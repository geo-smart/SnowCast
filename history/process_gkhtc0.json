[{
  "history_id" : "n2uj3rgpwym",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704644803696,
  "history_end_time" : 1704644803696,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ts32qrxeby9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704600008033,
  "history_end_time" : 1704600008033,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ueak13tff1n",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704566156613,
  "history_end_time" : 1704566156613,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tlwajtfecja",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704565587369,
  "history_end_time" : 1704565587369,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "lg6we5dtq2e",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704564424150,
  "history_end_time" : 1704564424150,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ueszkf94u5r",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704562992164,
  "history_end_time" : 1704562992164,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "h5r3nv06liu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704561889784,
  "history_end_time" : 1704561889784,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ifefvq8xjbz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704561861155,
  "history_end_time" : 1704561887033,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "fdf483ghv5z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704555479194,
  "history_end_time" : 1704555479194,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "g09dwt30zco",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704555028184,
  "history_end_time" : 1704555028184,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tx452po98gd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704553241757,
  "history_end_time" : 1704553241757,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "euzkd72nm4z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704552254605,
  "history_end_time" : 1704552254605,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "j49x2wvlzo7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704513607250,
  "history_end_time" : 1704513607250,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "s8hobjw6nm7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704427207355,
  "history_end_time" : 1704427207355,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "79ut9pxmche",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704340807418,
  "history_end_time" : 1704340807418,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "slom6fsi10t",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704330109285,
  "history_end_time" : 1704330109285,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wq0kq203wn9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704329364838,
  "history_end_time" : 1704329364838,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xt7080jf88y",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704254407351,
  "history_end_time" : 1704254407351,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "23spq3mchae",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704208947948,
  "history_end_time" : 1704208947948,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ittev2hfszx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704207352008,
  "history_end_time" : 1704207352008,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "txwlkba3nqp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704205859359,
  "history_end_time" : 1704205859359,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ficz13oa7sn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704168007265,
  "history_end_time" : 1704168007265,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0d7vdgspyi0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704081607320,
  "history_end_time" : 1704081607320,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "cnirjqgxb9s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703995208147,
  "history_end_time" : 1703995208147,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8fixo36r63s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703962871393,
  "history_end_time" : 1703962871393,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "384hcy5m5pl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703960265434,
  "history_end_time" : 1703960265434,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rrx23ykagha",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703959737829,
  "history_end_time" : 1703959737829,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ljqsiay55xb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703958611575,
  "history_end_time" : 1703958611575,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3oj86aay78k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703955838213,
  "history_end_time" : 1703955838213,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "em7apbemd1a",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703954150347,
  "history_end_time" : 1703954150347,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "v0g5sm2rl2e",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703915768059,
  "history_end_time" : 1703915768059,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "89ayx56ibdx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703915283474,
  "history_end_time" : 1703915283474,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "sg1p5mocs7z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703914476628,
  "history_end_time" : 1703914476628,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "4w3eesv4959",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703912302160,
  "history_end_time" : 1703912302160,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "k7kasx82evj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703908806958,
  "history_end_time" : 1703908806958,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "lke847g8is2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703906215366,
  "history_end_time" : 1703906215366,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "dew1f11ifeh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703900919132,
  "history_end_time" : 1703900919132,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "4n4rvq516l9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703899837753,
  "history_end_time" : 1703899837753,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "1e6fhsgnsal",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703897422938,
  "history_end_time" : 1703897422938,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vgx4jdfe1po",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703896125570,
  "history_end_time" : 1703896125570,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wh0cmxs7u67",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703890275978,
  "history_end_time" : 1703890275978,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "zxhnh125wjq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703886800797,
  "history_end_time" : 1703886800797,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wgebmmrk01s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703885997753,
  "history_end_time" : 1703885997753,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "82tfth86wwa",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703880194703,
  "history_end_time" : 1703880194703,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "cjkkg50l2r0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703872753021,
  "history_end_time" : 1703872753021,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "kesz8q84i9z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703869828226,
  "history_end_time" : 1703869828226,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9iljvx8chj3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703868616917,
  "history_end_time" : 1703868616917,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "67c73u2vu2h",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703867114034,
  "history_end_time" : 1703867114034,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "gz3lfw6rk0h",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703864885432,
  "history_end_time" : 1703864885432,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "cehgr9hbsf6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703862637339,
  "history_end_time" : 1703862637339,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9zosi1gqrrk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703827227314,
  "history_end_time" : 1703827227314,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "irb12oufzn5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703822411297,
  "history_end_time" : 1703822411297,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "1k9ie4zolue",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703786924624,
  "history_end_time" : 1703789718819,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "gb0fgzueb84",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703786053477,
  "history_end_time" : 1703786917609,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "lcd82u7p2h5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703778395381,
  "history_end_time" : 1703778395381,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "fvz3agsi8p7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703739034316,
  "history_end_time" : 1703739034316,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "jli36p83yvc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703738754372,
  "history_end_time" : 1703792459275,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "2gr3b3evv47",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703736166895,
  "history_end_time" : 1703737316875,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "s26aaul8zaf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703694763548,
  "history_end_time" : 1703694763548,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tnhbyd924wu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703659541192,
  "history_end_time" : 1703659541192,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "piayfdoryhu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703658144677,
  "history_end_time" : 1703658144677,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8o694b4djyp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703650855762,
  "history_end_time" : 1703650855762,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "p4b0reakb2y",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703646751522,
  "history_end_time" : 1703650812433,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "h7lpsplwmaz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703642120887,
  "history_end_time" : 1703646749620,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "x3x3invzlrs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703641988919,
  "history_end_time" : 1703642074627,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "mudmdr7876m",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703629665496,
  "history_end_time" : 1703629665496,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vxmwkmhl1i1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703626687971,
  "history_end_time" : 1703627783050,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "s3hysj2gx49",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703625782084,
  "history_end_time" : 1703625782084,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "lvqa13ep9zu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703624783822,
  "history_end_time" : 1703624783822,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "bh9rue2ze91",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702875592815,
  "history_end_time" : 1702875592815,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "k1smhcp4kqm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702871264367,
  "history_end_time" : 1702871264367,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6iz7nmcsws3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702867996381,
  "history_end_time" : 1702867996381,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "lkwjmqokoo1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702866593378,
  "history_end_time" : 1702866593378,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "undbqklobja",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702866137598,
  "history_end_time" : 1702866137598,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "5qb8mwo7qcq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702657305603,
  "history_end_time" : 1702657305603,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "l1w3lqg63qt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702633223035,
  "history_end_time" : 1702633223035,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "zwz9cavy3p2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702633156903,
  "history_end_time" : 1702633163895,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "mm37tk8tabs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702274520873,
  "history_end_time" : 1702274520873,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "w3oz9hdl4y2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702257109190,
  "history_end_time" : 1702257109190,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9brha148uit",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702253506498,
  "history_end_time" : 1702253506498,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "muwhficlc8f",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702047800919,
  "history_end_time" : 1702047800919,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "kydy820u9yw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702046671837,
  "history_end_time" : 1702047789480,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "054qoskls63",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701838624006,
  "history_end_time" : 1701838624006,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3cz5ag9e9es",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701272631465,
  "history_end_time" : 1701272875105,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "z5t0akhwlhc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701272152694,
  "history_end_time" : 1701272363347,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "b2ra70wbm9u",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701269761318,
  "history_end_time" : 1701269761318,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "biqev88r3b8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701245472006,
  "history_end_time" : 1701245472006,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ztib4gebif3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701234300609,
  "history_end_time" : 1701234300609,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "awlrwru2vq8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701232375265,
  "history_end_time" : 1701234158040,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "xy0fz1p9s8c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701231048659,
  "history_end_time" : 1701231048659,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "65sds4f3m2l",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933455,
  "history_end_time" : 1701230952342,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "9drmiex4s5p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230796324,
  "history_end_time" : 1701230932248,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "bjw91v3upns",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230384875,
  "history_end_time" : 1701230384875,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "d633gjrvmk2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701229983500,
  "history_end_time" : 1701229983500,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rx2f00944ug",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701228899363,
  "history_end_time" : 1701228899363,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rycqw1ls75g",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701228374794,
  "history_end_time" : 1701228374794,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "p5z34hd85bl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701228236333,
  "history_end_time" : 1701228236333,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "atn4laixmav",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701228118497,
  "history_end_time" : 1701228118497,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6rp6kyad2aw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701228056519,
  "history_end_time" : 1701228056519,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ja4jopgifwo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701227912503,
  "history_end_time" : 1701227912503,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "nzaku87rr44",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701013937398,
  "history_end_time" : 1701015920036,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "0q53pxfze0y",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700974688685,
  "history_end_time" : 1700974688685,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "h0q8p8w1ydx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700885116804,
  "history_end_time" : 1700885116804,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "byon5b2x2y7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700471590177,
  "history_end_time" : 1700471590177,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "4qs70oib19c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700468936405,
  "history_end_time" : 1700468936405,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "awudukpxr02",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700461922977,
  "history_end_time" : 1700462913673,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "3b4szsmiwts",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500131,
  "history_end_time" : 1700448500131,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xwb4pf25fo1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700447319839,
  "history_end_time" : 1700447319839,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "yqp2r9v9l12",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700230067237,
  "history_end_time" : 1700230067237,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "dylf1d5vf98",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700229012361,
  "history_end_time" : 1700229012361,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "g4591fam2yv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700210213804,
  "history_end_time" : 1700210213804,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ek533pj6uyy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700209780156,
  "history_end_time" : 1700209780156,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0sq7308akiv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700209729241,
  "history_end_time" : 1700209729241,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "yy05k9rzzis",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700203478641,
  "history_end_time" : 1700204245681,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "pnc0o7lqcou",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700201828257,
  "history_end_time" : 1700201828257,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "dmljwfy269b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700200332849,
  "history_end_time" : 1700200332849,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "38deq9kia3l",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700145667865,
  "history_end_time" : 1700145667865,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "re2cn92e8l9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700143295300,
  "history_end_time" : 1700143295300,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "kuereqdnszm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700141615808,
  "history_end_time" : 1700141615808,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "55uj2g9nz6d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700134126832,
  "history_end_time" : 1700134126832,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "c0thbkqb43k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700133783697,
  "history_end_time" : 1700133783697,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "kue1kl6uowu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699992839753,
  "history_end_time" : 1699992839753,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ety5pm6l1lj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699982145396,
  "history_end_time" : 1699982145396,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rics4b82wuu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699941614795,
  "history_end_time" : 1699941614795,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vno4seunl29",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699939440542,
  "history_end_time" : 1699939440542,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "gjw7612fcba",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699937910460,
  "history_end_time" : 1699937910460,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "a1dyuz5klcs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699805634608,
  "history_end_time" : 1699806085191,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "sk1zzxiaask",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699684154055,
  "history_end_time" : 1699684154055,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "dbn8txcgd0u",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699681071332,
  "history_end_time" : 1699681071332,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "s6wj4t42psk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698762678671,
  "history_end_time" : 1698762678671,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "or9cauy2ba8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698762637961,
  "history_end_time" : 1698762637961,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8gi9vzyer89",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698276496952,
  "history_end_time" : 1698276496952,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "aa3bc5eito8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698252277357,
  "history_end_time" : 1698252277357,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "a8kjbf3396o",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698251392459,
  "history_end_time" : 1698251392459,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "30zu9xdbm0l",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698228210941,
  "history_end_time" : 1698228210941,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "me9yj56qmd4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698227897131,
  "history_end_time" : 1698227897131,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "por8gvslt8k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698163737285,
  "history_end_time" : 1698163737285,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "jbmmsayyka7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698163445819,
  "history_end_time" : 1698163445819,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "b0ro9namz7z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698163121532,
  "history_end_time" : 1698163121532,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "mxamf5a1eis",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698160809355,
  "history_end_time" : 1698160809355,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "utzfr859m43",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698157805167,
  "history_end_time" : 1698157805167,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "k1xe7s44gg1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698152099732,
  "history_end_time" : 1698152099732,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "4sf2pokjfyt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698095495725,
  "history_end_time" : 1698095495725,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tucpc8858ho",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698075453561,
  "history_end_time" : 1698075453561,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "bpp39robej2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1697349529992,
  "history_end_time" : 1697349529992,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ejyq0iuogd4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1697348852258,
  "history_end_time" : 1697348852258,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "o5donu02t1s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1697189923544,
  "history_end_time" : 1697189923544,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "c5pfj2tmyq8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1697188523285,
  "history_end_time" : 1697188523285,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "x3zyv2hrzrr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1697187892249,
  "history_end_time" : 1697187892249,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "l8h5dxbx9ma",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1697187367954,
  "history_end_time" : 1697187367954,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "veb9xsqftnc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696863953199,
  "history_end_time" : 1696863953199,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "g54vyu7hlc2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696862402937,
  "history_end_time" : 1696862402937,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "r2aq20evo0c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696832263660,
  "history_end_time" : 1696832263660,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ddl683voj3z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696831867365,
  "history_end_time" : 1696831867365,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9fdzy9kdkut",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696830174318,
  "history_end_time" : 1696830174318,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "fyj3ailtkfk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696787541898,
  "history_end_time" : 1696787541898,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "bpw1ff9a80l",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696786838181,
  "history_end_time" : 1696786838181,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "kb6s4161zlu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696771780872,
  "history_end_time" : 1696771780872,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "sqn5wvz3753",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696602943929,
  "history_end_time" : 1696602943929,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "p41uhwqfytg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696432484313,
  "history_end_time" : 1696432484313,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "dr2go0udlgo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696432299749,
  "history_end_time" : 1696432482232,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "zu3ssd5lw5p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695827991079,
  "history_end_time" : 1695827991079,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "nor9gexetpp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695827889169,
  "history_end_time" : 1695827964214,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "3xu9mmeh0y4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695827855637,
  "history_end_time" : 1695827867005,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "luaiazll1uv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695696616108,
  "history_end_time" : 1695696616108,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "g5j0crvcgsu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695694257321,
  "history_end_time" : 1695694257321,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "v4cfy9cd4zh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695693585740,
  "history_end_time" : 1695693585740,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3zl6z9q8z59",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695693149357,
  "history_end_time" : 1695693149357,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6cv23sl9sg6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695580915839,
  "history_end_time" : 1695580915839,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "jp85jo66nss",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695576291647,
  "history_end_time" : 1695576291647,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "u7ajj1vsy3u",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695575931004,
  "history_end_time" : 1695575931004,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xhu8kny23y2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695535769204,
  "history_end_time" : 1695535769204,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "630t9z55ah6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695535478678,
  "history_end_time" : 1695535478678,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "d1wjlsauatr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695535214017,
  "history_end_time" : 1695535214017,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ig01nwfiqom",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695534943580,
  "history_end_time" : 1695534943580,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "seodxvhdrpb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695534671820,
  "history_end_time" : 1695534671820,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "onvu3ieaami",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695533024088,
  "history_end_time" : 1695533024088,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "jqtca21l1lp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695529187858,
  "history_end_time" : 1695529187858,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "gd5s3an3k4s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695528505174,
  "history_end_time" : 1695528505174,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "args7d25lmt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695515862386,
  "history_end_time" : 1695515862386,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "d5g77eotfx5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695506423836,
  "history_end_time" : 1695506423836,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "o56cvzgjw3d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695418741331,
  "history_end_time" : 1695418741331,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xpxum3du82x",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695417619669,
  "history_end_time" : 1695417619669,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "dqanc15of7l",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695417171272,
  "history_end_time" : 1695417171272,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6zw7pqzih4z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695417052725,
  "history_end_time" : 1695417052725,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vzepmwysd6l",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695416916013,
  "history_end_time" : 1695416916013,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xjj5pnwqvli",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695106488971,
  "history_end_time" : 1695106488971,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "yzvzuex4421",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695106316191,
  "history_end_time" : 1695106316191,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ongfxel6drj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695054045020,
  "history_end_time" : 1695054045020,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "dhsmcb8v47n",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695054019752,
  "history_end_time" : 1695054032321,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "rk9306u8hf5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695053979883,
  "history_end_time" : 1695054019272,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "sl90cee97wl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695053793398,
  "history_end_time" : 1695053793398,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "eaooqmnuhns",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695053733398,
  "history_end_time" : 1695053733398,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "fvq8czpo55f",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694971144814,
  "history_end_time" : 1694972839685,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ulwf53fol74",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970707917,
  "history_end_time" : 1694970707917,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "999n59qskqf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594754,
  "history_end_time" : 1694970594754,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "22i0aj0tagg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970131522,
  "history_end_time" : 1694970131522,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0m7ntiw5h2m",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694969350034,
  "history_end_time" : 1694969350034,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "jmloe3jbp3k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694905307653,
  "history_end_time" : 1694905307653,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tull47ezkik",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887121,
  "history_end_time" : 1694897887121,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "2im4ps60qfv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531335782,
  "history_end_time" : 1691531335782,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "qy01mqdblrq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531292706,
  "history_end_time" : 1691531292706,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "kmqc02dr5pj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531254581,
  "history_end_time" : 1691531284898,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "5kwo4jzlxvc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531163691,
  "history_end_time" : 1691531163691,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "0tckpj4v9dk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531120828,
  "history_end_time" : 1691531120828,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "4p7tivfbwqx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531060856,
  "history_end_time" : 1691531060856,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "2cgzuhmtil0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530848272,
  "history_end_time" : 1691530848272,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "c26xgfqg6vu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717683,
  "history_end_time" : 1691530721103,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "3d4559ze7hy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530690096,
  "history_end_time" : 1691530716747,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "iduqbshluf8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530620992,
  "history_end_time" : 1691530622436,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "xzhx1anwsri",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530617155,
  "history_end_time" : 1691530617155,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "gdbtdmko0a3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530599764,
  "history_end_time" : 1691530614282,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "qbe4nt0c3cq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1689632033812,
  "history_end_time" : 1689632033812,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rtpxn6zu2t2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1689631636465,
  "history_end_time" : 1689631636465,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "2qimrknblt5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1689135058040,
  "history_end_time" : 1689135058040,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vbljpyd8vdn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1688416860238,
  "history_end_time" : 1688416907376,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "jf7wuu",
  "indicator" : "Stopped"
},{
  "history_id" : "a10fltjlvek",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1688416833717,
  "history_end_time" : 1688416848468,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "jf7wuu",
  "indicator" : "Stopped"
},{
  "history_id" : "vohlr7tk3qf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1688416668378,
  "history_end_time" : 1688416822956,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "jf7wuu",
  "indicator" : "Stopped"
},{
  "history_id" : "wk565ef5uyg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1688416628776,
  "history_end_time" : 1688416660674,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "jf7wuu",
  "indicator" : "Stopped"
},{
  "history_id" : "rkn0pr6ngzi",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1688416567321,
  "history_end_time" : 1688416575017,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "jf7wuu",
  "indicator" : "Stopped"
},{
  "history_id" : "yngc53xmk0j",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/Users/joe\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n[[0.00038069]]\n[[78266]]\n[[[  33.666668 -109.291664]]]\n[[0.00016855]]\n[[202538]]\n[[[  38.5      -106.333336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00012986]]\n[[373281]]\n[[[  44.583332 -107.208336]]]\n[[3.87134852e-05]]\n[[362136]]\n[[[  44.166668 -107.125   ]]]\n[[0.00015419]]\n[[330682]]\n[[[  43.   -109.75]]]\n[[0.00023674]]\n[[319102]]\n[[[  42.583332 -108.833336]]]\n[[8.83810871e-05]]\n[[267286]]\n[[[  40.791668 -110.875   ]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00034885]]\n[[116759]]\n[[[  35.25 -108.25]]]\n[[0.00034215]]\n[[107190]]\n[[[  34.875 -111.625]]]\n[[0.0001517]]\n[[104042]]\n[[[  34.75     -111.416664]]]\n[[0.00013821]]\n[[216303]]\n[[[  39.   -106.75]]]\n[[0.00016356]]\n[[174581]]\n[[[  37.458332 -108.458336]]]\n[[0.00033874]]\n[[457740]]\n[[[  48.416668 -113.916664]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00035391]]\n[[177932]]\n[[[  37.583332 -107.25    ]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00012476]]\n[[269738]]\n[[[  40.875    -110.541664]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00039874]]\n[[459718]]\n[[[  48.541668 -120.75    ]]]\n[[0.00028356]]\n[[162526]]\n[[[  37.       -106.291664]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00014601]]\n[[183432]]\n[[[  37.791668 -108.166664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00019668]]\n[[266072]]\n[[[  40.75  -110.625]]]\n[[0.00034733]]\n[[255062]]\n[[[  40.375 -116.875]]]\n[[9.15488263e-05]]\n[[283188]]\n[[[  41.333332 -106.375   ]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022445]]\n[[185613]]\n[[[  37.875    -109.458336]]]\n[[0.00025986]]\n[[240815]]\n[[[  39.875 -111.25 ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00012938]]\n[[252783]]\n[[[  40.291668 -111.25    ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00020894]]\n[[263633]]\n[[[  40.666668 -110.958336]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00026128]]\n[[234882]]\n[[[  39.666668 -110.416664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0003611]]\n[[269634]]\n[[[  40.875    -115.208336]]]\n[[0.00030221]]\n[[272081]]\n[[[  40.958332 -115.083336]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00020455]]\n[[139488]]\n[[[  36.125    -105.541664]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00012719]]\n[[426421]]\n[[[  46.791668 -121.75    ]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00036997]]\n[[84911]]\n[[[  33.958332 -109.5     ]]]\n[[7.66642558e-05]]\n[[292876]]\n[[[  41.666668 -115.333336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00011195]]\n[[400403]]\n[[[  45.666668 -113.958336]]]\n[[0.00033222]]\n[[369791]]\n[[[  44.458332 -113.      ]]]\nfinished\n",
  "history_begin_time" : 1687546867411,
  "history_end_time" : 1687547253140,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "7nouk4jcaxe",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1687463685022,
  "history_end_time" : 1687463685022,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "1a8nw8qpruq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1687463635417,
  "history_end_time" : 1687463635417,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "nlqr52ay1y8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1686236147016,
  "history_end_time" : 1686237909495,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "jf7wuu",
  "indicator" : "Stopped"
},{
  "history_id" : "9xo8z1l1ljp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1686235960974,
  "history_end_time" : 1686235985414,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "jf7wuu",
  "indicator" : "Stopped"
},{
  "history_id" : "x1em0a2r41m",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1686235529628,
  "history_end_time" : 1686235529628,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "jf7wuu",
  "indicator" : "Skipped"
},{
  "history_id" : "j0or864zm16",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1686235448228,
  "history_end_time" : 1686235482630,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "jf7wuu",
  "indicator" : "Stopped"
},{
  "history_id" : "i5ui6gykqzu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1686235402213,
  "history_end_time" : 1686235424783,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "jf7wuu",
  "indicator" : "Stopped"
},{
  "history_id" : "w4uzok71s3v",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1686153654213,
  "history_end_time" : 1686153654213,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "jf7wuu",
  "indicator" : "Skipped"
},{
  "history_id" : "qbp1gb7gso7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1682984798067,
  "history_end_time" : 1682984800295,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "jf7wuu",
  "indicator" : "Stopped"
},{
  "history_id" : "y4qmmeqzasf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1681516927641,
  "history_end_time" : 1681516927641,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "VOeQqGyG48p4",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \n#turn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n<class 'netCDF4._netCDF4.Dataset'>\nroot group (NETCDF4 data model, file format HDF5):\n    dimensions(sizes): lat(621), lon(1405), time(365), time_str_len(11)\n    variables(dimensions): |S1 crs(), float32 lat(lat), float32 lon(lon), float32 time(time), |S1 time_str(time_str_len, time), int16 SWE(time, lat, lon), int16 DEPTH(time, lat, lon)\n    groups: \n<class 'netCDF4._netCDF4.Dimension'>: name = 'lat', size = 621\n<class 'netCDF4._netCDF4.Dimension'>: name = 'lon', size = 1405\n<class 'netCDF4._netCDF4.Dimension'>: name = 'time', size = 365\n<class 'netCDF4._netCDF4.Dimension'>: name = 'time_str_len', size = 11\n<class 'netCDF4._netCDF4.Variable'>\n|S1 crs()\n    grid_mapping_name: latitude_longitude\n    long_name: CRS definition\n    longitude_of_prime_meridian: 0.0\n    semi_major_axis: 6378137.0\n    inverse_flattening: 298.257222101\n    spatial_ref: GEOGCS[\"NAD83\",DATUM[\"North_American_Datum_1983\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6269\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.01745329251994328,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4269\"]]\n    GeoTransform: -125.0208 0.04166662697178698 0 49.9375 0 -0.04166662697178698 \nunlimited dimensions: \ncurrent shape = ()\nfilling on, default _FillValue of \u0000 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 lat(lat)\n    long_name: latitude\n    units: degrees north\nunlimited dimensions: \ncurrent shape = (621,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 lon(lon)\n    long_name: longitude\n    units: degrees east\nunlimited dimensions: \ncurrent shape = (1405,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 time(time)\n    long_name: time\n    units: days since 1900-01-01\nunlimited dimensions: \ncurrent shape = (365,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\n|S1 time_str(time_str_len, time)\n    long_name: time (string)\n    format: dd-mmm-yyyy\nunlimited dimensions: \ncurrent shape = (11, 365)\nfilling on, default _FillValue of \u0000 used\n<class 'netCDF4._netCDF4.Variable'>\nint16 SWE(time, lat, lon)\n    _FillValue: -999\n    long_name: Snow Water Equivalent\n    grid_mapping: crs\n    units: millimeters h20\nunlimited dimensions: \ncurrent shape = (365, 621, 1405)\nfilling on\n<class 'netCDF4._netCDF4.Variable'>\nint16 DEPTH(time, lat, lon)\n    _FillValue: -999\n    long_name: Snow Depth\n    grid_mapping: crs\n    units: millimeters snow thickness\nunlimited dimensions: \ncurrent shape = (365, 621, 1405)\nfilling on\n",
  "history_begin_time" : 1681516557802,
  "history_end_time" : 1681516563213,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "6xy9oql7e7k",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1681039708132,
  "history_end_time" : 1681039709669,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "0kjm5xgjbuf",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1681039689582,
  "history_end_time" : 1681039697767,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "0p8ma9x99ef",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1681007820843,
  "history_end_time" : 1681007822391,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "rih5ccxirnd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1679442743784,
  "history_end_time" : 1679442743784,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "jf7wuu",
  "indicator" : "Skipped"
},{
  "history_id" : "awk4b6f2oo1",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "  File \"data_nsidc_4km_swe.py\", line 35\n    github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n                                                      ^\nSyntaxError: invalid syntax\n",
  "history_begin_time" : 1679332585713,
  "history_end_time" : 1679332589286,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "jf7wuu",
  "indicator" : "Done"
},{
  "history_id" : "xxeoyk2meo7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1679191258497,
  "history_end_time" : 1679191258497,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "2jifky",
  "indicator" : "Skipped"
},{
  "history_id" : "bkjw9gczirk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1679091534986,
  "history_end_time" : 1679091744969,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "hyjmkewdh3t",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1679091527370,
  "history_end_time" : 1679091533670,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "0rhd9idttuh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678887964096,
  "history_end_time" : 1678888215698,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "ob0fcj9fkdv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678887944949,
  "history_end_time" : 1678887946446,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "e21xmqev8bl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678886689970,
  "history_end_time" : 1678887010629,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "94f8iebb7on",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678884557542,
  "history_end_time" : 1678884986357,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "sw9xpidsghm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678884501895,
  "history_end_time" : 1678884535354,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "n5hoa064oh3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678884207342,
  "history_end_time" : 1678884438333,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "406cvoox4b3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678884125217,
  "history_end_time" : 1678884140269,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "imc1lxtipoi",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678884039385,
  "history_end_time" : 1678884042290,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "2p4rltgkvw6",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "",
  "history_begin_time" : 1678883325736,
  "history_end_time" : 1678883775455,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "avib731xrt8",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n",
  "history_begin_time" : 1678756772393,
  "history_end_time" : 1678756782466,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "klg63o6hvsn",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n[[0.00038069]]\n[[78266]]\n[[[  33.666668 -109.291664]]]\n[[0.00016855]]\n[[202538]]\n[[[  38.5      -106.333336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00012986]]\n[[373281]]\n[[[  44.583332 -107.208336]]]\n[[3.87134852e-05]]\n[[362136]]\n[[[  44.166668 -107.125   ]]]\n[[0.00015419]]\n[[330682]]\n[[[  43.   -109.75]]]\n[[0.00023674]]\n[[319102]]\n[[[  42.583332 -108.833336]]]\n[[8.83810871e-05]]\n[[267286]]\n[[[  40.791668 -110.875   ]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00034885]]\n[[116759]]\n[[[  35.25 -108.25]]]\n[[0.00034215]]\n[[107190]]\n[[[  34.875 -111.625]]]\n[[0.0001517]]\n[[104042]]\n[[[  34.75     -111.416664]]]\n[[0.00013821]]\n[[216303]]\n[[[  39.   -106.75]]]\n[[0.00016356]]\n[[174581]]\n[[[  37.458332 -108.458336]]]\n[[0.00033874]]\n[[457740]]\n[[[  48.416668 -113.916664]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00035391]]\n[[177932]]\n[[[  37.583332 -107.25    ]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00012476]]\n[[269738]]\n[[[  40.875    -110.541664]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00039874]]\n[[459718]]\n[[[  48.541668 -120.75    ]]]\n[[0.00028356]]\n[[162526]]\n[[[  37.       -106.291664]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00014601]]\n[[183432]]\n[[[  37.791668 -108.166664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00019668]]\n[[266072]]\n[[[  40.75  -110.625]]]\n[[0.00034733]]\n[[255062]]\n[[[  40.375 -116.875]]]\n[[9.15488263e-05]]\n[[283188]]\n[[[  41.333332 -106.375   ]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022445]]\n[[185613]]\n[[[  37.875    -109.458336]]]\n[[0.00025986]]\n[[240815]]\n[[[  39.875 -111.25 ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00012938]]\n[[252783]]\n[[[  40.291668 -111.25    ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00020894]]\n[[263633]]\n[[[  40.666668 -110.958336]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00026128]]\n[[234882]]\n[[[  39.666668 -110.416664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0003611]]\n[[269634]]\n[[[  40.875    -115.208336]]]\n[[0.00030221]]\n[[272081]]\n[[[  40.958332 -115.083336]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00020455]]\n[[139488]]\n[[[  36.125    -105.541664]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00012719]]\n[[426421]]\n[[[  46.791668 -121.75    ]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00036997]]\n[[84911]]\n[[[  33.958332 -109.5     ]]]\n[[7.66642558e-05]]\n[[292876]]\n[[[  41.666668 -115.333336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00011195]]\n[[400403]]\n[[[  45.666668 -113.958336]]]\n[[0.00033222]]\n[[369791]]\n[[[  44.458332 -113.      ]]]\nfinished\n",
  "history_begin_time" : 1678749631780,
  "history_end_time" : 1678750224424,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "pd08ok58kov",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1678748548090,
  "history_end_time" : 1678748550742,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "4w4fgsmni3q",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n[[0.00038069]]\n[[78266]]\n[[[  33.666668 -109.291664]]]\n[[0.00016855]]\n[[202538]]\n[[[  38.5      -106.333336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00012986]]\n[[373281]]\n[[[  44.583332 -107.208336]]]\n[[3.87134852e-05]]\n[[362136]]\n[[[  44.166668 -107.125   ]]]\n[[0.00015419]]\n[[330682]]\n[[[  43.   -109.75]]]\n[[0.00023674]]\n[[319102]]\n[[[  42.583332 -108.833336]]]\n[[8.83810871e-05]]\n[[267286]]\n[[[  40.791668 -110.875   ]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00034885]]\n[[116759]]\n[[[  35.25 -108.25]]]\n[[0.00034215]]\n[[107190]]\n[[[  34.875 -111.625]]]\n[[0.0001517]]\n[[104042]]\n[[[  34.75     -111.416664]]]\n[[0.00013821]]\n[[216303]]\n[[[  39.   -106.75]]]\n[[0.00016356]]\n[[174581]]\n[[[  37.458332 -108.458336]]]\n[[0.00033874]]\n[[457740]]\n[[[  48.416668 -113.916664]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00035391]]\n[[177932]]\n[[[  37.583332 -107.25    ]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00012476]]\n[[269738]]\n[[[  40.875    -110.541664]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00039874]]\n[[459718]]\n[[[  48.541668 -120.75    ]]]\n[[0.00028356]]\n[[162526]]\n[[[  37.       -106.291664]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00014601]]\n[[183432]]\n[[[  37.791668 -108.166664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00019668]]\n[[266072]]\n[[[  40.75  -110.625]]]\n[[0.00034733]]\n[[255062]]\n[[[  40.375 -116.875]]]\n[[9.15488263e-05]]\n[[283188]]\n[[[  41.333332 -106.375   ]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022445]]\n[[185613]]\n[[[  37.875    -109.458336]]]\n[[0.00025986]]\n[[240815]]\n[[[  39.875 -111.25 ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00012938]]\n[[252783]]\n[[[  40.291668 -111.25    ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00020894]]\n[[263633]]\n[[[  40.666668 -110.958336]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00026128]]\n[[234882]]\n[[[  39.666668 -110.416664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0003611]]\n[[269634]]\n[[[  40.875    -115.208336]]]\n[[0.00030221]]\n[[272081]]\n[[[  40.958332 -115.083336]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00020455]]\n[[139488]]\n[[[  36.125    -105.541664]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00012719]]\n[[426421]]\n[[[  46.791668 -121.75    ]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00036997]]\n[[84911]]\n[[[  33.958332 -109.5     ]]]\n[[7.66642558e-05]]\n[[292876]]\n[[[  41.666668 -115.333336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00011195]]\n[[400403]]\n[[[  45.666668 -113.958336]]]\n[[0.00033222]]\n[[369791]]\n[[[  44.458332 -113.      ]]]\nfinished\n",
  "history_begin_time" : 1678747245458,
  "history_end_time" : 1678747792561,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "r73k6bvf2d2",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n[[0.00038069]]\n[[78266]]\n[[[  33.666668 -109.291664]]]\n[[0.00016855]]\n[[202538]]\n[[[  38.5      -106.333336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00012986]]\n[[373281]]\n[[[  44.583332 -107.208336]]]\n[[3.87134852e-05]]\n[[362136]]\n[[[  44.166668 -107.125   ]]]\n[[0.00015419]]\n[[330682]]\n[[[  43.   -109.75]]]\n[[0.00023674]]\n[[319102]]\n[[[  42.583332 -108.833336]]]\n[[8.83810871e-05]]\n[[267286]]\n[[[  40.791668 -110.875   ]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00034885]]\n[[116759]]\n[[[  35.25 -108.25]]]\n[[0.00034215]]\n[[107190]]\n[[[  34.875 -111.625]]]\n[[0.0001517]]\n[[104042]]\n[[[  34.75     -111.416664]]]\n[[0.00013821]]\n[[216303]]\n[[[  39.   -106.75]]]\n[[0.00016356]]\n[[174581]]\n[[[  37.458332 -108.458336]]]\n[[0.00033874]]\n[[457740]]\n[[[  48.416668 -113.916664]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00035391]]\n[[177932]]\n[[[  37.583332 -107.25    ]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00012476]]\n[[269738]]\n[[[  40.875    -110.541664]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00039874]]\n[[459718]]\n[[[  48.541668 -120.75    ]]]\n[[0.00028356]]\n[[162526]]\n[[[  37.       -106.291664]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00014601]]\n[[183432]]\n[[[  37.791668 -108.166664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00019668]]\n[[266072]]\n[[[  40.75  -110.625]]]\n[[0.00034733]]\n[[255062]]\n[[[  40.375 -116.875]]]\n[[9.15488263e-05]]\n[[283188]]\n[[[  41.333332 -106.375   ]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022445]]\n[[185613]]\n[[[  37.875    -109.458336]]]\n[[0.00025986]]\n[[240815]]\n[[[  39.875 -111.25 ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00012938]]\n[[252783]]\n[[[  40.291668 -111.25    ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00020894]]\n[[263633]]\n[[[  40.666668 -110.958336]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00026128]]\n[[234882]]\n[[[  39.666668 -110.416664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0003611]]\n[[269634]]\n[[[  40.875    -115.208336]]]\n[[0.00030221]]\n[[272081]]\n[[[  40.958332 -115.083336]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00020455]]\n[[139488]]\n[[[  36.125    -105.541664]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00012719]]\n[[426421]]\n[[[  46.791668 -121.75    ]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00036997]]\n[[84911]]\n[[[  33.958332 -109.5     ]]]\n[[7.66642558e-05]]\n[[292876]]\n[[[  41.666668 -115.333336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00011195]]\n[[400403]]\n[[[  45.666668 -113.958336]]]\n[[0.00033222]]\n[[369791]]\n[[[  44.458332 -113.      ]]]\nfinished\n",
  "history_begin_time" : 1678746481633,
  "history_end_time" : 1678747059375,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "ixzljx5ybrj",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n[[0.00038069]]\n[[78266]]\n[[[  33.666668 -109.291664]]]\n[[0.00016855]]\n[[202538]]\n[[[  38.5      -106.333336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00012986]]\n[[373281]]\n[[[  44.583332 -107.208336]]]\n[[3.87134852e-05]]\n[[362136]]\n[[[  44.166668 -107.125   ]]]\n[[0.00015419]]\n[[330682]]\n[[[  43.   -109.75]]]\n[[0.00023674]]\n[[319102]]\n[[[  42.583332 -108.833336]]]\n[[8.83810871e-05]]\n[[267286]]\n[[[  40.791668 -110.875   ]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00034885]]\n[[116759]]\n[[[  35.25 -108.25]]]\n[[0.00034215]]\n[[107190]]\n[[[  34.875 -111.625]]]\n[[0.0001517]]\n[[104042]]\n[[[  34.75     -111.416664]]]\n[[0.00013821]]\n[[216303]]\n[[[  39.   -106.75]]]\n[[0.00016356]]\n[[174581]]\n[[[  37.458332 -108.458336]]]\n[[0.00033874]]\n[[457740]]\n[[[  48.416668 -113.916664]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00035391]]\n[[177932]]\n[[[  37.583332 -107.25    ]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00012476]]\n[[269738]]\n[[[  40.875    -110.541664]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00039874]]\n[[459718]]\n[[[  48.541668 -120.75    ]]]\n[[0.00028356]]\n[[162526]]\n[[[  37.       -106.291664]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00014601]]\n[[183432]]\n[[[  37.791668 -108.166664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00019668]]\n[[266072]]\n[[[  40.75  -110.625]]]\n[[0.00034733]]\n[[255062]]\n[[[  40.375 -116.875]]]\n[[9.15488263e-05]]\n[[283188]]\n[[[  41.333332 -106.375   ]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022445]]\n[[185613]]\n[[[  37.875    -109.458336]]]\n[[0.00025986]]\n[[240815]]\n[[[  39.875 -111.25 ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00012938]]\n[[252783]]\n[[[  40.291668 -111.25    ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00020894]]\n[[263633]]\n[[[  40.666668 -110.958336]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00026128]]\n[[234882]]\n[[[  39.666668 -110.416664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0003611]]\n[[269634]]\n[[[  40.875    -115.208336]]]\n[[0.00030221]]\n[[272081]]\n[[[  40.958332 -115.083336]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00020455]]\n[[139488]]\n[[[  36.125    -105.541664]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00012719]]\n[[426421]]\n[[[  46.791668 -121.75    ]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00036997]]\n[[84911]]\n[[[  33.958332 -109.5     ]]]\n[[7.66642558e-05]]\n[[292876]]\n[[[  41.666668 -115.333336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00011195]]\n[[400403]]\n[[[  45.666668 -113.958336]]]\n[[0.00033222]]\n[[369791]]\n[[[  44.458332 -113.      ]]]\nfinished\n",
  "history_begin_time" : 1678743869212,
  "history_end_time" : 1678744427682,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "oxyghuj6ybb",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n",
  "history_begin_time" : 1678743637904,
  "history_end_time" : 1678743713958,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "769quxpfzz9",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n[[0.00038069]]\n[[78266]]\n[[[  33.666668 -109.291664]]]\n[[0.00016855]]\n[[202538]]\n[[[  38.5      -106.333336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00012986]]\n[[373281]]\n[[[  44.583332 -107.208336]]]\n[[3.87134852e-05]]\n[[362136]]\n[[[  44.166668 -107.125   ]]]\n[[0.00015419]]\n[[330682]]\n[[[  43.   -109.75]]]\n[[0.00023674]]\n[[319102]]\n[[[  42.583332 -108.833336]]]\n[[8.83810871e-05]]\n[[267286]]\n[[[  40.791668 -110.875   ]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00034885]]\n[[116759]]\n[[[  35.25 -108.25]]]\n[[0.00034215]]\n[[107190]]\n[[[  34.875 -111.625]]]\n[[0.0001517]]\n[[104042]]\n[[[  34.75     -111.416664]]]\n[[0.00013821]]\n[[216303]]\n[[[  39.   -106.75]]]\n[[0.00016356]]\n[[174581]]\n[[[  37.458332 -108.458336]]]\n[[0.00033874]]\n[[457740]]\n[[[  48.416668 -113.916664]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00035391]]\n[[177932]]\n[[[  37.583332 -107.25    ]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00012476]]\n[[269738]]\n[[[  40.875    -110.541664]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00039874]]\n[[459718]]\n[[[  48.541668 -120.75    ]]]\n[[0.00028356]]\n[[162526]]\n[[[  37.       -106.291664]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00014601]]\n[[183432]]\n[[[  37.791668 -108.166664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00019668]]\n[[266072]]\n[[[  40.75  -110.625]]]\n[[0.00034733]]\n[[255062]]\n[[[  40.375 -116.875]]]\n[[9.15488263e-05]]\n[[283188]]\n[[[  41.333332 -106.375   ]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022445]]\n[[185613]]\n[[[  37.875    -109.458336]]]\n[[0.00025986]]\n[[240815]]\n[[[  39.875 -111.25 ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00012938]]\n[[252783]]\n[[[  40.291668 -111.25    ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00020894]]\n[[263633]]\n[[[  40.666668 -110.958336]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00026128]]\n[[234882]]\n[[[  39.666668 -110.416664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0003611]]\n[[269634]]\n[[[  40.875    -115.208336]]]\n[[0.00030221]]\n[[272081]]\n[[[  40.958332 -115.083336]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00020455]]\n[[139488]]\n[[[  36.125    -105.541664]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00012719]]\n[[426421]]\n[[[  46.791668 -121.75    ]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00036997]]\n[[84911]]\n[[[  33.958332 -109.5     ]]]\n[[7.66642558e-05]]\n[[292876]]\n[[[  41.666668 -115.333336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00011195]]\n[[400403]]\n[[[  45.666668 -113.958336]]]\n[[0.00033222]]\n[[369791]]\n[[[  44.458332 -113.      ]]]\nfinished\n",
  "history_begin_time" : 1678743144982,
  "history_end_time" : 1678743713940,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "gtafrkrz5rb",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n",
  "history_begin_time" : 1678742584238,
  "history_end_time" : 1678742592509,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "yaj36vk735i",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n[[0.00038069]]\n[[78266]]\n[[[  33.666668 -109.291664]]]\n[[0.00016855]]\n[[202538]]\n[[[  38.5      -106.333336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00012986]]\n[[373281]]\n[[[  44.583332 -107.208336]]]\n[[3.87134852e-05]]\n[[362136]]\n[[[  44.166668 -107.125   ]]]\n[[0.00015419]]\n[[330682]]\n[[[  43.   -109.75]]]\n[[0.00023674]]\n[[319102]]\n[[[  42.583332 -108.833336]]]\n[[8.83810871e-05]]\n[[267286]]\n[[[  40.791668 -110.875   ]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00034885]]\n[[116759]]\n[[[  35.25 -108.25]]]\n[[0.00034215]]\n[[107190]]\n[[[  34.875 -111.625]]]\n[[0.0001517]]\n[[104042]]\n[[[  34.75     -111.416664]]]\n[[0.00013821]]\n[[216303]]\n[[[  39.   -106.75]]]\n[[0.00016356]]\n[[174581]]\n[[[  37.458332 -108.458336]]]\n[[0.00033874]]\n[[457740]]\n[[[  48.416668 -113.916664]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00035391]]\n[[177932]]\n[[[  37.583332 -107.25    ]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00012476]]\n[[269738]]\n[[[  40.875    -110.541664]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00039874]]\n[[459718]]\n[[[  48.541668 -120.75    ]]]\n[[0.00028356]]\n[[162526]]\n[[[  37.       -106.291664]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00014601]]\n[[183432]]\n[[[  37.791668 -108.166664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00019668]]\n[[266072]]\n[[[  40.75  -110.625]]]\n[[0.00034733]]\n[[255062]]\n[[[  40.375 -116.875]]]\n[[9.15488263e-05]]\n[[283188]]\n[[[  41.333332 -106.375   ]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022445]]\n[[185613]]\n[[[  37.875    -109.458336]]]\n[[0.00025986]]\n[[240815]]\n[[[  39.875 -111.25 ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00012938]]\n[[252783]]\n[[[  40.291668 -111.25    ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00020894]]\n[[263633]]\n[[[  40.666668 -110.958336]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00026128]]\n[[234882]]\n[[[  39.666668 -110.416664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0003611]]\n[[269634]]\n[[[  40.875    -115.208336]]]\n[[0.00030221]]\n[[272081]]\n[[[  40.958332 -115.083336]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00020455]]\n[[139488]]\n[[[  36.125    -105.541664]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00012719]]\n[[426421]]\n[[[  46.791668 -121.75    ]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00036997]]\n[[84911]]\n[[[  33.958332 -109.5     ]]]\n[[7.66642558e-05]]\n[[292876]]\n[[[  41.666668 -115.333336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00011195]]\n[[400403]]\n[[[  45.666668 -113.958336]]]\n[[0.00033222]]\n[[369791]]\n[[[  44.458332 -113.      ]]]\nfinished\n",
  "history_begin_time" : 1678742210246,
  "history_end_time" : 1678742785163,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "mhrmjb1856d",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n",
  "history_begin_time" : 1678725425457,
  "history_end_time" : 1678725437008,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "yq7uai46qfz",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n",
  "history_begin_time" : 1678723112469,
  "history_end_time" : 1678725408355,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "4k6yg2ka44l",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n[[0.00038069]]\n[[78266]]\n[[[  33.666668 -109.291664]]]\n[[0.00016855]]\n[[202538]]\n[[[  38.5      -106.333336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00012986]]\n[[373281]]\n[[[  44.583332 -107.208336]]]\n[[3.87134852e-05]]\n[[362136]]\n[[[  44.166668 -107.125   ]]]\n[[0.00015419]]\n[[330682]]\n[[[  43.   -109.75]]]\n[[0.00023674]]\n[[319102]]\n[[[  42.583332 -108.833336]]]\n[[8.83810871e-05]]\n[[267286]]\n[[[  40.791668 -110.875   ]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00034885]]\n[[116759]]\n[[[  35.25 -108.25]]]\n[[0.00034215]]\n[[107190]]\n[[[  34.875 -111.625]]]\n[[0.0001517]]\n[[104042]]\n[[[  34.75     -111.416664]]]\n[[0.00013821]]\n[[216303]]\n[[[  39.   -106.75]]]\n[[0.00016356]]\n[[174581]]\n[[[  37.458332 -108.458336]]]\n[[0.00033874]]\n[[457740]]\n[[[  48.416668 -113.916664]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00035391]]\n[[177932]]\n[[[  37.583332 -107.25    ]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00012476]]\n[[269738]]\n[[[  40.875    -110.541664]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00039874]]\n[[459718]]\n[[[  48.541668 -120.75    ]]]\n[[0.00028356]]\n[[162526]]\n[[[  37.       -106.291664]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00014601]]\n[[183432]]\n[[[  37.791668 -108.166664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00019668]]\n[[266072]]\n[[[  40.75  -110.625]]]\n[[0.00034733]]\n[[255062]]\n[[[  40.375 -116.875]]]\n[[9.15488263e-05]]\n[[283188]]\n[[[  41.333332 -106.375   ]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022445]]\n[[185613]]\n[[[  37.875    -109.458336]]]\n[[0.00025986]]\n[[240815]]\n[[[  39.875 -111.25 ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00012938]]\n[[252783]]\n[[[  40.291668 -111.25    ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00020894]]\n[[263633]]\n[[[  40.666668 -110.958336]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00026128]]\n[[234882]]\n[[[  39.666668 -110.416664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0003611]]\n[[269634]]\n[[[  40.875    -115.208336]]]\n[[0.00030221]]\n[[272081]]\n[[[  40.958332 -115.083336]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00020455]]\n[[139488]]\n[[[  36.125    -105.541664]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00012719]]\n[[426421]]\n[[[  46.791668 -121.75    ]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00036997]]\n[[84911]]\n[[[  33.958332 -109.5     ]]]\n[[7.66642558e-05]]\n[[292876]]\n[[[  41.666668 -115.333336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00011195]]\n[[400403]]\n[[[  45.666668 -113.958336]]]\n[[0.00033222]]\n[[369791]]\n[[[  44.458332 -113.      ]]]\nfinished\n",
  "history_begin_time" : 1678649002267,
  "history_end_time" : 1678649553983,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "5n3g453dksr",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n",
  "history_begin_time" : 1678648375789,
  "history_end_time" : 1678648386068,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "8qmuv9yzx1h",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n[[0.00038069]]\n[[78266]]\n[[[  33.666668 -109.291664]]]\n[[0.00016855]]\n[[202538]]\n[[[  38.5      -106.333336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00012986]]\n[[373281]]\n[[[  44.583332 -107.208336]]]\n[[3.87134852e-05]]\n[[362136]]\n[[[  44.166668 -107.125   ]]]\n[[0.00015419]]\n[[330682]]\n[[[  43.   -109.75]]]\n[[0.00023674]]\n[[319102]]\n[[[  42.583332 -108.833336]]]\n[[8.83810871e-05]]\n[[267286]]\n[[[  40.791668 -110.875   ]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00034885]]\n[[116759]]\n[[[  35.25 -108.25]]]\n[[0.00034215]]\n[[107190]]\n[[[  34.875 -111.625]]]\n[[0.0001517]]\n[[104042]]\n[[[  34.75     -111.416664]]]\n[[0.00013821]]\n[[216303]]\n[[[  39.   -106.75]]]\n[[0.00016356]]\n[[174581]]\n[[[  37.458332 -108.458336]]]\n[[0.00033874]]\n[[457740]]\n[[[  48.416668 -113.916664]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00035391]]\n[[177932]]\n[[[  37.583332 -107.25    ]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00012476]]\n[[269738]]\n[[[  40.875    -110.541664]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00039874]]\n[[459718]]\n[[[  48.541668 -120.75    ]]]\n[[0.00028356]]\n[[162526]]\n[[[  37.       -106.291664]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00014601]]\n[[183432]]\n[[[  37.791668 -108.166664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00019668]]\n[[266072]]\n[[[  40.75  -110.625]]]\n[[0.00034733]]\n[[255062]]\n[[[  40.375 -116.875]]]\n[[9.15488263e-05]]\n[[283188]]\n[[[  41.333332 -106.375   ]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022445]]\n[[185613]]\n[[[  37.875    -109.458336]]]\n[[0.00025986]]\n[[240815]]\n[[[  39.875 -111.25 ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00012938]]\n[[252783]]\n[[[  40.291668 -111.25    ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00020894]]\n[[263633]]\n[[[  40.666668 -110.958336]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00026128]]\n[[234882]]\n[[[  39.666668 -110.416664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0003611]]\n[[269634]]\n[[[  40.875    -115.208336]]]\n[[0.00030221]]\n[[272081]]\n[[[  40.958332 -115.083336]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00020455]]\n[[139488]]\n[[[  36.125    -105.541664]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00012719]]\n[[426421]]\n[[[  46.791668 -121.75    ]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00036997]]\n[[84911]]\n[[[  33.958332 -109.5     ]]]\n[[7.66642558e-05]]\n[[292876]]\n[[[  41.666668 -115.333336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00011195]]\n[[400403]]\n[[[  45.666668 -113.958336]]]\n[[0.00033222]]\n[[369791]]\n[[[  44.458332 -113.      ]]]\nfinished\n",
  "history_begin_time" : 1678648337618,
  "history_end_time" : 1678648935503,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "zqajxc6ef2p",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n",
  "history_begin_time" : 1678564609207,
  "history_end_time" : 1678564613046,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "ulnu9lxysv7",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n[[0.00038069]]\n[[78266]]\n[[[  33.666668 -109.291664]]]\n[[0.00016855]]\n[[202538]]\n[[[  38.5      -106.333336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00012986]]\n[[373281]]\n[[[  44.583332 -107.208336]]]\n[[3.87134852e-05]]\n[[362136]]\n[[[  44.166668 -107.125   ]]]\n[[0.00015419]]\n[[330682]]\n[[[  43.   -109.75]]]\n[[0.00023674]]\n[[319102]]\n[[[  42.583332 -108.833336]]]\n[[8.83810871e-05]]\n[[267286]]\n[[[  40.791668 -110.875   ]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00034885]]\n[[116759]]\n[[[  35.25 -108.25]]]\n[[0.00034215]]\n[[107190]]\n[[[  34.875 -111.625]]]\n[[0.0001517]]\n[[104042]]\n[[[  34.75     -111.416664]]]\n[[0.00013821]]\n[[216303]]\n[[[  39.   -106.75]]]\n[[0.00016356]]\n[[174581]]\n[[[  37.458332 -108.458336]]]\n[[0.00033874]]\n[[457740]]\n[[[  48.416668 -113.916664]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00035391]]\n[[177932]]\n[[[  37.583332 -107.25    ]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00012476]]\n[[269738]]\n[[[  40.875    -110.541664]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00039874]]\n[[459718]]\n[[[  48.541668 -120.75    ]]]\n[[0.00028356]]\n[[162526]]\n[[[  37.       -106.291664]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00014601]]\n[[183432]]\n[[[  37.791668 -108.166664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00019668]]\n[[266072]]\n[[[  40.75  -110.625]]]\n[[0.00034733]]\n[[255062]]\n[[[  40.375 -116.875]]]\n[[9.15488263e-05]]\n[[283188]]\n[[[  41.333332 -106.375   ]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022445]]\n[[185613]]\n[[[  37.875    -109.458336]]]\n[[0.00025986]]\n[[240815]]\n[[[  39.875 -111.25 ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00012938]]\n[[252783]]\n[[[  40.291668 -111.25    ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00020894]]\n[[263633]]\n[[[  40.666668 -110.958336]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00026128]]\n[[234882]]\n[[[  39.666668 -110.416664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0003611]]\n[[269634]]\n[[[  40.875    -115.208336]]]\n[[0.00030221]]\n[[272081]]\n[[[  40.958332 -115.083336]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00020455]]\n[[139488]]\n[[[  36.125    -105.541664]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00012719]]\n[[426421]]\n[[[  46.791668 -121.75    ]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00036997]]\n[[84911]]\n[[[  33.958332 -109.5     ]]]\n[[7.66642558e-05]]\n[[292876]]\n[[[  41.666668 -115.333336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00011195]]\n[[400403]]\n[[[  45.666668 -113.958336]]]\n[[0.00033222]]\n[[369791]]\n[[[  44.458332 -113.      ]]]\nfinished\n",
  "history_begin_time" : 1678564544623,
  "history_end_time" : 1678565062906,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "yjp44ar6duj",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n",
  "history_begin_time" : 1678557923705,
  "history_end_time" : 1678557923650,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Running"
},{
  "history_id" : "tk7gcdbfrok",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n",
  "history_begin_time" : 1678557706480,
  "history_end_time" : 1678557898797,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "6k915e2gfef",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n[[0.00038069]]\n[[78266]]\n[[[  33.666668 -109.291664]]]\n[[0.00016855]]\n[[202538]]\n[[[  38.5      -106.333336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00012986]]\n[[373281]]\n[[[  44.583332 -107.208336]]]\n[[3.87134852e-05]]\n[[362136]]\n[[[  44.166668 -107.125   ]]]\n[[0.00015419]]\n[[330682]]\n[[[  43.   -109.75]]]\n[[0.00023674]]\n[[319102]]\n[[[  42.583332 -108.833336]]]\n[[8.83810871e-05]]\n[[267286]]\n[[[  40.791668 -110.875   ]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00034885]]\n[[116759]]\n[[[  35.25 -108.25]]]\n[[0.00034215]]\n[[107190]]\n[[[  34.875 -111.625]]]\n[[0.0001517]]\n[[104042]]\n[[[  34.75     -111.416664]]]\n[[0.00013821]]\n[[216303]]\n[[[  39.   -106.75]]]\n[[0.00016356]]\n[[174581]]\n[[[  37.458332 -108.458336]]]\n[[0.00033874]]\n[[457740]]\n[[[  48.416668 -113.916664]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00035391]]\n[[177932]]\n[[[  37.583332 -107.25    ]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00012476]]\n[[269738]]\n[[[  40.875    -110.541664]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00039874]]\n[[459718]]\n[[[  48.541668 -120.75    ]]]\n[[0.00028356]]\n[[162526]]\n[[[  37.       -106.291664]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00014601]]\n[[183432]]\n[[[  37.791668 -108.166664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00019668]]\n[[266072]]\n[[[  40.75  -110.625]]]\n[[0.00034733]]\n[[255062]]\n[[[  40.375 -116.875]]]\n[[9.15488263e-05]]\n[[283188]]\n[[[  41.333332 -106.375   ]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022445]]\n[[185613]]\n[[[  37.875    -109.458336]]]\n[[0.00025986]]\n[[240815]]\n[[[  39.875 -111.25 ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00012938]]\n[[252783]]\n[[[  40.291668 -111.25    ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00020894]]\n[[263633]]\n[[[  40.666668 -110.958336]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00026128]]\n[[234882]]\n[[[  39.666668 -110.416664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0003611]]\n[[269634]]\n[[[  40.875    -115.208336]]]\n[[0.00030221]]\n[[272081]]\n[[[  40.958332 -115.083336]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00020455]]\n[[139488]]\n[[[  36.125    -105.541664]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00012719]]\n[[426421]]\n[[[  46.791668 -121.75    ]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00036997]]\n[[84911]]\n[[[  33.958332 -109.5     ]]]\n[[7.66642558e-05]]\n[[292876]]\n[[[  41.666668 -115.333336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00011195]]\n[[400403]]\n[[[  45.666668 -113.958336]]]\n[[0.00033222]]\n[[369791]]\n[[[  44.458332 -113.      ]]]\nfinished\n",
  "history_begin_time" : 1678497028896,
  "history_end_time" : 1678497575427,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "hptyp75w8q3",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n[[0.00038069]]\n[[78266]]\n[[[  33.666668 -109.291664]]]\n[[0.00016855]]\n[[202538]]\n[[[  38.5      -106.333336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00012986]]\n[[373281]]\n[[[  44.583332 -107.208336]]]\n[[3.87134852e-05]]\n[[362136]]\n[[[  44.166668 -107.125   ]]]\n[[0.00015419]]\n[[330682]]\n[[[  43.   -109.75]]]\n[[0.00023674]]\n[[319102]]\n[[[  42.583332 -108.833336]]]\n[[8.83810871e-05]]\n[[267286]]\n[[[  40.791668 -110.875   ]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00034885]]\n[[116759]]\n[[[  35.25 -108.25]]]\n[[0.00034215]]\n[[107190]]\n[[[  34.875 -111.625]]]\n[[0.0001517]]\n[[104042]]\n[[[  34.75     -111.416664]]]\n[[0.00013821]]\n[[216303]]\n[[[  39.   -106.75]]]\n[[0.00016356]]\n[[174581]]\n[[[  37.458332 -108.458336]]]\n[[0.00033874]]\n[[457740]]\n[[[  48.416668 -113.916664]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00035391]]\n[[177932]]\n[[[  37.583332 -107.25    ]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00012476]]\n[[269738]]\n[[[  40.875    -110.541664]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00039874]]\n[[459718]]\n[[[  48.541668 -120.75    ]]]\n[[0.00028356]]\n[[162526]]\n[[[  37.       -106.291664]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00014601]]\n[[183432]]\n[[[  37.791668 -108.166664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00019668]]\n[[266072]]\n[[[  40.75  -110.625]]]\n[[0.00034733]]\n[[255062]]\n[[[  40.375 -116.875]]]\n[[9.15488263e-05]]\n[[283188]]\n[[[  41.333332 -106.375   ]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022445]]\n[[185613]]\n[[[  37.875    -109.458336]]]\n[[0.00025986]]\n[[240815]]\n[[[  39.875 -111.25 ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00012938]]\n[[252783]]\n[[[  40.291668 -111.25    ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00020894]]\n[[263633]]\n[[[  40.666668 -110.958336]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00026128]]\n[[234882]]\n[[[  39.666668 -110.416664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0003611]]\n[[269634]]\n[[[  40.875    -115.208336]]]\n[[0.00030221]]\n[[272081]]\n[[[  40.958332 -115.083336]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00020455]]\n[[139488]]\n[[[  36.125    -105.541664]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00012719]]\n[[426421]]\n[[[  46.791668 -121.75    ]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00036997]]\n[[84911]]\n[[[  33.958332 -109.5     ]]]\n[[7.66642558e-05]]\n[[292876]]\n[[[  41.666668 -115.333336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00011195]]\n[[400403]]\n[[[  45.666668 -113.958336]]]\n[[0.00033222]]\n[[369791]]\n[[[  44.458332 -113.      ]]]\nfinished\n",
  "history_begin_time" : 1678330214634,
  "history_end_time" : 1678330719179,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "3vtqty3dc2v",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n",
  "history_begin_time" : 1678326965508,
  "history_end_time" : 1694185584954,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "ubw23bcpxc9",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n[[0.00038069]]\n[[78266]]\n[[[  33.666668 -109.291664]]]\n[[0.00016855]]\n[[202538]]\n[[[  38.5      -106.333336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00012986]]\n[[373281]]\n[[[  44.583332 -107.208336]]]\n[[3.87134852e-05]]\n[[362136]]\n[[[  44.166668 -107.125   ]]]\n[[0.00015419]]\n[[330682]]\n[[[  43.   -109.75]]]\n[[0.00023674]]\n[[319102]]\n[[[  42.583332 -108.833336]]]\n[[8.83810871e-05]]\n[[267286]]\n[[[  40.791668 -110.875   ]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00034885]]\n[[116759]]\n[[[  35.25 -108.25]]]\n[[0.00034215]]\n[[107190]]\n[[[  34.875 -111.625]]]\n[[0.0001517]]\n[[104042]]\n[[[  34.75     -111.416664]]]\n[[0.00013821]]\n[[216303]]\n[[[  39.   -106.75]]]\n[[0.00016356]]\n[[174581]]\n[[[  37.458332 -108.458336]]]\n[[0.00033874]]\n[[457740]]\n[[[  48.416668 -113.916664]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00035391]]\n[[177932]]\n[[[  37.583332 -107.25    ]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00012476]]\n[[269738]]\n[[[  40.875    -110.541664]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00039874]]\n[[459718]]\n[[[  48.541668 -120.75    ]]]\n[[0.00028356]]\n[[162526]]\n[[[  37.       -106.291664]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00014601]]\n[[183432]]\n[[[  37.791668 -108.166664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00019668]]\n[[266072]]\n[[[  40.75  -110.625]]]\n[[0.00034733]]\n[[255062]]\n[[[  40.375 -116.875]]]\n[[9.15488263e-05]]\n[[283188]]\n[[[  41.333332 -106.375   ]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022445]]\n[[185613]]\n[[[  37.875    -109.458336]]]\n[[0.00025986]]\n[[240815]]\n[[[  39.875 -111.25 ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00012938]]\n[[252783]]\n[[[  40.291668 -111.25    ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00020894]]\n[[263633]]\n[[[  40.666668 -110.958336]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00026128]]\n[[234882]]\n[[[  39.666668 -110.416664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0003611]]\n[[269634]]\n[[[  40.875    -115.208336]]]\n",
  "history_begin_time" : 1678312075021,
  "history_end_time" : 1678312605881,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "nue7y7blatq",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n",
  "history_begin_time" : 1678312031691,
  "history_end_time" : 1678312065074,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "fwpcea8xrar",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n[[0.00038069]]\n[[78266]]\n[[[  33.666668 -109.291664]]]\n[[0.00016855]]\n[[202538]]\n[[[  38.5      -106.333336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00012986]]\n[[373281]]\n[[[  44.583332 -107.208336]]]\n[[3.87134852e-05]]\n[[362136]]\n[[[  44.166668 -107.125   ]]]\n[[0.00015419]]\n[[330682]]\n[[[  43.   -109.75]]]\n[[0.00023674]]\n[[319102]]\n[[[  42.583332 -108.833336]]]\n[[8.83810871e-05]]\n[[267286]]\n[[[  40.791668 -110.875   ]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00034885]]\n[[116759]]\n[[[  35.25 -108.25]]]\n[[0.00034215]]\n[[107190]]\n[[[  34.875 -111.625]]]\n[[0.0001517]]\n[[104042]]\n[[[  34.75     -111.416664]]]\n[[0.00013821]]\n[[216303]]\n[[[  39.   -106.75]]]\n[[0.00016356]]\n[[174581]]\n[[[  37.458332 -108.458336]]]\n[[0.00033874]]\n[[457740]]\n[[[  48.416668 -113.916664]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00035391]]\n[[177932]]\n[[[  37.583332 -107.25    ]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00012476]]\n[[269738]]\n[[[  40.875    -110.541664]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00039874]]\n[[459718]]\n[[[  48.541668 -120.75    ]]]\n[[0.00028356]]\n[[162526]]\n[[[  37.       -106.291664]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00014601]]\n[[183432]]\n[[[  37.791668 -108.166664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00019668]]\n[[266072]]\n[[[  40.75  -110.625]]]\n[[0.00034733]]\n[[255062]]\n[[[  40.375 -116.875]]]\n[[9.15488263e-05]]\n[[283188]]\n[[[  41.333332 -106.375   ]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022445]]\n[[185613]]\n[[[  37.875    -109.458336]]]\n[[0.00025986]]\n[[240815]]\n[[[  39.875 -111.25 ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00012938]]\n[[252783]]\n[[[  40.291668 -111.25    ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00020894]]\n[[263633]]\n[[[  40.666668 -110.958336]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00026128]]\n[[234882]]\n[[[  39.666668 -110.416664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0003611]]\n[[269634]]\n[[[  40.875    -115.208336]]]\n[[0.00030221]]\n[[272081]]\n[[[  40.958332 -115.083336]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00020455]]\n[[139488]]\n[[[  36.125    -105.541664]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00012719]]\n[[426421]]\n[[[  46.791668 -121.75    ]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00036997]]\n[[84911]]\n[[[  33.958332 -109.5     ]]]\n[[7.66642558e-05]]\n[[292876]]\n[[[  41.666668 -115.333336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00011195]]\n[[400403]]\n[[[  45.666668 -113.958336]]]\n[[0.00033222]]\n[[369791]]\n[[[  44.458332 -113.      ]]]\nfinished\n",
  "history_begin_time" : 1678312002024,
  "history_end_time" : 1694185586319,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "co2vudh0ndt",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n",
  "history_begin_time" : 1678283531010,
  "history_end_time" : 1678283551026,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "0p6r5ihc0cp",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n[[0.00038069]]\n[[78266]]\n[[[  33.666668 -109.291664]]]\n[[0.00016855]]\n[[202538]]\n[[[  38.5      -106.333336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00012986]]\n[[373281]]\n[[[  44.583332 -107.208336]]]\n[[3.87134852e-05]]\n[[362136]]\n[[[  44.166668 -107.125   ]]]\n[[0.00015419]]\n[[330682]]\n[[[  43.   -109.75]]]\n[[0.00023674]]\n[[319102]]\n[[[  42.583332 -108.833336]]]\n[[8.83810871e-05]]\n[[267286]]\n[[[  40.791668 -110.875   ]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00034885]]\n[[116759]]\n[[[  35.25 -108.25]]]\n[[0.00034215]]\n[[107190]]\n[[[  34.875 -111.625]]]\n[[0.0001517]]\n[[104042]]\n[[[  34.75     -111.416664]]]\n[[0.00013821]]\n[[216303]]\n[[[  39.   -106.75]]]\n[[0.00016356]]\n[[174581]]\n[[[  37.458332 -108.458336]]]\n[[0.00033874]]\n[[457740]]\n[[[  48.416668 -113.916664]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00035391]]\n[[177932]]\n[[[  37.583332 -107.25    ]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00012476]]\n[[269738]]\n[[[  40.875    -110.541664]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00039874]]\n[[459718]]\n[[[  48.541668 -120.75    ]]]\n[[0.00028356]]\n[[162526]]\n[[[  37.       -106.291664]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00014601]]\n[[183432]]\n[[[  37.791668 -108.166664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00019668]]\n[[266072]]\n[[[  40.75  -110.625]]]\n[[0.00034733]]\n[[255062]]\n[[[  40.375 -116.875]]]\n[[9.15488263e-05]]\n[[283188]]\n[[[  41.333332 -106.375   ]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022445]]\n[[185613]]\n[[[  37.875    -109.458336]]]\n[[0.00025986]]\n[[240815]]\n[[[  39.875 -111.25 ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00012938]]\n[[252783]]\n[[[  40.291668 -111.25    ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00020894]]\n[[263633]]\n[[[  40.666668 -110.958336]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00026128]]\n[[234882]]\n[[[  39.666668 -110.416664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0003611]]\n[[269634]]\n[[[  40.875    -115.208336]]]\n[[0.00030221]]\n[[272081]]\n[[[  40.958332 -115.083336]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00020455]]\n[[139488]]\n[[[  36.125    -105.541664]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00012719]]\n[[426421]]\n[[[  46.791668 -121.75    ]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00036997]]\n[[84911]]\n[[[  33.958332 -109.5     ]]]\n[[7.66642558e-05]]\n[[292876]]\n[[[  41.666668 -115.333336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00011195]]\n[[400403]]\n[[[  45.666668 -113.958336]]]\n[[0.00033222]]\n[[369791]]\n[[[  44.458332 -113.      ]]]\nfinished\n",
  "history_begin_time" : 1678241847023,
  "history_end_time" : 1694185588077,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "dsxqr923psu",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n[[0.00038069]]\n[[78266]]\n[[[  33.666668 -109.291664]]]\n[[0.00016855]]\n[[202538]]\n[[[  38.5      -106.333336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00012986]]\n[[373281]]\n[[[  44.583332 -107.208336]]]\n[[3.87134852e-05]]\n[[362136]]\n[[[  44.166668 -107.125   ]]]\n[[0.00015419]]\n[[330682]]\n[[[  43.   -109.75]]]\n[[0.00023674]]\n[[319102]]\n[[[  42.583332 -108.833336]]]\n[[8.83810871e-05]]\n[[267286]]\n[[[  40.791668 -110.875   ]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00034885]]\n[[116759]]\n[[[  35.25 -108.25]]]\n[[0.00034215]]\n[[107190]]\n[[[  34.875 -111.625]]]\n[[0.0001517]]\n[[104042]]\n[[[  34.75     -111.416664]]]\n[[0.00013821]]\n[[216303]]\n[[[  39.   -106.75]]]\n[[0.00016356]]\n[[174581]]\n[[[  37.458332 -108.458336]]]\n[[0.00033874]]\n[[457740]]\n[[[  48.416668 -113.916664]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00035391]]\n[[177932]]\n[[[  37.583332 -107.25    ]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00012476]]\n[[269738]]\n[[[  40.875    -110.541664]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00039874]]\n[[459718]]\n[[[  48.541668 -120.75    ]]]\n[[0.00028356]]\n[[162526]]\n[[[  37.       -106.291664]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00014601]]\n[[183432]]\n[[[  37.791668 -108.166664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00019668]]\n[[266072]]\n[[[  40.75  -110.625]]]\n[[0.00034733]]\n[[255062]]\n[[[  40.375 -116.875]]]\n[[9.15488263e-05]]\n[[283188]]\n[[[  41.333332 -106.375   ]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022445]]\n[[185613]]\n[[[  37.875    -109.458336]]]\n[[0.00025986]]\n[[240815]]\n[[[  39.875 -111.25 ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00012938]]\n[[252783]]\n[[[  40.291668 -111.25    ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00020894]]\n[[263633]]\n[[[  40.666668 -110.958336]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00026128]]\n[[234882]]\n[[[  39.666668 -110.416664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0003611]]\n[[269634]]\n[[[  40.875    -115.208336]]]\n[[0.00030221]]\n[[272081]]\n[[[  40.958332 -115.083336]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00020455]]\n[[139488]]\n[[[  36.125    -105.541664]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00012719]]\n[[426421]]\n[[[  46.791668 -121.75    ]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00036997]]\n[[84911]]\n[[[  33.958332 -109.5     ]]]\n[[7.66642558e-05]]\n[[292876]]\n[[[  41.666668 -115.333336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00011195]]\n[[400403]]\n[[[  45.666668 -113.958336]]]\n[[0.00033222]]\n[[369791]]\n[[[  44.458332 -113.      ]]]\nfinished\n",
  "history_begin_time" : 1678206375332,
  "history_end_time" : 1678206948375,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "8jpc4g9hn12",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n",
  "history_begin_time" : 1678201930522,
  "history_end_time" : 1678201934493,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "62fjemdy12k",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n[[0.00038069]]\n[[78266]]\n[[[  33.666668 -109.291664]]]\n[[0.00016855]]\n[[202538]]\n[[[  38.5      -106.333336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00012986]]\n[[373281]]\n[[[  44.583332 -107.208336]]]\n[[3.87134852e-05]]\n[[362136]]\n[[[  44.166668 -107.125   ]]]\n[[0.00015419]]\n[[330682]]\n[[[  43.   -109.75]]]\n[[0.00023674]]\n[[319102]]\n[[[  42.583332 -108.833336]]]\n[[8.83810871e-05]]\n[[267286]]\n[[[  40.791668 -110.875   ]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00034885]]\n[[116759]]\n[[[  35.25 -108.25]]]\n[[0.00034215]]\n[[107190]]\n[[[  34.875 -111.625]]]\n[[0.0001517]]\n[[104042]]\n[[[  34.75     -111.416664]]]\n[[0.00013821]]\n[[216303]]\n[[[  39.   -106.75]]]\n[[0.00016356]]\n[[174581]]\n[[[  37.458332 -108.458336]]]\n[[0.00033874]]\n[[457740]]\n[[[  48.416668 -113.916664]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00035391]]\n[[177932]]\n[[[  37.583332 -107.25    ]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00012476]]\n[[269738]]\n[[[  40.875    -110.541664]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00039874]]\n[[459718]]\n[[[  48.541668 -120.75    ]]]\n[[0.00028356]]\n[[162526]]\n[[[  37.       -106.291664]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00014601]]\n[[183432]]\n[[[  37.791668 -108.166664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00019668]]\n[[266072]]\n[[[  40.75  -110.625]]]\n[[0.00034733]]\n[[255062]]\n[[[  40.375 -116.875]]]\n[[9.15488263e-05]]\n[[283188]]\n[[[  41.333332 -106.375   ]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022445]]\n[[185613]]\n[[[  37.875    -109.458336]]]\n[[0.00025986]]\n[[240815]]\n[[[  39.875 -111.25 ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00012938]]\n[[252783]]\n[[[  40.291668 -111.25    ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00020894]]\n[[263633]]\n[[[  40.666668 -110.958336]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00026128]]\n[[234882]]\n[[[  39.666668 -110.416664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0003611]]\n[[269634]]\n[[[  40.875    -115.208336]]]\n[[0.00030221]]\n[[272081]]\n[[[  40.958332 -115.083336]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00020455]]\n[[139488]]\n[[[  36.125    -105.541664]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00012719]]\n[[426421]]\n[[[  46.791668 -121.75    ]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00036997]]\n[[84911]]\n[[[  33.958332 -109.5     ]]]\n[[7.66642558e-05]]\n[[292876]]\n[[[  41.666668 -115.333336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00011195]]\n[[400403]]\n[[[  45.666668 -113.958336]]]\n[[0.00033222]]\n[[369791]]\n[[[  44.458332 -113.      ]]]\nfinished\n",
  "history_begin_time" : 1678201343500,
  "history_end_time" : 1678201865860,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "v8jalmllhi0",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n",
  "history_begin_time" : 1678155216043,
  "history_end_time" : 1678155220312,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "onhy7mea769",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n[[0.00038069]]\n[[78266]]\n[[[  33.666668 -109.291664]]]\n[[0.00016855]]\n[[202538]]\n[[[  38.5      -106.333336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00012986]]\n[[373281]]\n[[[  44.583332 -107.208336]]]\n[[3.87134852e-05]]\n[[362136]]\n[[[  44.166668 -107.125   ]]]\n[[0.00015419]]\n[[330682]]\n[[[  43.   -109.75]]]\n[[0.00023674]]\n[[319102]]\n[[[  42.583332 -108.833336]]]\n[[8.83810871e-05]]\n[[267286]]\n[[[  40.791668 -110.875   ]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00034885]]\n[[116759]]\n[[[  35.25 -108.25]]]\n[[0.00034215]]\n[[107190]]\n[[[  34.875 -111.625]]]\n[[0.0001517]]\n[[104042]]\n[[[  34.75     -111.416664]]]\n[[0.00013821]]\n[[216303]]\n[[[  39.   -106.75]]]\n[[0.00016356]]\n[[174581]]\n[[[  37.458332 -108.458336]]]\n[[0.00033874]]\n[[457740]]\n[[[  48.416668 -113.916664]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n",
  "history_begin_time" : 1678144752914,
  "history_end_time" : 1678154846288,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "ssm6f9blgr7",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n",
  "history_begin_time" : 1677858790160,
  "history_end_time" : 1694185596832,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "q1uqoi3owzo",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n",
  "history_begin_time" : 1677809689231,
  "history_end_time" : 1694185596456,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "bg6o1x3hh30",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n",
  "history_begin_time" : 1677809448973,
  "history_end_time" : 1694185620807,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "yq8hireo5yw",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n",
  "history_begin_time" : 1677809016848,
  "history_end_time" : 1694185619658,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "24kjam2zd5w",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n",
  "history_begin_time" : 1677793015028,
  "history_end_time" : 1694185612780,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "uxyanug4wea",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n[[0.00038069]]\n[[78266]]\n[[[  33.666668 -109.291664]]]\n[[0.00016855]]\n[[202538]]\n[[[  38.5      -106.333336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00012986]]\n[[373281]]\n[[[  44.583332 -107.208336]]]\n[[3.87134852e-05]]\n[[362136]]\n[[[  44.166668 -107.125   ]]]\n[[0.00015419]]\n[[330682]]\n[[[  43.   -109.75]]]\n[[0.00023674]]\n[[319102]]\n[[[  42.583332 -108.833336]]]\n[[8.83810871e-05]]\n[[267286]]\n[[[  40.791668 -110.875   ]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00034885]]\n[[116759]]\n[[[  35.25 -108.25]]]\n[[0.00034215]]\n[[107190]]\n[[[  34.875 -111.625]]]\n[[0.0001517]]\n[[104042]]\n[[[  34.75     -111.416664]]]\n[[0.00013821]]\n[[216303]]\n[[[  39.   -106.75]]]\n[[0.00016356]]\n[[174581]]\n[[[  37.458332 -108.458336]]]\n[[0.00033874]]\n[[457740]]\n[[[  48.416668 -113.916664]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00035391]]\n[[177932]]\n[[[  37.583332 -107.25    ]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00012476]]\n[[269738]]\n[[[  40.875    -110.541664]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00039874]]\n[[459718]]\n[[[  48.541668 -120.75    ]]]\n[[0.00028356]]\n[[162526]]\n[[[  37.       -106.291664]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00014601]]\n[[183432]]\n[[[  37.791668 -108.166664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00019668]]\n[[266072]]\n[[[  40.75  -110.625]]]\n[[0.00034733]]\n[[255062]]\n[[[  40.375 -116.875]]]\n[[9.15488263e-05]]\n[[283188]]\n[[[  41.333332 -106.375   ]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022445]]\n[[185613]]\n[[[  37.875    -109.458336]]]\n[[0.00025986]]\n[[240815]]\n[[[  39.875 -111.25 ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00012938]]\n[[252783]]\n[[[  40.291668 -111.25    ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00020894]]\n[[263633]]\n[[[  40.666668 -110.958336]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00026128]]\n[[234882]]\n[[[  39.666668 -110.416664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0003611]]\n[[269634]]\n[[[  40.875    -115.208336]]]\n[[0.00030221]]\n[[272081]]\n[[[  40.958332 -115.083336]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00020455]]\n[[139488]]\n[[[  36.125    -105.541664]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00012719]]\n[[426421]]\n[[[  46.791668 -121.75    ]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00036997]]\n[[84911]]\n[[[  33.958332 -109.5     ]]]\n[[7.66642558e-05]]\n[[292876]]\n[[[  41.666668 -115.333336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00011195]]\n[[400403]]\n[[[  45.666668 -113.958336]]]\n[[0.00033222]]\n[[369791]]\n[[[  44.458332 -113.      ]]]\nfinished\n",
  "history_begin_time" : 1677791611919,
  "history_end_time" : 1694185611224,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "ne3ku7q1n0a",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n[[0.00038069]]\n[[78266]]\n[[[  33.666668 -109.291664]]]\n[[0.00016855]]\n[[202538]]\n[[[  38.5      -106.333336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00012986]]\n[[373281]]\n[[[  44.583332 -107.208336]]]\n[[3.87134852e-05]]\n[[362136]]\n[[[  44.166668 -107.125   ]]]\n[[0.00015419]]\n[[330682]]\n[[[  43.   -109.75]]]\n[[0.00023674]]\n[[319102]]\n[[[  42.583332 -108.833336]]]\n[[8.83810871e-05]]\n[[267286]]\n[[[  40.791668 -110.875   ]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00034885]]\n[[116759]]\n[[[  35.25 -108.25]]]\n[[0.00034215]]\n[[107190]]\n[[[  34.875 -111.625]]]\n[[0.0001517]]\n[[104042]]\n[[[  34.75     -111.416664]]]\n[[0.00013821]]\n[[216303]]\n[[[  39.   -106.75]]]\n[[0.00016356]]\n[[174581]]\n[[[  37.458332 -108.458336]]]\n[[0.00033874]]\n[[457740]]\n[[[  48.416668 -113.916664]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00035391]]\n[[177932]]\n[[[  37.583332 -107.25    ]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00012476]]\n[[269738]]\n[[[  40.875    -110.541664]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00039874]]\n[[459718]]\n[[[  48.541668 -120.75    ]]]\n[[0.00028356]]\n[[162526]]\n[[[  37.       -106.291664]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00014601]]\n[[183432]]\n[[[  37.791668 -108.166664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00019668]]\n[[266072]]\n[[[  40.75  -110.625]]]\n[[0.00034733]]\n[[255062]]\n[[[  40.375 -116.875]]]\n[[9.15488263e-05]]\n[[283188]]\n[[[  41.333332 -106.375   ]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022445]]\n[[185613]]\n[[[  37.875    -109.458336]]]\n[[0.00025986]]\n[[240815]]\n[[[  39.875 -111.25 ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00012938]]\n[[252783]]\n[[[  40.291668 -111.25    ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00020894]]\n[[263633]]\n[[[  40.666668 -110.958336]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00026128]]\n[[234882]]\n[[[  39.666668 -110.416664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0003611]]\n[[269634]]\n[[[  40.875    -115.208336]]]\n[[0.00030221]]\n[[272081]]\n[[[  40.958332 -115.083336]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00020455]]\n[[139488]]\n[[[  36.125    -105.541664]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00012719]]\n[[426421]]\n[[[  46.791668 -121.75    ]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00036997]]\n[[84911]]\n[[[  33.958332 -109.5     ]]]\n[[7.66642558e-05]]\n[[292876]]\n[[[  41.666668 -115.333336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00011195]]\n[[400403]]\n[[[  45.666668 -113.958336]]]\n[[0.00033222]]\n[[369791]]\n[[[  44.458332 -113.      ]]]\nfinished\n",
  "history_begin_time" : 1677787790692,
  "history_end_time" : 1694185610913,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "r0mkypilred",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n[[0.00038069]]\n[[78266]]\n[[[  33.666668 -109.291664]]]\n[[0.00016855]]\n[[202538]]\n[[[  38.5      -106.333336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00012986]]\n[[373281]]\n[[[  44.583332 -107.208336]]]\n[[3.87134852e-05]]\n[[362136]]\n[[[  44.166668 -107.125   ]]]\n[[0.00015419]]\n[[330682]]\n[[[  43.   -109.75]]]\n[[0.00023674]]\n[[319102]]\n[[[  42.583332 -108.833336]]]\n[[8.83810871e-05]]\n[[267286]]\n[[[  40.791668 -110.875   ]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00034885]]\n[[116759]]\n[[[  35.25 -108.25]]]\n[[0.00034215]]\n[[107190]]\n[[[  34.875 -111.625]]]\n[[0.0001517]]\n[[104042]]\n[[[  34.75     -111.416664]]]\n[[0.00013821]]\n[[216303]]\n[[[  39.   -106.75]]]\n[[0.00016356]]\n[[174581]]\n[[[  37.458332 -108.458336]]]\n[[0.00033874]]\n[[457740]]\n[[[  48.416668 -113.916664]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00035391]]\n[[177932]]\n[[[  37.583332 -107.25    ]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00012476]]\n[[269738]]\n[[[  40.875    -110.541664]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00039874]]\n[[459718]]\n[[[  48.541668 -120.75    ]]]\n[[0.00028356]]\n[[162526]]\n[[[  37.       -106.291664]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00014601]]\n[[183432]]\n[[[  37.791668 -108.166664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00019668]]\n[[266072]]\n[[[  40.75  -110.625]]]\n[[0.00034733]]\n[[255062]]\n[[[  40.375 -116.875]]]\n[[9.15488263e-05]]\n[[283188]]\n[[[  41.333332 -106.375   ]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022445]]\n[[185613]]\n[[[  37.875    -109.458336]]]\n[[0.00025986]]\n[[240815]]\n[[[  39.875 -111.25 ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00012938]]\n[[252783]]\n[[[  40.291668 -111.25    ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00020894]]\n[[263633]]\n[[[  40.666668 -110.958336]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00026128]]\n[[234882]]\n[[[  39.666668 -110.416664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0003611]]\n[[269634]]\n[[[  40.875    -115.208336]]]\n[[0.00030221]]\n[[272081]]\n[[[  40.958332 -115.083336]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00020455]]\n[[139488]]\n[[[  36.125    -105.541664]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00012719]]\n[[426421]]\n[[[  46.791668 -121.75    ]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00036997]]\n[[84911]]\n[[[  33.958332 -109.5     ]]]\n[[7.66642558e-05]]\n[[292876]]\n[[[  41.666668 -115.333336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00011195]]\n[[400403]]\n[[[  45.666668 -113.958336]]]\n[[0.00033222]]\n[[369791]]\n[[[  44.458332 -113.      ]]]\nfinished\n",
  "history_begin_time" : 1677787291921,
  "history_end_time" : 1677788035560,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "plezrdz08vx",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n[[0.00038069]]\n[[78266]]\n[[[  33.666668 -109.291664]]]\n[[0.00016855]]\n[[202538]]\n[[[  38.5      -106.333336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00012986]]\n[[373281]]\n[[[  44.583332 -107.208336]]]\n[[3.87134852e-05]]\n[[362136]]\n[[[  44.166668 -107.125   ]]]\n[[0.00015419]]\n[[330682]]\n[[[  43.   -109.75]]]\n[[0.00023674]]\n[[319102]]\n[[[  42.583332 -108.833336]]]\n[[8.83810871e-05]]\n[[267286]]\n[[[  40.791668 -110.875   ]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00034885]]\n[[116759]]\n[[[  35.25 -108.25]]]\n[[0.00034215]]\n[[107190]]\n[[[  34.875 -111.625]]]\n[[0.0001517]]\n[[104042]]\n[[[  34.75     -111.416664]]]\n[[0.00013821]]\n[[216303]]\n[[[  39.   -106.75]]]\n[[0.00016356]]\n[[174581]]\n[[[  37.458332 -108.458336]]]\n[[0.00033874]]\n[[457740]]\n[[[  48.416668 -113.916664]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00035391]]\n[[177932]]\n[[[  37.583332 -107.25    ]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00012476]]\n[[269738]]\n[[[  40.875    -110.541664]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00039874]]\n[[459718]]\n[[[  48.541668 -120.75    ]]]\n[[0.00028356]]\n[[162526]]\n[[[  37.       -106.291664]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00014601]]\n[[183432]]\n[[[  37.791668 -108.166664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00019668]]\n[[266072]]\n[[[  40.75  -110.625]]]\n[[0.00034733]]\n[[255062]]\n[[[  40.375 -116.875]]]\n[[9.15488263e-05]]\n[[283188]]\n[[[  41.333332 -106.375   ]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022445]]\n[[185613]]\n[[[  37.875    -109.458336]]]\n[[0.00025986]]\n[[240815]]\n[[[  39.875 -111.25 ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00012938]]\n[[252783]]\n[[[  40.291668 -111.25    ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00020894]]\n[[263633]]\n[[[  40.666668 -110.958336]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00026128]]\n[[234882]]\n[[[  39.666668 -110.416664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0003611]]\n[[269634]]\n[[[  40.875    -115.208336]]]\n[[0.00030221]]\n[[272081]]\n[[[  40.958332 -115.083336]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00020455]]\n[[139488]]\n[[[  36.125    -105.541664]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00012719]]\n[[426421]]\n[[[  46.791668 -121.75    ]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00036997]]\n[[84911]]\n[[[  33.958332 -109.5     ]]]\n[[7.66642558e-05]]\n[[292876]]\n[[[  41.666668 -115.333336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00011195]]\n[[400403]]\n[[[  45.666668 -113.958336]]]\n[[0.00033222]]\n[[369791]]\n[[[  44.458332 -113.      ]]]\nfinished\n",
  "history_begin_time" : 1677786019637,
  "history_end_time" : 1677786568720,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "flnnc4i26h5",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "",
  "history_begin_time" : 1677784218474,
  "history_end_time" : 1677784272026,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "35omev1t4vn",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n[[0.00038069]]\n[[78266]]\n[[[  33.666668 -109.291664]]]\n[[0.00016855]]\n[[202538]]\n[[[  38.5      -106.333336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00012986]]\n[[373281]]\n[[[  44.583332 -107.208336]]]\n[[3.87134852e-05]]\n[[362136]]\n[[[  44.166668 -107.125   ]]]\n[[0.00015419]]\n[[330682]]\n[[[  43.   -109.75]]]\n[[0.00023674]]\n[[319102]]\n[[[  42.583332 -108.833336]]]\n[[8.83810871e-05]]\n[[267286]]\n[[[  40.791668 -110.875   ]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00034885]]\n[[116759]]\n[[[  35.25 -108.25]]]\n[[0.00034215]]\n[[107190]]\n[[[  34.875 -111.625]]]\n[[0.0001517]]\n[[104042]]\n[[[  34.75     -111.416664]]]\n[[0.00013821]]\n[[216303]]\n[[[  39.   -106.75]]]\n[[0.00016356]]\n[[174581]]\n[[[  37.458332 -108.458336]]]\n[[0.00033874]]\n[[457740]]\n[[[  48.416668 -113.916664]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00035391]]\n[[177932]]\n[[[  37.583332 -107.25    ]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00012476]]\n[[269738]]\n[[[  40.875    -110.541664]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00039874]]\n[[459718]]\n[[[  48.541668 -120.75    ]]]\n[[0.00028356]]\n[[162526]]\n[[[  37.       -106.291664]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00014601]]\n[[183432]]\n[[[  37.791668 -108.166664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00019668]]\n[[266072]]\n[[[  40.75  -110.625]]]\n[[0.00034733]]\n[[255062]]\n[[[  40.375 -116.875]]]\n[[9.15488263e-05]]\n[[283188]]\n[[[  41.333332 -106.375   ]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022445]]\n[[185613]]\n[[[  37.875    -109.458336]]]\n[[0.00025986]]\n[[240815]]\n[[[  39.875 -111.25 ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00012938]]\n[[252783]]\n[[[  40.291668 -111.25    ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00020894]]\n[[263633]]\n[[[  40.666668 -110.958336]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00026128]]\n[[234882]]\n[[[  39.666668 -110.416664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0003611]]\n[[269634]]\n[[[  40.875    -115.208336]]]\n[[0.00030221]]\n[[272081]]\n[[[  40.958332 -115.083336]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00020455]]\n[[139488]]\n[[[  36.125    -105.541664]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00012719]]\n[[426421]]\n[[[  46.791668 -121.75    ]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00036997]]\n[[84911]]\n[[[  33.958332 -109.5     ]]]\n[[7.66642558e-05]]\n[[292876]]\n[[[  41.666668 -115.333336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00011195]]\n[[400403]]\n[[[  45.666668 -113.958336]]]\n[[0.00033222]]\n[[369791]]\n[[[  44.458332 -113.      ]]]\nfinished\n",
  "history_begin_time" : 1677782902314,
  "history_end_time" : 1677783440871,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "1dy27r2lx67",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n[[0.00038069]]\n[[78266]]\n[[[  33.666668 -109.291664]]]\n[[0.00016855]]\n[[202538]]\n[[[  38.5      -106.333336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00012986]]\n[[373281]]\n[[[  44.583332 -107.208336]]]\n[[3.87134852e-05]]\n[[362136]]\n[[[  44.166668 -107.125   ]]]\n[[0.00015419]]\n[[330682]]\n[[[  43.   -109.75]]]\n[[0.00023674]]\n[[319102]]\n[[[  42.583332 -108.833336]]]\n[[8.83810871e-05]]\n[[267286]]\n[[[  40.791668 -110.875   ]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00034885]]\n[[116759]]\n[[[  35.25 -108.25]]]\n[[0.00034215]]\n[[107190]]\n[[[  34.875 -111.625]]]\n[[0.0001517]]\n[[104042]]\n[[[  34.75     -111.416664]]]\n[[0.00013821]]\n[[216303]]\n[[[  39.   -106.75]]]\n[[0.00016356]]\n[[174581]]\n[[[  37.458332 -108.458336]]]\n[[0.00033874]]\n[[457740]]\n[[[  48.416668 -113.916664]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00035391]]\n[[177932]]\n[[[  37.583332 -107.25    ]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00012476]]\n[[269738]]\n[[[  40.875    -110.541664]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00039874]]\n[[459718]]\n[[[  48.541668 -120.75    ]]]\n[[0.00028356]]\n[[162526]]\n[[[  37.       -106.291664]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00014601]]\n[[183432]]\n[[[  37.791668 -108.166664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00019668]]\n[[266072]]\n[[[  40.75  -110.625]]]\n[[0.00034733]]\n[[255062]]\n[[[  40.375 -116.875]]]\n[[9.15488263e-05]]\n[[283188]]\n[[[  41.333332 -106.375   ]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022445]]\n[[185613]]\n[[[  37.875    -109.458336]]]\n[[0.00025986]]\n[[240815]]\n[[[  39.875 -111.25 ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00012938]]\n[[252783]]\n[[[  40.291668 -111.25    ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00020894]]\n[[263633]]\n[[[  40.666668 -110.958336]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00026128]]\n[[234882]]\n[[[  39.666668 -110.416664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0003611]]\n[[269634]]\n[[[  40.875    -115.208336]]]\n[[0.00030221]]\n[[272081]]\n[[[  40.958332 -115.083336]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00020455]]\n[[139488]]\n[[[  36.125    -105.541664]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00012719]]\n[[426421]]\n[[[  46.791668 -121.75    ]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00036997]]\n[[84911]]\n[[[  33.958332 -109.5     ]]]\n[[7.66642558e-05]]\n[[292876]]\n[[[  41.666668 -115.333336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00011195]]\n[[400403]]\n[[[  45.666668 -113.958336]]]\n[[0.00033222]]\n[[369791]]\n[[[  44.458332 -113.      ]]]\nfinished\n",
  "history_begin_time" : 1677781745986,
  "history_end_time" : 1677782294543,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "i9xi3pxbhtb",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n",
  "history_begin_time" : 1677719047525,
  "history_end_time" : 1677719052811,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "ue3g8k07etq",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n",
  "history_begin_time" : 1677718179249,
  "history_end_time" : 1677718184554,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "jsnycdzvsku",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n",
  "history_begin_time" : 1677679556428,
  "history_end_time" : 1677679561207,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "l7o3acsemfc",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n[[0.00038069]]\n[[78266]]\n[[[  33.666668 -109.291664]]]\n[[0.00016855]]\n[[202538]]\n[[[  38.5      -106.333336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00012986]]\n[[373281]]\n[[[  44.583332 -107.208336]]]\n[[3.87134852e-05]]\n[[362136]]\n[[[  44.166668 -107.125   ]]]\n[[0.00015419]]\n[[330682]]\n[[[  43.   -109.75]]]\n[[0.00023674]]\n[[319102]]\n[[[  42.583332 -108.833336]]]\n[[8.83810871e-05]]\n[[267286]]\n[[[  40.791668 -110.875   ]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00034885]]\n[[116759]]\n[[[  35.25 -108.25]]]\n[[0.00034215]]\n[[107190]]\n[[[  34.875 -111.625]]]\n[[0.0001517]]\n[[104042]]\n[[[  34.75     -111.416664]]]\n[[0.00013821]]\n[[216303]]\n[[[  39.   -106.75]]]\n[[0.00016356]]\n[[174581]]\n[[[  37.458332 -108.458336]]]\n[[0.00033874]]\n[[457740]]\n[[[  48.416668 -113.916664]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00035391]]\n[[177932]]\n[[[  37.583332 -107.25    ]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00012476]]\n[[269738]]\n[[[  40.875    -110.541664]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00039874]]\n[[459718]]\n[[[  48.541668 -120.75    ]]]\n[[0.00028356]]\n[[162526]]\n[[[  37.       -106.291664]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00014601]]\n[[183432]]\n[[[  37.791668 -108.166664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00019668]]\n[[266072]]\n[[[  40.75  -110.625]]]\n[[0.00034733]]\n[[255062]]\n[[[  40.375 -116.875]]]\n[[9.15488263e-05]]\n[[283188]]\n[[[  41.333332 -106.375   ]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022445]]\n[[185613]]\n[[[  37.875    -109.458336]]]\n[[0.00025986]]\n[[240815]]\n[[[  39.875 -111.25 ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00012938]]\n[[252783]]\n[[[  40.291668 -111.25    ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00020894]]\n[[263633]]\n[[[  40.666668 -110.958336]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00026128]]\n[[234882]]\n[[[  39.666668 -110.416664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0003611]]\n[[269634]]\n[[[  40.875    -115.208336]]]\n[[0.00030221]]\n[[272081]]\n[[[  40.958332 -115.083336]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00020455]]\n[[139488]]\n[[[  36.125    -105.541664]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00012719]]\n[[426421]]\n[[[  46.791668 -121.75    ]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00036997]]\n[[84911]]\n[[[  33.958332 -109.5     ]]]\n[[7.66642558e-05]]\n[[292876]]\n[[[  41.666668 -115.333336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00011195]]\n[[400403]]\n[[[  45.666668 -113.958336]]]\n[[0.00033222]]\n[[369791]]\n[[[  44.458332 -113.      ]]]\nfinished\n",
  "history_begin_time" : 1677679508080,
  "history_end_time" : 1677680015819,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "w8dj5rzrf46",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n",
  "history_begin_time" : 1677636287715,
  "history_end_time" : 1677636292401,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "c7pat0nqbq2",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n",
  "history_begin_time" : 1677636151167,
  "history_end_time" : 1677636155902,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "i2or13g7pjw",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "Usage: /usr/bin/python3.6-config --prefix|--exec-prefix|--includes|--libs|--cflags|--ldflags|--extension-suffix|--help|--abiflags|--configdir\n",
  "history_begin_time" : 1677636137627,
  "history_end_time" : 1677636142814,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "8qklLTES6n2s",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "sh: /usr/lib/python3.6: Is a directory\n",
  "history_begin_time" : 1677636118930,
  "history_end_time" : 1677636120167,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "le5xitqhj50",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n[[0.00038069]]\n[[78266]]\n[[[  33.666668 -109.291664]]]\n[[0.00016855]]\n[[202538]]\n[[[  38.5      -106.333336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00012986]]\n[[373281]]\n[[[  44.583332 -107.208336]]]\n[[3.87134852e-05]]\n[[362136]]\n[[[  44.166668 -107.125   ]]]\n[[0.00015419]]\n[[330682]]\n[[[  43.   -109.75]]]\n[[0.00023674]]\n[[319102]]\n[[[  42.583332 -108.833336]]]\n[[8.83810871e-05]]\n[[267286]]\n[[[  40.791668 -110.875   ]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00034885]]\n[[116759]]\n[[[  35.25 -108.25]]]\n[[0.00034215]]\n[[107190]]\n[[[  34.875 -111.625]]]\n[[0.0001517]]\n[[104042]]\n[[[  34.75     -111.416664]]]\n[[0.00013821]]\n[[216303]]\n[[[  39.   -106.75]]]\n[[0.00016356]]\n[[174581]]\n[[[  37.458332 -108.458336]]]\n[[0.00033874]]\n[[457740]]\n[[[  48.416668 -113.916664]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00035391]]\n[[177932]]\n[[[  37.583332 -107.25    ]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00012476]]\n[[269738]]\n[[[  40.875    -110.541664]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00039874]]\n[[459718]]\n[[[  48.541668 -120.75    ]]]\n[[0.00028356]]\n[[162526]]\n[[[  37.       -106.291664]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00014601]]\n[[183432]]\n[[[  37.791668 -108.166664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00019668]]\n[[266072]]\n[[[  40.75  -110.625]]]\n[[0.00034733]]\n[[255062]]\n[[[  40.375 -116.875]]]\n[[9.15488263e-05]]\n[[283188]]\n[[[  41.333332 -106.375   ]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022445]]\n[[185613]]\n[[[  37.875    -109.458336]]]\n[[0.00025986]]\n[[240815]]\n[[[  39.875 -111.25 ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00012938]]\n[[252783]]\n[[[  40.291668 -111.25    ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00020894]]\n[[263633]]\n[[[  40.666668 -110.958336]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00026128]]\n[[234882]]\n[[[  39.666668 -110.416664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0003611]]\n[[269634]]\n[[[  40.875    -115.208336]]]\n[[0.00030221]]\n[[272081]]\n[[[  40.958332 -115.083336]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00020455]]\n[[139488]]\n[[[  36.125    -105.541664]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00012719]]\n[[426421]]\n[[[  46.791668 -121.75    ]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00036997]]\n[[84911]]\n[[[  33.958332 -109.5     ]]]\n[[7.66642558e-05]]\n[[292876]]\n[[[  41.666668 -115.333336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00011195]]\n[[400403]]\n[[[  45.666668 -113.958336]]]\n[[0.00033222]]\n[[369791]]\n[[[  44.458332 -113.      ]]]\nfinished\n",
  "history_begin_time" : 1677636064680,
  "history_end_time" : 1677636866588,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "e1yri4tan1n",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "",
  "history_begin_time" : 1677635882553,
  "history_end_time" : 1677635885301,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "o4t2b45rshi",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n[[0.00038069]]\n[[78266]]\n[[[  33.666668 -109.291664]]]\n[[0.00016855]]\n[[202538]]\n[[[  38.5      -106.333336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00012986]]\n[[373281]]\n[[[  44.583332 -107.208336]]]\n[[3.87134852e-05]]\n[[362136]]\n[[[  44.166668 -107.125   ]]]\n[[0.00015419]]\n[[330682]]\n[[[  43.   -109.75]]]\n[[0.00023674]]\n[[319102]]\n[[[  42.583332 -108.833336]]]\n[[8.83810871e-05]]\n[[267286]]\n[[[  40.791668 -110.875   ]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00034885]]\n[[116759]]\n[[[  35.25 -108.25]]]\n[[0.00034215]]\n[[107190]]\n[[[  34.875 -111.625]]]\n[[0.0001517]]\n[[104042]]\n[[[  34.75     -111.416664]]]\n[[0.00013821]]\n[[216303]]\n[[[  39.   -106.75]]]\n[[0.00016356]]\n[[174581]]\n[[[  37.458332 -108.458336]]]\n[[0.00033874]]\n[[457740]]\n[[[  48.416668 -113.916664]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00035391]]\n[[177932]]\n[[[  37.583332 -107.25    ]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00012476]]\n[[269738]]\n[[[  40.875    -110.541664]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00039874]]\n[[459718]]\n[[[  48.541668 -120.75    ]]]\n[[0.00028356]]\n[[162526]]\n[[[  37.       -106.291664]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00014601]]\n[[183432]]\n[[[  37.791668 -108.166664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00019668]]\n[[266072]]\n[[[  40.75  -110.625]]]\n[[0.00034733]]\n[[255062]]\n[[[  40.375 -116.875]]]\n[[9.15488263e-05]]\n[[283188]]\n[[[  41.333332 -106.375   ]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022445]]\n[[185613]]\n[[[  37.875    -109.458336]]]\n[[0.00025986]]\n[[240815]]\n[[[  39.875 -111.25 ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00012938]]\n[[252783]]\n[[[  40.291668 -111.25    ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00020894]]\n[[263633]]\n[[[  40.666668 -110.958336]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00026128]]\n[[234882]]\n[[[  39.666668 -110.416664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0003611]]\n[[269634]]\n[[[  40.875    -115.208336]]]\n[[0.00030221]]\n[[272081]]\n[[[  40.958332 -115.083336]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00020455]]\n[[139488]]\n[[[  36.125    -105.541664]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00012719]]\n[[426421]]\n[[[  46.791668 -121.75    ]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00036997]]\n[[84911]]\n[[[  33.958332 -109.5     ]]]\n[[7.66642558e-05]]\n[[292876]]\n[[[  41.666668 -115.333336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00011195]]\n[[400403]]\n[[[  45.666668 -113.958336]]]\n[[0.00033222]]\n[[369791]]\n[[[  44.458332 -113.      ]]]\nfinished\n",
  "history_begin_time" : 1677617763585,
  "history_end_time" : 1677618274073,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "omrrwxvlh9c",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n",
  "history_begin_time" : 1677606171857,
  "history_end_time" : 1677606176193,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "3f4ccl242ny",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n[[0.00038069]]\n[[78266]]\n[[[  33.666668 -109.291664]]]\n[[0.00016855]]\n[[202538]]\n[[[  38.5      -106.333336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00012986]]\n[[373281]]\n[[[  44.583332 -107.208336]]]\n[[3.87134852e-05]]\n[[362136]]\n[[[  44.166668 -107.125   ]]]\n[[0.00015419]]\n[[330682]]\n[[[  43.   -109.75]]]\n[[0.00023674]]\n[[319102]]\n[[[  42.583332 -108.833336]]]\n[[8.83810871e-05]]\n[[267286]]\n[[[  40.791668 -110.875   ]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00034885]]\n[[116759]]\n[[[  35.25 -108.25]]]\n[[0.00034215]]\n[[107190]]\n[[[  34.875 -111.625]]]\n[[0.0001517]]\n[[104042]]\n[[[  34.75     -111.416664]]]\n[[0.00013821]]\n[[216303]]\n[[[  39.   -106.75]]]\n[[0.00016356]]\n[[174581]]\n[[[  37.458332 -108.458336]]]\n[[0.00033874]]\n[[457740]]\n[[[  48.416668 -113.916664]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00035391]]\n[[177932]]\n[[[  37.583332 -107.25    ]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00012476]]\n[[269738]]\n[[[  40.875    -110.541664]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00039874]]\n[[459718]]\n[[[  48.541668 -120.75    ]]]\n[[0.00028356]]\n[[162526]]\n[[[  37.       -106.291664]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00014601]]\n[[183432]]\n[[[  37.791668 -108.166664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00019668]]\n[[266072]]\n[[[  40.75  -110.625]]]\n[[0.00034733]]\n[[255062]]\n[[[  40.375 -116.875]]]\n[[9.15488263e-05]]\n[[283188]]\n[[[  41.333332 -106.375   ]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022445]]\n[[185613]]\n[[[  37.875    -109.458336]]]\n[[0.00025986]]\n[[240815]]\n[[[  39.875 -111.25 ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00012938]]\n[[252783]]\n[[[  40.291668 -111.25    ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00020894]]\n[[263633]]\n[[[  40.666668 -110.958336]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00026128]]\n[[234882]]\n[[[  39.666668 -110.416664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0003611]]\n[[269634]]\n[[[  40.875    -115.208336]]]\n[[0.00030221]]\n[[272081]]\n[[[  40.958332 -115.083336]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00020455]]\n[[139488]]\n[[[  36.125    -105.541664]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00012719]]\n[[426421]]\n[[[  46.791668 -121.75    ]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00036997]]\n[[84911]]\n[[[  33.958332 -109.5     ]]]\n[[7.66642558e-05]]\n[[292876]]\n[[[  41.666668 -115.333336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00011195]]\n[[400403]]\n[[[  45.666668 -113.958336]]]\n[[0.00033222]]\n[[369791]]\n[[[  44.458332 -113.      ]]]\nfinished\n",
  "history_begin_time" : 1677606114377,
  "history_end_time" : 1677606644310,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "y17y3qd9ime",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \n#turn_nsidc_nc_to_csv()",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/gokulprathin/gw-workspace/y17y3qd9ime/data_nsidc_4km_swe.py\", line 24, in <module>\n    import netCDF4 as nc\nModuleNotFoundError: No module named 'netCDF4'\n",
  "history_begin_time" : 1677582849539,
  "history_end_time" : 1677582851335,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "16v76s3rgqb",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n",
  "history_begin_time" : 1677525427069,
  "history_end_time" : 1677525438353,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "fm7zvu9607f",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n",
  "history_begin_time" : 1677462328154,
  "history_end_time" : 1694185608920,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "moe8sfpqky1",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n",
  "history_begin_time" : 1677462314459,
  "history_end_time" : 1694185608703,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "fbxjrb61qmy",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n",
  "history_begin_time" : 1677462314459,
  "history_end_time" : 1694185608646,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "aqe9m0hqydv",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n",
  "history_begin_time" : 1677428743416,
  "history_end_time" : 1677428748154,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "d9td5s9km8d",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n[[0.00038069]]\n[[78266]]\n[[[  33.666668 -109.291664]]]\n[[0.00016855]]\n[[202538]]\n[[[  38.5      -106.333336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00012986]]\n[[373281]]\n[[[  44.583332 -107.208336]]]\n[[3.87134852e-05]]\n[[362136]]\n[[[  44.166668 -107.125   ]]]\n[[0.00015419]]\n[[330682]]\n[[[  43.   -109.75]]]\n[[0.00023674]]\n[[319102]]\n[[[  42.583332 -108.833336]]]\n[[8.83810871e-05]]\n[[267286]]\n[[[  40.791668 -110.875   ]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00034885]]\n[[116759]]\n[[[  35.25 -108.25]]]\n[[0.00034215]]\n[[107190]]\n[[[  34.875 -111.625]]]\n[[0.0001517]]\n[[104042]]\n[[[  34.75     -111.416664]]]\n[[0.00013821]]\n[[216303]]\n[[[  39.   -106.75]]]\n[[0.00016356]]\n[[174581]]\n[[[  37.458332 -108.458336]]]\n[[0.00033874]]\n[[457740]]\n[[[  48.416668 -113.916664]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00035391]]\n[[177932]]\n[[[  37.583332 -107.25    ]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00012476]]\n[[269738]]\n[[[  40.875    -110.541664]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00039874]]\n[[459718]]\n[[[  48.541668 -120.75    ]]]\n[[0.00028356]]\n[[162526]]\n[[[  37.       -106.291664]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00014601]]\n[[183432]]\n[[[  37.791668 -108.166664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00019668]]\n[[266072]]\n[[[  40.75  -110.625]]]\n[[0.00034733]]\n[[255062]]\n[[[  40.375 -116.875]]]\n[[9.15488263e-05]]\n[[283188]]\n[[[  41.333332 -106.375   ]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022445]]\n[[185613]]\n[[[  37.875    -109.458336]]]\n[[0.00025986]]\n[[240815]]\n[[[  39.875 -111.25 ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00012938]]\n[[252783]]\n[[[  40.291668 -111.25    ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00020894]]\n[[263633]]\n[[[  40.666668 -110.958336]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00026128]]\n[[234882]]\n[[[  39.666668 -110.416664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0003611]]\n[[269634]]\n[[[  40.875    -115.208336]]]\n[[0.00030221]]\n[[272081]]\n[[[  40.958332 -115.083336]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00020455]]\n[[139488]]\n[[[  36.125    -105.541664]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00012719]]\n[[426421]]\n[[[  46.791668 -121.75    ]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00036997]]\n[[84911]]\n[[[  33.958332 -109.5     ]]]\n[[7.66642558e-05]]\n[[292876]]\n[[[  41.666668 -115.333336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00011195]]\n[[400403]]\n[[[  45.666668 -113.958336]]]\n[[0.00033222]]\n[[369791]]\n[[[  44.458332 -113.      ]]]\nfinished\n",
  "history_begin_time" : 1677428687605,
  "history_end_time" : 1677429200147,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "sk8djo6z45b",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n",
  "history_begin_time" : 1677426263346,
  "history_end_time" : 1677426271468,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "4v9j29rcbrr",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n",
  "history_begin_time" : 1677379891639,
  "history_end_time" : 1677379890252,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Running"
},{
  "history_id" : "5ntqt4d8lm8",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n",
  "history_begin_time" : 1677379839467,
  "history_end_time" : 1677379837938,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Running"
},{
  "history_id" : "2z413m4mbgm",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n",
  "history_begin_time" : 1677352479020,
  "history_end_time" : 1677352483416,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "zypkbej32yq",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n[[0.00038069]]\n[[78266]]\n[[[  33.666668 -109.291664]]]\n[[0.00016855]]\n[[202538]]\n[[[  38.5      -106.333336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00012986]]\n[[373281]]\n[[[  44.583332 -107.208336]]]\n[[3.87134852e-05]]\n[[362136]]\n[[[  44.166668 -107.125   ]]]\n[[0.00015419]]\n[[330682]]\n[[[  43.   -109.75]]]\n[[0.00023674]]\n[[319102]]\n[[[  42.583332 -108.833336]]]\n[[8.83810871e-05]]\n[[267286]]\n[[[  40.791668 -110.875   ]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00034885]]\n[[116759]]\n[[[  35.25 -108.25]]]\n[[0.00034215]]\n[[107190]]\n[[[  34.875 -111.625]]]\n[[0.0001517]]\n[[104042]]\n[[[  34.75     -111.416664]]]\n[[0.00013821]]\n[[216303]]\n[[[  39.   -106.75]]]\n[[0.00016356]]\n[[174581]]\n[[[  37.458332 -108.458336]]]\n[[0.00033874]]\n[[457740]]\n[[[  48.416668 -113.916664]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00035391]]\n[[177932]]\n[[[  37.583332 -107.25    ]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00012476]]\n[[269738]]\n[[[  40.875    -110.541664]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00039874]]\n[[459718]]\n[[[  48.541668 -120.75    ]]]\n[[0.00028356]]\n[[162526]]\n[[[  37.       -106.291664]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00014601]]\n[[183432]]\n[[[  37.791668 -108.166664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00019668]]\n[[266072]]\n[[[  40.75  -110.625]]]\n[[0.00034733]]\n[[255062]]\n[[[  40.375 -116.875]]]\n[[9.15488263e-05]]\n[[283188]]\n[[[  41.333332 -106.375   ]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022445]]\n[[185613]]\n[[[  37.875    -109.458336]]]\n[[0.00025986]]\n[[240815]]\n[[[  39.875 -111.25 ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00012938]]\n[[252783]]\n[[[  40.291668 -111.25    ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00020894]]\n[[263633]]\n[[[  40.666668 -110.958336]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00026128]]\n[[234882]]\n[[[  39.666668 -110.416664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0003611]]\n[[269634]]\n[[[  40.875    -115.208336]]]\n[[0.00030221]]\n[[272081]]\n[[[  40.958332 -115.083336]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00020455]]\n[[139488]]\n[[[  36.125    -105.541664]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00012719]]\n[[426421]]\n[[[  46.791668 -121.75    ]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00036997]]\n[[84911]]\n[[[  33.958332 -109.5     ]]]\n[[7.66642558e-05]]\n[[292876]]\n[[[  41.666668 -115.333336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00011195]]\n[[400403]]\n[[[  45.666668 -113.958336]]]\n[[0.00033222]]\n[[369791]]\n[[[  44.458332 -113.      ]]]\nfinished\n",
  "history_begin_time" : 1677352391500,
  "history_end_time" : 1677356964559,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "whkjb1hjykk",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n[[0.00038069]]\n[[78266]]\n[[[  33.666668 -109.291664]]]\n[[0.00016855]]\n[[202538]]\n[[[  38.5      -106.333336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00012986]]\n[[373281]]\n[[[  44.583332 -107.208336]]]\n[[3.87134852e-05]]\n[[362136]]\n[[[  44.166668 -107.125   ]]]\n[[0.00015419]]\n[[330682]]\n[[[  43.   -109.75]]]\n[[0.00023674]]\n[[319102]]\n[[[  42.583332 -108.833336]]]\n[[8.83810871e-05]]\n[[267286]]\n[[[  40.791668 -110.875   ]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00034885]]\n[[116759]]\n[[[  35.25 -108.25]]]\n[[0.00034215]]\n[[107190]]\n[[[  34.875 -111.625]]]\n[[0.0001517]]\n[[104042]]\n[[[  34.75     -111.416664]]]\n[[0.00013821]]\n[[216303]]\n[[[  39.   -106.75]]]\n[[0.00016356]]\n[[174581]]\n[[[  37.458332 -108.458336]]]\n[[0.00033874]]\n[[457740]]\n[[[  48.416668 -113.916664]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00035391]]\n[[177932]]\n[[[  37.583332 -107.25    ]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00012476]]\n[[269738]]\n[[[  40.875    -110.541664]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00039874]]\n[[459718]]\n[[[  48.541668 -120.75    ]]]\n[[0.00028356]]\n[[162526]]\n[[[  37.       -106.291664]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00014601]]\n[[183432]]\n[[[  37.791668 -108.166664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00019668]]\n[[266072]]\n[[[  40.75  -110.625]]]\n[[0.00034733]]\n[[255062]]\n[[[  40.375 -116.875]]]\n[[9.15488263e-05]]\n[[283188]]\n[[[  41.333332 -106.375   ]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022445]]\n[[185613]]\n[[[  37.875    -109.458336]]]\n[[0.00025986]]\n[[240815]]\n[[[  39.875 -111.25 ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00012938]]\n[[252783]]\n[[[  40.291668 -111.25    ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00020894]]\n[[263633]]\n[[[  40.666668 -110.958336]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00026128]]\n[[234882]]\n[[[  39.666668 -110.416664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0003611]]\n[[269634]]\n[[[  40.875    -115.208336]]]\n[[0.00030221]]\n[[272081]]\n[[[  40.958332 -115.083336]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00020455]]\n[[139488]]\n[[[  36.125    -105.541664]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00012719]]\n[[426421]]\n[[[  46.791668 -121.75    ]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00036997]]\n[[84911]]\n[[[  33.958332 -109.5     ]]]\n[[7.66642558e-05]]\n[[292876]]\n[[[  41.666668 -115.333336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00011195]]\n[[400403]]\n[[[  45.666668 -113.958336]]]\n[[0.00033222]]\n[[369791]]\n[[[  44.458332 -113.      ]]]\nfinished\n",
  "history_begin_time" : 1677352335934,
  "history_end_time" : 1677352864181,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "76zop15mc37",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n[[0.00038069]]\n[[78266]]\n[[[  33.666668 -109.291664]]]\n[[0.00016855]]\n[[202538]]\n[[[  38.5      -106.333336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00012986]]\n[[373281]]\n[[[  44.583332 -107.208336]]]\n[[3.87134852e-05]]\n[[362136]]\n[[[  44.166668 -107.125   ]]]\n[[0.00015419]]\n[[330682]]\n[[[  43.   -109.75]]]\n[[0.00023674]]\n[[319102]]\n[[[  42.583332 -108.833336]]]\n[[8.83810871e-05]]\n[[267286]]\n[[[  40.791668 -110.875   ]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00034885]]\n[[116759]]\n[[[  35.25 -108.25]]]\n[[0.00034215]]\n[[107190]]\n[[[  34.875 -111.625]]]\n[[0.0001517]]\n[[104042]]\n[[[  34.75     -111.416664]]]\n[[0.00013821]]\n[[216303]]\n[[[  39.   -106.75]]]\n[[0.00016356]]\n[[174581]]\n[[[  37.458332 -108.458336]]]\n[[0.00033874]]\n[[457740]]\n[[[  48.416668 -113.916664]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00035391]]\n[[177932]]\n[[[  37.583332 -107.25    ]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00012476]]\n[[269738]]\n[[[  40.875    -110.541664]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00039874]]\n[[459718]]\n[[[  48.541668 -120.75    ]]]\n[[0.00028356]]\n[[162526]]\n[[[  37.       -106.291664]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00014601]]\n[[183432]]\n[[[  37.791668 -108.166664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00019668]]\n[[266072]]\n[[[  40.75  -110.625]]]\n[[0.00034733]]\n[[255062]]\n[[[  40.375 -116.875]]]\n[[9.15488263e-05]]\n[[283188]]\n[[[  41.333332 -106.375   ]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022445]]\n[[185613]]\n[[[  37.875    -109.458336]]]\n[[0.00025986]]\n[[240815]]\n[[[  39.875 -111.25 ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00012938]]\n[[252783]]\n[[[  40.291668 -111.25    ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00020894]]\n[[263633]]\n[[[  40.666668 -110.958336]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00026128]]\n[[234882]]\n[[[  39.666668 -110.416664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0003611]]\n[[269634]]\n[[[  40.875    -115.208336]]]\n[[0.00030221]]\n[[272081]]\n[[[  40.958332 -115.083336]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00020455]]\n[[139488]]\n[[[  36.125    -105.541664]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00012719]]\n[[426421]]\n[[[  46.791668 -121.75    ]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00036997]]\n[[84911]]\n[[[  33.958332 -109.5     ]]]\n[[7.66642558e-05]]\n[[292876]]\n[[[  41.666668 -115.333336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00011195]]\n[[400403]]\n[[[  45.666668 -113.958336]]]\n[[0.00033222]]\n[[369791]]\n[[[  44.458332 -113.      ]]]\nfinished\n",
  "history_begin_time" : 1677344120505,
  "history_end_time" : 1677344640007,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "zd25cqam3fv",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n",
  "history_begin_time" : 1677282604019,
  "history_end_time" : 1677282611105,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "7m4ksoewi01",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n[[0.00038069]]\n[[78266]]\n[[[  33.666668 -109.291664]]]\n[[0.00016855]]\n[[202538]]\n[[[  38.5      -106.333336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00012986]]\n[[373281]]\n[[[  44.583332 -107.208336]]]\n[[3.87134852e-05]]\n[[362136]]\n[[[  44.166668 -107.125   ]]]\n[[0.00015419]]\n[[330682]]\n[[[  43.   -109.75]]]\n[[0.00023674]]\n[[319102]]\n[[[  42.583332 -108.833336]]]\n[[8.83810871e-05]]\n[[267286]]\n[[[  40.791668 -110.875   ]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00034885]]\n[[116759]]\n[[[  35.25 -108.25]]]\n[[0.00034215]]\n[[107190]]\n[[[  34.875 -111.625]]]\n[[0.0001517]]\n[[104042]]\n[[[  34.75     -111.416664]]]\n[[0.00013821]]\n[[216303]]\n[[[  39.   -106.75]]]\n[[0.00016356]]\n[[174581]]\n[[[  37.458332 -108.458336]]]\n[[0.00033874]]\n[[457740]]\n[[[  48.416668 -113.916664]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00035391]]\n[[177932]]\n[[[  37.583332 -107.25    ]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00012476]]\n[[269738]]\n[[[  40.875    -110.541664]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00039874]]\n[[459718]]\n[[[  48.541668 -120.75    ]]]\n[[0.00028356]]\n[[162526]]\n[[[  37.       -106.291664]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00014601]]\n[[183432]]\n[[[  37.791668 -108.166664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00019668]]\n[[266072]]\n[[[  40.75  -110.625]]]\n[[0.00034733]]\n[[255062]]\n[[[  40.375 -116.875]]]\n[[9.15488263e-05]]\n[[283188]]\n[[[  41.333332 -106.375   ]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022445]]\n[[185613]]\n[[[  37.875    -109.458336]]]\n[[0.00025986]]\n[[240815]]\n[[[  39.875 -111.25 ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00012938]]\n[[252783]]\n[[[  40.291668 -111.25    ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00020894]]\n[[263633]]\n[[[  40.666668 -110.958336]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00026128]]\n[[234882]]\n[[[  39.666668 -110.416664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0003611]]\n[[269634]]\n[[[  40.875    -115.208336]]]\n[[0.00030221]]\n[[272081]]\n[[[  40.958332 -115.083336]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00020455]]\n[[139488]]\n[[[  36.125    -105.541664]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00012719]]\n[[426421]]\n[[[  46.791668 -121.75    ]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00036997]]\n[[84911]]\n[[[  33.958332 -109.5     ]]]\n[[7.66642558e-05]]\n[[292876]]\n[[[  41.666668 -115.333336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00011195]]\n[[400403]]\n[[[  45.666668 -113.958336]]]\n[[0.00033222]]\n[[369791]]\n[[[  44.458332 -113.      ]]]\nfinished\n",
  "history_begin_time" : 1677273712384,
  "history_end_time" : 1677274236757,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "8vbewsc9b73",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "sh: /usr/include/python3.6m: Is a directory\n",
  "history_begin_time" : 1677273699654,
  "history_end_time" : 1677273703950,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "rlgrc3ycde7",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "Usage: /usr/bin/python3.6-config --prefix|--exec-prefix|--includes|--libs|--cflags|--ldflags|--extension-suffix|--help|--abiflags|--configdir\n",
  "history_begin_time" : 1677273674853,
  "history_end_time" : 1677273679536,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "a01mskw03gx",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "sh: /usr/local/lib/python3.6: Is a directory\n",
  "history_begin_time" : 1677273658143,
  "history_end_time" : 1677273665454,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "y4dtqk3zh8d",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n",
  "history_begin_time" : 1677273536690,
  "history_end_time" : 1677273543896,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "yndi2oiwiq4",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "sh: /usr/include/python3.6: Is a directory\n",
  "history_begin_time" : 1677273519989,
  "history_end_time" : 1677273525488,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "98pr0hp0t6k",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "sh: /usr/local/lib/python3.6: Is a directory\n",
  "history_begin_time" : 1677273371777,
  "history_end_time" : 1677273374364,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Failed"
},{
  "history_id" : "fnj7suq0rky",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "sh: /usr/lib/python3.6: Is a directory\n",
  "history_begin_time" : 1677273340949,
  "history_end_time" : 1677273345444,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "c9quuv0tq0a",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "sh: /usr/lib/python3.6: Is a directory\n",
  "history_begin_time" : 1677273324101,
  "history_end_time" : 1677273332234,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "jbamqrhumjc",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n[[0.00038069]]\n[[78266]]\n[[[  33.666668 -109.291664]]]\n[[0.00016855]]\n[[202538]]\n[[[  38.5      -106.333336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00012986]]\n[[373281]]\n[[[  44.583332 -107.208336]]]\n[[3.87134852e-05]]\n[[362136]]\n[[[  44.166668 -107.125   ]]]\n[[0.00015419]]\n[[330682]]\n[[[  43.   -109.75]]]\n[[0.00023674]]\n[[319102]]\n[[[  42.583332 -108.833336]]]\n[[8.83810871e-05]]\n[[267286]]\n[[[  40.791668 -110.875   ]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00034885]]\n[[116759]]\n[[[  35.25 -108.25]]]\n[[0.00034215]]\n[[107190]]\n[[[  34.875 -111.625]]]\n[[0.0001517]]\n[[104042]]\n[[[  34.75     -111.416664]]]\n[[0.00013821]]\n[[216303]]\n[[[  39.   -106.75]]]\n[[0.00016356]]\n[[174581]]\n[[[  37.458332 -108.458336]]]\n[[0.00033874]]\n[[457740]]\n[[[  48.416668 -113.916664]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00035391]]\n[[177932]]\n[[[  37.583332 -107.25    ]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00012476]]\n[[269738]]\n[[[  40.875    -110.541664]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00039874]]\n[[459718]]\n[[[  48.541668 -120.75    ]]]\n[[0.00028356]]\n[[162526]]\n[[[  37.       -106.291664]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00014601]]\n[[183432]]\n[[[  37.791668 -108.166664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00019668]]\n[[266072]]\n[[[  40.75  -110.625]]]\n[[0.00034733]]\n[[255062]]\n[[[  40.375 -116.875]]]\n[[9.15488263e-05]]\n[[283188]]\n[[[  41.333332 -106.375   ]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022445]]\n[[185613]]\n[[[  37.875    -109.458336]]]\n[[0.00025986]]\n[[240815]]\n[[[  39.875 -111.25 ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00012938]]\n[[252783]]\n[[[  40.291668 -111.25    ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00020894]]\n[[263633]]\n[[[  40.666668 -110.958336]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00026128]]\n[[234882]]\n[[[  39.666668 -110.416664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0003611]]\n[[269634]]\n[[[  40.875    -115.208336]]]\n[[0.00030221]]\n[[272081]]\n[[[  40.958332 -115.083336]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00020455]]\n[[139488]]\n[[[  36.125    -105.541664]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00012719]]\n[[426421]]\n[[[  46.791668 -121.75    ]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00036997]]\n[[84911]]\n[[[  33.958332 -109.5     ]]]\n[[7.66642558e-05]]\n[[292876]]\n[[[  41.666668 -115.333336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00011195]]\n[[400403]]\n[[[  45.666668 -113.958336]]]\n[[0.00033222]]\n[[369791]]\n[[[  44.458332 -113.      ]]]\nfinished\n",
  "history_begin_time" : 1677273146712,
  "history_end_time" : 1677273999456,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "bqzefixr7ux",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n",
  "history_begin_time" : 1677273100322,
  "history_end_time" : 1677273134493,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "mddwoia360x",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "",
  "history_begin_time" : 1677201276856,
  "history_end_time" : 1677201283691,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "yyl87uuctr6",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n",
  "history_begin_time" : 1677192311338,
  "history_end_time" : 1677192316084,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "4zaz41fxe95",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n",
  "history_begin_time" : 1677192268838,
  "history_end_time" : 1677192273760,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "b0hhki6sr3t",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n",
  "history_begin_time" : 1677191917492,
  "history_end_time" : 1677191922320,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "o4RXTAP2wskk",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n",
  "history_begin_time" : 1677191877373,
  "history_end_time" : 1677191946487,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "akETxLGW7okD",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n<class 'netCDF4._netCDF4.Dataset'>\nroot group (NETCDF4 data model, file format HDF5):\n    dimensions(sizes): lat(621), lon(1405), time(365), time_str_len(11)\n    variables(dimensions): |S1 crs(), float32 lat(lat), float32 lon(lon), float32 time(time), |S1 time_str(time_str_len, time), int16 SWE(time, lat, lon), int16 DEPTH(time, lat, lon)\n    groups: \n<class 'netCDF4._netCDF4.Dimension'>: name = 'lat', size = 621\n<class 'netCDF4._netCDF4.Dimension'>: name = 'lon', size = 1405\n<class 'netCDF4._netCDF4.Dimension'>: name = 'time', size = 365\n<class 'netCDF4._netCDF4.Dimension'>: name = 'time_str_len', size = 11\n<class 'netCDF4._netCDF4.Variable'>\n|S1 crs()\n    grid_mapping_name: latitude_longitude\n    long_name: CRS definition\n    longitude_of_prime_meridian: 0.0\n    semi_major_axis: 6378137.0\n    inverse_flattening: 298.257222101\n    spatial_ref: GEOGCS[\"NAD83\",DATUM[\"North_American_Datum_1983\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6269\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.01745329251994328,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4269\"]]\n    GeoTransform: -125.0208 0.04166662697178698 0 49.9375 0 -0.04166662697178698 \nunlimited dimensions: \ncurrent shape = ()\nfilling on, default _FillValue of \u0000 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 lat(lat)\n    long_name: latitude\n    units: degrees north\nunlimited dimensions: \ncurrent shape = (621,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 lon(lon)\n    long_name: longitude\n    units: degrees east\nunlimited dimensions: \ncurrent shape = (1405,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 time(time)\n    long_name: time\n    units: days since 1900-01-01\nunlimited dimensions: \ncurrent shape = (365,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\n|S1 time_str(time_str_len, time)\n    long_name: time (string)\n    format: dd-mmm-yyyy\nunlimited dimensions: \ncurrent shape = (11, 365)\nfilling on, default _FillValue of \u0000 used\n<class 'netCDF4._netCDF4.Variable'>\nint16 SWE(time, lat, lon)\n    _FillValue: -999\n    long_name: Snow Water Equivalent\n    grid_mapping: crs\n    units: millimeters h20\nunlimited dimensions: \ncurrent shape = (365, 621, 1405)\nfilling on\n<class 'netCDF4._netCDF4.Variable'>\nint16 DEPTH(time, lat, lon)\n    _FillValue: -999\n    long_name: Snow Depth\n    grid_mapping: crs\n    units: millimeters snow thickness\nunlimited dimensions: \ncurrent shape = (365, 621, 1405)\nfilling on\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n",
  "history_begin_time" : 1677191651050,
  "history_end_time" : 1677191877386,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "C2cHP33ZqkNq",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\n#print(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\n#print(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    #print(dim)\nfor var in nsidc_data_ds.variables.values():\n    #print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    #print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    #print(dist)\n    #print(ind)\n    #print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "  File \"/home/chetana/gw-workspace/C2cHP33ZqkNq/data_nsidc_4km_swe.py\", line 51\n    for var in nsidc_data_ds.variables.values():\n    ^\nIndentationError: expected an indented block\n",
  "history_begin_time" : 1677191605570,
  "history_end_time" : 1677191606977,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "JGqOtuI5CyMb",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n<class 'netCDF4._netCDF4.Dataset'>\nroot group (NETCDF4 data model, file format HDF5):\n    dimensions(sizes): lat(621), lon(1405), time(365), time_str_len(11)\n    variables(dimensions): |S1 crs(), float32 lat(lat), float32 lon(lon), float32 time(time), |S1 time_str(time_str_len, time), int16 SWE(time, lat, lon), int16 DEPTH(time, lat, lon)\n    groups: \n<class 'netCDF4._netCDF4.Dimension'>: name = 'lat', size = 621\n<class 'netCDF4._netCDF4.Dimension'>: name = 'lon', size = 1405\n<class 'netCDF4._netCDF4.Dimension'>: name = 'time', size = 365\n<class 'netCDF4._netCDF4.Dimension'>: name = 'time_str_len', size = 11\n<class 'netCDF4._netCDF4.Variable'>\n|S1 crs()\n    grid_mapping_name: latitude_longitude\n    long_name: CRS definition\n    longitude_of_prime_meridian: 0.0\n    semi_major_axis: 6378137.0\n    inverse_flattening: 298.257222101\n    spatial_ref: GEOGCS[\"NAD83\",DATUM[\"North_American_Datum_1983\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6269\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.01745329251994328,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4269\"]]\n    GeoTransform: -125.0208 0.04166662697178698 0 49.9375 0 -0.04166662697178698 \nunlimited dimensions: \ncurrent shape = ()\nfilling on, default _FillValue of \u0000 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 lat(lat)\n    long_name: latitude\n    units: degrees north\nunlimited dimensions: \ncurrent shape = (621,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 lon(lon)\n    long_name: longitude\n    units: degrees east\nunlimited dimensions: \ncurrent shape = (1405,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 time(time)\n    long_name: time\n    units: days since 1900-01-01\nunlimited dimensions: \ncurrent shape = (365,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\n|S1 time_str(time_str_len, time)\n    long_name: time (string)\n    format: dd-mmm-yyyy\nunlimited dimensions: \ncurrent shape = (11, 365)\nfilling on, default _FillValue of \u0000 used\n<class 'netCDF4._netCDF4.Variable'>\nint16 SWE(time, lat, lon)\n    _FillValue: -999\n    long_name: Snow Water Equivalent\n    grid_mapping: crs\n    units: millimeters h20\nunlimited dimensions: \ncurrent shape = (365, 621, 1405)\nfilling on\n<class 'netCDF4._netCDF4.Variable'>\nint16 DEPTH(time, lat, lon)\n    _FillValue: -999\n    long_name: Snow Depth\n    grid_mapping: crs\n    units: millimeters snow thickness\nunlimited dimensions: \ncurrent shape = (365, 621, 1405)\nfilling on\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n",
  "history_begin_time" : 1677191527779,
  "history_end_time" : 1677191605587,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "0pV5YN52guQO",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n<class 'netCDF4._netCDF4.Dataset'>\nroot group (NETCDF4 data model, file format HDF5):\n    dimensions(sizes): lat(621), lon(1405), time(365), time_str_len(11)\n    variables(dimensions): |S1 crs(), float32 lat(lat), float32 lon(lon), float32 time(time), |S1 time_str(time_str_len, time), int16 SWE(time, lat, lon), int16 DEPTH(time, lat, lon)\n    groups: \n<class 'netCDF4._netCDF4.Dimension'>: name = 'lat', size = 621\n<class 'netCDF4._netCDF4.Dimension'>: name = 'lon', size = 1405\n<class 'netCDF4._netCDF4.Dimension'>: name = 'time', size = 365\n<class 'netCDF4._netCDF4.Dimension'>: name = 'time_str_len', size = 11\n<class 'netCDF4._netCDF4.Variable'>\n|S1 crs()\n    grid_mapping_name: latitude_longitude\n    long_name: CRS definition\n    longitude_of_prime_meridian: 0.0\n    semi_major_axis: 6378137.0\n    inverse_flattening: 298.257222101\n    spatial_ref: GEOGCS[\"NAD83\",DATUM[\"North_American_Datum_1983\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6269\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.01745329251994328,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4269\"]]\n    GeoTransform: -125.0208 0.04166662697178698 0 49.9375 0 -0.04166662697178698 \nunlimited dimensions: \ncurrent shape = ()\nfilling on, default _FillValue of \u0000 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 lat(lat)\n    long_name: latitude\n    units: degrees north\nunlimited dimensions: \ncurrent shape = (621,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 lon(lon)\n    long_name: longitude\n    units: degrees east\nunlimited dimensions: \ncurrent shape = (1405,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 time(time)\n    long_name: time\n    units: days since 1900-01-01\nunlimited dimensions: \ncurrent shape = (365,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\n|S1 time_str(time_str_len, time)\n    long_name: time (string)\n    format: dd-mmm-yyyy\nunlimited dimensions: \ncurrent shape = (11, 365)\nfilling on, default _FillValue of \u0000 used\n<class 'netCDF4._netCDF4.Variable'>\nint16 SWE(time, lat, lon)\n    _FillValue: -999\n    long_name: Snow Water Equivalent\n    grid_mapping: crs\n    units: millimeters h20\nunlimited dimensions: \ncurrent shape = (365, 621, 1405)\nfilling on\n<class 'netCDF4._netCDF4.Variable'>\nint16 DEPTH(time, lat, lon)\n    _FillValue: -999\n    long_name: Snow Depth\n    grid_mapping: crs\n    units: millimeters snow thickness\nunlimited dimensions: \ncurrent shape = (365, 621, 1405)\nfilling on\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/0pV5YN52guQO/data_nsidc_4km_swe.py\", line 187, in <module>\n    turn_nsidc_nc_to_csv()\n  File \"/home/chetana/gw-workspace/0pV5YN52guQO/data_nsidc_4km_swe.py\", line 161, in turn_nsidc_nc_to_csv\n    lat_val, lon_val = find_nearest_2(latitude, longitude)\n  File \"/home/chetana/gw-workspace/0pV5YN52guQO/data_nsidc_4km_swe.py\", line 127, in find_nearest_2\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\nNameError: name 'lat_lon_pairs_rad' is not defined\n",
  "history_begin_time" : 1677191323670,
  "history_end_time" : 1677191326941,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "3WoW2wj1eMtx",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n<class 'netCDF4._netCDF4.Dataset'>\nroot group (NETCDF4 data model, file format HDF5):\n    dimensions(sizes): lat(621), lon(1405), time(365), time_str_len(11)\n    variables(dimensions): |S1 crs(), float32 lat(lat), float32 lon(lon), float32 time(time), |S1 time_str(time_str_len, time), int16 SWE(time, lat, lon), int16 DEPTH(time, lat, lon)\n    groups: \n<class 'netCDF4._netCDF4.Dimension'>: name = 'lat', size = 621\n<class 'netCDF4._netCDF4.Dimension'>: name = 'lon', size = 1405\n<class 'netCDF4._netCDF4.Dimension'>: name = 'time', size = 365\n<class 'netCDF4._netCDF4.Dimension'>: name = 'time_str_len', size = 11\n<class 'netCDF4._netCDF4.Variable'>\n|S1 crs()\n    grid_mapping_name: latitude_longitude\n    long_name: CRS definition\n    longitude_of_prime_meridian: 0.0\n    semi_major_axis: 6378137.0\n    inverse_flattening: 298.257222101\n    spatial_ref: GEOGCS[\"NAD83\",DATUM[\"North_American_Datum_1983\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6269\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.01745329251994328,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4269\"]]\n    GeoTransform: -125.0208 0.04166662697178698 0 49.9375 0 -0.04166662697178698 \nunlimited dimensions: \ncurrent shape = ()\nfilling on, default _FillValue of \u0000 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 lat(lat)\n    long_name: latitude\n    units: degrees north\nunlimited dimensions: \ncurrent shape = (621,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 lon(lon)\n    long_name: longitude\n    units: degrees east\nunlimited dimensions: \ncurrent shape = (1405,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 time(time)\n    long_name: time\n    units: days since 1900-01-01\nunlimited dimensions: \ncurrent shape = (365,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\n|S1 time_str(time_str_len, time)\n    long_name: time (string)\n    format: dd-mmm-yyyy\nunlimited dimensions: \ncurrent shape = (11, 365)\nfilling on, default _FillValue of \u0000 used\n<class 'netCDF4._netCDF4.Variable'>\nint16 SWE(time, lat, lon)\n    _FillValue: -999\n    long_name: Snow Water Equivalent\n    grid_mapping: crs\n    units: millimeters h20\nunlimited dimensions: \ncurrent shape = (365, 621, 1405)\nfilling on\n<class 'netCDF4._netCDF4.Variable'>\nint16 DEPTH(time, lat, lon)\n    _FillValue: -999\n    long_name: Snow Depth\n    grid_mapping: crs\n    units: millimeters snow thickness\nunlimited dimensions: \ncurrent shape = (365, 621, 1405)\nfilling on\nfile doesn't exist, generating new\n[[ 25.166666 -81.125   ]\n [ 25.166666 -81.083336]\n [ 25.166666 -81.041664]\n ...\n [ 49.333332 -94.958336]\n [ 49.333332 -94.916664]\n [ 49.333332 -94.875   ]]\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/3WoW2wj1eMtx/data_nsidc_4km_swe.py\", line 187, in <module>\n    turn_nsidc_nc_to_csv()\n  File \"/home/chetana/gw-workspace/3WoW2wj1eMtx/data_nsidc_4km_swe.py\", line 161, in turn_nsidc_nc_to_csv\n    lat_val, lon_val = find_nearest_2(latitude, longitude)\n  File \"/home/chetana/gw-workspace/3WoW2wj1eMtx/data_nsidc_4km_swe.py\", line 127, in find_nearest_2\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\nNameError: name 'lat_lon_pairs_rad' is not defined\n",
  "history_begin_time" : 1677189476109,
  "history_end_time" : 1677189704214,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "fgf4ibb1xqr",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/fgf4ibb1xqr/data_nsidc_4km_swe.py\", line 46, in <module>\n    nsidc_data_ds = nc.Dataset(nsidc_data_file)\n  File \"src/netCDF4/_netCDF4.pyx\", line 2307, in netCDF4._netCDF4.Dataset.__init__\n  File \"src/netCDF4/_netCDF4.pyx\", line 1925, in netCDF4._netCDF4._ensure_nc_success\nFileNotFoundError: [Errno 2] No such file or directory: b'/home/chetana/Documents/data/4km_SWE_Depth_WY2019_v01.nc'\n",
  "history_begin_time" : 1677184297120,
  "history_end_time" : 1677184300943,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Failed"
},{
  "history_id" : "ucswnzwnje4",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/ucswnzwnje4/data_nsidc_4km_swe.py\", line 46, in <module>\n    nsidc_data_ds = nc.Dataset(nsidc_data_file)\n  File \"src/netCDF4/_netCDF4.pyx\", line 2307, in netCDF4._netCDF4.Dataset.__init__\n  File \"src/netCDF4/_netCDF4.pyx\", line 1925, in netCDF4._netCDF4._ensure_nc_success\nFileNotFoundError: [Errno 2] No such file or directory: b'/home/chetana/Documents/data/4km_SWE_Depth_WY2019_v01.nc'\n",
  "history_begin_time" : 1677184173866,
  "history_end_time" : 1677184192555,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Failed"
},{
  "history_id" : "h9RHidto6ll9",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/h9RHidto6ll9/data_nsidc_4km_swe.py\", line 46, in <module>\n    nsidc_data_ds = nc.Dataset(nsidc_data_file)\n  File \"src/netCDF4/_netCDF4.pyx\", line 2307, in netCDF4._netCDF4.Dataset.__init__\n  File \"src/netCDF4/_netCDF4.pyx\", line 1925, in netCDF4._netCDF4._ensure_nc_success\nFileNotFoundError: [Errno 2] No such file or directory: b'/home/chetana/Documents/data/4km_SWE_Depth_WY2019_v01.nc'\n",
  "history_begin_time" : 1677114397073,
  "history_end_time" : 1677114399317,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "fsktu3ucF4NS",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/fsktu3ucF4NS/data_nsidc_4km_swe.py\", line 46, in <module>\n    nsidc_data_ds = nc.Dataset(nsidc_data_file)\n  File \"src/netCDF4/_netCDF4.pyx\", line 2307, in netCDF4._netCDF4.Dataset.__init__\n  File \"src/netCDF4/_netCDF4.pyx\", line 1925, in netCDF4._netCDF4._ensure_nc_success\nFileNotFoundError: [Errno 2] No such file or directory: b'/home/chetana/Documents/data/4km_SWE_Depth_WY2019_v01.nc'\n",
  "history_begin_time" : 1677113516581,
  "history_end_time" : 1677113518786,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "xtbk8ve10qa",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/xtbk8ve10qa/data_nsidc_4km_swe.py\", line 46, in <module>\n    nsidc_data_ds = nc.Dataset(nsidc_data_file)\n  File \"src/netCDF4/_netCDF4.pyx\", line 2307, in netCDF4._netCDF4.Dataset.__init__\n  File \"src/netCDF4/_netCDF4.pyx\", line 1925, in netCDF4._netCDF4._ensure_nc_success\nFileNotFoundError: [Errno 2] No such file or directory: b'/home/chetana/Documents/data/4km_SWE_Depth_WY2019_v01.nc'\n",
  "history_begin_time" : 1677113477118,
  "history_end_time" : 1677113481389,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Failed"
},{
  "history_id" : "v2vWUbLQNz8l",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\nTraceback (most recent call last):\n  File \"data_nsidc_4km_swe.py\", line 39, in <module>\n    station_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 685, in parser_f\n    return _read(filepath_or_buffer, kwds)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 457, in _read\n    parser = TextFileReader(fp_or_buf, **kwds)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 895, in __init__\n    self._make_engine(self.engine)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 1135, in _make_engine\n    self._engine = CParserWrapper(self.f, **self.options)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 1917, in __init__\n    self._reader = parsers.TextReader(src, **kwds)\n  File \"pandas/_libs/parsers.pyx\", line 382, in pandas._libs.parsers.TextReader.__cinit__\n  File \"pandas/_libs/parsers.pyx\", line 689, in pandas._libs.parsers.TextReader._setup_parser_source\nFileNotFoundError: [Errno 2] File b'/home/chetana/Documents/GitHub/SnowCast/data/ready_for_training/station_cell_mapping.csv' does not exist: b'/home/chetana/Documents/GitHub/SnowCast/data/ready_for_training/station_cell_mapping.csv'\n",
  "history_begin_time" : 1677108260001,
  "history_end_time" : 1677108262239,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "se6yr0t37ib",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "Usage: /usr/bin/python3.6m-config --prefix|--exec-prefix|--includes|--libs|--cflags|--ldflags|--extension-suffix|--help|--abiflags|--configdir\n",
  "history_begin_time" : 1677108239361,
  "history_end_time" : 1677108241918,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Failed"
},{
  "history_id" : "j25mzx5fi7h",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\nTraceback (most recent call last):\n  File \"data_nsidc_4km_swe.py\", line 39, in <module>\n    station_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 685, in parser_f\n    return _read(filepath_or_buffer, kwds)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 457, in _read\n    parser = TextFileReader(fp_or_buf, **kwds)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 895, in __init__\n    self._make_engine(self.engine)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 1135, in _make_engine\n    self._engine = CParserWrapper(self.f, **self.options)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 1917, in __init__\n    self._reader = parsers.TextReader(src, **kwds)\n  File \"pandas/_libs/parsers.pyx\", line 382, in pandas._libs.parsers.TextReader.__cinit__\n  File \"pandas/_libs/parsers.pyx\", line 689, in pandas._libs.parsers.TextReader._setup_parser_source\nFileNotFoundError: [Errno 2] File b'/home/chetana/Documents/GitHub/SnowCast/data/ready_for_training/station_cell_mapping.csv' does not exist: b'/home/chetana/Documents/GitHub/SnowCast/data/ready_for_training/station_cell_mapping.csv'\n",
  "history_begin_time" : 1677108212733,
  "history_end_time" : 1677108229740,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "k9j9z2nez9c",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\nTraceback (most recent call last):\n  File \"data_nsidc_4km_swe.py\", line 39, in <module>\n    station_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 685, in parser_f\n    return _read(filepath_or_buffer, kwds)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 457, in _read\n    parser = TextFileReader(fp_or_buf, **kwds)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 895, in __init__\n    self._make_engine(self.engine)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 1135, in _make_engine\n    self._engine = CParserWrapper(self.f, **self.options)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 1917, in __init__\n    self._reader = parsers.TextReader(src, **kwds)\n  File \"pandas/_libs/parsers.pyx\", line 382, in pandas._libs.parsers.TextReader.__cinit__\n  File \"pandas/_libs/parsers.pyx\", line 689, in pandas._libs.parsers.TextReader._setup_parser_source\nFileNotFoundError: [Errno 2] File b'/home/chetana/Documents/GitHub/SnowCast/data/ready_for_training/station_cell_mapping.csv' does not exist: b'/home/chetana/Documents/GitHub/SnowCast/data/ready_for_training/station_cell_mapping.csv'\n",
  "history_begin_time" : 1677107870636,
  "history_end_time" : 1677107874188,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Failed"
},{
  "history_id" : "4ex1kg2rblh",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\nTraceback (most recent call last):\n  File \"data_nsidc_4km_swe.py\", line 39, in <module>\n    station_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 685, in parser_f\n    return _read(filepath_or_buffer, kwds)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 457, in _read\n    parser = TextFileReader(fp_or_buf, **kwds)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 895, in __init__\n    self._make_engine(self.engine)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 1135, in _make_engine\n    self._engine = CParserWrapper(self.f, **self.options)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 1917, in __init__\n    self._reader = parsers.TextReader(src, **kwds)\n  File \"pandas/_libs/parsers.pyx\", line 382, in pandas._libs.parsers.TextReader.__cinit__\n  File \"pandas/_libs/parsers.pyx\", line 689, in pandas._libs.parsers.TextReader._setup_parser_source\nFileNotFoundError: [Errno 2] File b'/home/chetana/Documents/GitHub/SnowCast/data/ready_for_training/station_cell_mapping.csv' does not exist: b'/home/chetana/Documents/GitHub/SnowCast/data/ready_for_training/station_cell_mapping.csv'\n",
  "history_begin_time" : 1677107758324,
  "history_end_time" : 1677107761968,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Failed"
},{
  "history_id" : "3ur916fkgl1",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "sh: /usr/local/lib/python3.6: Is a directory\n",
  "history_begin_time" : 1677107718853,
  "history_end_time" : 1677107721451,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Failed"
},{
  "history_id" : "h5krz4jyl5q",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "Usage: /usr/bin/python3.6-config --prefix|--exec-prefix|--includes|--libs|--cflags|--ldflags|--extension-suffix|--help|--abiflags|--configdir\n",
  "history_begin_time" : 1677107637628,
  "history_end_time" : 1677107705677,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "ufv2sex5gsa",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "  File \"data_nsidc_4km_swe.py\", line 35\n    github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n                                                      ^\nSyntaxError: invalid syntax\n",
  "history_begin_time" : 1677107604123,
  "history_end_time" : 1677107608764,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "tt6ce75gig1",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "sh: /usr/lib/python3.7: Is a directory\n",
  "history_begin_time" : 1677107557250,
  "history_end_time" : 1677107562722,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "76bvmnlklfz",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "sh: /usr/local/lib/python3.6: Is a directory\n",
  "history_begin_time" : 1677107501311,
  "history_end_time" : 1677107503886,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Failed"
},{
  "history_id" : "n7td1hmjq3h",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "sh: /usr/lib/python3.6: Is a directory\n",
  "history_begin_time" : 1677107474989,
  "history_end_time" : 1677107477505,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Failed"
},{
  "history_id" : "oteh9ejuy3b",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\nTraceback (most recent call last):\n  File \"data_nsidc_4km_swe.py\", line 39, in <module>\n    station_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 685, in parser_f\n    return _read(filepath_or_buffer, kwds)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 457, in _read\n    parser = TextFileReader(fp_or_buf, **kwds)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 895, in __init__\n    self._make_engine(self.engine)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 1135, in _make_engine\n    self._engine = CParserWrapper(self.f, **self.options)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 1917, in __init__\n    self._reader = parsers.TextReader(src, **kwds)\n  File \"pandas/_libs/parsers.pyx\", line 382, in pandas._libs.parsers.TextReader.__cinit__\n  File \"pandas/_libs/parsers.pyx\", line 689, in pandas._libs.parsers.TextReader._setup_parser_source\nFileNotFoundError: [Errno 2] File b'/home/chetana/Documents/GitHub/SnowCast/data/ready_for_training/station_cell_mapping.csv' does not exist: b'/home/chetana/Documents/GitHub/SnowCast/data/ready_for_training/station_cell_mapping.csv'\n",
  "history_begin_time" : 1677106516624,
  "history_end_time" : 1677106520209,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "ikr5103b8a6",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "",
  "history_begin_time" : 1677106477674,
  "history_end_time" : 1677106481037,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "gg8u0qvb4lj",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "",
  "history_begin_time" : 1677106431833,
  "history_end_time" : 1677106435001,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "ph2gv2a43ib",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\nTraceback (most recent call last):\n  File \"data_nsidc_4km_swe.py\", line 39, in <module>\n    station_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 685, in parser_f\n    return _read(filepath_or_buffer, kwds)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 457, in _read\n    parser = TextFileReader(fp_or_buf, **kwds)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 895, in __init__\n    self._make_engine(self.engine)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 1135, in _make_engine\n    self._engine = CParserWrapper(self.f, **self.options)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 1917, in __init__\n    self._reader = parsers.TextReader(src, **kwds)\n  File \"pandas/_libs/parsers.pyx\", line 382, in pandas._libs.parsers.TextReader.__cinit__\n  File \"pandas/_libs/parsers.pyx\", line 689, in pandas._libs.parsers.TextReader._setup_parser_source\nFileNotFoundError: [Errno 2] File b'/home/chetana/Documents/GitHub/SnowCast/data/ready_for_training/station_cell_mapping.csv' does not exist: b'/home/chetana/Documents/GitHub/SnowCast/data/ready_for_training/station_cell_mapping.csv'\n",
  "history_begin_time" : 1677106135795,
  "history_end_time" : 1677106147567,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "rc5v62so7d3",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "",
  "history_begin_time" : 1677106011390,
  "history_end_time" : 1677106017561,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "ra84wuq32h4",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "",
  "history_begin_time" : 1677030844287,
  "history_end_time" : 1677030849538,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "o89h02ch1j3",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\nTraceback (most recent call last):\n  File \"data_nsidc_4km_swe.py\", line 39, in <module>\n    station_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 685, in parser_f\n    return _read(filepath_or_buffer, kwds)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 457, in _read\n    parser = TextFileReader(fp_or_buf, **kwds)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 895, in __init__\n    self._make_engine(self.engine)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 1135, in _make_engine\n    self._engine = CParserWrapper(self.f, **self.options)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 1917, in __init__\n    self._reader = parsers.TextReader(src, **kwds)\n  File \"pandas/_libs/parsers.pyx\", line 382, in pandas._libs.parsers.TextReader.__cinit__\n  File \"pandas/_libs/parsers.pyx\", line 689, in pandas._libs.parsers.TextReader._setup_parser_source\nFileNotFoundError: [Errno 2] File b'/home/chetana/Documents/GitHub/SnowCast/data/ready_for_training/station_cell_mapping.csv' does not exist: b'/home/chetana/Documents/GitHub/SnowCast/data/ready_for_training/station_cell_mapping.csv'\n",
  "history_begin_time" : 1677030771590,
  "history_end_time" : 1677030775209,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Failed"
},{
  "history_id" : "ki2nsv5gth5",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "",
  "history_begin_time" : 1677030672965,
  "history_end_time" : 1677030675859,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "owa6sqquxxl",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "",
  "history_begin_time" : 1677030564898,
  "history_end_time" : 1677030567678,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "s5gp37d4r3k",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\nTraceback (most recent call last):\n  File \"data_nsidc_4km_swe.py\", line 39, in <module>\n    station_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 685, in parser_f\n    return _read(filepath_or_buffer, kwds)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 457, in _read\n    parser = TextFileReader(fp_or_buf, **kwds)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 895, in __init__\n    self._make_engine(self.engine)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 1135, in _make_engine\n    self._engine = CParserWrapper(self.f, **self.options)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 1917, in __init__\n    self._reader = parsers.TextReader(src, **kwds)\n  File \"pandas/_libs/parsers.pyx\", line 382, in pandas._libs.parsers.TextReader.__cinit__\n  File \"pandas/_libs/parsers.pyx\", line 689, in pandas._libs.parsers.TextReader._setup_parser_source\nFileNotFoundError: [Errno 2] File b'/home/chetana/Documents/GitHub/SnowCast/data/ready_for_training/station_cell_mapping.csv' does not exist: b'/home/chetana/Documents/GitHub/SnowCast/data/ready_for_training/station_cell_mapping.csv'\n",
  "history_begin_time" : 1677025529702,
  "history_end_time" : 1677025533151,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Failed"
},{
  "history_id" : "b2ortcon25r",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "",
  "history_begin_time" : 1677025468643,
  "history_end_time" : 1677025471182,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "t6xizch46i9",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "Traceback (most recent call last):\n  File \"data_nsidc_4km_swe.py\", line 26, in <module>\n    import geojson\nModuleNotFoundError: No module named 'geojson'\n",
  "history_begin_time" : 1677017827215,
  "history_end_time" : 1677017830281,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "nicpz1cl5kp",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "Traceback (most recent call last):\n  File \"data_nsidc_4km_swe.py\", line 26, in <module>\n    import geojson\nModuleNotFoundError: No module named 'geojson'\n",
  "history_begin_time" : 1677017218555,
  "history_end_time" : 1677017221616,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "7rfnraypbx9",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "Traceback (most recent call last):\n  File \"data_nsidc_4km_swe.py\", line 26, in <module>\n    import geojson\nModuleNotFoundError: No module named 'geojson'\n",
  "history_begin_time" : 1677016681973,
  "history_end_time" : 1677016685100,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "qkg26sucmbf",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "Traceback (most recent call last):\n  File \"data_nsidc_4km_swe.py\", line 26, in <module>\n    import geojson\nModuleNotFoundError: No module named 'geojson'\n",
  "history_begin_time" : 1677016143326,
  "history_end_time" : 1677016146548,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Failed"
},{
  "history_id" : "2xhnmgll2r2",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "Traceback (most recent call last):\n  File \"data_nsidc_4km_swe.py\", line 26, in <module>\n    import geojson\nModuleNotFoundError: No module named 'geojson'\n",
  "history_begin_time" : 1677016064073,
  "history_end_time" : 1677016067514,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Failed"
},{
  "history_id" : "rex6m8ciilr",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "  File \"data_nsidc_4km_swe.py\", line 35\n    github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n                                                      ^\nSyntaxError: invalid syntax\n",
  "history_begin_time" : 1677015790087,
  "history_end_time" : 1677015791264,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "9jf82rf2nft",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "  File \"data_nsidc_4km_swe.py\", line 35\n    github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n                                                      ^\nSyntaxError: invalid syntax\n",
  "history_begin_time" : 1677015740953,
  "history_end_time" : 1677015743607,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "41d57i34mpz",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "  File \"data_nsidc_4km_swe.py\", line 35\n    github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n                                                      ^\nSyntaxError: invalid syntax\n",
  "history_begin_time" : 1677014886690,
  "history_end_time" : 1677014887938,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "wlgmj28f42m",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "  File \"data_nsidc_4km_swe.py\", line 35\n    github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n                                                      ^\nSyntaxError: invalid syntax\n",
  "history_begin_time" : 1677014797833,
  "history_end_time" : 1677014799145,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "LYcvy8jNmWnj",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "  File \"data_nsidc_4km_swe.py\", line 35\n    github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n                                                      ^\nSyntaxError: invalid syntax\n",
  "history_begin_time" : 1677014744639,
  "history_end_time" : 1677014745909,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "kqnatulz9hc",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "  File \"data_nsidc_4km_swe.py\", line 35\n    github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n                                                      ^\nSyntaxError: invalid syntax\n",
  "history_begin_time" : 1677014229756,
  "history_end_time" : 1677014231159,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "unrzrv4bzwy",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "  File \"data_nsidc_4km_swe.py\", line 35\n    github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n                                                      ^\nSyntaxError: invalid syntax\n",
  "history_begin_time" : 1677013835610,
  "history_end_time" : 1677013836877,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "hp3crw8umrp",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "  File \"data_nsidc_4km_swe.py\", line 35\n    github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n                                                      ^\nSyntaxError: invalid syntax\n",
  "history_begin_time" : 1677011875107,
  "history_end_time" : 1677011876407,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "hgzg5bpktua",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "  File \"data_nsidc_4km_swe.py\", line 35\n    github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n                                                      ^\nSyntaxError: invalid syntax\n",
  "history_begin_time" : 1677008200756,
  "history_end_time" : 1677008201974,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "e6r54cxgnr5",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "  File \"data_nsidc_4km_swe.py\", line 35\n    github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n                                                      ^\nSyntaxError: invalid syntax\n",
  "history_begin_time" : 1677008166766,
  "history_end_time" : 1677008168017,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "8d08pvdvzfp",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "  File \"data_nsidc_4km_swe.py\", line 35\n    github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n                                                      ^\nSyntaxError: invalid syntax\n",
  "history_begin_time" : 1677002001733,
  "history_end_time" : 1677002003039,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "8gznse4cpfy",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\bharg\\gw-workspace\\8gznse4cpfy\\data_nsidc_4km_swe.py\", line 24, in <module>\n    import netCDF4 as nc\nModuleNotFoundError: No module named 'netCDF4'\n",
  "history_begin_time" : 1677001732300,
  "history_end_time" : 1677001733205,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "0mn2nhqa818",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\bharg\\gw-workspace\\0mn2nhqa818\\data_nsidc_4km_swe.py\", line 24, in <module>\n    import netCDF4 as nc\nModuleNotFoundError: No module named 'netCDF4'\n",
  "history_begin_time" : 1677001594661,
  "history_end_time" : 1677001595417,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "cwgypfthwae",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\bharg\\gw-workspace\\cwgypfthwae\\data_nsidc_4km_swe.py\", line 24, in <module>\n    import netCDF4 as nc\nModuleNotFoundError: No module named 'netCDF4'\n",
  "history_begin_time" : 1677000537412,
  "history_end_time" : 1677000538199,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "q5fxxgq3sxn",
  "history_input" : null,
  "history_output" : "Exhausted available authentication methods",
  "history_begin_time" : 1676999722771,
  "history_end_time" : 1676999727043,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "yb51al",
  "indicator" : "Failed"
},{
  "history_id" : "e1ttq12hkj2",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\bharg\\gw-workspace\\e1ttq12hkj2\\data_nsidc_4km_swe.py\", line 24, in <module>\n    import netCDF4 as nc\nModuleNotFoundError: No module named 'netCDF4'\n",
  "history_begin_time" : 1676999599340,
  "history_end_time" : 1676999602673,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "dj42ukva60v",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\bharg\\gw-workspace\\dj42ukva60v\\data_nsidc_4km_swe.py\", line 24, in <module>\n    import netCDF4 as nc\nModuleNotFoundError: No module named 'netCDF4'\n",
  "history_begin_time" : 1676862212774,
  "history_end_time" : 1676862216515,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "yh4drfxun03",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\bharg\\gw-workspace\\yh4drfxun03\\data_nsidc_4km_swe.py\", line 24, in <module>\n    import netCDF4 as nc\nModuleNotFoundError: No module named 'netCDF4'\n",
  "history_begin_time" : 1676329536756,
  "history_end_time" : 1676329537682,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "85nkk2oxws1",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\bharg\\gw-workspace\\85nkk2oxws1\\data_nsidc_4km_swe.py\", line 24, in <module>\n    import netCDF4 as nc\nModuleNotFoundError: No module named 'netCDF4'\n",
  "history_begin_time" : 1676329492365,
  "history_end_time" : 1676329496367,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "upca9ngfgxi",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \n#turn_nsidc_nc_to_csv()",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/gokulprathin/gw-workspace/upca9ngfgxi/data_nsidc_4km_swe.py\", line 24, in <module>\n    import netCDF4 as nc\nModuleNotFoundError: No module named 'netCDF4'\n",
  "history_begin_time" : 1676063614076,
  "history_end_time" : 1676063616832,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "hG0IT9p3yIST",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \n#turn_nsidc_nc_to_csv()",
  "history_output" : "/Users/joe\n<class 'netCDF4._netCDF4.Dataset'>\nroot group (NETCDF4 data model, file format HDF5):\n    dimensions(sizes): lat(621), lon(1405), time(365), time_str_len(11)\n    variables(dimensions): |S1 crs(), float32 lat(lat), float32 lon(lon), float32 time(time), |S1 time_str(time_str_len, time), int16 SWE(time, lat, lon), int16 DEPTH(time, lat, lon)\n    groups: \n<class 'netCDF4._netCDF4.Dimension'>: name = 'lat', size = 621\n<class 'netCDF4._netCDF4.Dimension'>: name = 'lon', size = 1405\n<class 'netCDF4._netCDF4.Dimension'>: name = 'time', size = 365\n<class 'netCDF4._netCDF4.Dimension'>: name = 'time_str_len', size = 11\n<class 'netCDF4._netCDF4.Variable'>\n|S1 crs()\n    grid_mapping_name: latitude_longitude\n    long_name: CRS definition\n    longitude_of_prime_meridian: 0.0\n    semi_major_axis: 6378137.0\n    inverse_flattening: 298.257222101\n    spatial_ref: GEOGCS[\"NAD83\",DATUM[\"North_American_Datum_1983\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6269\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.01745329251994328,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4269\"]]\n    GeoTransform: -125.0208 0.04166662697178698 0 49.9375 0 -0.04166662697178698 \nunlimited dimensions: \ncurrent shape = ()\nfilling on, default _FillValue of \u0000 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 lat(lat)\n    long_name: latitude\n    units: degrees north\nunlimited dimensions: \ncurrent shape = (621,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 lon(lon)\n    long_name: longitude\n    units: degrees east\nunlimited dimensions: \ncurrent shape = (1405,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 time(time)\n    long_name: time\n    units: days since 1900-01-01\nunlimited dimensions: \ncurrent shape = (365,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\n|S1 time_str(time_str_len, time)\n    long_name: time (string)\n    format: dd-mmm-yyyy\nunlimited dimensions: \ncurrent shape = (11, 365)\nfilling on, default _FillValue of \u0000 used\n<class 'netCDF4._netCDF4.Variable'>\nint16 SWE(time, lat, lon)\n    _FillValue: -999\n    long_name: Snow Water Equivalent\n    grid_mapping: crs\n    units: millimeters h20\nunlimited dimensions: \ncurrent shape = (365, 621, 1405)\nfilling on\n<class 'netCDF4._netCDF4.Variable'>\nint16 DEPTH(time, lat, lon)\n    _FillValue: -999\n    long_name: Snow Depth\n    grid_mapping: crs\n    units: millimeters snow thickness\nunlimited dimensions: \ncurrent shape = (365, 621, 1405)\nfilling on\n",
  "history_begin_time" : 1675814265778,
  "history_end_time" : 1675814267174,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "wtGK6u8Lu9MX",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \n#turn_nsidc_nc_to_csv()",
  "history_output" : "/Users/joe\n<class 'netCDF4._netCDF4.Dataset'>\nroot group (NETCDF4 data model, file format HDF5):\n    dimensions(sizes): lat(621), lon(1405), time(365), time_str_len(11)\n    variables(dimensions): |S1 crs(), float32 lat(lat), float32 lon(lon), float32 time(time), |S1 time_str(time_str_len, time), int16 SWE(time, lat, lon), int16 DEPTH(time, lat, lon)\n    groups: \n<class 'netCDF4._netCDF4.Dimension'>: name = 'lat', size = 621\n<class 'netCDF4._netCDF4.Dimension'>: name = 'lon', size = 1405\n<class 'netCDF4._netCDF4.Dimension'>: name = 'time', size = 365\n<class 'netCDF4._netCDF4.Dimension'>: name = 'time_str_len', size = 11\n<class 'netCDF4._netCDF4.Variable'>\n|S1 crs()\n    grid_mapping_name: latitude_longitude\n    long_name: CRS definition\n    longitude_of_prime_meridian: 0.0\n    semi_major_axis: 6378137.0\n    inverse_flattening: 298.257222101\n    spatial_ref: GEOGCS[\"NAD83\",DATUM[\"North_American_Datum_1983\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6269\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.01745329251994328,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4269\"]]\n    GeoTransform: -125.0208 0.04166662697178698 0 49.9375 0 -0.04166662697178698 \nunlimited dimensions: \ncurrent shape = ()\nfilling on, default _FillValue of \u0000 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 lat(lat)\n    long_name: latitude\n    units: degrees north\nunlimited dimensions: \ncurrent shape = (621,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 lon(lon)\n    long_name: longitude\n    units: degrees east\nunlimited dimensions: \ncurrent shape = (1405,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 time(time)\n    long_name: time\n    units: days since 1900-01-01\nunlimited dimensions: \ncurrent shape = (365,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\n|S1 time_str(time_str_len, time)\n    long_name: time (string)\n    format: dd-mmm-yyyy\nunlimited dimensions: \ncurrent shape = (11, 365)\nfilling on, default _FillValue of \u0000 used\n<class 'netCDF4._netCDF4.Variable'>\nint16 SWE(time, lat, lon)\n    _FillValue: -999\n    long_name: Snow Water Equivalent\n    grid_mapping: crs\n    units: millimeters h20\nunlimited dimensions: \ncurrent shape = (365, 621, 1405)\nfilling on\n<class 'netCDF4._netCDF4.Variable'>\nint16 DEPTH(time, lat, lon)\n    _FillValue: -999\n    long_name: Snow Depth\n    grid_mapping: crs\n    units: millimeters snow thickness\nunlimited dimensions: \ncurrent shape = (365, 621, 1405)\nfilling on\n",
  "history_begin_time" : 1675814224340,
  "history_end_time" : 1675814227602,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "dprfuljmt9a",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \n#turn_nsidc_nc_to_csv()",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/gokulprathin/gw-workspace/dprfuljmt9a/data_nsidc_4km_swe.py\", line 24, in <module>\n    import netCDF4 as nc\nModuleNotFoundError: No module named 'netCDF4'\n",
  "history_begin_time" : 1675783783121,
  "history_end_time" : 1675783786118,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "nttrbpifg0p",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \n#turn_nsidc_nc_to_csv()",
  "history_output" : "/Users/joe\n<class 'netCDF4._netCDF4.Dataset'>\nroot group (NETCDF4 data model, file format HDF5):\n    dimensions(sizes): lat(621), lon(1405), time(365), time_str_len(11)\n    variables(dimensions): |S1 crs(), float32 lat(lat), float32 lon(lon), float32 time(time), |S1 time_str(time_str_len, time), int16 SWE(time, lat, lon), int16 DEPTH(time, lat, lon)\n    groups: \n<class 'netCDF4._netCDF4.Dimension'>: name = 'lat', size = 621\n<class 'netCDF4._netCDF4.Dimension'>: name = 'lon', size = 1405\n<class 'netCDF4._netCDF4.Dimension'>: name = 'time', size = 365\n<class 'netCDF4._netCDF4.Dimension'>: name = 'time_str_len', size = 11\n<class 'netCDF4._netCDF4.Variable'>\n|S1 crs()\n    grid_mapping_name: latitude_longitude\n    long_name: CRS definition\n    longitude_of_prime_meridian: 0.0\n    semi_major_axis: 6378137.0\n    inverse_flattening: 298.257222101\n    spatial_ref: GEOGCS[\"NAD83\",DATUM[\"North_American_Datum_1983\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6269\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.01745329251994328,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4269\"]]\n    GeoTransform: -125.0208 0.04166662697178698 0 49.9375 0 -0.04166662697178698 \nunlimited dimensions: \ncurrent shape = ()\nfilling on, default _FillValue of \u0000 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 lat(lat)\n    long_name: latitude\n    units: degrees north\nunlimited dimensions: \ncurrent shape = (621,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 lon(lon)\n    long_name: longitude\n    units: degrees east\nunlimited dimensions: \ncurrent shape = (1405,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 time(time)\n    long_name: time\n    units: days since 1900-01-01\nunlimited dimensions: \ncurrent shape = (365,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\n|S1 time_str(time_str_len, time)\n    long_name: time (string)\n    format: dd-mmm-yyyy\nunlimited dimensions: \ncurrent shape = (11, 365)\nfilling on, default _FillValue of \u0000 used\n<class 'netCDF4._netCDF4.Variable'>\nint16 SWE(time, lat, lon)\n    _FillValue: -999\n    long_name: Snow Water Equivalent\n    grid_mapping: crs\n    units: millimeters h20\nunlimited dimensions: \ncurrent shape = (365, 621, 1405)\nfilling on\n<class 'netCDF4._netCDF4.Variable'>\nint16 DEPTH(time, lat, lon)\n    _FillValue: -999\n    long_name: Snow Depth\n    grid_mapping: crs\n    units: millimeters snow thickness\nunlimited dimensions: \ncurrent shape = (365, 621, 1405)\nfilling on\n",
  "history_begin_time" : 1672014983303,
  "history_end_time" : 1672014984903,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "b64fp4ak1hb",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \n#turn_nsidc_nc_to_csv()",
  "history_output" : "/Users/joe\n<class 'netCDF4._netCDF4.Dataset'>\nroot group (NETCDF4 data model, file format HDF5):\n    dimensions(sizes): lat(621), lon(1405), time(365), time_str_len(11)\n    variables(dimensions): |S1 crs(), float32 lat(lat), float32 lon(lon), float32 time(time), |S1 time_str(time_str_len, time), int16 SWE(time, lat, lon), int16 DEPTH(time, lat, lon)\n    groups: \n<class 'netCDF4._netCDF4.Dimension'>: name = 'lat', size = 621\n<class 'netCDF4._netCDF4.Dimension'>: name = 'lon', size = 1405\n<class 'netCDF4._netCDF4.Dimension'>: name = 'time', size = 365\n<class 'netCDF4._netCDF4.Dimension'>: name = 'time_str_len', size = 11\n<class 'netCDF4._netCDF4.Variable'>\n|S1 crs()\n    grid_mapping_name: latitude_longitude\n    long_name: CRS definition\n    longitude_of_prime_meridian: 0.0\n    semi_major_axis: 6378137.0\n    inverse_flattening: 298.257222101\n    spatial_ref: GEOGCS[\"NAD83\",DATUM[\"North_American_Datum_1983\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6269\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.01745329251994328,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4269\"]]\n    GeoTransform: -125.0208 0.04166662697178698 0 49.9375 0 -0.04166662697178698 \nunlimited dimensions: \ncurrent shape = ()\nfilling on, default _FillValue of \u0000 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 lat(lat)\n    long_name: latitude\n    units: degrees north\nunlimited dimensions: \ncurrent shape = (621,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 lon(lon)\n    long_name: longitude\n    units: degrees east\nunlimited dimensions: \ncurrent shape = (1405,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 time(time)\n    long_name: time\n    units: days since 1900-01-01\nunlimited dimensions: \ncurrent shape = (365,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\n|S1 time_str(time_str_len, time)\n    long_name: time (string)\n    format: dd-mmm-yyyy\nunlimited dimensions: \ncurrent shape = (11, 365)\nfilling on, default _FillValue of \u0000 used\n<class 'netCDF4._netCDF4.Variable'>\nint16 SWE(time, lat, lon)\n    _FillValue: -999\n    long_name: Snow Water Equivalent\n    grid_mapping: crs\n    units: millimeters h20\nunlimited dimensions: \ncurrent shape = (365, 621, 1405)\nfilling on\n<class 'netCDF4._netCDF4.Variable'>\nint16 DEPTH(time, lat, lon)\n    _FillValue: -999\n    long_name: Snow Depth\n    grid_mapping: crs\n    units: millimeters snow thickness\nunlimited dimensions: \ncurrent shape = (365, 621, 1405)\nfilling on\n",
  "history_begin_time" : 1671944383330,
  "history_end_time" : 1671944386621,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "ai5rhbfkom1",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \n#turn_nsidc_nc_to_csv()",
  "history_output" : "/Users/joe\n<class 'netCDF4._netCDF4.Dataset'>\nroot group (NETCDF4 data model, file format HDF5):\n    dimensions(sizes): lat(621), lon(1405), time(365), time_str_len(11)\n    variables(dimensions): |S1 crs(), float32 lat(lat), float32 lon(lon), float32 time(time), |S1 time_str(time_str_len, time), int16 SWE(time, lat, lon), int16 DEPTH(time, lat, lon)\n    groups: \n<class 'netCDF4._netCDF4.Dimension'>: name = 'lat', size = 621\n<class 'netCDF4._netCDF4.Dimension'>: name = 'lon', size = 1405\n<class 'netCDF4._netCDF4.Dimension'>: name = 'time', size = 365\n<class 'netCDF4._netCDF4.Dimension'>: name = 'time_str_len', size = 11\n<class 'netCDF4._netCDF4.Variable'>\n|S1 crs()\n    grid_mapping_name: latitude_longitude\n    long_name: CRS definition\n    longitude_of_prime_meridian: 0.0\n    semi_major_axis: 6378137.0\n    inverse_flattening: 298.257222101\n    spatial_ref: GEOGCS[\"NAD83\",DATUM[\"North_American_Datum_1983\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6269\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.01745329251994328,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4269\"]]\n    GeoTransform: -125.0208 0.04166662697178698 0 49.9375 0 -0.04166662697178698 \nunlimited dimensions: \ncurrent shape = ()\nfilling on, default _FillValue of \u0000 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 lat(lat)\n    long_name: latitude\n    units: degrees north\nunlimited dimensions: \ncurrent shape = (621,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 lon(lon)\n    long_name: longitude\n    units: degrees east\nunlimited dimensions: \ncurrent shape = (1405,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 time(time)\n    long_name: time\n    units: days since 1900-01-01\nunlimited dimensions: \ncurrent shape = (365,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\n|S1 time_str(time_str_len, time)\n    long_name: time (string)\n    format: dd-mmm-yyyy\nunlimited dimensions: \ncurrent shape = (11, 365)\nfilling on, default _FillValue of \u0000 used\n<class 'netCDF4._netCDF4.Variable'>\nint16 SWE(time, lat, lon)\n    _FillValue: -999\n    long_name: Snow Water Equivalent\n    grid_mapping: crs\n    units: millimeters h20\nunlimited dimensions: \ncurrent shape = (365, 621, 1405)\nfilling on\n<class 'netCDF4._netCDF4.Variable'>\nint16 DEPTH(time, lat, lon)\n    _FillValue: -999\n    long_name: Snow Depth\n    grid_mapping: crs\n    units: millimeters snow thickness\nunlimited dimensions: \ncurrent shape = (365, 621, 1405)\nfilling on\n",
  "history_begin_time" : 1670910617811,
  "history_end_time" : 1670910618806,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "ssnabwmmduw",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \n#turn_nsidc_nc_to_csv()",
  "history_output" : "/Users/joe\n<class 'netCDF4._netCDF4.Dataset'>\nroot group (NETCDF4 data model, file format HDF5):\n    dimensions(sizes): lat(621), lon(1405), time(365), time_str_len(11)\n    variables(dimensions): |S1 crs(), float32 lat(lat), float32 lon(lon), float32 time(time), |S1 time_str(time_str_len, time), int16 SWE(time, lat, lon), int16 DEPTH(time, lat, lon)\n    groups: \n<class 'netCDF4._netCDF4.Dimension'>: name = 'lat', size = 621\n<class 'netCDF4._netCDF4.Dimension'>: name = 'lon', size = 1405\n<class 'netCDF4._netCDF4.Dimension'>: name = 'time', size = 365\n<class 'netCDF4._netCDF4.Dimension'>: name = 'time_str_len', size = 11\n<class 'netCDF4._netCDF4.Variable'>\n|S1 crs()\n    grid_mapping_name: latitude_longitude\n    long_name: CRS definition\n    longitude_of_prime_meridian: 0.0\n    semi_major_axis: 6378137.0\n    inverse_flattening: 298.257222101\n    spatial_ref: GEOGCS[\"NAD83\",DATUM[\"North_American_Datum_1983\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6269\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.01745329251994328,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4269\"]]\n    GeoTransform: -125.0208 0.04166662697178698 0 49.9375 0 -0.04166662697178698 \nunlimited dimensions: \ncurrent shape = ()\nfilling on, default _FillValue of \u0000 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 lat(lat)\n    long_name: latitude\n    units: degrees north\nunlimited dimensions: \ncurrent shape = (621,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 lon(lon)\n    long_name: longitude\n    units: degrees east\nunlimited dimensions: \ncurrent shape = (1405,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 time(time)\n    long_name: time\n    units: days since 1900-01-01\nunlimited dimensions: \ncurrent shape = (365,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\n|S1 time_str(time_str_len, time)\n    long_name: time (string)\n    format: dd-mmm-yyyy\nunlimited dimensions: \ncurrent shape = (11, 365)\nfilling on, default _FillValue of \u0000 used\n<class 'netCDF4._netCDF4.Variable'>\nint16 SWE(time, lat, lon)\n    _FillValue: -999\n    long_name: Snow Water Equivalent\n    grid_mapping: crs\n    units: millimeters h20\nunlimited dimensions: \ncurrent shape = (365, 621, 1405)\nfilling on\n<class 'netCDF4._netCDF4.Variable'>\nint16 DEPTH(time, lat, lon)\n    _FillValue: -999\n    long_name: Snow Depth\n    grid_mapping: crs\n    units: millimeters snow thickness\nunlimited dimensions: \ncurrent shape = (365, 621, 1405)\nfilling on\n",
  "history_begin_time" : 1670910501523,
  "history_end_time" : 1670910503098,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "sfxiedgwoa9",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \n#turn_nsidc_nc_to_csv()",
  "history_output" : "/Users/joe\n<class 'netCDF4._netCDF4.Dataset'>\nroot group (NETCDF4 data model, file format HDF5):\n    dimensions(sizes): lat(621), lon(1405), time(365), time_str_len(11)\n    variables(dimensions): |S1 crs(), float32 lat(lat), float32 lon(lon), float32 time(time), |S1 time_str(time_str_len, time), int16 SWE(time, lat, lon), int16 DEPTH(time, lat, lon)\n    groups: \n<class 'netCDF4._netCDF4.Dimension'>: name = 'lat', size = 621\n<class 'netCDF4._netCDF4.Dimension'>: name = 'lon', size = 1405\n<class 'netCDF4._netCDF4.Dimension'>: name = 'time', size = 365\n<class 'netCDF4._netCDF4.Dimension'>: name = 'time_str_len', size = 11\n<class 'netCDF4._netCDF4.Variable'>\n|S1 crs()\n    grid_mapping_name: latitude_longitude\n    long_name: CRS definition\n    longitude_of_prime_meridian: 0.0\n    semi_major_axis: 6378137.0\n    inverse_flattening: 298.257222101\n    spatial_ref: GEOGCS[\"NAD83\",DATUM[\"North_American_Datum_1983\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6269\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.01745329251994328,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4269\"]]\n    GeoTransform: -125.0208 0.04166662697178698 0 49.9375 0 -0.04166662697178698 \nunlimited dimensions: \ncurrent shape = ()\nfilling on, default _FillValue of \u0000 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 lat(lat)\n    long_name: latitude\n    units: degrees north\nunlimited dimensions: \ncurrent shape = (621,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 lon(lon)\n    long_name: longitude\n    units: degrees east\nunlimited dimensions: \ncurrent shape = (1405,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 time(time)\n    long_name: time\n    units: days since 1900-01-01\nunlimited dimensions: \ncurrent shape = (365,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\n|S1 time_str(time_str_len, time)\n    long_name: time (string)\n    format: dd-mmm-yyyy\nunlimited dimensions: \ncurrent shape = (11, 365)\nfilling on, default _FillValue of \u0000 used\n<class 'netCDF4._netCDF4.Variable'>\nint16 SWE(time, lat, lon)\n    _FillValue: -999\n    long_name: Snow Water Equivalent\n    grid_mapping: crs\n    units: millimeters h20\nunlimited dimensions: \ncurrent shape = (365, 621, 1405)\nfilling on\n<class 'netCDF4._netCDF4.Variable'>\nint16 DEPTH(time, lat, lon)\n    _FillValue: -999\n    long_name: Snow Depth\n    grid_mapping: crs\n    units: millimeters snow thickness\nunlimited dimensions: \ncurrent shape = (365, 621, 1405)\nfilling on\n",
  "history_begin_time" : 1670910269210,
  "history_end_time" : 1670910270584,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "6cFp6gO0HPrb",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \n#turn_nsidc_nc_to_csv()",
  "history_output" : "/Users/joe\n<class 'netCDF4._netCDF4.Dataset'>\nroot group (NETCDF4 data model, file format HDF5):\n    dimensions(sizes): lat(621), lon(1405), time(365), time_str_len(11)\n    variables(dimensions): |S1 crs(), float32 lat(lat), float32 lon(lon), float32 time(time), |S1 time_str(time_str_len, time), int16 SWE(time, lat, lon), int16 DEPTH(time, lat, lon)\n    groups: \n<class 'netCDF4._netCDF4.Dimension'>: name = 'lat', size = 621\n<class 'netCDF4._netCDF4.Dimension'>: name = 'lon', size = 1405\n<class 'netCDF4._netCDF4.Dimension'>: name = 'time', size = 365\n<class 'netCDF4._netCDF4.Dimension'>: name = 'time_str_len', size = 11\n<class 'netCDF4._netCDF4.Variable'>\n|S1 crs()\n    grid_mapping_name: latitude_longitude\n    long_name: CRS definition\n    longitude_of_prime_meridian: 0.0\n    semi_major_axis: 6378137.0\n    inverse_flattening: 298.257222101\n    spatial_ref: GEOGCS[\"NAD83\",DATUM[\"North_American_Datum_1983\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6269\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.01745329251994328,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4269\"]]\n    GeoTransform: -125.0208 0.04166662697178698 0 49.9375 0 -0.04166662697178698 \nunlimited dimensions: \ncurrent shape = ()\nfilling on, default _FillValue of \u0000 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 lat(lat)\n    long_name: latitude\n    units: degrees north\nunlimited dimensions: \ncurrent shape = (621,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 lon(lon)\n    long_name: longitude\n    units: degrees east\nunlimited dimensions: \ncurrent shape = (1405,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 time(time)\n    long_name: time\n    units: days since 1900-01-01\nunlimited dimensions: \ncurrent shape = (365,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\n|S1 time_str(time_str_len, time)\n    long_name: time (string)\n    format: dd-mmm-yyyy\nunlimited dimensions: \ncurrent shape = (11, 365)\nfilling on, default _FillValue of \u0000 used\n<class 'netCDF4._netCDF4.Variable'>\nint16 SWE(time, lat, lon)\n    _FillValue: -999\n    long_name: Snow Water Equivalent\n    grid_mapping: crs\n    units: millimeters h20\nunlimited dimensions: \ncurrent shape = (365, 621, 1405)\nfilling on\n<class 'netCDF4._netCDF4.Variable'>\nint16 DEPTH(time, lat, lon)\n    _FillValue: -999\n    long_name: Snow Depth\n    grid_mapping: crs\n    units: millimeters snow thickness\nunlimited dimensions: \ncurrent shape = (365, 621, 1405)\nfilling on\n",
  "history_begin_time" : 1670910197058,
  "history_end_time" : 1670910200475,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "B3IOyWx9TciW",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# generate valid pairs, or just load if they already exist\nif not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n    print(\"file doesn't exist, generating new\")\n    gen_pairs()\nlat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\nlat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n# comment out if bulk writing!!\n# all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\nfor ind, current_cell_id in enumerate(scmd):\n    # comment out if bulk writing\n    # all_cells_df = pd.DataFrame(columns=columns)\n\n    # Location information\n    longitude = station_cell_mapper_df['lon'][ind]\n    latitude = station_cell_mapper_df['lat'][ind]\n\n    print(latitude)\n    print(longitude)\n\n    # find closest lat long\n    lat_val, lon_val = find_nearest_2(latitude, longitude)\n    lat_idx = np.where(lat == lat_val)[0]\n    lon_idx = np.where(lon == lon_val)[0]\n    print(lat_val)\n    print(lon_val)\n\n    depth_time = depth[:, lat_idx, lon_idx]\n    swe_time = swe[:, lat_idx, lon_idx]\n\n    for ele in time:\n        time_index = int(ele.data - days_1900_start)\n        time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n        depth_val = depth_time[time_index][0][0]\n        swe_val = swe_time[time_index][0][0]\n\n        all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n    # comment out if bulk writing\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n# uncomment to bulk write at end of program\nall_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\nprint(\"finished\")",
  "history_output" : "Running",
  "history_begin_time" : 1670019847159,
  "history_end_time" : 1670770307178,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "a9n0blqr2i2",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\nnsidc_data_file = f\"{homedir}/Documents/Geoweaver/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# generate valid pairs, or just load if they already exist\nif not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n    print(\"file doesn't exist, generating new\")\n    gen_pairs()\nlat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\nlat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n# comment out if bulk writing!!\n# all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\nfor ind, current_cell_id in enumerate(scmd):\n    # comment out if bulk writing\n    # all_cells_df = pd.DataFrame(columns=columns)\n\n    # Location information\n    longitude = station_cell_mapper_df['lon'][ind]\n    latitude = station_cell_mapper_df['lat'][ind]\n\n    print(latitude)\n    print(longitude)\n\n    # find closest lat long\n    lat_val, lon_val = find_nearest_2(latitude, longitude)\n    lat_idx = np.where(lat == lat_val)[0]\n    lon_idx = np.where(lon == lon_val)[0]\n    print(lat_val)\n    print(lon_val)\n\n    depth_time = depth[:, lat_idx, lon_idx]\n    swe_time = swe[:, lat_idx, lon_idx]\n\n    for ele in time:\n        time_index = int(ele.data - days_1900_start)\n        time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n        depth_val = depth_time[time_index][0][0]\n        swe_val = swe_time[time_index][0][0]\n\n        all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n    # comment out if bulk writing\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n# uncomment to bulk write at end of program\nall_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\nprint(\"finished\")",
  "history_output" : "/Users/joe\nTraceback (most recent call last):\n  File \"/Users/joe/gw-workspace/a9n0blqr2i2/data_nsidc_4km_swe.py\", line 39, in <module>\n    nsidc_data_ds = nc.Dataset(nsidc_data_file)\n  File \"src/netCDF4/_netCDF4.pyx\", line 2307, in netCDF4._netCDF4.Dataset.__init__\n  File \"src/netCDF4/_netCDF4.pyx\", line 1925, in netCDF4._netCDF4._ensure_nc_success\nFileNotFoundError: [Errno 2] No such file or directory: b'/Users/joe/Documents/Geoweaver/4km_SWE_Depth_WY2019_v01.nc'\n",
  "history_begin_time" : 1670019639086,
  "history_end_time" : 1670019643796,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "nQee4hNNrkbB",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\"\"\"\nimport os\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY2019_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-08-31'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\n\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n    \n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\n\n# np.set_printoptions(threshold=sys.maxsize)\n# print(lat)\n# print(lon)\n# np.set_printoptions(threshold=False)\n# print(len(lat))\n# print(len(lon))\n\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n# print(days_1900_start)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# generate valid pairs, or just load if they already exist\nif not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n    print(\"file doesn't exist, generating new\")\n    gen_pairs()\nlat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\nlat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n# comment out if bulk writing!!\n# all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n# test1, test2 = find_nearest_2(41.993149, -120.1787155)\n# print(test1)\n# print(test2)\n\nfor ind, current_cell_id in enumerate(scmd):\n# for i in range(1):\n    # comment out if bulk writing\n    # all_cells_df = pd.DataFrame(columns=columns)\n\n    # Location information\n    longitude = station_cell_mapper_df['lon'][ind]\n    latitude = station_cell_mapper_df['lat'][ind]\n\n    print(latitude)\n    print(longitude)\n\n    # find closest lat long\n    lat_val, lon_val = find_nearest_2(latitude, longitude)\n    lat_idx = np.where(lat == lat_val)[0]\n    lon_idx = np.where(lon == lon_val)[0]\n    # print(lat_idx)\n    # print(lon_idx)\n    print(lat_val)\n    print(lon_val)\n\n    depth_time = depth[:, lat_idx, lon_idx]\n    swe_time = swe[:, lat_idx, lon_idx]\n\n    for ele in time:\n        # print(ele)\n        time_index = int(ele.data - days_1900_start)\n        time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n        # print(time_index)\n        # print(type(ele))\n        depth_val = depth_time[time_index][0][0]\n        swe_val = swe_time[time_index][0][0]\n\n        all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n        # print(ind)\n\n    # print(all_cells_df)\n    # comment out if bulk writing\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n\n# for column_name in all_cells_df:\n#     if len(all_cells_df[column_name]) > 0:\n#         all_cells_df[column_name].to_csv(f\"{dfolder}/{column_name}.csv\")\n\n# uncomment to bulk write at end of program\nall_cells_df.to_csv(f\"{dfolder}/test.csv\")\n\nprint(\"finished\")",
  "history_output" : "/Users/joe\n<class 'netCDF4._netCDF4.Dataset'>\nroot group (NETCDF4 data model, file format HDF5):\n    dimensions(sizes): lat(621), lon(1405), time(365), time_str_len(11)\n    variables(dimensions): |S1 crs(), float32 lat(lat), float32 lon(lon), float32 time(time), |S1 time_str(time_str_len, time), int16 SWE(time, lat, lon), int16 DEPTH(time, lat, lon)\n    groups: \n<class 'netCDF4._netCDF4.Dimension'>: name = 'lat', size = 621\n<class 'netCDF4._netCDF4.Dimension'>: name = 'lon', size = 1405\n<class 'netCDF4._netCDF4.Dimension'>: name = 'time', size = 365\n<class 'netCDF4._netCDF4.Dimension'>: name = 'time_str_len', size = 11\n<class 'netCDF4._netCDF4.Variable'>\n|S1 crs()\n    grid_mapping_name: latitude_longitude\n    long_name: CRS definition\n    longitude_of_prime_meridian: 0.0\n    semi_major_axis: 6378137.0\n    inverse_flattening: 298.257222101\n    spatial_ref: GEOGCS[\"NAD83\",DATUM[\"North_American_Datum_1983\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6269\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.01745329251994328,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4269\"]]\n    GeoTransform: -125.0208 0.04166662697178698 0 49.9375 0 -0.04166662697178698 \nunlimited dimensions: \ncurrent shape = ()\nfilling on, default _FillValue of \u0000 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 lat(lat)\n    long_name: latitude\n    units: degrees north\nunlimited dimensions: \ncurrent shape = (621,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 lon(lon)\n    long_name: longitude\n    units: degrees east\nunlimited dimensions: \ncurrent shape = (1405,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 time(time)\n    long_name: time\n    units: days since 1900-01-01\nunlimited dimensions: \ncurrent shape = (365,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\n|S1 time_str(time_str_len, time)\n    long_name: time (string)\n    format: dd-mmm-yyyy\nunlimited dimensions: \ncurrent shape = (11, 365)\nfilling on, default _FillValue of \u0000 used\n<class 'netCDF4._netCDF4.Variable'>\nint16 SWE(time, lat, lon)\n    _FillValue: -999\n    long_name: Snow Water Equivalent\n    grid_mapping: crs\n    units: millimeters h20\nunlimited dimensions: \ncurrent shape = (365, 621, 1405)\nfilling on\n<class 'netCDF4._netCDF4.Variable'>\nint16 DEPTH(time, lat, lon)\n    _FillValue: -999\n    long_name: Snow Depth\n    grid_mapping: crs\n    units: millimeters snow thickness\nunlimited dimensions: \ncurrent shape = (365, 621, 1405)\nfilling on\nfile doesn't exist, generating new\n[[ 25.166666 -81.125   ]\n [ 25.166666 -81.083336]\n [ 25.166666 -81.041664]\n ...\n [ 49.333332 -94.958336]\n [ 49.333332 -94.916664]\n [ 49.333332 -94.875   ]]\n41.993149\n-120.1787155\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n42.0\n-120.166664\n37.7271545\n-119.1366695\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n37.708332\n-119.125\n38.9181445\n-120.205665\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n38.916668\n-120.208336\n37.0706085\n-118.7683605\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n37.083332\n-118.75\n36.364939\n-118.2922535\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n36.375\n-118.291664\n41.227482\n-122.8017965\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n41.208332\n-122.791664\n37.092108\n-118.498866\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n37.083332\n-118.5\n36.725784\n-118.8402255\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n36.708332\n-118.833336\n39.814023\n-121.3195755\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n39.833332\n-121.333336\n40.780052\n-121.7866995\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n40.791668\n-121.791664\n39.67588\n-120.61889\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n39.666668\n-120.625\n38.4905185\n-119.8014235\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n38.5\n-119.791664\n38.708156\n-120.0439685\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n38.708332\n-120.041664\n38.1663575\n-120.0529515\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n38.166668\n-120.041664\n38.9251335\n-119.9182045\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n38.916668\n-119.916664\n41.2004515\n-122.5233185\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n41.208332\n-122.541664\n38.708156\n-120.0439685\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n38.708332\n-120.041664\n41.993149\n-120.1787155\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n42.0\n-120.166664\n37.4067365\n-119.4870125\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n37.416668\n-119.5\n36.466145\n-118.5437815\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n36.458332\n-118.541664\n36.7833635\n-118.4270005\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n36.791668\n-118.416664\n38.911155\n-120.3763445\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n38.916668\n-120.375\n36.364939\n-118.2922535\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n36.375\n-118.291664\n36.364939\n-118.2922535\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n36.375\n-118.291664\n41.993149\n-120.1787155\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n42.0\n-120.166664\n38.708156\n-120.0439685\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n38.708332\n-120.041664\n37.89748\n-119.2624335\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n37.916668\n-119.25\n38.279274\n-119.6127765\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n38.291668\n-119.625\n37.634731\n-119.0827705\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n37.625\n-119.083336\n41.993149\n-120.1787155\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n42.0\n-120.166664\n38.4905185\n-119.8014235\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n38.5\n-119.791664\n38.9181445\n-120.205665\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n38.916668\n-120.208336\n38.708156\n-120.0439685\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n38.708332\n-120.041664\n38.9251335\n-119.9182045\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n38.916668\n-119.916664\n38.9181445\n-120.205665\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n38.916668\n-120.208336\n36.408329\n-118.5797145\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n36.416668\n-118.583336\n37.798171\n-119.1995515\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n37.791668\n-119.208336\n37.7626715\n-119.7744735\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n37.75\n-119.791664\n38.911155\n-120.3763445\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n38.916668\n-120.375\n36.5672195\n-118.7683605\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n36.583332\n-118.75\n38.194603\n-119.8912545\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n38.208332\n-119.875\n37.5564365\n-119.2354845\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n37.541668\n-119.25\n37.4709305\n-119.2893835\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n37.458332\n-119.291664\n39.67588\n-120.61889\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n39.666668\n-120.625\n38.9251335\n-119.9182045\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n38.916668\n-119.916664\n41.2004515\n-122.5233185\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n41.208332\n-122.541664\n39.814023\n-121.3195755\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n39.833332\n-121.333336\n37.2209785\n-119.2175185\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n37.208332\n-119.208336\n38.9251335\n-119.9182045\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n38.916668\n-119.916664\n39.814023\n-121.3195755\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n39.833332\n-121.333336\n38.152231\n-119.6666755\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n38.166668\n-119.666664\n38.9251335\n-119.9182045\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n38.916668\n-119.916664\n38.911155\n-120.3763445\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n38.916668\n-120.375\n39.67588\n-120.61889\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n39.666668\n-120.625\n39.67588\n-120.61889\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n39.666668\n-120.625\n39.67588\n-120.61889\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n39.666668\n-120.625\n39.7864165\n-120.8794015\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n39.791668\n-120.875\n38.50458\n-119.62176\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n38.5\n-119.625\n40.780052\n-121.7866995\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n40.791668\n-121.791664\n38.279274\n-119.6127765\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n38.291668\n-119.625\n38.279274\n-119.6127765\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n38.291668\n-119.625\n39.67588\n-120.61889\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n39.666668\n-120.625\n41.1666475\n-121.9394135\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n41.166668\n-121.958336\n37.6205015\n-119.028872\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n37.625\n-119.041664\n38.50458\n-119.62176\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n38.5\n-119.625\n38.9251335\n-119.9182045\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n38.916668\n-119.916664\n38.9251335\n-119.9182045\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n38.916668\n-119.916664\n36.740183\n-118.7054785\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n36.75\n-118.708336\n38.0603395\n-119.6666755\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n38.041668\n-119.666664\n41.2004515\n-122.5233185\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n41.208332\n-122.541664\n36.364939\n-118.2922535\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n36.375\n-118.291664\n38.50458\n-119.62176\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n38.5\n-119.625\n36.314286\n-118.6156465\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n36.333332\n-118.625\n38.911155\n-120.3763445\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n38.916668\n-120.375\n39.67588\n-120.61889\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n39.666668\n-120.625\n37.4495385\n-118.7593775\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n37.458332\n-118.75\n38.9181445\n-120.205665\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n38.916668\n-120.208336\n41.227482\n-122.8017965\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n41.208332\n-122.791664\n39.7864165\n-120.8794015\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n39.791668\n-120.875\n38.708156\n-120.0439685\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n38.708332\n-120.041664\n41.227482\n-122.8017965\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n41.208332\n-122.791664\n41.2004515\n-122.5233185\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n41.208332\n-122.541664\n38.50458\n-119.62176\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n38.5\n-119.625\n41.227482\n-122.8017965\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n41.208332\n-122.791664\n38.708156\n-120.0439685\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n38.708332\n-120.041664\n38.4905185\n-119.8014235\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n38.5\n-119.791664\n41.2004515\n-122.5233185\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n41.208332\n-122.541664\n38.279274\n-119.6127765\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n38.291668\n-119.625\n38.4905185\n-119.8014235\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n38.5\n-119.791664\n38.9181445\n-120.205665\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n38.916668\n-120.208336\n39.7864165\n-120.8794015\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n39.791668\n-120.875\n37.1780465\n-118.561748\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n37.166668\n-118.541664\n38.9181445\n-120.205665\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n38.916668\n-120.208336\n38.9181445\n-120.205665\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n38.916668\n-120.208336\n37.156571\n-119.1995515\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n37.166668\n-119.208336\n37.833654\n-119.4510805\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n37.833332\n-119.458336\n37.8762105\n-119.3432825\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n37.875\n-119.333336\n37.19236\n-118.9390405\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n37.208332\n-118.958336\n36.682572\n-118.4270005\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n36.666668\n-118.416664\n37.385326\n-118.912091\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n37.375\n-118.916664\n38.0391175\n-119.3073495\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n38.041668\n-119.291664\n38.911155\n-120.3763445\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n38.916668\n-120.375\n38.9181445\n-120.205665\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n38.916668\n-120.208336\n37.862028\n-119.6576925\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n37.875\n-119.666664\n36.3432345\n-118.5886975\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n36.333332\n-118.583336\n37.0347625\n-118.912091\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n37.041668\n-118.916664\n42.7892825\n-121.9753465\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n42.791668\n-121.958336\n37.8549355\n-105.4373615\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n37.875\n-105.416664\n46.913419\n-110.8542025\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n46.916668\n-110.875\n42.696921\n-120.798553\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n42.708332\n-120.791664\n48.693141\n-121.912464\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n48.708332\n-121.916664\n46.178208\n-121.9304305\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n46.166668\n-121.916664\n41.6852895\n-111.4201415\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n41.666668\n-111.416664\n39.7657045\n-105.9044855\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n39.75\n-105.916664\n41.071904\n-106.9465315\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n41.083332\n-106.958336\n45.186647\n-115.9746\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n45.166668\n-115.958336\n36.002412\n-106.5512725\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n36.0\n-106.541664\n40.3980345\n-106.6051715\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n40.416668\n-106.625\n40.3980345\n-105.8505865\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n40.416668\n-105.833336\n40.5347195\n-105.8865195\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n40.541668\n-105.875\n40.8140545\n-106.748902\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n40.833332\n-106.75\n33.358253500000004\n-107.8268805\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n33.375\n-107.833336\n40.5620235\n-111.6537035\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n40.541668\n-111.666664\n39.599784\n-106.51534\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n39.583332\n-106.5\n47.880235\n-117.088511\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n47.875\n-117.083336\n43.190109\n-122.1370425\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n43.208332\n-122.125\n41.3557235\n-106.2278795\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n41.375\n-106.208336\n41.3557235\n-106.2278795\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n41.375\n-106.208336\n33.358253500000004\n-107.8268805\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n33.375\n-107.833336\n38.9251335\n-119.9182045\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n38.916668\n-119.916664\n38.708156\n-120.0439685\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n38.708332\n-120.041664\n38.50458\n-119.62176\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n38.5\n-119.625\n48.9710655\n-115.9566335\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n48.958332\n-115.958336\n40.936319\n-111.8154005\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n40.916668\n-111.833336\n37.4281405\n-106.6231375\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n37.416668\n-106.625\n40.3980345\n-106.6051715\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n40.416668\n-106.625\n37.527945\n-113.0550755\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n37.541668\n-113.041664\n37.527945\n-113.0550755\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n37.541668\n-113.041664\n38.708156\n-120.0439685\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n38.708332\n-120.041664\n47.2742445\n-121.3375425\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n47.291668\n-121.333336\n42.7892825\n-121.9753465\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n42.791668\n-121.958336\n46.23416\n-117.3939385\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n46.25\n-117.375\n43.5687975\n-111.213529\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n43.583332\n-111.208336\n44.613976\n-122.2268745\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n44.625\n-122.208336\n47.0666095\n-121.5890705\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n47.083332\n-121.583336\n41.6852895\n-111.4201415\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n41.666668\n-111.416664\n37.9754145\n-111.8333665\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n37.958332\n-111.833336\n38.708156\n-106.4255085\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n38.708332\n-106.416664\n39.294562\n-106.5512725\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n39.291668\n-106.541664\n46.178208\n-121.9304305\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n46.166668\n-121.916664\n45.218295\n-110.2343655\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n45.208332\n-110.25\n47.7112645\n-123.4485835\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n47.708332\n-123.458336\n46.178208\n-121.9304305\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n46.166668\n-121.916664\n36.3070475\n-115.678156\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n36.291668\n-115.666664\n36.3070475\n-115.678156\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n36.291668\n-115.666664\n41.6852895\n-111.4201415\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n41.666668\n-111.416664\n41.6852895\n-111.4201415\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n41.666668\n-111.416664\n41.6852895\n-111.4201415\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n41.666668\n-111.416664\n41.071904\n-106.9465315\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n41.083332\n-106.958336\n39.599784\n-106.51534\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n39.583332\n-106.5\n34.9710915\n-111.509973\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n34.958332\n-111.5\n40.432232\n-105.7338055\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n40.416668\n-105.75\n40.432232\n-105.7338055\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n40.416668\n-105.75\n37.8691195\n-106.6051715\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n37.875\n-106.625\n33.650385\n-109.3091005\n[[0.00038069]]\n[[78266]]\n[[[  33.666668 -109.291664]]]\n33.666668\n-109.291664\n38.4905185\n-106.335677\n[[0.00016855]]\n[[202538]]\n[[[  38.5      -106.333336]]]\n38.5\n-106.333336\n46.3582945\n-121.0770305\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n46.375\n-121.083336\n44.581994\n-107.1980595\n[[0.00012986]]\n[[373281]]\n[[[  44.583332 -107.208336]]]\n44.583332\n-107.208336\n44.164622\n-107.1261945\n[[3.87134852e-05]]\n[[362136]]\n[[[  44.166668 -107.125   ]]]\n44.166668\n-107.125\n43.0064475\n-109.7582585\n[[0.00015419]]\n[[330682]]\n[[[  43.   -109.75]]]\n43.0\n-109.75\n42.5713525\n-108.8419765\n[[0.00023674]]\n[[319102]]\n[[[  42.583332 -108.833336]]]\n42.583332\n-108.833336\n40.7936545\n-110.8811525\n[[8.83810871e-05]]\n[[267286]]\n[[[  40.791668 -110.875   ]]]\n40.791668\n-110.875\n41.7791425\n-116.0284985\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n41.791668\n-116.041664\n35.2356645\n-108.2670545\n[[0.00034885]]\n[[116759]]\n[[[  35.25 -108.25]]]\n35.25\n-108.25\n34.860599500000006\n-111.6087875\n[[0.00034215]]\n[[107190]]\n[[[  34.875 -111.625]]]\n34.875\n-111.625\n34.742577999999995\n-111.4111585\n[[0.0001517]]\n[[104042]]\n[[[  34.75     -111.416664]]]\n34.75\n-111.416664\n38.994985\n-106.7578855\n[[0.00013821]]\n[[216303]]\n[[[  39.   -106.75]]]\n39.0\n-106.75\n37.4566695\n-108.4467175\n[[0.00016356]]\n[[174581]]\n[[[  37.458332 -108.458336]]]\n37.458332\n-108.458336\n48.4315575\n-113.9354245\n[[0.00033874]]\n[[457740]]\n[[[  48.416668 -113.916664]]]\n48.416668\n-113.916664\n39.495881\n-111.7255685\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n39.5\n-111.708336\n39.565167\n-115.8398525\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n39.583332\n-115.833336\n37.5777975\n-112.9023615\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n37.583332\n-112.916664\n37.9754145\n-111.8333665\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n37.958332\n-111.833336\n39.565167\n-115.8398525\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n39.583332\n-115.833336\n39.565167\n-115.8398525\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n39.583332\n-115.833336\n37.5777975\n-112.9023615\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n37.583332\n-112.916664\n39.565167\n-115.8398525\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n39.583332\n-115.833336\n38.483487\n-112.390322\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n38.5\n-112.375\n48.7227805\n-120.6548225\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n48.708332\n-120.666664\n37.5991525\n-107.2339925\n[[0.00035391]]\n[[177932]]\n[[[  37.583332 -107.25    ]]]\n37.583332\n-107.25\n40.5347195\n-105.8865195\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n40.541668\n-105.875\n40.882007\n-110.5397925\n[[0.00012476]]\n[[269738]]\n[[[  40.875    -110.541664]]]\n40.875\n-110.541664\n44.613976\n-122.2268745\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n44.625\n-122.208336\n44.613976\n-122.2268745\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n44.625\n-122.208336\n36.473369000000005\n-105.1948165\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n36.458332\n-105.208336\n36.473369000000005\n-105.1948165\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n36.458332\n-105.208336\n36.473369000000005\n-105.1948165\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n36.458332\n-105.208336\n48.5208855\n-120.735671\n[[0.00039874]]\n[[459718]]\n[[[  48.541668 -120.75    ]]]\n48.541668\n-120.75\n37.006073\n-106.2727945\n[[0.00028356]]\n[[162526]]\n[[[  37.       -106.291664]]]\n37.0\n-106.291664\n39.134482\n-111.4381075\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n39.125\n-111.458336\n37.7910725\n-108.1772235\n[[0.00014601]]\n[[183432]]\n[[[  37.791668 -108.166664]]]\n37.791668\n-108.166664\n39.7657045\n-105.9044855\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n39.75\n-105.916664\n40.199347\n-105.5990585\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n40.208332\n-105.583336\n40.739226\n-110.6206405\n[[0.00019668]]\n[[266072]]\n[[[  40.75  -110.625]]]\n40.75\n-110.625\n40.3569745\n-116.8639325\n[[0.00034733]]\n[[255062]]\n[[[  40.375 -116.875]]]\n40.375\n-116.875\n41.328746\n-106.3716095\n[[9.15488263e-05]]\n[[283188]]\n[[[  41.333332 -106.375   ]]]\n41.333332\n-106.375\n37.5777975\n-112.9023615\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n37.583332\n-112.916664\n40.4937435\n-112.6149005\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n40.5\n-112.625\n37.8691195\n-109.4438475\n[[0.00022445]]\n[[185613]]\n[[[  37.875    -109.458336]]]\n37.875\n-109.458336\n39.889883\n-111.2494615\n[[0.00025986]]\n[[240815]]\n[[[  39.875 -111.25 ]]]\n39.875\n-111.25\n39.322364500000006\n-111.4920065\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n39.333332\n-111.5\n40.295338\n-111.2584445\n[[0.00012938]]\n[[252783]]\n[[[  40.291668 -111.25    ]]]\n40.291668\n-111.25\n39.134482\n-111.4381075\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n39.125\n-111.458336\n40.6779395\n-110.9530175\n[[0.00020894]]\n[[263633]]\n[[[  40.666668 -110.958336]]]\n40.666668\n-110.958336\n39.322364500000006\n-111.4920065\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n39.333332\n-111.5\n39.322364500000006\n-111.4920065\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n39.333332\n-111.5\n39.134482\n-111.4381075\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n39.125\n-111.458336\n46.3582945\n-121.0770305\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n46.375\n-121.083336\n39.67588\n-110.4319945\n[[0.00026128]]\n[[234882]]\n[[[  39.666668 -110.416664]]]\n39.666668\n-110.416664\n38.9251335\n-119.9182045\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n38.916668\n-119.916664\n40.895589\n-115.211032\n[[0.0003611]]\n[[269634]]\n[[[  40.875    -115.208336]]]\n40.875\n-115.208336\n40.943105\n-115.094251\n[[0.00030221]]\n[[272081]]\n[[[  40.958332 -115.083336]]]\n40.958332\n-115.083336\n40.4937435\n-112.6149005\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n40.5\n-112.625\n37.5777975\n-112.9023615\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n37.583332\n-112.916664\n37.9754145\n-111.8333665\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n37.958332\n-111.833336\n40.199347\n-105.5990585\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n40.208332\n-105.583336\n41.071904\n-106.9465315\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n41.083332\n-106.958336\n36.125859\n-105.5271935\n[[0.00020455]]\n[[139488]]\n[[[  36.125    -105.541664]]]\n36.125\n-105.541664\n47.880235\n-117.088511\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n47.875\n-117.083336\n46.7843995\n-121.7507675\n[[0.00012719]]\n[[426421]]\n[[[  46.791668 -121.75    ]]]\n46.791668\n-121.75\n41.993149\n-120.1787155\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n42.0\n-120.166664\n48.7227805\n-120.6548225\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n48.708332\n-120.666664\n37.9754145\n-111.8333665\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n37.958332\n-111.833336\n41.7791425\n-116.0284985\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n41.791668\n-116.041664\n46.178208\n-121.9304305\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n46.166668\n-121.916664\n37.9754145\n-111.8333665\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n37.958332\n-111.833336\n33.9787815\n-109.50673\n[[0.00036997]]\n[[84911]]\n[[[  33.958332 -109.5     ]]]\n33.958332\n-109.5\n41.6651605\n-115.327813\n[[7.66642558e-05]]\n[[292876]]\n[[[  41.666668 -115.333336]]]\n41.666668\n-115.333336\n39.7864165\n-120.8794015\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n39.791668\n-120.875\n38.483487\n-112.390322\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n38.5\n-112.375\n39.495881\n-111.7255685\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n39.5\n-111.708336\n45.6720715\n-113.9533905\n[[0.00011195]]\n[[400403]]\n[[[  45.666668 -113.958336]]]\n45.666668\n-113.958336\n44.473122\n-112.9832105\n[[0.00033222]]\n[[369791]]\n[[[  44.458332 -113.      ]]]\n44.458332\n-113.0\nfinished\n",
  "history_begin_time" : 1668706065524,
  "history_end_time" : 1668706577070,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "rHYeTykMOuVW",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY2019_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-08-31'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\n\n# np.set_printoptions(threshold=sys.maxsize)\n# print(lat)\n# print(lon)\n# np.set_printoptions(threshold=False)\n# print(len(lat))\n# print(len(lon))\n\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n# print(days_1900_start)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# generate valid pairs, or just load if they already exist\nif not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n    print(\"file doesn't exist, generating new\")\n    gen_pairs()\nlat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\nlat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n# comment out if bulk writing!!\n# all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n# test1, test2 = find_nearest_2(41.993149, -120.1787155)\n# print(test1)\n# print(test2)\n\nfor ind, current_cell_id in enumerate(scmd):\n# for i in range(1):\n    # comment out if bulk writing\n    # all_cells_df = pd.DataFrame(columns=columns)\n\n    # Location information\n    longitude = station_cell_mapper_df['lon'][ind]\n    latitude = station_cell_mapper_df['lat'][ind]\n\n    print(latitude)\n    print(longitude)\n\n    # find closest lat long\n    lat_val, lon_val = find_nearest_2(latitude, longitude)\n    lat_idx = np.where(lat == lat_val)[0]\n    lon_idx = np.where(lon == lon_val)[0]\n    # print(lat_idx)\n    # print(lon_idx)\n    print(lat_val)\n    print(lon_val)\n\n    depth_time = depth[:, lat_idx, lon_idx]\n    swe_time = swe[:, lat_idx, lon_idx]\n\n    for ele in time:\n        # print(ele)\n        time_index = int(ele.data - days_1900_start)\n        time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n        # print(time_index)\n        # print(type(ele))\n        depth_val = depth_time[time_index][0][0]\n        swe_val = swe_time[time_index][0][0]\n\n        all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n        # print(ind)\n\n    # print(all_cells_df)\n    # comment out if bulk writing\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n\n# for column_name in all_cells_df:\n#     if len(all_cells_df[column_name]) > 0:\n#         all_cells_df[column_name].to_csv(f\"{dfolder}/{column_name}.csv\")\n\n# uncomment to bulk write at end of program\nall_cells_df.to_csv(f\"{dfolder}/test.csv\")\n\nprint(\"finished\")",
  "history_output" : "/Users/joe\n<class 'netCDF4._netCDF4.Dataset'>\nroot group (NETCDF4 data model, file format HDF5):\n    dimensions(sizes): lat(621), lon(1405), time(365), time_str_len(11)\n    variables(dimensions): |S1 crs(), float32 lat(lat), float32 lon(lon), float32 time(time), |S1 time_str(time_str_len, time), int16 SWE(time, lat, lon), int16 DEPTH(time, lat, lon)\n    groups: \n<class 'netCDF4._netCDF4.Dimension'>: name = 'lat', size = 621\n<class 'netCDF4._netCDF4.Dimension'>: name = 'lon', size = 1405\n<class 'netCDF4._netCDF4.Dimension'>: name = 'time', size = 365\n<class 'netCDF4._netCDF4.Dimension'>: name = 'time_str_len', size = 11\n<class 'netCDF4._netCDF4.Variable'>\n|S1 crs()\n    grid_mapping_name: latitude_longitude\n    long_name: CRS definition\n    longitude_of_prime_meridian: 0.0\n    semi_major_axis: 6378137.0\n    inverse_flattening: 298.257222101\n    spatial_ref: GEOGCS[\"NAD83\",DATUM[\"North_American_Datum_1983\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6269\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.01745329251994328,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4269\"]]\n    GeoTransform: -125.0208 0.04166662697178698 0 49.9375 0 -0.04166662697178698 \nunlimited dimensions: \ncurrent shape = ()\nfilling on, default _FillValue of \u0000 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 lat(lat)\n    long_name: latitude\n    units: degrees north\nunlimited dimensions: \ncurrent shape = (621,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 lon(lon)\n    long_name: longitude\n    units: degrees east\nunlimited dimensions: \ncurrent shape = (1405,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 time(time)\n    long_name: time\n    units: days since 1900-01-01\nunlimited dimensions: \ncurrent shape = (365,)\nfilling on, default _FillValue of 9.969209968386869e+36 used\n<class 'netCDF4._netCDF4.Variable'>\n|S1 time_str(time_str_len, time)\n    long_name: time (string)\n    format: dd-mmm-yyyy\nunlimited dimensions: \ncurrent shape = (11, 365)\nfilling on, default _FillValue of \u0000 used\n<class 'netCDF4._netCDF4.Variable'>\nint16 SWE(time, lat, lon)\n    _FillValue: -999\n    long_name: Snow Water Equivalent\n    grid_mapping: crs\n    units: millimeters h20\nunlimited dimensions: \ncurrent shape = (365, 621, 1405)\nfilling on\n<class 'netCDF4._netCDF4.Variable'>\nint16 DEPTH(time, lat, lon)\n    _FillValue: -999\n    long_name: Snow Depth\n    grid_mapping: crs\n    units: millimeters snow thickness\nunlimited dimensions: \ncurrent shape = (365, 621, 1405)\nfilling on\nfile doesn't exist, generating new\n[[ 25.166666 -81.125   ]\n [ 25.166666 -81.083336]\n [ 25.166666 -81.041664]\n ...\n [ 49.333332 -94.958336]\n [ 49.333332 -94.916664]\n [ 49.333332 -94.875   ]]\nTraceback (most recent call last):\n  File \"/Users/joe/gw-workspace/rHYeTykMOuVW/data_nsidc_4km_swe.py\", line 133, in <module>\n    gen_pairs()\n  File \"/Users/joe/gw-workspace/rHYeTykMOuVW/data_nsidc_4km_swe.py\", line 113, in gen_pairs\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n  File \"<__array_function__ internals>\", line 5, in save\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/npyio.py\", line 525, in save\n    file_ctx = open(file, \"wb\")\nFileNotFoundError: [Errno 2] No such file or directory: '/Users/joe/Documents/GitHub/SnowCast/data/sim_training/nsidc//valid_pairs.npy'\n",
  "history_begin_time" : 1668705741464,
  "history_end_time" : 1668705938173,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "ziDnegQddoS4",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY2019_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-08-31'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\n\n# np.set_printoptions(threshold=sys.maxsize)\n# print(lat)\n# print(lon)\n# np.set_printoptions(threshold=False)\n# print(len(lat))\n# print(len(lon))\n\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n# print(days_1900_start)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# generate valid pairs, or just load if they already exist\nif not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n    print(\"file doesn't exist, generating new\")\n    gen_pairs()\nlat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\nlat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n# comment out if bulk writing!!\n# all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n# test1, test2 = find_nearest_2(41.993149, -120.1787155)\n# print(test1)\n# print(test2)\n\nfor ind, current_cell_id in enumerate(scmd):\n# for i in range(1):\n    # comment out if bulk writing\n    # all_cells_df = pd.DataFrame(columns=columns)\n\n    # Location information\n    longitude = station_cell_mapper_df['lon'][ind]\n    latitude = station_cell_mapper_df['lat'][ind]\n\n    print(latitude)\n    print(longitude)\n\n    # find closest lat long\n    lat_val, lon_val = find_nearest_2(latitude, longitude)\n    lat_idx = np.where(lat == lat_val)[0]\n    lon_idx = np.where(lon == lon_val)[0]\n    # print(lat_idx)\n    # print(lon_idx)\n    print(lat_val)\n    print(lon_val)\n\n    depth_time = depth[:, lat_idx, lon_idx]\n    swe_time = swe[:, lat_idx, lon_idx]\n\n    for ele in time:\n        # print(ele)\n        time_index = int(ele.data - days_1900_start)\n        time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n        # print(time_index)\n        # print(type(ele))\n        depth_val = depth_time[time_index][0][0]\n        swe_val = swe_time[time_index][0][0]\n\n        all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n        # print(ind)\n\n    # print(all_cells_df)\n    # comment out if bulk writing\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n\n# for column_name in all_cells_df:\n#     if len(all_cells_df[column_name]) > 0:\n#         all_cells_df[column_name].to_csv(f\"{dfolder}/{column_name}.csv\")\n\n# uncomment to bulk write at end of program\nall_cells_df.to_csv(f\"{dfolder}/test.csv\")\n\nprint(\"finished\")",
  "history_output" : "/Users/joe\nTraceback (most recent call last):\n  File \"/Users/joe/gw-workspace/ziDnegQddoS4/data_nsidc_4km_swe.py\", line 31, in <module>\n    nsidc_data_ds = nc.Dataset(nsidc_data_file)\n  File \"src/netCDF4/_netCDF4.pyx\", line 2463, in netCDF4._netCDF4.Dataset.__init__\n  File \"src/netCDF4/_netCDF4.pyx\", line 2026, in netCDF4._netCDF4._ensure_nc_success\nFileNotFoundError: [Errno 2] No such file or directory: b'/Users/joe/Documents/data/4km_SWE_Depth_WY2019_v01.nc'\n",
  "history_begin_time" : 1668705548868,
  "history_end_time" : 1668705551535,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "iNxbIXr1k5fi",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nnsidc_data_file = f\"{homedir}/Documents/Geoweaver/4km_SWE_Depth_WY2019_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-08-31'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\n\n# np.set_printoptions(threshold=sys.maxsize)\n# print(lat)\n# print(lon)\n# np.set_printoptions(threshold=False)\n# print(len(lat))\n# print(len(lon))\n\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n# print(days_1900_start)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# generate valid pairs, or just load if they already exist\nif not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n    print(\"file doesn't exist, generating new\")\n    gen_pairs()\nlat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\nlat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n# comment out if bulk writing!!\n# all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n# test1, test2 = find_nearest_2(41.993149, -120.1787155)\n# print(test1)\n# print(test2)\n\nfor ind, current_cell_id in enumerate(scmd):\n# for i in range(1):\n    # comment out if bulk writing\n    # all_cells_df = pd.DataFrame(columns=columns)\n\n    # Location information\n    longitude = station_cell_mapper_df['lon'][ind]\n    latitude = station_cell_mapper_df['lat'][ind]\n\n    print(latitude)\n    print(longitude)\n\n    # find closest lat long\n    lat_val, lon_val = find_nearest_2(latitude, longitude)\n    lat_idx = np.where(lat == lat_val)[0]\n    lon_idx = np.where(lon == lon_val)[0]\n    # print(lat_idx)\n    # print(lon_idx)\n    print(lat_val)\n    print(lon_val)\n\n    depth_time = depth[:, lat_idx, lon_idx]\n    swe_time = swe[:, lat_idx, lon_idx]\n\n    for ele in time:\n        # print(ele)\n        time_index = int(ele.data - days_1900_start)\n        time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n        # print(time_index)\n        # print(type(ele))\n        depth_val = depth_time[time_index][0][0]\n        swe_val = swe_time[time_index][0][0]\n\n        all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n        # print(ind)\n\n    # print(all_cells_df)\n    # comment out if bulk writing\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n\n# for column_name in all_cells_df:\n#     if len(all_cells_df[column_name]) > 0:\n#         all_cells_df[column_name].to_csv(f\"{dfolder}/{column_name}.csv\")\n\n# uncomment to bulk write at end of program\nall_cells_df.to_csv(f\"{dfolder}/test.csv\")\n\nprint(\"finished\")",
  "history_output" : "/Users/joe\nTraceback (most recent call last):\n  File \"/Users/joe/gw-workspace/iNxbIXr1k5fi/data_nsidc_4km_swe.py\", line 31, in <module>\n    nsidc_data_ds = nc.Dataset(nsidc_data_file)\n  File \"src/netCDF4/_netCDF4.pyx\", line 2463, in netCDF4._netCDF4.Dataset.__init__\n  File \"src/netCDF4/_netCDF4.pyx\", line 2026, in netCDF4._netCDF4._ensure_nc_success\nFileNotFoundError: [Errno 2] No such file or directory: b'/Users/joe/Documents/Geoweaver/4km_SWE_Depth_WY2019_v01.nc'\n",
  "history_begin_time" : 1668705088912,
  "history_end_time" : 1668705094500,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "gkpXpkOawQrE",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nnsidc_data_file = f\"{homedir}/Documents/Geoweaver/4km_SWE_Depth_WY2019_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-08-31'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\n\n# np.set_printoptions(threshold=sys.maxsize)\n# print(lat)\n# print(lon)\n# np.set_printoptions(threshold=False)\n# print(len(lat))\n# print(len(lon))\n\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n# print(days_1900_start)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# generate valid pairs, or just load if they already exist\nif not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n    print(\"file doesn't exist, generating new\")\n    gen_pairs()\nlat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\nlat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n# comment out if bulk writing!!\n# all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n# test1, test2 = find_nearest_2(41.993149, -120.1787155)\n# print(test1)\n# print(test2)\n\nfor ind, current_cell_id in enumerate(scmd):\n# for i in range(1):\n    # comment out if bulk writing\n    # all_cells_df = pd.DataFrame(columns=columns)\n\n    # Location information\n    longitude = station_cell_mapper_df['lon'][ind]\n    latitude = station_cell_mapper_df['lat'][ind]\n\n    print(latitude)\n    print(longitude)\n\n    # find closest lat long\n    lat_val, lon_val = find_nearest_2(latitude, longitude)\n    lat_idx = np.where(lat == lat_val)[0]\n    lon_idx = np.where(lon == lon_val)[0]\n    # print(lat_idx)\n    # print(lon_idx)\n    print(lat_val)\n    print(lon_val)\n\n    depth_time = depth[:, lat_idx, lon_idx]\n    swe_time = swe[:, lat_idx, lon_idx]\n\n    for ele in time:\n        # print(ele)\n        time_index = int(ele.data - days_1900_start)\n        time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n        # print(time_index)\n        # print(type(ele))\n        depth_val = depth_time[time_index][0][0]\n        swe_val = swe_time[time_index][0][0]\n\n        all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n        # print(ind)\n\n    # print(all_cells_df)\n    # comment out if bulk writing\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n\n# for column_name in all_cells_df:\n#     if len(all_cells_df[column_name]) > 0:\n#         all_cells_df[column_name].to_csv(f\"{dfolder}/{column_name}.csv\")\n\n# uncomment to bulk write at end of program\nall_cells_df.to_csv(f\"{dfolder}/test.csv\")\n\nprint(\"finished\")",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/joe/gw-workspace/gkpXpkOawQrE/data_nsidc_4km_swe.py\", line 14, in <module>\n    import netCDF4 as nc\nModuleNotFoundError: No module named 'netCDF4'\n",
  "history_begin_time" : 1668704995560,
  "history_end_time" : 1668704998794,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "yux62yv3y5c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667484654221,
  "history_end_time" : 1667484654221,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ue9fut4blx3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667410736845,
  "history_end_time" : 1667410736845,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "nsrhbvomno3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667410652251,
  "history_end_time" : 1667410705883,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "eo8glalycyz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667410544351,
  "history_end_time" : 1667410624033,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "lrhh6zzit0y",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677809171461,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "jjdkp9hgusk",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678201703980,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "dl6z6f035cl",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677785529432,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "fgcm24k20dn",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677797113231,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "2rpiblj4ta6",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677809840769,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "4288e6ezp9n",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677959722648,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "hn522y58xdx",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677107538163,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "ohet9bwwbvz",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677784516788,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "bhd8pmu9sjp",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677785383333,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "2zrf1oow3ns",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677796528249,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "dis1w9qw6tb",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677809306587,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "t8utw41pi1s",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677809554694,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "l7tr0q5fm0e",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677809573436,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "huwmqckrmkj",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677867648674,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "kj8d88t1pah",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677958291216,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "r1z5a3k501y",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677958754130,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "1cwo0lqudbw",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677958849891,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "xs0hlbnqk2a",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677958952869,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "sb0i5suzfkn",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677959583154,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "9ri3cbf9vlh",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678201687064,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "kshyy5je2oo",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678206143139,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "7olidaf6507",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678756684823,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "zj4v2jkw985",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678887836232,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},]
