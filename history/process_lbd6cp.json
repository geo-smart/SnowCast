[{
  "history_id" : "5evn6wd84j5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704644803698,
  "history_end_time" : 1704644803698,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ic5trswpadh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704600008043,
  "history_end_time" : 1704600008043,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "avo0igj6c24",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704566156614,
  "history_end_time" : 1704566156614,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3rh5fmwwk8z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704565587370,
  "history_end_time" : 1704565587370,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "85znwji7u7a",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704564424151,
  "history_end_time" : 1704564424151,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "i24h19khdsu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704562992165,
  "history_end_time" : 1704562992165,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0a417p3mcwa",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704561889785,
  "history_end_time" : 1704561889785,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "l5idxrphb37",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704561861156,
  "history_end_time" : 1704561887033,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "zargkhrxwka",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704555479195,
  "history_end_time" : 1704555479195,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "r8xbwzufy3g",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704555028185,
  "history_end_time" : 1704555028185,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "40ox84t60gh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704553241758,
  "history_end_time" : 1704553241758,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "n8uf8w9x6r9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704552254606,
  "history_end_time" : 1704552254606,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "uhza8qs8we8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704513607260,
  "history_end_time" : 1704513607260,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "sdnx7cfyzei",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704427207364,
  "history_end_time" : 1704427207364,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "p3lx54kmnvr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704340807428,
  "history_end_time" : 1704340807428,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "qr9n03dd2cf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704330109287,
  "history_end_time" : 1704330109287,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "5v9k1q9kuqu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704329364839,
  "history_end_time" : 1704329364839,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "bp9ddjh5of0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704254407361,
  "history_end_time" : 1704254407361,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "oz3ehm6m5c4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704208947948,
  "history_end_time" : 1704208947948,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "m3d6z9w9tnw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704207352009,
  "history_end_time" : 1704207352009,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "s0wejqywgnd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704205859360,
  "history_end_time" : 1704205859360,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "2qvp55oordv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704168007275,
  "history_end_time" : 1704168007275,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "hoeba497eq1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704081607330,
  "history_end_time" : 1704081607330,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "26p0gccz28e",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703995208157,
  "history_end_time" : 1703995208157,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "d9bhs5dps28",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703962871394,
  "history_end_time" : 1703962871394,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "gz2sc7uoxwn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703960265435,
  "history_end_time" : 1703960265435,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "gwns1untrb4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703959737831,
  "history_end_time" : 1703959737831,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "4ir57no7m1v",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703958611576,
  "history_end_time" : 1703958611576,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "zlxyhlnyuan",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703955838214,
  "history_end_time" : 1703955838214,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ijkal1rdx6i",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703954150349,
  "history_end_time" : 1703954150349,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8caqtc1pgp1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703915768060,
  "history_end_time" : 1703915768060,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "nxjwrdoo0qn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703915283475,
  "history_end_time" : 1703915283475,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wcy750g6nay",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703914476629,
  "history_end_time" : 1703914476629,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6j3lghuhdfb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703912302161,
  "history_end_time" : 1703912302161,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3227uucykg4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703908806968,
  "history_end_time" : 1703908806968,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "pbel2t3neny",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703906215367,
  "history_end_time" : 1703906215367,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0d61fxjf6rz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703900919132,
  "history_end_time" : 1703900919132,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0qkfeus9hxi",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703899837754,
  "history_end_time" : 1703899837754,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3k352zmarep",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703897422939,
  "history_end_time" : 1703897422939,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6fktlfaiq0g",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703896125571,
  "history_end_time" : 1703896125571,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "mlx66ab1nli",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703890275979,
  "history_end_time" : 1703890275979,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9uevcza6bjm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703886800797,
  "history_end_time" : 1703886800797,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "56250l5xrpt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703885997754,
  "history_end_time" : 1703885997754,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ixr88pv5ng6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703880194704,
  "history_end_time" : 1703880194704,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "bko7uie9mxs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703872753022,
  "history_end_time" : 1703872753022,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "pjg9af81vcv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703869828227,
  "history_end_time" : 1703869828227,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xnc6zfgk5nk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703868616918,
  "history_end_time" : 1703868616918,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "qu78vfnf6st",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703867114035,
  "history_end_time" : 1703867114035,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "buajyfp89g9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703864885433,
  "history_end_time" : 1703864885433,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "32h35fa01wx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703862637341,
  "history_end_time" : 1703862637341,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6qob3u89m8n",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703827227316,
  "history_end_time" : 1703827227316,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "n59i5yoezsw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703822411308,
  "history_end_time" : 1703822411308,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "22dr05a16u8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703786924625,
  "history_end_time" : 1703789718819,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "WLbUgGBp2BhS",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for SWE prediction.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for SWE prediction.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import compute_sample_weight\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_all_active_stations_v1.csv_sorted.csv_cleaned_nodata.csv_time_series_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(\"data.shape = \", data.shape)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of training input: \", X.describe())\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        #scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        #X_train_scaled = scaler.fit_transform(X_train)\n        #X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=10, columns=X.columns)\n\n        self.train_x, self.train_y = X_train, y_train\n        self.test_x, self.test_y = X_test, y_test\n        #self.train_x, self.train_y = X_train_scaled, y_train\n        #self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n        # Fit the classifier\n        self.classifier.fit(self.train_x, self.train_y)\n\n        # Make predictions\n        predictions = self.classifier.predict(self.train_x)\n\n        # Calculate absolute errors\n        errors = np.abs(self.train_y - predictions)\n\n        # Assign weights based on errors (higher errors get higher weights)\n        weights = compute_sample_weight('balanced', errors)\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value',\n  'SWE',\n  'cumulative_SWE',\n#   'cumulative_relative_humidity_rmin',\n#   'cumulative_air_temperature_tmmx', \n#   'cumulative_air_temperature_tmmn',\n#   'cumulative_relative_humidity_rmax',\n#   'cumulative_potential_evapotranspiration',\n#   'cumulative_wind_speed',\n  'cumulative_fsca',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'Elevation',\t\n  'Slope',\t\n  'Curvature',\t\n  'Aspect',\t\n  'Eastness',\t\n  'Northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n#   hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n#   hole.post_processing()\n\n",
  "history_output" : "today date = 2023-12-28\ntest start date:  2023-06-11\ntest end date:  2023-10-11\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_all_active_stations_v1.csv_sorted.csv_cleaned_nodata.csv_time_series_cumulative_v1.csv\ndata.shape =  (1075041, 107)\n         date       lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2018-10-01  32.92342  ...               3.500000              0.0\n1  2018-10-02  32.92342  ...               8.700000              0.0\n2  2018-10-03  32.92342  ...              13.100000              0.0\n3  2018-10-04  32.92342  ...              18.100000              0.0\n4  2018-10-05  32.92342  ...              22.800001              0.0\n[5 rows x 107 columns]\nrequired features: Index(['SWE', 'cumulative_SWE', 'cumulative_fsca', 'fsca',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'Elevation',\n       'Slope', 'Curvature', 'Aspect', 'Eastness', 'Northness'],\n      dtype='object')\ndescribe the statistics of training input:                  SWE  cumulative_SWE  ...      Eastness     Northness\ncount  1.075041e+06    1.075041e+06  ...  1.075041e+06  1.075041e+06\nmean   8.476187e+00    2.607697e+03  ... -6.903257e-02 -9.360435e-02\nstd    1.868724e+01    4.441010e+03  ...  5.708660e-01  5.880218e-01\nmin    0.000000e+00    0.000000e+00  ... -7.853979e-01 -7.853982e-01\n25%    0.000000e+00    0.000000e+00  ... -6.455206e-01 -6.945519e-01\n50%    0.000000e+00    8.160000e+02  ... -1.286902e-01 -2.071670e-01\n75%    0.000000e+00    3.673030e+03  ...  5.119705e-01  5.013258e-01\nmax    2.340000e+02    5.077595e+04  ...  7.853982e-01  7.853976e-01\n[8 rows x 14 columns]\ndescribe the statistics of swe_value:  count    1.075041e+06\nmean     6.038476e+00\nstd      1.005967e+01\nmin     -8.000000e+00\n25%      0.000000e+00\n50%      7.000000e-01\n75%      8.700000e+00\nmax      1.021000e+02\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'cumulative_SWE', 'cumulative_fsca', 'fsca',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'Elevation',\n       'Slope', 'Curvature', 'Aspect', 'Eastness', 'Northness'],\n      dtype='object')\ntraining data row number:  1075041\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.8730205005371873\nMSE is 4.503850910017361\nR2 score is 0.9552963326382309\nRMSE is 2.1222278176523277\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232812180003.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-15.png\n",
  "history_begin_time" : 1703786292747,
  "history_end_time" : 1703786425607,
  "history_notes" : "check the value range of the input variables, especially the cumulative ones. ",
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "8e01awcou9l",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703786053478,
  "history_end_time" : 1703786917610,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "81fd2504ybn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703778395382,
  "history_end_time" : 1703778395382,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "1y6idcxykwb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703739034324,
  "history_end_time" : 1703739034324,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "1b9zggsl4s3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703738754388,
  "history_end_time" : 1703792459276,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "yl3336H1Lvz8",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for SWE prediction.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for SWE prediction.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import compute_sample_weight\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_all_active_stations_v1.csv_sorted.csv_cleaned_nodata.csv_time_series_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(\"data.shape = \", data.shape)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        #scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        #X_train_scaled = scaler.fit_transform(X_train)\n        #X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=10, columns=X.columns)\n\n        self.train_x, self.train_y = X_train, y_train\n        self.test_x, self.test_y = X_test, y_test\n        #self.train_x, self.train_y = X_train_scaled, y_train\n        #self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n        # Fit the classifier\n        self.classifier.fit(self.train_x, self.train_y)\n\n        # Make predictions\n        predictions = self.classifier.predict(self.train_x)\n\n        # Calculate absolute errors\n        errors = np.abs(self.train_y - predictions)\n\n        # Assign weights based on errors (higher errors get higher weights)\n        weights = compute_sample_weight('balanced', errors)\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value',\n  'SWE',\n  'cumulative_SWE',\n#   'cumulative_relative_humidity_rmin',\n#   'cumulative_air_temperature_tmmx', \n#   'cumulative_air_temperature_tmmn',\n#   'cumulative_relative_humidity_rmax',\n#   'cumulative_potential_evapotranspiration',\n#   'cumulative_wind_speed',\n  'cumulative_fsca',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'Elevation',\t\n  'Slope',\t\n  'Curvature',\t\n  'Aspect',\t\n  'Eastness',\t\n  'Northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n#   hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n#   hole.post_processing()\n\n",
  "history_output" : "today date = 2023-12-28\n2023-12-25\ntest start date:  2023-12-25\ntest end date:  2023-10-11\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_all_active_stations_v1.csv_sorted.csv_cleaned_nodata.csv_time_series_cumulative_v1.csv\ndata.shape =  (1075041, 107)\n         date       lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2018-10-01  32.92342  ...               3.500000              0.0\n1  2018-10-02  32.92342  ...               8.700000              0.0\n2  2018-10-03  32.92342  ...              13.100000              0.0\n3  2018-10-04  32.92342  ...              18.100000              0.0\n4  2018-10-05  32.92342  ...              22.800001              0.0\n[5 rows x 107 columns]\nrequired features: Index(['SWE', 'cumulative_SWE', 'cumulative_fsca', 'fsca',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'Elevation',\n       'Slope', 'Curvature', 'Aspect', 'Eastness', 'Northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    1.075041e+06\nmean     6.038476e+00\nstd      1.005967e+01\nmin     -8.000000e+00\n25%      0.000000e+00\n50%      7.000000e-01\n75%      8.700000e+00\nmax      1.021000e+02\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'cumulative_SWE', 'cumulative_fsca', 'fsca',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'Elevation',\n       'Slope', 'Curvature', 'Aspect', 'Eastness', 'Northness'],\n      dtype='object')\ntraining data row number:  1075041\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.8666319547088731\nMSE is 4.43375011015278\nR2 score is 0.9559921289471118\nRMSE is 2.1056471950810707\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232812042431.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-15.png\n",
  "history_begin_time" : 1703737380308,
  "history_end_time" : 1703737496840,
  "history_notes" : "Use the Elevation from DEM in meters",
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "at1yfwebo1h",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703736166896,
  "history_end_time" : 1703737316876,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "fSKperZKNRN4",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for SWE prediction.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for SWE prediction.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import compute_sample_weight\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_all_active_stations_v1.csv_sorted.csv_cleaned_nodata.csv_time_series_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(\"data.shape = \", data.shape)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        #scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        #X_train_scaled = scaler.fit_transform(X_train)\n        #X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=10, columns=X.columns)\n\n        self.train_x, self.train_y = X_train, y_train\n        self.test_x, self.test_y = X_test, y_test\n        #self.train_x, self.train_y = X_train_scaled, y_train\n        #self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n        # Fit the classifier\n        self.classifier.fit(self.train_x, self.train_y)\n\n        # Make predictions\n        predictions = self.classifier.predict(self.train_x)\n\n        # Calculate absolute errors\n        errors = np.abs(self.train_y - predictions)\n\n        # Assign weights based on errors (higher errors get higher weights)\n        weights = compute_sample_weight('balanced', errors)\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value',\n  'SWE',\n  'cumulative_SWE',\n#   'cumulative_relative_humidity_rmin',\n#   'cumulative_air_temperature_tmmx', \n#   'cumulative_air_temperature_tmmn',\n#   'cumulative_relative_humidity_rmax',\n#   'cumulative_potential_evapotranspiration',\n#   'cumulative_wind_speed',\n  'cumulative_fsca',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'Elevation',\n  'Slope',\t\n  'Curvature',\t\n  'Aspect',\t\n  'Eastness',\t\n  'Northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n#   hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n#   hole.post_processing()\n\n",
  "history_output" : "today date = 2023-12-27\ntest start date:  2023-05-10\ntest end date:  2023-12-11\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_all_active_stations_v1.csv_sorted.csv_cleaned_nodata.csv_time_series_cumulative_v1.csv\ndata.shape =  (1075041, 107)\n         date       lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2018-10-01  32.92342  ...               3.500000              0.0\n1  2018-10-02  32.92342  ...               8.700000              0.0\n2  2018-10-03  32.92342  ...              13.100000              0.0\n3  2018-10-04  32.92342  ...              18.100000              0.0\n4  2018-10-05  32.92342  ...              22.800001              0.0\n[5 rows x 107 columns]\nrequired features: Index(['SWE', 'cumulative_SWE', 'cumulative_fsca', 'fsca',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'Elevation',\n       'Slope', 'Curvature', 'Aspect', 'Eastness', 'Northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    1.075041e+06\nmean     6.038476e+00\nstd      1.005967e+01\nmin     -8.000000e+00\n25%      0.000000e+00\n50%      7.000000e-01\n75%      8.700000e+00\nmax      1.021000e+02\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'cumulative_SWE', 'cumulative_fsca', 'fsca',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'Elevation',\n       'Slope', 'Curvature', 'Aspect', 'Eastness', 'Northness'],\n      dtype='object')\ntraining data row number:  1075041\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.8547305659297985\nMSE is 4.338169262321434\nR2 score is 0.9569408314048475\nRMSE is 2.082827228149621\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232712164836.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-15.png\n",
  "history_begin_time" : 1703695603403,
  "history_end_time" : 1703695736761,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "1robludpwoe",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703694763550,
  "history_end_time" : 1703694763550,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "lbhpe0abl4z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703659541193,
  "history_end_time" : 1703659541193,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xyvk3plrsm5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703658144678,
  "history_end_time" : 1703658144678,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "nmdr3ftiwkt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703650855763,
  "history_end_time" : 1703650855763,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "182q05c5luj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703646751524,
  "history_end_time" : 1703650812434,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "czcngyl176f",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703642120887,
  "history_end_time" : 1703646749620,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "1cf9tkhsgq6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703641988920,
  "history_end_time" : 1703642074628,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "3lb59wg3xdy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703629665497,
  "history_end_time" : 1703629665497,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "nc7xgprr935",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703626687972,
  "history_end_time" : 1703627783050,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ag3yozz0ekv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703625782085,
  "history_end_time" : 1703625782085,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ro8u4b3vg3p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703624783851,
  "history_end_time" : 1703624783851,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0fRB5uMHGAre",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for SWE prediction.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for SWE prediction.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import compute_sample_weight\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_all_active_stations_v1.csv_sorted.csv_cleaned_nodata.csv_time_series_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(\"data.shape = \", data.shape)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        #scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        #X_train_scaled = scaler.fit_transform(X_train)\n        #X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=10, columns=X.columns)\n\n        self.train_x, self.train_y = X_train, y_train\n        self.test_x, self.test_y = X_test, y_test\n        #self.train_x, self.train_y = X_train_scaled, y_train\n        #self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n        # Fit the classifier\n        self.classifier.fit(self.train_x, self.train_y)\n\n        # Make predictions\n        predictions = self.classifier.predict(self.train_x)\n\n        # Calculate absolute errors\n        errors = np.abs(self.train_y - predictions)\n\n        # Assign weights based on errors (higher errors get higher weights)\n        weights = compute_sample_weight('balanced', errors)\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value',\n  'SWE',\n  'cumulative_SWE',\n#   'cumulative_relative_humidity_rmin',\n#   'cumulative_air_temperature_tmmx', \n#   'cumulative_air_temperature_tmmn',\n#   'cumulative_relative_humidity_rmax',\n#   'cumulative_potential_evapotranspiration',\n#   'cumulative_wind_speed',\n  'cumulative_fsca',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t\n  'Slope',\t\n  'Curvature',\t\n  'Aspect',\t\n  'Eastness',\t\n  'Northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n#   hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n#   hole.post_processing()\n\n",
  "history_output" : "today date = 2023-12-26\ntest start date:  2023-01-20\ntest end date:  2023-10-11\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_all_active_stations_v1.csv_sorted.csv_cleaned_nodata.csv_time_series_cumulative_v1.csv\ndata.shape =  (1075041, 107)\n         date       lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2018-10-01  32.92342  ...               3.500000              0.0\n1  2018-10-02  32.92342  ...               8.700000              0.0\n2  2018-10-03  32.92342  ...              13.100000              0.0\n3  2018-10-04  32.92342  ...              18.100000              0.0\n4  2018-10-05  32.92342  ...              22.800001              0.0\n[5 rows x 107 columns]\nrequired features: Index(['SWE', 'cumulative_SWE', 'cumulative_fsca', 'fsca',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'elevation',\n       'Slope', 'Curvature', 'Aspect', 'Eastness', 'Northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    1.075041e+06\nmean     6.038476e+00\nstd      1.005967e+01\nmin     -8.000000e+00\n25%      0.000000e+00\n50%      7.000000e-01\n75%      8.700000e+00\nmax      1.021000e+02\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'cumulative_SWE', 'cumulative_fsca', 'fsca',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'elevation',\n       'Slope', 'Curvature', 'Aspect', 'Eastness', 'Northness'],\n      dtype='object')\ntraining data row number:  1075041\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.8809877054758952\nMSE is 4.55758745380442\nR2 score is 0.9547629622788159\nRMSE is 2.1348506865362786\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232612204214.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-15.png\n",
  "history_begin_time" : 1703623245280,
  "history_end_time" : 1703623382473,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "znROnOYuTbYv",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for SWE prediction.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for SWE prediction.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import compute_sample_weight\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_all_active_stations_v1.csv_sorted.csv_time_series_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(\"data.shape = \", data.shape)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        #scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        #X_train_scaled = scaler.fit_transform(X_train)\n        #X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=10, columns=X.columns)\n\n        self.train_x, self.train_y = X_train, y_train\n        self.test_x, self.test_y = X_test, y_test\n        #self.train_x, self.train_y = X_train_scaled, y_train\n        #self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n        # Fit the classifier\n        self.classifier.fit(self.train_x, self.train_y)\n\n        # Make predictions\n        predictions = self.classifier.predict(self.train_x)\n\n        # Calculate absolute errors\n        errors = np.abs(self.train_y - predictions)\n\n        # Assign weights based on errors (higher errors get higher weights)\n        weights = compute_sample_weight('balanced', errors)\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value',\n  'SWE',\n  'cumulative_SWE',\n#   'cumulative_relative_humidity_rmin',\n#   'cumulative_air_temperature_tmmx', \n#   'cumulative_air_temperature_tmmn',\n#   'cumulative_relative_humidity_rmax',\n#   'cumulative_potential_evapotranspiration',\n#   'cumulative_wind_speed',\n  'cumulative_fsca',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t\n  'Slope',\t\n  'Curvature',\t\n  'Aspect',\t\n  'Eastness',\t\n  'Northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n#   hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n#   hole.post_processing()\n\n",
  "history_output" : "today date = 2023-12-26\ntest start date:  2023-01-20\ntest end date:  2023-10-11\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_all_active_stations_v1.csv_sorted.csv_time_series_cumulative_v1.csv\n/home/chetana/gw-workspace/znROnOYuTbYv/model_creation_et.py:99: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n  data = pd.read_csv(training_data_path)\ndata.shape =  (4373449, 107)\n         date       lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2018-10-01  31.41666  ...                    4.5              0.0\n1  2018-10-02  31.41666  ...                   11.4              0.0\n2  2018-10-03  31.41666  ...                   15.0              0.0\n3  2018-10-04  31.41666  ...                   18.6              0.0\n4  2018-10-05  31.41666  ...                   21.0              0.0\n[5 rows x 107 columns]\nrequired features: Index(['SWE', 'cumulative_SWE', 'cumulative_fsca', 'fsca',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'elevation',\n       'Slope', 'Curvature', 'Aspect', 'Eastness', 'Northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    4.373449e+06\nmean     1.484323e+00\nstd      5.624509e+00\nmin     -8.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      0.000000e+00\nmax      1.021000e+02\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'cumulative_SWE', 'cumulative_fsca', 'fsca',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'elevation',\n       'Slope', 'Curvature', 'Aspect', 'Eastness', 'Northness'],\n      dtype='object')\ntraining data row number:  4373449\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.4677045638205191\nMSE is 3.0011413113194227\nR2 score is 0.9050113525076955\nRMSE is 1.7323802444381033\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232612203502.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-15.png\n",
  "history_begin_time" : 1703622325699,
  "history_end_time" : 1703622954880,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "0HDKBmOAuMWy",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for SWE prediction.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for SWE prediction.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import compute_sample_weight\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_all_active_stations_v1.csv_sorted.csv_time_series_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        #scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        #X_train_scaled = scaler.fit_transform(X_train)\n        #X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=10, columns=X.columns)\n\n        self.train_x, self.train_y = X_train, y_train\n        self.test_x, self.test_y = X_test, y_test\n        #self.train_x, self.train_y = X_train_scaled, y_train\n        #self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n        # Fit the classifier\n        self.classifier.fit(self.train_x, self.train_y)\n\n        # Make predictions\n        predictions = self.classifier.predict(self.train_x)\n\n        # Calculate absolute errors\n        errors = np.abs(self.train_y - predictions)\n\n        # Assign weights based on errors (higher errors get higher weights)\n        weights = compute_sample_weight('balanced', errors)\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value',\n  'SWE',\n  'cumulative_SWE',\n#   'cumulative_relative_humidity_rmin',\n#   'cumulative_air_temperature_tmmx', \n#   'cumulative_air_temperature_tmmn',\n#   'cumulative_relative_humidity_rmax',\n#   'cumulative_potential_evapotranspiration',\n#   'cumulative_wind_speed',\n  'cumulative_fsca',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t\n  'slope',\t\n  'curvature',\t\n  'aspect',\t\n  'eastness',\t\n  'northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  #hole.preprocessing(chosen_columns = selected_columns)\n  hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  #hole.post_processing(chosen_columns = selected_columns)\n  hole.post_processing()\n\n",
  "history_output" : "today date = 2023-12-26\ntest start date:  2023-01-20\ntest end date:  2023-10-11\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_all_active_stations_v1.csv_sorted.csv_time_series_cumulative_v1.csv\n/home/chetana/gw-workspace/0HDKBmOAuMWy/model_creation_et.py:99: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n  data = pd.read_csv(training_data_path)\n         date       lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  31.41666  ...                    5.1              0.0\n1  2019-10-02  31.41666  ...                    7.3              0.0\n2  2019-10-03  31.41666  ...                   10.2              0.0\n3  2019-10-04  31.41666  ...                   13.4              0.0\n4  2019-10-05  31.41666  ...                   15.8              0.0\n[5 rows x 107 columns]\nall non-numeric columns are dropped:  Index(['station_name', 'stationTriplet'], dtype='object')\nrequired features: Index(['SWE', 'change_in_swe_inch', 'snow_depth', 'air_temperature_observed_f',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       ...\n       'cumulative_SWE', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n       'cumulative_fsca'],\n      dtype='object', length=101)\ndescribe the statistics of swe_value:  count    3.288946e+06\nmean     1.207488e+00\nstd      4.703094e+00\nmin     -8.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      0.000000e+00\nmax      1.021000e+02\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'change_in_swe_inch', 'snow_depth', 'air_temperature_observed_f',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       ...\n       'cumulative_SWE', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n       'cumulative_fsca'],\n      dtype='object', length=101)\ntraining data row number:  3288946\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.12879506681463687\nMSE is 0.41654204942686857\nR2 score is 0.9812685910917338\nRMSE is 0.6454006890505065\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232612201219.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1703618135610,
  "history_end_time" : 1703621575158,
  "history_notes" : "Train a new model with the latest training data including all empty stations. This takes much longer than usual. Should remove all the empty rows.",
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "fsX54116fTH3",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for SWE prediction.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for SWE prediction.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import compute_sample_weight\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_time_series_cumulative_v3.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        #scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        #X_train_scaled = scaler.fit_transform(X_train)\n        #X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=10, columns=X.columns)\n\n        self.train_x, self.train_y = X_train, y_train\n        self.test_x, self.test_y = X_test, y_test\n        #self.train_x, self.train_y = X_train_scaled, y_train\n        #self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n        # Fit the classifier\n        self.classifier.fit(self.train_x, self.train_y)\n\n        # Make predictions\n        predictions = self.classifier.predict(self.train_x)\n\n        # Calculate absolute errors\n        errors = np.abs(self.train_y - predictions)\n\n        # Assign weights based on errors (higher errors get higher weights)\n        weights = compute_sample_weight('balanced', errors)\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value',\n  'SWE',\n  'cumulative_SWE',\n#   'cumulative_relative_humidity_rmin',\n#   'cumulative_air_temperature_tmmx', \n#   'cumulative_air_temperature_tmmn',\n#   'cumulative_relative_humidity_rmax',\n#   'cumulative_potential_evapotranspiration',\n#   'cumulative_wind_speed',\n  'cumulative_fsca',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t\n  'slope',\t\n  'curvature',\t\n  'aspect',\t\n  'eastness',\t\n  'northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  #hole.preprocessing(chosen_columns = selected_columns)\n  hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  #hole.post_processing(chosen_columns = selected_columns)\n  hole.post_processing()\n\n",
  "history_output" : "today date = 2023-12-18\ntest start date:  2023-01-20\ntest end date:  2023-10-11\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_time_series_cumulative_v3.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    2.8              0.0\n1  2019-10-01  33.358254  ...                    5.6              0.0\n2  2019-10-01  33.358254  ...                    8.4              0.0\n3  2019-10-02  33.358254  ...                   11.2              0.0\n4  2019-10-02  33.358254  ...                   14.0              0.0\n[5 rows x 112 columns]\nall non-numeric columns are dropped:  Index(['cell_id', 'station_id'], dtype='object')\nrequired features: Index(['SWE', 'Flag', 'potential_evapotranspiration', 'precipitation_amount',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'air_temperature_tmmn', 'air_temperature_tmmx',\n       'mean_vapor_pressure_deficit', 'wind_speed',\n       ...\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n       'cumulative_fsca'],\n      dtype='object', length=106)\ndescribe the statistics of swe_value:  count    797888.000000\nmean          2.378197\nstd           3.859433\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           3.700000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'Flag', 'potential_evapotranspiration', 'precipitation_amount',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'air_temperature_tmmn', 'air_temperature_tmmx',\n       'mean_vapor_pressure_deficit', 'wind_speed',\n       ...\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n       'cumulative_fsca'],\n      dtype='object', length=106)\ntraining data row number:  797888\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.11421289902116886\nMSE is 0.0809615241950645\nR2 score is 0.9945880349179462\nRMSE is 0.2845373862870475\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231812150222.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1702911303325,
  "history_end_time" : 1702911747005,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "rzx3o7vwhg8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702875592816,
  "history_end_time" : 1702875592816,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ec0ojfr21ep",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702871264368,
  "history_end_time" : 1702871264368,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "NK5VKD5YNCXK",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for SWE prediction.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for SWE prediction.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import compute_sample_weight\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_time_series_cumulative_v3.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        #scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        #X_train_scaled = scaler.fit_transform(X_train)\n        #X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=10, columns=X.columns)\n\n        self.train_x, self.train_y = X_train, y_train\n        self.test_x, self.test_y = X_test, y_test\n        #self.train_x, self.train_y = X_train_scaled, y_train\n        #self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n        # Fit the classifier\n        self.classifier.fit(self.train_x, self.train_y)\n\n        # Make predictions\n        predictions = self.classifier.predict(self.train_x)\n\n        # Calculate absolute errors\n        errors = np.abs(self.train_y - predictions)\n\n        # Assign weights based on errors (higher errors get higher weights)\n        weights = compute_sample_weight('balanced', errors)\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value',\n  'SWE',\n  'cumulative_SWE',\n#   'cumulative_relative_humidity_rmin',\n#   'cumulative_air_temperature_tmmx', \n#   'cumulative_air_temperature_tmmn',\n#   'cumulative_relative_humidity_rmax',\n#   'cumulative_potential_evapotranspiration',\n#   'cumulative_wind_speed',\n  'cumulative_fsca',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t\n  'slope',\t\n  'curvature',\t\n  'aspect',\t\n  'eastness',\t\n  'northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n  #hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n  #hole.post_processing()\n\n",
  "history_output" : "today date = 2023-12-18\ntest start date:  2022-12-23\ntest end date:  2023-10-11\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_time_series_cumulative_v3.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    2.8              0.0\n1  2019-10-01  33.358254  ...                    5.6              0.0\n2  2019-10-01  33.358254  ...                    8.4              0.0\n3  2019-10-02  33.358254  ...                   11.2              0.0\n4  2019-10-02  33.358254  ...                   14.0              0.0\n[5 rows x 112 columns]\nrequired features: Index(['SWE', 'cumulative_SWE', 'cumulative_fsca', 'fsca',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    797888.000000\nmean          2.378197\nstd           3.859433\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           3.700000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'cumulative_SWE', 'cumulative_fsca', 'fsca',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  797888\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.37815319393110947\nMSE is 1.441932540982853\nR2 score is 0.9036123808183935\nRMSE is 1.2008049554290043\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231812031747.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-15.png\n",
  "history_begin_time" : 1702869415931,
  "history_end_time" : 1702869476675,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "gJEpED3bqSkF",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for SWE prediction.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for SWE prediction.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import compute_sample_weight\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_time_series_cumulative_v3.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        #scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        #X_train_scaled = scaler.fit_transform(X_train)\n        #X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=10, columns=X.columns)\n\n        self.train_x, self.train_y = X_train, y_train\n        self.test_x, self.test_y = X_test, y_test\n        #self.train_x, self.train_y = X_train_scaled, y_train\n        #self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n        # Fit the classifier\n#         self.classifier.fit(self.train_x, self.train_y)\n\n#         # Make predictions\n#         predictions = self.classifier.predict(self.train_x)\n\n#         # Calculate absolute errors\n#         errors = np.abs(self.train_y - predictions)\n\n#         # Assign weights based on errors (higher errors get higher weights)\n#         weights = compute_sample_weight('balanced', errors)\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value',\n  'SWE',\n  'cumulative_SWE',\n#   'cumulative_relative_humidity_rmin',\n#   'cumulative_air_temperature_tmmx', \n#   'cumulative_air_temperature_tmmn',\n#   'cumulative_relative_humidity_rmax',\n#   'cumulative_potential_evapotranspiration',\n#   'cumulative_wind_speed',\n  'cumulative_fsca',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t\n  'slope',\t\n  'curvature',\t\n  'aspect',\t\n  'eastness',\t\n  'northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n  #hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n  #hole.post_processing()\n\n",
  "history_output" : "today date = 2023-12-18\ntest start date:  2022-12-23\ntest end date:  2023-10-11\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_time_series_cumulative_v3.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    2.8              0.0\n1  2019-10-01  33.358254  ...                    5.6              0.0\n2  2019-10-01  33.358254  ...                    8.4              0.0\n3  2019-10-02  33.358254  ...                   11.2              0.0\n4  2019-10-02  33.358254  ...                   14.0              0.0\n[5 rows x 112 columns]\nrequired features: Index(['SWE', 'cumulative_SWE', 'cumulative_fsca', 'fsca',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    797888.000000\nmean          2.378197\nstd           3.859433\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           3.700000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'cumulative_SWE', 'cumulative_fsca', 'fsca',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  797888\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 1.955903307503866\nMSE is 7.5998777307726355\nR2 score is 0.49197753728397486\nRMSE is 2.756787574473709\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231812031614.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-15.png\n",
  "history_begin_time" : 1702869352347,
  "history_end_time" : 1702869380114,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "6g96fPWmSos7",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for SWE prediction.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for SWE prediction.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import compute_sample_weight\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_time_series_cumulative_v3.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        #scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        #X_train_scaled = scaler.fit_transform(X_train)\n        #X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=1.70, columns=X.columns)\n\n        self.train_x, self.train_y = X_train, y_train\n        self.test_x, self.test_y = X_test, y_test\n        #self.train_x, self.train_y = X_train_scaled, y_train\n        #self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n        # Fit the classifier\n#         self.classifier.fit(self.train_x, self.train_y)\n\n#         # Make predictions\n#         predictions = self.classifier.predict(self.train_x)\n\n#         # Calculate absolute errors\n#         errors = np.abs(self.train_y - predictions)\n\n#         # Assign weights based on errors (higher errors get higher weights)\n#         weights = compute_sample_weight('balanced', errors)\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value',\n  'SWE',\n  'cumulative_SWE',\n#   'cumulative_relative_humidity_rmin',\n#   'cumulative_air_temperature_tmmx', \n#   'cumulative_air_temperature_tmmn',\n#   'cumulative_relative_humidity_rmax',\n#   'cumulative_potential_evapotranspiration',\n#   'cumulative_wind_speed',\n  'cumulative_fsca',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t\n  'slope',\t\n  'curvature',\t\n  'aspect',\t\n  'eastness',\t\n  'northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n  #hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n  #hole.post_processing()\n\n",
  "history_output" : "today date = 2023-12-18\ntest start date:  2022-12-23\ntest end date:  2023-10-11\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_time_series_cumulative_v3.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    2.8              0.0\n1  2019-10-01  33.358254  ...                    5.6              0.0\n2  2019-10-01  33.358254  ...                    8.4              0.0\n3  2019-10-02  33.358254  ...                   11.2              0.0\n4  2019-10-02  33.358254  ...                   14.0              0.0\n[5 rows x 112 columns]\nrequired features: Index(['SWE', 'cumulative_SWE', 'cumulative_fsca', 'fsca',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    797888.000000\nmean          2.378197\nstd           3.859433\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           3.700000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'cumulative_SWE', 'cumulative_fsca', 'fsca',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  797888\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 1.9526208557119014\nMSE is 7.638779388408168\nR2 score is 0.4893771117750595\nRMSE is 2.7638341825095383\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231812031421.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-15.png\n",
  "history_begin_time" : 1702869239009,
  "history_end_time" : 1702869267879,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "o7l5zh9fkqw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702867996381,
  "history_end_time" : 1702867996381,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wwmt056gcz4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702866593379,
  "history_end_time" : 1702866593379,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "kectv26e8ys",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702866137600,
  "history_end_time" : 1702866137600,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xydnu4uzqdf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702657305605,
  "history_end_time" : 1702657305605,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "80zxmn1nud9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702633223036,
  "history_end_time" : 1702633223036,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "si04fx7n0mv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702633156905,
  "history_end_time" : 1702633163895,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "JYQ4rPmij2tT",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for SWE prediction.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for SWE prediction.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import compute_sample_weight\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_time_series_cumulative_v3.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        #scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        #X_train_scaled = scaler.fit_transform(X_train)\n        #X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=1.70, columns=X.columns)\n\n        self.train_x, self.train_y = X_train, y_train\n        self.test_x, self.test_y = X_test, y_test\n        #self.train_x, self.train_y = X_train_scaled, y_train\n        #self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n        # Fit the classifier\n#         self.classifier.fit(self.train_x, self.train_y)\n\n#         # Make predictions\n#         predictions = self.classifier.predict(self.train_x)\n\n#         # Calculate absolute errors\n#         errors = np.abs(self.train_y - predictions)\n\n#         # Assign weights based on errors (higher errors get higher weights)\n#         weights = compute_sample_weight('balanced', errors)\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value',\n  'SWE',\n  'cumulative_SWE',\n#   'cumulative_relative_humidity_rmin',\n#   'cumulative_air_temperature_tmmx', \n#   'cumulative_air_temperature_tmmn',\n#   'cumulative_relative_humidity_rmax',\n#   'cumulative_potential_evapotranspiration',\n#   'cumulative_wind_speed',\n  'cumulative_fsca',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t\n  'slope',\t\n  'curvature',\t\n  'aspect',\t\n  'eastness',\t\n  'northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n  #hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n  #hole.post_processing()\n\n",
  "history_output" : "today date = 2023-12-15\ntest start date:  2022-10-04\ntest end date:  2023-10-11\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_time_series_cumulative_v3.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    2.8              0.0\n1  2019-10-01  33.358254  ...                    5.6              0.0\n2  2019-10-01  33.358254  ...                    8.4              0.0\n3  2019-10-02  33.358254  ...                   11.2              0.0\n4  2019-10-02  33.358254  ...                   14.0              0.0\n[5 rows x 112 columns]\nrequired features: Index(['SWE', 'cumulative_SWE', 'cumulative_fsca', 'fsca',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    797888.000000\nmean          2.378197\nstd           3.859433\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           3.700000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'cumulative_SWE', 'cumulative_fsca', 'fsca',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  797888\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 1.9526208557119014\nMSE is 7.638779388408168\nR2 score is 0.4893771117750595\nRMSE is 2.7638341825095383\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231512092308.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-15.png\n",
  "history_begin_time" : 1702632166277,
  "history_end_time" : 1702632194670,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "pxRU8Xj8e81M",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for SWE prediction.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for SWE prediction.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import compute_sample_weight\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_time_series_cumulative_v3.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        #scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        #X_train_scaled = scaler.fit_transform(X_train)\n        #X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=1.70, columns=X.columns)\n\n        self.train_x, self.train_y = X_train, y_train\n        self.test_x, self.test_y = X_test, y_test\n        #self.train_x, self.train_y = X_train_scaled, y_train\n        #self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n        # Fit the classifier\n#         self.classifier.fit(self.train_x, self.train_y)\n\n#         # Make predictions\n#         predictions = self.classifier.predict(self.train_x)\n\n#         # Calculate absolute errors\n#         errors = np.abs(self.train_y - predictions)\n\n#         # Assign weights based on errors (higher errors get higher weights)\n#         weights = compute_sample_weight('balanced', errors)\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value',\n  'SWE',\n  'cumulative_SWE',\n#   'cumulative_relative_humidity_rmin',\n#   'cumulative_air_temperature_tmmx', \n#   'cumulative_air_temperature_tmmn',\n#   'cumulative_relative_humidity_rmax',\n#   'cumulative_potential_evapotranspiration',\n#   'cumulative_wind_speed',\n  'cumulative_fsca',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t\n  'eastness',\t'northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  #hole.preprocessing(chosen_columns = selected_columns)\n  hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  #hole.post_processing(chosen_columns = selected_columns)\n  hole.post_processing()\n\n",
  "history_output" : "today date = 2023-12-15\ntest start date:  2022-10-04\ntest end date:  2023-10-11\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_time_series_cumulative_v3.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    2.8              0.0\n1  2019-10-01  33.358254  ...                    5.6              0.0\n2  2019-10-01  33.358254  ...                    8.4              0.0\n3  2019-10-02  33.358254  ...                   11.2              0.0\n4  2019-10-02  33.358254  ...                   14.0              0.0\n[5 rows x 112 columns]\nall non-numeric columns are dropped:  Index(['cell_id', 'station_id'], dtype='object')\nrequired features: Index(['SWE', 'Flag', 'potential_evapotranspiration', 'precipitation_amount',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'air_temperature_tmmn', 'air_temperature_tmmx',\n       'mean_vapor_pressure_deficit', 'wind_speed',\n       ...\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n       'cumulative_fsca'],\n      dtype='object', length=106)\ndescribe the statistics of swe_value:  count    797888.000000\nmean          2.378197\nstd           3.859433\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           3.700000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'Flag', 'potential_evapotranspiration', 'precipitation_amount',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'air_temperature_tmmn', 'air_temperature_tmmx',\n       'mean_vapor_pressure_deficit', 'wind_speed',\n       ...\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n       'cumulative_fsca'],\n      dtype='object', length=106)\ntraining data row number:  797888\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 2.3961203486696174\nMSE is 12.114515124885639\nR2 score is 0.1901914706555089\nRMSE is 3.4805912033569295\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231512091926.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1702631866233,
  "history_end_time" : 1702631970004,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "oa9am1r66at",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702274520874,
  "history_end_time" : 1702274520874,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wicr6d4c6x3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702257109191,
  "history_end_time" : 1702257109191,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ca7psga1uig",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702253506499,
  "history_end_time" : 1702253506499,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "qbgay75jdni",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702047800921,
  "history_end_time" : 1702047800921,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "oimblktyzfn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702046671839,
  "history_end_time" : 1702047789480,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "nLtRf6rDx2fa",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for SWE prediction.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for SWE prediction.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import compute_sample_weight\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_time_series_cumulative_v3.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        #scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        #X_train_scaled = scaler.fit_transform(X_train)\n        #X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=1.70, columns=X.columns)\n\n        self.train_x, self.train_y = X_train, y_train\n        self.test_x, self.test_y = X_test, y_test\n        #self.train_x, self.train_y = X_train_scaled, y_train\n        #self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n        # Fit the classifier\n#         self.classifier.fit(self.train_x, self.train_y)\n\n#         # Make predictions\n#         predictions = self.classifier.predict(self.train_x)\n\n#         # Calculate absolute errors\n#         errors = np.abs(self.train_y - predictions)\n\n#         # Assign weights based on errors (higher errors get higher weights)\n#         weights = compute_sample_weight('balanced', errors)\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value',\n  'SWE',\n  'cumulative_SWE',\n#   'cumulative_relative_humidity_rmin',\n#   'cumulative_air_temperature_tmmx', \n#   'cumulative_air_temperature_tmmn',\n#   'cumulative_relative_humidity_rmax',\n#   'cumulative_potential_evapotranspiration',\n#   'cumulative_wind_speed',\n  'cumulative_fsca',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t\n  'eastness',\t'northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n  # hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n  # hole.post_processing()\n\n",
  "history_output" : "today date = 2023-12-08\ntest start date:  2022-10-05\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_time_series_cumulative_v3.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    2.8              0.0\n1  2019-10-01  33.358254  ...                    5.6              0.0\n2  2019-10-01  33.358254  ...                    8.4              0.0\n3  2019-10-02  33.358254  ...                   11.2              0.0\n4  2019-10-02  33.358254  ...                   14.0              0.0\n[5 rows x 112 columns]\nrequired features: Index(['SWE', 'cumulative_SWE', 'cumulative_fsca', 'fsca',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    797888.000000\nmean          2.378197\nstd           3.859433\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           3.700000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'cumulative_SWE', 'cumulative_fsca', 'fsca',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  797888\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 1.9526208557119014\nMSE is 7.638779388408168\nR2 score is 0.4893771117750595\nRMSE is 2.7638341825095383\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230812134934.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-15.png\n",
  "history_begin_time" : 1702043352326,
  "history_end_time" : 1702043379211,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "zywcouxQTa7l",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for SWE prediction.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for SWE prediction.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import compute_sample_weight\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_time_series_cumulative_v3.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=1.70, columns=X.columns)\n\n        self.train_x, self.train_y = X_train_scaled, y_train\n        self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n        # Fit the classifier\n#         self.classifier.fit(self.train_x, self.train_y)\n\n#         # Make predictions\n#         predictions = self.classifier.predict(self.train_x)\n\n#         # Calculate absolute errors\n#         errors = np.abs(self.train_y - predictions)\n\n#         # Assign weights based on errors (higher errors get higher weights)\n#         weights = compute_sample_weight('balanced', errors)\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value',\n  'SWE',\n  'cumulative_SWE',\n#   'cumulative_relative_humidity_rmin',\n#   'cumulative_air_temperature_tmmx', \n#   'cumulative_air_temperature_tmmn',\n#   'cumulative_relative_humidity_rmax',\n#   'cumulative_potential_evapotranspiration',\n#   'cumulative_wind_speed',\n  'cumulative_fsca',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t\n  'eastness',\t'northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n  # hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n  # hole.post_processing()\n\n",
  "history_output" : "today date = 2023-12-08\ntest start date:  2022-10-05\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_time_series_cumulative_v3.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    2.8              0.0\n1  2019-10-01  33.358254  ...                    5.6              0.0\n2  2019-10-01  33.358254  ...                    8.4              0.0\n3  2019-10-02  33.358254  ...                   11.2              0.0\n4  2019-10-02  33.358254  ...                   14.0              0.0\n[5 rows x 112 columns]\nrequired features: Index(['SWE', 'cumulative_SWE', 'cumulative_fsca', 'fsca',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    797888.000000\nmean          2.378197\nstd           3.859433\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           3.700000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'cumulative_SWE', 'cumulative_fsca', 'fsca',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  797888\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 1.9806521378142423\nMSE is 7.831832016483281\nR2 score is 0.476472289484116\nRMSE is 2.7985410514200577\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230812062120.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-15.png\n",
  "history_begin_time" : 1702016458687,
  "history_end_time" : 1702016486576,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "sce20upujq8",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for SWE prediction.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for SWE prediction.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import compute_sample_weight\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_time_series_cumulative_v3.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=1.70, columns=X.columns)\n\n        self.train_x, self.train_y = X_train_scaled, y_train\n        self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n        # Fit the classifier\n#         self.classifier.fit(self.train_x, self.train_y)\n\n#         # Make predictions\n#         predictions = self.classifier.predict(self.train_x)\n\n#         # Calculate absolute errors\n#         errors = np.abs(self.train_y - predictions)\n\n#         # Assign weights based on errors (higher errors get higher weights)\n#         weights = compute_sample_weight('balanced', errors)\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value',\n  'SWE',\n  'cumulative_SWE',\n  'cumulative_relative_humidity_rmin',\n  'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n  'cumulative_relative_humidity_rmax',\n  'cumulative_potential_evapotranspiration',\n  'cumulative_fsca',\n  'cumulative_wind_speed',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t\n  'eastness',\t'northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n  # hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n  # hole.post_processing()\n\n",
  "history_output" : "today date = 2023-12-06\ntest start date:  2022-10-05\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_time_series_cumulative_v3.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    2.8              0.0\n1  2019-10-01  33.358254  ...                    5.6              0.0\n2  2019-10-01  33.358254  ...                    8.4              0.0\n3  2019-10-02  33.358254  ...                   11.2              0.0\n4  2019-10-02  33.358254  ...                   14.0              0.0\n[5 rows x 112 columns]\nrequired features: Index(['SWE', 'cumulative_SWE', 'cumulative_relative_humidity_rmin',\n       'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    797888.000000\nmean          2.378197\nstd           3.859433\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           3.700000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'cumulative_SWE', 'cumulative_relative_humidity_rmin',\n       'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  797888\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 2.150261910932204\nMSE is 9.492780630358052\nR2 score is 0.36544429204031437\nRMSE is 3.0810356425004324\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230612045731.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-21.png\n",
  "history_begin_time" : 1701838626012,
  "history_end_time" : 1701838656356,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "U10lXQOwTmR1",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import compute_sample_weight\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_time_series_cumulative_v3.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=1.70, columns=X.columns)\n\n        self.train_x, self.train_y = X_train_scaled, y_train\n        self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n        # Fit the classifier\n#         self.classifier.fit(self.train_x, self.train_y)\n\n#         # Make predictions\n#         predictions = self.classifier.predict(self.train_x)\n\n#         # Calculate absolute errors\n#         errors = np.abs(self.train_y - predictions)\n\n#         # Assign weights based on errors (higher errors get higher weights)\n#         weights = compute_sample_weight('balanced', errors)\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value',\n  'SWE',\n#   'cumulative_SWE',\n#   'cumulative_relative_humidity_rmin',\n#   'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n#   'cumulative_relative_humidity_rmax',\n#   'cumulative_potential_evapotranspiration',\n#   'cumulative_fsca',\n#   'cumulative_wind_speed',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t\n  'eastness',\t'northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n  # hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n  # hole.post_processing()\n\n",
  "history_output" : "today date = 2023-12-05\ntest start date:  2022-10-05\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_time_series_cumulative_v3.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    2.8              0.0\n1  2019-10-01  33.358254  ...                    5.6              0.0\n2  2019-10-01  33.358254  ...                    8.4              0.0\n3  2019-10-02  33.358254  ...                   11.2              0.0\n4  2019-10-02  33.358254  ...                   14.0              0.0\n[5 rows x 112 columns]\nrequired features: Index(['SWE', 'fsca', 'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    797888.000000\nmean          2.378197\nstd           3.859433\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           3.700000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'fsca', 'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  797888\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 6.301172644010014\nMSE is 49.60030505667523\nR2 score is -2.315588752741226\nRMSE is 7.042748402198905\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230512071709.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-13.png\n",
  "history_begin_time" : 1701760608016,
  "history_end_time" : 1701760631380,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "kwj6tDOipjdo",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import compute_sample_weight\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_time_series_cumulative_v3.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=1.70, columns=X.columns)\n\n        self.train_x, self.train_y = X_train_scaled, y_train\n        self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n        # Fit the classifier\n#         self.classifier.fit(self.train_x, self.train_y)\n\n#         # Make predictions\n#         predictions = self.classifier.predict(self.train_x)\n\n#         # Calculate absolute errors\n#         errors = np.abs(self.train_y - predictions)\n\n#         # Assign weights based on errors (higher errors get higher weights)\n#         weights = compute_sample_weight('balanced', errors)\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value',\n  'SWE',\n  'cumulative_SWE',\n  'cumulative_relative_humidity_rmin',\n  'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n  'cumulative_relative_humidity_rmax',\n  'cumulative_potential_evapotranspiration',\n  'cumulative_fsca',\n  'cumulative_wind_speed',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t\n  'eastness',\t'northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n  # hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n  # hole.post_processing()\n\n",
  "history_output" : "today date = 2023-12-04\ntest start date:  2022-10-05\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_time_series_cumulative_v3.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    2.8              0.0\n1  2019-10-01  33.358254  ...                    5.6              0.0\n2  2019-10-01  33.358254  ...                    8.4              0.0\n3  2019-10-02  33.358254  ...                   11.2              0.0\n4  2019-10-02  33.358254  ...                   14.0              0.0\n[5 rows x 112 columns]\nrequired features: Index(['SWE', 'cumulative_SWE', 'cumulative_relative_humidity_rmin',\n       'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    797888.000000\nmean          2.378197\nstd           3.859433\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           3.700000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'cumulative_SWE', 'cumulative_relative_humidity_rmin',\n       'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  797888\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 2.150261910932204\nMSE is 9.492780630358052\nR2 score is 0.36544429204031437\nRMSE is 3.0810356425004324\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230412053416.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-21.png\n",
  "history_begin_time" : 1701668028489,
  "history_end_time" : 1701668060750,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "0IXEJwl00fjO",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import compute_sample_weight\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_time_series_cumulative_v3.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=1.70, columns=X.columns)\n\n        self.train_x, self.train_y = X_train_scaled, y_train\n        self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n        # Fit the classifier\n#         self.classifier.fit(self.train_x, self.train_y)\n\n#         # Make predictions\n#         predictions = self.classifier.predict(self.train_x)\n\n#         # Calculate absolute errors\n#         errors = np.abs(self.train_y - predictions)\n\n#         # Assign weights based on errors (higher errors get higher weights)\n#         weights = compute_sample_weight('balanced', errors)\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value',\n  'SWE',\n  'cumulative_SWE',\n  'cumulative_relative_humidity_rmin',\n  'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n  'cumulative_relative_humidity_rmax',\n  'cumulative_potential_evapotranspiration',\n  'cumulative_fsca',\n  'cumulative_wind_speed',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t\n  'eastness',\t'northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n  # hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n  # hole.post_processing()\n\n",
  "history_output" : "today date = 2023-12-04\ntest start date:  2022-10-05\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_time_series_cumulative_v3.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    2.8              0.0\n1  2019-10-01  33.358254  ...                    5.6              0.0\n2  2019-10-01  33.358254  ...                    8.4              0.0\n3  2019-10-02  33.358254  ...                   11.2              0.0\n4  2019-10-02  33.358254  ...                   14.0              0.0\n[5 rows x 112 columns]\nrequired features: Index(['SWE', 'cumulative_SWE', 'cumulative_relative_humidity_rmin',\n       'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    797888.000000\nmean          2.378197\nstd           3.859433\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           3.700000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'cumulative_SWE', 'cumulative_relative_humidity_rmin',\n       'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  797888\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 2.150261910932204\nMSE is 9.492780630358052\nR2 score is 0.36544429204031437\nRMSE is 3.0810356425004324\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230412025515.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-21.png\n",
  "history_begin_time" : 1701658489524,
  "history_end_time" : 1701658520765,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "mbor5af5imc",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import compute_sample_weight\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=1.70, columns=X.columns)\n\n        self.train_x, self.train_y = X_train_scaled, y_train\n        self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n        #self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n        # Fit the classifier\n        self.classifier.fit(self.train_x, self.train_y)\n\n#         # Make predictions\n#         predictions = self.classifier.predict(self.train_x)\n\n#         # Calculate absolute errors\n#         errors = np.abs(self.train_y - predictions)\n\n#         # Assign weights based on errors (higher errors get higher weights)\n#         weights = compute_sample_weight('balanced', errors)\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', \n  'SWE', \n  'cumulative_SWE',\n  'cumulative_relative_humidity_rmin', \n  'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n  'cumulative_relative_humidity_rmax',\n  'cumulative_potential_evapotranspiration',\n  'cumulative_fsca',\n  'cumulative_wind_speed',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t\n  'eastness',\t'northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n  # hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n  # hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-12\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    4.9              0.0\n1  2019-10-01  33.358254  ...                    9.8              0.0\n2  2019-10-01  33.358254  ...                   14.7              0.0\n3  2019-10-02  33.358254  ...                   16.5              0.0\n4  2019-10-02  33.358254  ...                   18.3              0.0\n[5 rows x 112 columns]\nrequired features: Index(['SWE', 'cumulative_SWE', 'cumulative_relative_humidity_rmin',\n       'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'cumulative_SWE', 'cumulative_relative_humidity_rmin',\n       'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  712845\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.07089881732804498\nMSE is 0.09936662541228145\nR2 score is 0.9937526234648365\nRMSE is 0.3152247220829633\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232911154430.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-21.png\n",
  "history_begin_time" : 1701272633938,
  "history_end_time" : 1701272875106,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "52l9630p8de",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import compute_sample_weight\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=1.70, columns=X.columns)\n\n        self.train_x, self.train_y = X_train_scaled, y_train\n        self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n        # Fit the classifier\n#         self.classifier.fit(self.train_x, self.train_y)\n\n#         # Make predictions\n#         predictions = self.classifier.predict(self.train_x)\n\n#         # Calculate absolute errors\n#         errors = np.abs(self.train_y - predictions)\n\n#         # Assign weights based on errors (higher errors get higher weights)\n#         weights = compute_sample_weight('balanced', errors)\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', \n  'SWE', \n  'cumulative_SWE',\n  'cumulative_relative_humidity_rmin', \n  'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n  'cumulative_relative_humidity_rmax',\n  'cumulative_potential_evapotranspiration',\n  'cumulative_fsca',\n  'cumulative_wind_speed',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t\n  'eastness',\t'northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n  # hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n  # hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-12\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    4.9              0.0\n1  2019-10-01  33.358254  ...                    9.8              0.0\n2  2019-10-01  33.358254  ...                   14.7              0.0\n3  2019-10-02  33.358254  ...                   16.5              0.0\n4  2019-10-02  33.358254  ...                   18.3              0.0\n[5 rows x 112 columns]\nrequired features: Index(['SWE', 'cumulative_SWE', 'cumulative_relative_humidity_rmin',\n       'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'cumulative_SWE', 'cumulative_relative_humidity_rmin',\n       'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  712845\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 1.9573772737381891\nMSE is 8.450164641447078\nR2 score is 0.4687214134503537\nRMSE is 2.9069166898015975\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232911153621.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-21.png\n",
  "history_begin_time" : 1701272154432,
  "history_end_time" : 1701272363348,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "wun04cnvk5e",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import compute_sample_weight\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=1.70, columns=X.columns)\n\n        self.train_x, self.train_y = X_train_scaled, y_train\n        self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n        # Fit the classifier\n#         self.classifier.fit(self.train_x, self.train_y)\n\n#         # Make predictions\n#         predictions = self.classifier.predict(self.train_x)\n\n#         # Calculate absolute errors\n#         errors = np.abs(self.train_y - predictions)\n\n#         # Assign weights based on errors (higher errors get higher weights)\n#         weights = compute_sample_weight('balanced', errors)\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', \n  'SWE', \n  'cumulative_SWE',\n  'cumulative_relative_humidity_rmin', \n  'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n  'cumulative_relative_humidity_rmax',\n  'cumulative_potential_evapotranspiration',\n  'cumulative_fsca',\n  'cumulative_wind_speed',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t\n  'eastness',\t'northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n  # hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n  # hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-12\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    4.9              0.0\n1  2019-10-01  33.358254  ...                    9.8              0.0\n2  2019-10-01  33.358254  ...                   14.7              0.0\n3  2019-10-02  33.358254  ...                   16.5              0.0\n4  2019-10-02  33.358254  ...                   18.3              0.0\n[5 rows x 112 columns]\nrequired features: Index(['SWE', 'cumulative_SWE', 'cumulative_relative_humidity_rmin',\n       'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'cumulative_SWE', 'cumulative_relative_humidity_rmin',\n       'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  712845\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 1.9573772737381891\nMSE is 8.450164641447078\nR2 score is 0.4687214134503537\nRMSE is 2.9069166898015975\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232911145634.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-21.png\n",
  "history_begin_time" : 1701269763337,
  "history_end_time" : 1701269799889,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "3u0kgCHaKKok",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import compute_sample_weight\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=1.70, columns=X.columns)\n\n        self.train_x, self.train_y = X_train_scaled, y_train\n        self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n        # Fit the classifier\n#         self.classifier.fit(self.train_x, self.train_y)\n\n#         # Make predictions\n#         predictions = self.classifier.predict(self.train_x)\n\n#         # Calculate absolute errors\n#         errors = np.abs(self.train_y - predictions)\n\n#         # Assign weights based on errors (higher errors get higher weights)\n#         weights = compute_sample_weight('balanced', errors)\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', \n  'SWE', \n  'cumulative_SWE',\n  'cumulative_relative_humidity_rmin', \n  'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n  'cumulative_relative_humidity_rmax',\n  'cumulative_potential_evapotranspiration',\n  'cumulative_fsca',\n  'cumulative_wind_speed',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t\n  'eastness',\t'northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n  # hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n  # hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-12\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    4.9              0.0\n1  2019-10-01  33.358254  ...                    9.8              0.0\n2  2019-10-01  33.358254  ...                   14.7              0.0\n3  2019-10-02  33.358254  ...                   16.5              0.0\n4  2019-10-02  33.358254  ...                   18.3              0.0\n[5 rows x 112 columns]\nrequired features: Index(['SWE', 'cumulative_SWE', 'cumulative_relative_humidity_rmin',\n       'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'cumulative_SWE', 'cumulative_relative_humidity_rmin',\n       'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  712845\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 1.9573772737381891\nMSE is 8.450164641447078\nR2 score is 0.4687214134503537\nRMSE is 2.9069166898015975\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232911085537.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-21.png\n",
  "history_begin_time" : 1701248111713,
  "history_end_time" : 1701248141081,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "xmwubkvaby0",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import compute_sample_weight\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=1.70, columns=X.columns)\n\n        self.train_x, self.train_y = X_train_scaled, y_train\n        self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n        # Fit the classifier\n#         self.classifier.fit(self.train_x, self.train_y)\n\n#         # Make predictions\n#         predictions = self.classifier.predict(self.train_x)\n\n#         # Calculate absolute errors\n#         errors = np.abs(self.train_y - predictions)\n\n#         # Assign weights based on errors (higher errors get higher weights)\n#         weights = compute_sample_weight('balanced', errors)\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', \n  'SWE', \n  'cumulative_SWE',\n  'cumulative_relative_humidity_rmin', \n  'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n  'cumulative_relative_humidity_rmax',\n  'cumulative_potential_evapotranspiration',\n  'cumulative_fsca',\n  'cumulative_wind_speed',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t\n  'eastness',\t'northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n  # hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n  # hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-12\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    4.9            250.0\n1  2019-10-01  33.358254  ...                    9.8            500.0\n2  2019-10-01  33.358254  ...                   14.7            750.0\n3  2019-10-02  33.358254  ...                   16.5            750.0\n4  2019-10-02  33.358254  ...                   18.3            750.0\n[5 rows x 112 columns]\nrequired features: Index(['SWE', 'cumulative_SWE', 'cumulative_relative_humidity_rmin',\n       'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'cumulative_SWE', 'cumulative_relative_humidity_rmin',\n       'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  712845\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 2.2350686336612045\nMSE is 11.169684747767795\nR2 score is 0.2977397983593617\nRMSE is 3.3421078300629072\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232911081145.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-21.png\n",
  "history_begin_time" : 1701245473643,
  "history_end_time" : 1701245509711,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "970dal4tt67",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import compute_sample_weight\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=1.70, columns=X.columns)\n\n        self.train_x, self.train_y = X_train_scaled, y_train\n        self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n        # Fit the classifier\n#         self.classifier.fit(self.train_x, self.train_y)\n\n#         # Make predictions\n#         predictions = self.classifier.predict(self.train_x)\n\n#         # Calculate absolute errors\n#         errors = np.abs(self.train_y - predictions)\n\n#         # Assign weights based on errors (higher errors get higher weights)\n#         weights = compute_sample_weight('balanced', errors)\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', \n  'SWE', \n  'cumulative_SWE',\n  'cumulative_relative_humidity_rmin', \n  'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n  'cumulative_relative_humidity_rmax',\n  'cumulative_potential_evapotranspiration',\n  'cumulative_fsca',\n  'cumulative_wind_speed',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t\n  'eastness',\t'northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n  # hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n  # hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-12\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    4.9            250.0\n1  2019-10-01  33.358254  ...                    9.8            500.0\n2  2019-10-01  33.358254  ...                   14.7            750.0\n3  2019-10-02  33.358254  ...                   16.5            750.0\n4  2019-10-02  33.358254  ...                   18.3            750.0\n[5 rows x 112 columns]\nrequired features: Index(['SWE', 'cumulative_SWE', 'cumulative_relative_humidity_rmin',\n       'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'cumulative_SWE', 'cumulative_relative_humidity_rmin',\n       'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  712845\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 2.2350686336612045\nMSE is 11.169684747767795\nR2 score is 0.2977397983593617\nRMSE is 3.3421078300629072\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232911050528.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\n",
  "history_begin_time" : 1701234301297,
  "history_end_time" : 1701234331974,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "TM9d8JNXd71k",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import compute_sample_weight\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=1.70, columns=X.columns)\n\n        self.train_x, self.train_y = X_train_scaled, y_train\n        self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n        # Fit the classifier\n#         self.classifier.fit(self.train_x, self.train_y)\n\n#         # Make predictions\n#         predictions = self.classifier.predict(self.train_x)\n\n#         # Calculate absolute errors\n#         errors = np.abs(self.train_y - predictions)\n\n#         # Assign weights based on errors (higher errors get higher weights)\n#         weights = compute_sample_weight('balanced', errors)\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', \n  'SWE', \n  'cumulative_SWE',\n  'cumulative_relative_humidity_rmin', \n  'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n  'cumulative_relative_humidity_rmax',\n  'cumulative_potential_evapotranspiration',\n  'cumulative_fsca',\n  'cumulative_wind_speed',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t\n  'eastness',\t'northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n  # hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n  # hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-12\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    4.9            250.0\n1  2019-10-01  33.358254  ...                    9.8            500.0\n2  2019-10-01  33.358254  ...                   14.7            750.0\n3  2019-10-02  33.358254  ...                   16.5            750.0\n4  2019-10-02  33.358254  ...                   18.3            750.0\n[5 rows x 112 columns]\nrequired features: Index(['SWE', 'cumulative_SWE', 'cumulative_relative_humidity_rmin',\n       'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'cumulative_SWE', 'cumulative_relative_humidity_rmin',\n       'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  712845\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 2.2350686336612045\nMSE is 11.169684747767793\nR2 score is 0.2977397983593618\nRMSE is 3.342107830062907\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232911045039.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-21.png\n",
  "history_begin_time" : 1701233412987,
  "history_end_time" : 1701233442908,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "rnsiz31q033",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701232375266,
  "history_end_time" : 1701234158040,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "h4bmVNnkfkqg",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import compute_sample_weight\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=1.50, columns=X.columns)\n\n        self.train_x, self.train_y = X_train_scaled, y_train\n        self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n        # Fit the classifier\n        self.classifier.fit(self.train_x, self.train_y)\n\n        # Make predictions\n        predictions = self.classifier.predict(self.train_x)\n\n        # Calculate absolute errors\n        errors = np.abs(self.train_y - predictions)\n\n        # Assign weights based on errors (higher errors get higher weights)\n        weights = compute_sample_weight('balanced', errors)\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', \n  'SWE', \n  'cumulative_SWE',\n  'cumulative_relative_humidity_rmin', \n  'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n  'cumulative_relative_humidity_rmax',\n  'cumulative_potential_evapotranspiration',\n  'cumulative_fsca',\n  'cumulative_wind_speed',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t\n  'eastness',\t'northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n  # hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n  # hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-12\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    4.9            250.0\n1  2019-10-01  33.358254  ...                    9.8            500.0\n2  2019-10-01  33.358254  ...                   14.7            750.0\n3  2019-10-02  33.358254  ...                   16.5            750.0\n4  2019-10-02  33.358254  ...                   18.3            750.0\n[5 rows x 112 columns]\nrequired features: Index(['SWE', 'cumulative_SWE', 'cumulative_relative_humidity_rmin',\n       'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'cumulative_SWE', 'cumulative_relative_humidity_rmin',\n       'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  712845\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.07405299334485206\nMSE is 0.12379689118129371\nR2 score is 0.9922166442718239\nRMSE is 0.3518478238973402\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232911043237.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-21.png\n",
  "history_begin_time" : 1701232299490,
  "history_end_time" : 1701232360821,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "PdzbpGNa4WKL",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import compute_sample_weight\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=1.50, columns=X.columns)\n\n        self.train_x, self.train_y = X_train_scaled, y_train\n        self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n        # Fit the classifier\n        self.classifier.fit(self.train_x, self.train_y)\n\n        # Make predictions\n        predictions = self.classifier.predict(self.train_x)\n\n        # Calculate absolute errors\n        errors = np.abs(self.train_y - predictions)\n\n        # Assign weights based on errors (higher errors get higher weights)\n        weights = compute_sample_weight('auto', errors)\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', \n  'SWE', \n  'cumulative_SWE',\n  'cumulative_relative_humidity_rmin', \n  'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n  'cumulative_relative_humidity_rmax',\n  'cumulative_potential_evapotranspiration',\n  'cumulative_fsca',\n  'cumulative_wind_speed',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t\n  'eastness',\t'northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n  # hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n  # hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-12\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    4.9            250.0\n1  2019-10-01  33.358254  ...                    9.8            500.0\n2  2019-10-01  33.358254  ...                   14.7            750.0\n3  2019-10-02  33.358254  ...                   16.5            750.0\n4  2019-10-02  33.358254  ...                   18.3            750.0\n[5 rows x 112 columns]\nrequired features: Index(['SWE', 'cumulative_SWE', 'cumulative_relative_humidity_rmin',\n       'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'cumulative_SWE', 'cumulative_relative_humidity_rmin',\n       'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  712845\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/PdzbpGNa4WKL/model_creation_et.py\", line 267, in <module>\n    hole.train()\n  File \"/home/chetana/gw-workspace/PdzbpGNa4WKL/model_creation_et.py\", line 192, in train\n    weights = compute_sample_weight('auto', errors)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/class_weight.py\", line 126, in compute_sample_weight\n    raise ValueError(\nValueError: The only valid preset for class_weight is \"balanced\". Given \"auto\".\n",
  "history_begin_time" : 1701232242238,
  "history_end_time" : 1701232279790,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "lAO96CNWQKWU",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=1.50, columns=X.columns)\n\n        self.train_x, self.train_y = X_train_scaled, y_train\n        self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', \n  'SWE', \n  'cumulative_SWE',\n  'cumulative_relative_humidity_rmin', \n  'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n  'cumulative_relative_humidity_rmax',\n  'cumulative_potential_evapotranspiration',\n  'cumulative_fsca',\n  'cumulative_wind_speed',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t\n  'eastness',\t'northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n  # hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n  # hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-12\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    4.9            250.0\n1  2019-10-01  33.358254  ...                    9.8            500.0\n2  2019-10-01  33.358254  ...                   14.7            750.0\n3  2019-10-02  33.358254  ...                   16.5            750.0\n4  2019-10-02  33.358254  ...                   18.3            750.0\n[5 rows x 112 columns]\nrequired features: Index(['SWE', 'cumulative_SWE', 'cumulative_relative_humidity_rmin',\n       'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'cumulative_SWE', 'cumulative_relative_humidity_rmin',\n       'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  712845\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 2.2360424832043364\nMSE is 11.145724568288587\nR2 score is 0.2992462222963288\nRMSE is 3.338521314637453\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232911042638.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-21.png\n",
  "history_begin_time" : 1701231973297,
  "history_end_time" : 1701232002618,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "sjM1Y5XJjiAY",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=5.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train_scaled, y_train\n        self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', \n  'SWE', \n  'cumulative_SWE',\n  'cumulative_relative_humidity_rmin', \n  'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n  'cumulative_relative_humidity_rmax',\n  'cumulative_potential_evapotranspiration',\n  'cumulative_fsca',\n  'cumulative_wind_speed',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t\n  'eastness',\t'northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n  # hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n  # hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-12\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    4.9            250.0\n1  2019-10-01  33.358254  ...                    9.8            500.0\n2  2019-10-01  33.358254  ...                   14.7            750.0\n3  2019-10-02  33.358254  ...                   16.5            750.0\n4  2019-10-02  33.358254  ...                   18.3            750.0\n[5 rows x 112 columns]\nrequired features: Index(['SWE', 'cumulative_SWE', 'cumulative_relative_humidity_rmin',\n       'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'cumulative_SWE', 'cumulative_relative_humidity_rmin',\n       'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  712845\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 2.242680914499038\nMSE is 11.29325579164872\nR2 score is 0.2899706420982501\nRMSE is 3.3605439725807367\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232911042529.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-21.png\n",
  "history_begin_time" : 1701231904217,
  "history_end_time" : 1701231933790,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "9x2ajegvfon",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701231048660,
  "history_end_time" : 1701231048660,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "br0dqbwcfk2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933455,
  "history_end_time" : 1701230952342,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "jvmavvqkyos",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230796325,
  "history_end_time" : 1701230932249,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ncrr9bm78t4",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        #return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train_scaled, y_train\n        self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', \n  'SWE', \n  'cumulative_SWE',\n  'cumulative_relative_humidity_rmin', \n  'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n  'cumulative_relative_humidity_rmax',\n  'cumulative_potential_evapotranspiration',\n  'cumulative_fsca',\n  'cumulative_wind_speed',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t\n  'eastness',\t'northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n  # hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n  # hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    4.9            250.0\n1  2019-10-01  33.358254  ...                    9.8            500.0\n2  2019-10-01  33.358254  ...                   14.7            750.0\n3  2019-10-02  33.358254  ...                   16.5            750.0\n4  2019-10-02  33.358254  ...                   18.3            750.0\n[5 rows x 112 columns]\nrequired features: Index(['SWE', 'cumulative_SWE', 'cumulative_relative_humidity_rmin',\n       'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'cumulative_SWE', 'cumulative_relative_humidity_rmin',\n       'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  712845\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.0794359079993588\nMSE is 0.12681235123139709\nR2 score is 0.9920270563263586\nRMSE is 0.35610721872969253\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232911040023.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-21.png\n",
  "history_begin_time" : 1701230386573,
  "history_end_time" : 1701230428271,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "muc8k1zjd9m",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        #return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train_scaled, y_train\n        self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', \n  'SWE', \n  'cumulative_SWE',\n  'cumulative_relative_humidity_rmin', \n  'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n  'cumulative_relative_humidity_rmax',\n  'cumulative_potential_evapotranspiration',\n  'cumulative_fsca',\n  'cumulative_wind_speed',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t\n  'eastness',\t'northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n  # hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n  # hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    4.9            250.0\n1  2019-10-01  33.358254  ...                    9.8            500.0\n2  2019-10-01  33.358254  ...                   14.7            750.0\n3  2019-10-02  33.358254  ...                   16.5            750.0\n4  2019-10-02  33.358254  ...                   18.3            750.0\n[5 rows x 112 columns]\nrequired features: Index(['SWE', 'cumulative_SWE', 'cumulative_relative_humidity_rmin',\n       'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'cumulative_SWE', 'cumulative_relative_humidity_rmin',\n       'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  712845\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.0794359079993588\nMSE is 0.12681235123139709\nR2 score is 0.9920270563263586\nRMSE is 0.35610721872969253\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232911035338.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-21.png\n",
  "history_begin_time" : 1701229985415,
  "history_end_time" : 1701230022754,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "3wadrnmrqa2",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        #return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train_scaled, y_train\n        self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', \n  'SWE', \n  'cumulative_SWE',\n  'cumulative_relative_humidity_rmin', \n  'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n  'cumulative_relative_humidity_rmax',\n  'cumulative_potential_evapotranspiration',\n  'cumulative_fsca',\n  'cumulative_wind_speed',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t\n  'eastness',\t'northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n  # hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n  # hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    4.9            250.0\n1  2019-10-01  33.358254  ...                    9.8            500.0\n2  2019-10-01  33.358254  ...                   14.7            750.0\n3  2019-10-02  33.358254  ...                   16.5            750.0\n4  2019-10-02  33.358254  ...                   18.3            750.0\n[5 rows x 112 columns]\nrequired features: Index(['SWE', 'cumulative_SWE', 'cumulative_relative_humidity_rmin',\n       'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'cumulative_SWE', 'cumulative_relative_humidity_rmin',\n       'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  712845\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.0794359079993588\nMSE is 0.1268123512313971\nR2 score is 0.9920270563263586\nRMSE is 0.3561072187296926\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232911033538.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-21.png\n",
  "history_begin_time" : 1701228901786,
  "history_end_time" : 1701228941817,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "f8vlt6kr0ud",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        #return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train_scaled, y_train\n        self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', \n  'SWE', \n  'cumulative_SWE',\n  'cumulative_relative_humidity_rmin', \n  'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n  'cumulative_relative_humidity_rmax',\n  'cumulative_potential_evapotranspiration',\n  'cumulative_fsca',\n  'cumulative_wind_speed',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t\n  'eastness',\t'northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n  # hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n  # hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    4.9            250.0\n1  2019-10-01  33.358254  ...                    9.8            500.0\n2  2019-10-01  33.358254  ...                   14.7            750.0\n3  2019-10-02  33.358254  ...                   16.5            750.0\n4  2019-10-02  33.358254  ...                   18.3            750.0\n[5 rows x 112 columns]\nrequired features: Index(['SWE', 'cumulative_SWE', 'cumulative_relative_humidity_rmin',\n       'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'cumulative_SWE', 'cumulative_relative_humidity_rmin',\n       'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  712845\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.0794359079993588\nMSE is 0.12681235123139709\nR2 score is 0.9920270563263586\nRMSE is 0.35610721872969253\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232911032653.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\n",
  "history_begin_time" : 1701228374874,
  "history_end_time" : 1701228417385,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "k6420f0g7z7",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        #return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train_scaled, y_train\n        self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', \n  'SWE', \n#   'cumulative_SWE',\n#   'cumulative_relative_humidity_rmin', \n#   'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n#   'cumulative_relative_humidity_rmax',\n#   'cumulative_potential_evapotranspiration',\n#   'cumulative_fsca',\n#   'cumulative_wind_speed',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t\n  'eastness',\t'northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n  # hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n  # hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    4.9            250.0\n1  2019-10-01  33.358254  ...                    9.8            500.0\n2  2019-10-01  33.358254  ...                   14.7            750.0\n3  2019-10-02  33.358254  ...                   16.5            750.0\n4  2019-10-02  33.358254  ...                   18.3            750.0\n[5 rows x 112 columns]\nrequired features: Index(['SWE', 'fsca', 'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'fsca', 'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  712845\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.3072619344242066\nMSE is 1.2213446806607795\nR2 score is 0.923211641055047\nRMSE is 1.1051446424159959\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232911032424.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-13.png\n",
  "history_begin_time" : 1701228238418,
  "history_end_time" : 1701228270078,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "rbdtimppi35",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        #return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train_scaled, y_train\n        self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', \n#   'SWE', \n#   'cumulative_SWE',\n#   'cumulative_relative_humidity_rmin', \n#   'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n#   'cumulative_relative_humidity_rmax',\n#   'cumulative_potential_evapotranspiration',\n#   'cumulative_fsca',\n#   'cumulative_wind_speed',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n#   'elevation',\t'slope',\t'curvature',\t'aspect',\t\n#   'eastness',\t'northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n  # hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n  # hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    4.9            250.0\n1  2019-10-01  33.358254  ...                    9.8            500.0\n2  2019-10-01  33.358254  ...                   14.7            750.0\n3  2019-10-02  33.358254  ...                   16.5            750.0\n4  2019-10-02  33.358254  ...                   18.3            750.0\n[5 rows x 112 columns]\nrequired features: Index(['fsca', 'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['fsca', 'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax'],\n      dtype='object')\ntraining data row number:  712845\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.40708714605374247\nMSE is 1.8691939198301568\nR2 score is 0.8824800763237557\nRMSE is 1.367184669249241\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232911032218.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-6.png\n",
  "history_begin_time" : 1701228120223,
  "history_end_time" : 1701228142943,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "dp6a61cufwu",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        #return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train_scaled, y_train\n        self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', \n#   'SWE', \n#   'cumulative_SWE',\n#   'cumulative_relative_humidity_rmin', \n#   'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n#   'cumulative_relative_humidity_rmax',\n#   'cumulative_potential_evapotranspiration',\n#   'cumulative_fsca',\n#   'cumulative_wind_speed',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n#   'elevation',\t'slope',\t'curvature',\t'aspect',\t\n#   'eastness',\t'northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n  # hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n  # hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    4.9            250.0\n1  2019-10-01  33.358254  ...                    9.8            500.0\n2  2019-10-01  33.358254  ...                   14.7            750.0\n3  2019-10-02  33.358254  ...                   16.5            750.0\n4  2019-10-02  33.358254  ...                   18.3            750.0\n[5 rows x 112 columns]\nrequired features: Index(['fsca', 'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['fsca', 'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax'],\n      dtype='object')\ntraining data row number:  712845\n",
  "history_begin_time" : 1701228058558,
  "history_end_time" : 1701228076475,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "ng7xfaop1fl",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        #return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train_scaled, y_train\n        self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', \n#   'SWE', \n#   'cumulative_SWE',\n#   'cumulative_relative_humidity_rmin', \n#   'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n#   'cumulative_relative_humidity_rmax',\n#   'cumulative_potential_evapotranspiration',\n#   'cumulative_fsca',\n#   'cumulative_wind_speed',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t\n  'eastness',\t'northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n  # hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n  # hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    4.9            250.0\n1  2019-10-01  33.358254  ...                    9.8            500.0\n2  2019-10-01  33.358254  ...                   14.7            750.0\n3  2019-10-02  33.358254  ...                   16.5            750.0\n4  2019-10-02  33.358254  ...                   18.3            750.0\n[5 rows x 112 columns]\nrequired features: Index(['fsca', 'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['fsca', 'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  712845\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.3549389282560724\nMSE is 1.5495190050331116\nR2 score is 0.9025786713328665\nRMSE is 1.2447967725830236\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232911031900.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-12.png\n",
  "history_begin_time" : 1701227914780,
  "history_end_time" : 1701227944548,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "ARqtu5eWKIeA",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        #return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train_scaled, y_train\n        self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', \n#   'SWE', \n#   'cumulative_SWE',\n#   'cumulative_relative_humidity_rmin', \n#   'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n#   'cumulative_relative_humidity_rmax',\n#   'cumulative_potential_evapotranspiration',\n#   'cumulative_fsca',\n#   'cumulative_wind_speed',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t\n  'eastness',\t'northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n  # hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n  # hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    4.9            250.0\n1  2019-10-01  33.358254  ...                    9.8            500.0\n2  2019-10-01  33.358254  ...                   14.7            750.0\n3  2019-10-02  33.358254  ...                   16.5            750.0\n4  2019-10-02  33.358254  ...                   18.3            750.0\n[5 rows x 112 columns]\nrequired features: Index(['fsca', 'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['fsca', 'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  712845\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.3549389282560724\nMSE is 1.5495190050331116\nR2 score is 0.9025786713328665\nRMSE is 1.2447967725830236\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232911031808.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-12.png\n",
  "history_begin_time" : 1701227863226,
  "history_end_time" : 1701227892775,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "L4jrk7hsvxK2",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        #return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train_scaled, y_train\n        self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', \n#   'SWE', \n#   'cumulative_SWE',\n  'cumulative_relative_humidity_rmin', \n  'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n  'cumulative_relative_humidity_rmax',\n  'cumulative_potential_evapotranspiration',\n  'cumulative_fsca',\n  'cumulative_wind_speed',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t\n  'eastness',\t'northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n  # hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n  # hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    4.9            250.0\n1  2019-10-01  33.358254  ...                    9.8            500.0\n2  2019-10-01  33.358254  ...                   14.7            750.0\n3  2019-10-02  33.358254  ...                   16.5            750.0\n4  2019-10-02  33.358254  ...                   18.3            750.0\n[5 rows x 112 columns]\nrequired features: Index(['cumulative_relative_humidity_rmin', 'cumulative_air_temperature_tmmx',\n       'cumulative_air_temperature_tmmn', 'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['cumulative_relative_humidity_rmin', 'cumulative_air_temperature_tmmx',\n       'cumulative_air_temperature_tmmn', 'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  712845\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.13848004133829583\nMSE is 0.39347696989142983\nR2 score is 0.9752613236222155\nRMSE is 0.6272774265756977\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232911031509.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-19.png\n",
  "history_begin_time" : 1701227676988,
  "history_end_time" : 1701227713200,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "jlcHkLVKR1Wh",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        #return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train_scaled.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test_scaled.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', \n#   'SWE', \n#   'cumulative_SWE',\n  'cumulative_relative_humidity_rmin', \n  'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n  'cumulative_relative_humidity_rmax',\n  'cumulative_potential_evapotranspiration',\n  'cumulative_fsca',\n  'cumulative_wind_speed',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t\n  'eastness',\t'northness',\n]\n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n  # hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n  # hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    4.9            250.0\n1  2019-10-01  33.358254  ...                    9.8            500.0\n2  2019-10-01  33.358254  ...                   14.7            750.0\n3  2019-10-02  33.358254  ...                   16.5            750.0\n4  2019-10-02  33.358254  ...                   18.3            750.0\n[5 rows x 112 columns]\nrequired features: Index(['cumulative_relative_humidity_rmin', 'cumulative_air_temperature_tmmx',\n       'cumulative_air_temperature_tmmn', 'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['cumulative_relative_humidity_rmin', 'cumulative_air_temperature_tmmx',\n       'cumulative_air_temperature_tmmn', 'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  712845\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/jlcHkLVKR1Wh/model_creation_et.py\", line 252, in <module>\n    hole.preprocessing(chosen_columns = selected_columns)\n  File \"/home/chetana/gw-workspace/jlcHkLVKR1Wh/model_creation_et.py\", line 152, in preprocessing\n    self.train_x, self.train_y = X_train_scaled.to_numpy(), y_train.to_numpy()\nAttributeError: 'numpy.ndarray' object has no attribute 'to_numpy'\n",
  "history_begin_time" : 1701227641026,
  "history_end_time" : 1701227655713,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "PYnSajZX85TP",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        #return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', \n#   'SWE', \n#   'cumulative_SWE',\n  'cumulative_relative_humidity_rmin', \n  'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n  'cumulative_relative_humidity_rmax',\n  'cumulative_potential_evapotranspiration',\n  'cumulative_fsca',\n  'cumulative_wind_speed',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t\n  'eastness',\t'northness',\n]\n\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n# hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    4.9            250.0\n1  2019-10-01  33.358254  ...                    9.8            500.0\n2  2019-10-01  33.358254  ...                   14.7            750.0\n3  2019-10-02  33.358254  ...                   16.5            750.0\n4  2019-10-02  33.358254  ...                   18.3            750.0\n[5 rows x 112 columns]\nrequired features: Index(['cumulative_relative_humidity_rmin', 'cumulative_air_temperature_tmmx',\n       'cumulative_air_temperature_tmmn', 'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['cumulative_relative_humidity_rmin', 'cumulative_air_temperature_tmmx',\n       'cumulative_air_temperature_tmmn', 'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'fsca', 'air_temperature_tmmx',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  712845\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.14086996129852233\nMSE is 0.39645777281297556\nR2 score is 0.975073914639061\nRMSE is 0.6296489282234788\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232911025749.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-19.png\n",
  "history_begin_time" : 1701226636228,
  "history_end_time" : 1701226673722,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "XCXn8m8txf67",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        #return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', 'SWE', \n  'cumulative_SWE',\n#   'cumulative_relative_humidity_rmin', \n#   'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n#   'cumulative_relative_humidity_rmax',\n#   'cumulative_potential_evapotranspiration',\n  'cumulative_fsca',\n#   'cumulative_wind_speed',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t\n  'eastness',\t'northness',\n]\n\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n# hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    4.9            250.0\n1  2019-10-01  33.358254  ...                    9.8            500.0\n2  2019-10-01  33.358254  ...                   14.7            750.0\n3  2019-10-02  33.358254  ...                   16.5            750.0\n4  2019-10-02  33.358254  ...                   18.3            750.0\n[5 rows x 112 columns]\nrequired features: Index(['SWE', 'cumulative_SWE', 'cumulative_fsca', 'fsca',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'cumulative_SWE', 'cumulative_fsca', 'fsca',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  712845\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.12980309001157989\nMSE is 0.2938693136762008\nR2 score is 0.9815238542413216\nRMSE is 0.5420971441321203\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232911025402.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-15.png\n",
  "history_begin_time" : 1701226411295,
  "history_end_time" : 1701226447050,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "kpg31sjq07Kj",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        #return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n#         self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', 'SWE', \n  'cumulative_SWE',\n#   'cumulative_relative_humidity_rmin', \n#   'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n#   'cumulative_relative_humidity_rmax',\n#   'cumulative_potential_evapotranspiration',\n  'cumulative_fsca',\n#   'cumulative_wind_speed',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t\n  'eastness',\t'northness',\n]\n\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n# hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    4.9            250.0\n1  2019-10-01  33.358254  ...                    9.8            500.0\n2  2019-10-01  33.358254  ...                   14.7            750.0\n3  2019-10-02  33.358254  ...                   16.5            750.0\n4  2019-10-02  33.358254  ...                   18.3            750.0\n[5 rows x 112 columns]\nrequired features: Index(['SWE', 'cumulative_SWE', 'cumulative_fsca', 'fsca',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'cumulative_SWE', 'cumulative_fsca', 'fsca',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  712845\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/kpg31sjq07Kj/model_creation_et.py\", line 245, in <module>\n    hole.train()\n  File \"/home/chetana/gw-workspace/kpg31sjq07Kj/model_creation_et.py\", line 167, in train\n    self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\nNameError: name 'weights' is not defined\n",
  "history_begin_time" : 1701226381747,
  "history_end_time" : 1701226395807,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "7h2pi34icccM",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', 'SWE', \n  'cumulative_SWE',\n#   'cumulative_relative_humidity_rmin', \n#   'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n#   'cumulative_relative_humidity_rmax',\n#   'cumulative_potential_evapotranspiration',\n  'cumulative_fsca',\n#   'cumulative_wind_speed',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t\n  'eastness',\t'northness',\n]\n\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n# hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    4.9            250.0\n1  2019-10-01  33.358254  ...                    9.8            500.0\n2  2019-10-01  33.358254  ...                   14.7            750.0\n3  2019-10-02  33.358254  ...                   16.5            750.0\n4  2019-10-02  33.358254  ...                   18.3            750.0\n[5 rows x 112 columns]\nrequired features: Index(['SWE', 'cumulative_SWE', 'cumulative_fsca', 'fsca',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'cumulative_SWE', 'cumulative_fsca', 'fsca',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  712845\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.12993381409190127\nMSE is 0.2972190905478494\nR2 score is 0.9813132471351717\nRMSE is 0.5451780356432653\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232911023253.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-15.png\n",
  "history_begin_time" : 1701225144393,
  "history_end_time" : 1701225177369,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "DMlU8LsqXTWN",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', 'SWE', \n  'cumulative_SWE',\n#   'cumulative_relative_humidity_rmin', \n#   'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n#   'cumulative_relative_humidity_rmax',\n#   'cumulative_potential_evapotranspiration',\n  'cumulative_fsca',\n#   'cumulative_wind_speed',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t\n  'eastness',\t'northness', \n  'northness'\n]\n\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n# hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    4.9            250.0\n1  2019-10-01  33.358254  ...                    9.8            500.0\n2  2019-10-01  33.358254  ...                   14.7            750.0\n3  2019-10-02  33.358254  ...                   16.5            750.0\n4  2019-10-02  33.358254  ...                   18.3            750.0\n[5 rows x 112 columns]\nrequired features: Index(['SWE', 'cumulative_SWE', 'cumulative_fsca', 'fsca',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'cumulative_SWE', 'cumulative_fsca', 'fsca',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness', 'northness'],\n      dtype='object')\ntraining data row number:  712845\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.1300353010113948\nMSE is 0.2985240366143542\nR2 score is 0.9812312025915262\nRMSE is 0.5463735321319603\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232911023124.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-16.png\n",
  "history_begin_time" : 1701225043126,
  "history_end_time" : 1701225089865,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "lddaPzvQeKjB",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', 'SWE',  \n  'water_year', \n#   'cumulative_SWE','cumulative_relative_humidity_rmin', \n#   'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n#   'cumulative_relative_humidity_rmax',\n#   'cumulative_potential_evapotranspiration',\n#   'cumulative_fsca',\n#   'cumulative_wind_speed',\n  'fsca',\n  'air_temperature_tmmx', \n  'potential_evapotranspiration',\n  'elevation',\t'slope',\t'curvature',\t'aspect',\t\n  'eastness',\t'northness', 'air_temperature_tmmn', \n  'potential_evapotranspiration', 'relative_humidity_rmax', \n  'northness'\n]\n\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n# hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    4.9            250.0\n1  2019-10-01  33.358254  ...                    9.8            500.0\n2  2019-10-01  33.358254  ...                   14.7            750.0\n3  2019-10-02  33.358254  ...                   16.5            750.0\n4  2019-10-02  33.358254  ...                   18.3            750.0\n[5 rows x 112 columns]\nrequired features: Index(['SWE', 'water_year', 'fsca', 'air_temperature_tmmx',\n       'potential_evapotranspiration', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'water_year', 'fsca', 'air_temperature_tmmx',\n       'potential_evapotranspiration', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'northness'],\n      dtype='object')\ntraining data row number:  712845\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.2766753877589529\nMSE is 1.0658018974020567\nR2 score is 0.9329909238908357\nRMSE is 1.0323768194811702\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232911022943.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-16.png\n",
  "history_begin_time" : 1701224954439,
  "history_end_time" : 1701224986997,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "NzKRZVt2Enpw",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', 'SWE',  \n  'water_year', \n#   'cumulative_SWE','cumulative_relative_humidity_rmin', \n#   'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n#   'cumulative_relative_humidity_rmax',\n#   'cumulative_potential_evapotranspiration',\n#   'cumulative_fsca',\n#   'cumulative_wind_speed',\n  'air_temperature_tmmx', \n  'potential_evapotranspiration',\n  'elevation',\t'slope',\t'curvature',\t'aspect',\t\n  'eastness',\t'northness', 'air_temperature_tmmn', \n  'potential_evapotranspiration', 'relative_humidity_rmax', \n  'northness'\n]\n\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n# hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    4.9            250.0\n1  2019-10-01  33.358254  ...                    9.8            500.0\n2  2019-10-01  33.358254  ...                   14.7            750.0\n3  2019-10-02  33.358254  ...                   16.5            750.0\n4  2019-10-02  33.358254  ...                   18.3            750.0\n[5 rows x 112 columns]\nrequired features: Index(['SWE', 'water_year', 'air_temperature_tmmx',\n       'potential_evapotranspiration', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'water_year', 'air_temperature_tmmx',\n       'potential_evapotranspiration', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'northness'],\n      dtype='object')\ntraining data row number:  712845\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.3321014976852606\nMSE is 1.3480875115105515\nR2 score is 0.9152430682654834\nRMSE is 1.161071708169031\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232911022905.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-15.png\n",
  "history_begin_time" : 1701224916536,
  "history_end_time" : 1701224949510,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Xkn9uH5yodzO",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        print(\"all columns: \", data.columns)\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', 'SWE', 'cumulative_SWE', \n  'water_year', 'cumulative_relative_humidity_rmin', \n  'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn',\n  'cumulative_relative_humidity_rmax',\n  'cumulative_potential_evapotranspiration',\n  'cumulative_fsca',\n  'cumulative_wind_speed',\n  'air_temperature_tmmx', \n  'potential_evapotranspiration',\n  'elevation',\t'slope',\t'curvature',\t'aspect',\t\n  'eastness',\t'northness', 'air_temperature_tmmn', \n  'potential_evapotranspiration', 'relative_humidity_rmax', \n  'northness'\n]\n\n\n\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n# hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    4.9            250.0\n1  2019-10-01  33.358254  ...                    9.8            500.0\n2  2019-10-01  33.358254  ...                   14.7            750.0\n3  2019-10-02  33.358254  ...                   16.5            750.0\n4  2019-10-02  33.358254  ...                   18.3            750.0\n[5 rows x 112 columns]\nget swe statistics\ncount    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nall columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value', 'cell_id',\n       'station_id', 'potential_evapotranspiration', 'precipitation_amount',\n       ...\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n       'cumulative_fsca'],\n      dtype='object', length=112)\nrequired features: Index(['SWE', 'cumulative_SWE', 'water_year',\n       'cumulative_relative_humidity_rmin', 'cumulative_air_temperature_tmmx',\n       'cumulative_air_temperature_tmmn', 'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'air_temperature_tmmx',\n       'potential_evapotranspiration', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'cumulative_SWE', 'water_year',\n       'cumulative_relative_humidity_rmin', 'cumulative_air_temperature_tmmx',\n       'cumulative_air_temperature_tmmn', 'cumulative_relative_humidity_rmax',\n       'cumulative_potential_evapotranspiration', 'cumulative_fsca',\n       'cumulative_wind_speed', 'air_temperature_tmmx',\n       'potential_evapotranspiration', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'northness'],\n      dtype='object')\ntraining data row number:  712845\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.05448207871983666\nMSE is 0.07401306105630431\nR2 score is 0.9953466522686033\nRMSE is 0.2720534158144395\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232911022646.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-23.png\n",
  "history_begin_time" : 1701224767353,
  "history_end_time" : 1701224810595,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "pu6Biz2LuMja",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        print(\"all columns: \", data.columns)\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t'eastness',\t'northness', 'air_temperature_tmmn',  \n  'potential_evapotranspiration', 'relative_humidity_rmax',   'northness'\n]\n\n\n\n\nhole = ETHole()\n# hole.preprocessing(chosen_columns = selected_columns)\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n# hole.post_processing(chosen_columns = selected_columns)\nhole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    4.9            250.0\n1  2019-10-01  33.358254  ...                    9.8            500.0\n2  2019-10-01  33.358254  ...                   14.7            750.0\n3  2019-10-02  33.358254  ...                   16.5            750.0\n4  2019-10-02  33.358254  ...                   18.3            750.0\n[5 rows x 112 columns]\nget swe statistics\ncount    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nall columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value', 'cell_id',\n       'station_id', 'potential_evapotranspiration', 'precipitation_amount',\n       ...\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n       'cumulative_fsca'],\n      dtype='object', length=112)\nall non-numeric columns are dropped:  Index(['cell_id', 'station_id'], dtype='object')\nrequired features: Index(['SWE', 'Flag', 'potential_evapotranspiration', 'precipitation_amount',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'air_temperature_tmmn', 'air_temperature_tmmx',\n       'mean_vapor_pressure_deficit', 'wind_speed',\n       ...\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n       'cumulative_fsca'],\n      dtype='object', length=106)\ndescribe the statistics of swe_value:  count    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'Flag', 'potential_evapotranspiration', 'precipitation_amount',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'air_temperature_tmmn', 'air_temperature_tmmx',\n       'mean_vapor_pressure_deficit', 'wind_speed',\n       ...\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n       'cumulative_fsca'],\n      dtype='object', length=106)\ntraining data row number:  712845\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.19556802670987403\nMSE is 0.2101612990762367\nR2 score is 0.9867867429028531\nRMSE is 0.4584335274347162\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232911021734.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1701224120940,
  "history_end_time" : 1701224260636,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "A79XApuScZdt",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        print(\"all columns: \", data.columns)\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = df.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          df = df.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t'eastness',\t'northness', 'air_temperature_tmmn',  \n  'potential_evapotranspiration', 'relative_humidity_rmax',   'northness'\n]\n\n\n\n\nhole = ETHole()\n# hole.preprocessing(chosen_columns = selected_columns)\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n# hole.post_processing(chosen_columns = selected_columns)\nhole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    4.9            250.0\n1  2019-10-01  33.358254  ...                    9.8            500.0\n2  2019-10-01  33.358254  ...                   14.7            750.0\n3  2019-10-02  33.358254  ...                   16.5            750.0\n4  2019-10-02  33.358254  ...                   18.3            750.0\n[5 rows x 112 columns]\nget swe statistics\ncount    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nall columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value', 'cell_id',\n       'station_id', 'potential_evapotranspiration', 'precipitation_amount',\n       ...\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n       'cumulative_fsca'],\n      dtype='object', length=112)\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/A79XApuScZdt/model_creation_et.py\", line 237, in <module>\n    hole.preprocessing()\n  File \"/home/chetana/gw-workspace/A79XApuScZdt/model_creation_et.py\", line 118, in preprocessing\n    non_numeric_columns = df.select_dtypes(exclude=['number']).columns\nUnboundLocalError: local variable 'df' referenced before assignment\n",
  "history_begin_time" : 1701224088621,
  "history_end_time" : 1701224103040,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "x8HYYKVfxsOO",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        print(\"all columns: \", data.columns)\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          data = data.drop('station_id', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t'eastness',\t'northness', 'air_temperature_tmmn',  \n  'potential_evapotranspiration', 'relative_humidity_rmax',   'northness'\n]\n\n\n\n\nhole = ETHole()\n# hole.preprocessing(chosen_columns = selected_columns)\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n# hole.post_processing(chosen_columns = selected_columns)\nhole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    4.9            250.0\n1  2019-10-01  33.358254  ...                    9.8            500.0\n2  2019-10-01  33.358254  ...                   14.7            750.0\n3  2019-10-02  33.358254  ...                   16.5            750.0\n4  2019-10-02  33.358254  ...                   18.3            750.0\n[5 rows x 112 columns]\nget swe statistics\ncount    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nall columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value', 'cell_id',\n       'station_id', 'potential_evapotranspiration', 'precipitation_amount',\n       ...\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n       'cumulative_fsca'],\n      dtype='object', length=112)\nrequired features: Index(['SWE', 'Flag', 'cell_id', 'potential_evapotranspiration',\n       'precipitation_amount', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'air_temperature_tmmn',\n       'air_temperature_tmmx', 'mean_vapor_pressure_deficit',\n       ...\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n       'cumulative_fsca'],\n      dtype='object', length=107)\ndescribe the statistics of swe_value:  count    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'Flag', 'cell_id', 'potential_evapotranspiration',\n       'precipitation_amount', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'air_temperature_tmmn',\n       'air_temperature_tmmx', 'mean_vapor_pressure_deficit',\n       ...\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n       'cumulative_fsca'],\n      dtype='object', length=107)\ntraining data row number:  712845\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/x8HYYKVfxsOO/model_creation_et.py\", line 235, in <module>\n    hole.train()\n  File \"/home/chetana/gw-workspace/x8HYYKVfxsOO/model_creation_et.py\", line 168, in train\n    self.classifier.fit(self.train_x, self.train_y)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 348, in fit\n    X, y = self._validate_data(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 621, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1147, in check_X_y\n    X = check_array(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 917, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/_array_api.py\", line 380, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\nValueError: could not convert string to float: '552a5638-8fd2-46de-8569-5f5512102264'\n",
  "history_begin_time" : 1701223969056,
  "history_end_time" : 1701223989996,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "DnKXKxHv478V",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          data = data.drop('station_id', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t'eastness',\t'northness', 'air_temperature_tmmn',  \n  'potential_evapotranspiration', 'relative_humidity_rmax',   'northness'\n]\n\n\n\n\nhole = ETHole()\n# hole.preprocessing(chosen_columns = selected_columns)\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n# hole.post_processing(chosen_columns = selected_columns)\nhole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    4.9            250.0\n1  2019-10-01  33.358254  ...                    9.8            500.0\n2  2019-10-01  33.358254  ...                   14.7            750.0\n3  2019-10-02  33.358254  ...                   16.5            750.0\n4  2019-10-02  33.358254  ...                   18.3            750.0\n[5 rows x 112 columns]\nget swe statistics\ncount    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nrequired features: Index(['SWE', 'Flag', 'cell_id', 'potential_evapotranspiration',\n       'precipitation_amount', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'air_temperature_tmmn',\n       'air_temperature_tmmx', 'mean_vapor_pressure_deficit',\n       ...\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n       'cumulative_fsca'],\n      dtype='object', length=107)\ndescribe the statistics of swe_value:  count    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'Flag', 'cell_id', 'potential_evapotranspiration',\n       'precipitation_amount', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'air_temperature_tmmn',\n       'air_temperature_tmmx', 'mean_vapor_pressure_deficit',\n       ...\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n       'cumulative_fsca'],\n      dtype='object', length=107)\ntraining data row number:  712845\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/DnKXKxHv478V/model_creation_et.py\", line 234, in <module>\n    hole.train()\n  File \"/home/chetana/gw-workspace/DnKXKxHv478V/model_creation_et.py\", line 167, in train\n    self.classifier.fit(self.train_x, self.train_y)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 348, in fit\n    X, y = self._validate_data(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 621, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1147, in check_X_y\n    X = check_array(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 917, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/_array_api.py\", line 380, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\nValueError: could not convert string to float: '552a5638-8fd2-46de-8569-5f5512102264'\n",
  "history_begin_time" : 1701223905977,
  "history_end_time" : 1701223926524,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "gRyMvYNVY9xb",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n\t\t  data = data.drop('station_id', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t'eastness',\t'northness', 'air_temperature_tmmn',  \n  'potential_evapotranspiration', 'relative_humidity_rmax',   'northness'\n]\n\n\n\n\nhole = ETHole()\n# hole.preprocessing(chosen_columns = selected_columns)\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n# hole.post_processing(chosen_columns = selected_columns)\nhole.post_processing()\n\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/gRyMvYNVY9xb/model_creation_et.py\", line 117\n    data = data.drop('station_id', axis=1)\nTabError: inconsistent use of tabs and spaces in indentation\n",
  "history_begin_time" : 1701223898253,
  "history_end_time" : 1701223898307,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "DeJbK73h2FgK",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t'eastness',\t'northness', 'air_temperature_tmmn',  \n  'potential_evapotranspiration', 'relative_humidity_rmax',   'northness'\n]\n\n\n\n\nhole = ETHole()\n# hole.preprocessing(chosen_columns = selected_columns)\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n# hole.post_processing(chosen_columns = selected_columns)\nhole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    4.9            250.0\n1  2019-10-01  33.358254  ...                    9.8            500.0\n2  2019-10-01  33.358254  ...                   14.7            750.0\n3  2019-10-02  33.358254  ...                   16.5            750.0\n4  2019-10-02  33.358254  ...                   18.3            750.0\n[5 rows x 112 columns]\nget swe statistics\ncount    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nrequired features: Index(['SWE', 'Flag', 'cell_id', 'station_id', 'potential_evapotranspiration',\n       'precipitation_amount', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'air_temperature_tmmn',\n       'air_temperature_tmmx',\n       ...\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n       'cumulative_fsca'],\n      dtype='object', length=108)\ndescribe the statistics of swe_value:  count    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'Flag', 'cell_id', 'station_id', 'potential_evapotranspiration',\n       'precipitation_amount', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'air_temperature_tmmn',\n       'air_temperature_tmmx',\n       ...\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n       'cumulative_fsca'],\n      dtype='object', length=108)\ntraining data row number:  712845\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/DeJbK73h2FgK/model_creation_et.py\", line 233, in <module>\n    hole.train()\n  File \"/home/chetana/gw-workspace/DeJbK73h2FgK/model_creation_et.py\", line 166, in train\n    self.classifier.fit(self.train_x, self.train_y)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 348, in fit\n    X, y = self._validate_data(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 621, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1147, in check_X_y\n    X = check_array(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 917, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/_array_api.py\", line 380, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\nValueError: could not convert string to float: '552a5638-8fd2-46de-8569-5f5512102264'\n",
  "history_begin_time" : 1701223829622,
  "history_end_time" : 1701223850037,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "d5pv7L6U8FUc",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t'eastness',\t'northness', 'air_temperature_tmmn',  \n  'potential_evapotranspiration', 'relative_humidity_rmax',   'northness'\n]\n\n\n\n\nhole = ETHole()\n# hole.preprocessing(chosen_columns = selected_columns)\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n# hole.post_processing(chosen_columns = selected_columns)\nhole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v4_slope_corrected_cumulative_v1.csv\n         date        lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-01  33.358254  ...                    4.9            250.0\n1  2019-10-01  33.358254  ...                    9.8            500.0\n2  2019-10-01  33.358254  ...                   14.7            750.0\n3  2019-10-02  33.358254  ...                   16.5            750.0\n4  2019-10-02  33.358254  ...                   18.3            750.0\n[5 rows x 112 columns]\nget swe statistics\ncount    712845.000000\nmean          2.661918\nstd           3.989611\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/d5pv7L6U8FUc/model_creation_et.py\", line 232, in <module>\n    hole.preprocessing()\n  File \"/home/chetana/gw-workspace/d5pv7L6U8FUc/model_creation_et.py\", line 116, in preprocessing\n    data = data.drop('Unnamed: 0', axis=1)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\", line 5399, in drop\n    return super().drop(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\", line 4505, in drop\n    obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\", line 4546, in _drop_axis\n    new_axis = axis.drop(labels, errors=errors)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 6934, in drop\n    raise KeyError(f\"{list(labels[mask])} not found in axis\")\nKeyError: \"['Unnamed: 0'] not found in axis\"\n",
  "history_begin_time" : 1701223728624,
  "history_end_time" : 1701223751594,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "jU6g0hHFKE9S",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_1yr_cummulative_v2.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', \n  'elevation',\t'slope',\t'curvature',\t'aspect',\t'eastness',\t'northness', 'air_temperature_tmmn',  \n  'potential_evapotranspiration', 'relative_humidity_rmax',   'northness'\n]\n\n\n\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n#hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-26\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_1yr_cummulative_v2.csv\n         date       lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-03  37.19236  ...               6.800000                0\n1  2019-11-10  37.19236  ...              81.100001              500\n2  2019-10-19  37.19236  ...              40.800000              250\n3  2019-12-12  37.19236  ...             145.200002             4862\n4  2019-12-06  37.19236  ...             132.800002             3913\n[5 rows x 34 columns]\nget swe statistics\ncount    305624.000000\nmean          0.813043\nstd           1.305277\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           1.325000\nmax           3.400000\nName: swe_value, dtype: float64\nrequired features: Index(['elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    305624.000000\nmean          0.813043\nstd           1.305277\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           1.325000\nmax           3.400000\nName: swe_value, dtype: float64\ninput features and order:  Index(['elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'northness'],\n      dtype='object')\ntraining data row number:  305624\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.03142862611525333\nMSE is 0.04292793297156518\nR2 score is 0.9747310541231335\nRMSE is 0.20719057162806703\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232611182332.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-11.png\n",
  "history_begin_time" : 1701023003213,
  "history_end_time" : 1701023013760,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "J4nUfxLX3vSv",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_1yr_cummulative_v2.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', 'SWE', 'fsca',\n  'elevation',\t'slope',\t'curvature',\t'aspect',\t'eastness',\t'northness', 'air_temperature_tmmn',  \n  'potential_evapotranspiration', 'relative_humidity_rmax',   'northness'\n]\n\n\n\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n#hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-26\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_1yr_cummulative_v2.csv\n         date       lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-03  37.19236  ...               6.800000                0\n1  2019-11-10  37.19236  ...              81.100001              500\n2  2019-10-19  37.19236  ...              40.800000              250\n3  2019-12-12  37.19236  ...             145.200002             4862\n4  2019-12-06  37.19236  ...             132.800002             3913\n[5 rows x 34 columns]\nget swe statistics\ncount    305624.000000\nmean          0.813043\nstd           1.305277\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           1.325000\nmax           3.400000\nName: swe_value, dtype: float64\nrequired features: Index(['SWE', 'fsca', 'elevation', 'slope', 'curvature', 'aspect', 'eastness',\n       'northness', 'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    305624.000000\nmean          0.813043\nstd           1.305277\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           1.325000\nmax           3.400000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'fsca', 'elevation', 'slope', 'curvature', 'aspect', 'eastness',\n       'northness', 'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'northness'],\n      dtype='object')\ntraining data row number:  305624\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.023636602343932618\nMSE is 0.030896115769907494\nR2 score is 0.9818134202335732\nRMSE is 0.17577290965876252\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232611181715.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-13.png\n",
  "history_begin_time" : 1701022628843,
  "history_end_time" : 1701022636291,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "9DHN7vzN7b1m",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_1yr_cummulative_v2.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', 'SWE',\n  'elevation',\t'slope',\t'curvature',\t'aspect',\t'eastness',\t'northness', 'air_temperature_tmmn',  \n  'potential_evapotranspiration', 'relative_humidity_rmax',   'northness'\n]\n\n\n\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n#hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-26\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_1yr_cummulative_v2.csv\n         date       lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-03  37.19236  ...               6.800000                0\n1  2019-11-10  37.19236  ...              81.100001              500\n2  2019-10-19  37.19236  ...              40.800000              250\n3  2019-12-12  37.19236  ...             145.200002             4862\n4  2019-12-06  37.19236  ...             132.800002             3913\n[5 rows x 34 columns]\nget swe statistics\ncount    305624.000000\nmean          0.813043\nstd           1.305277\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           1.325000\nmax           3.400000\nName: swe_value, dtype: float64\nrequired features: Index(['SWE', 'elevation', 'slope', 'curvature', 'aspect', 'eastness',\n       'northness', 'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    305624.000000\nmean          0.813043\nstd           1.305277\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           1.325000\nmax           3.400000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'elevation', 'slope', 'curvature', 'aspect', 'eastness',\n       'northness', 'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'northness'],\n      dtype='object')\ntraining data row number:  305624\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.028716700345385832\nMSE is 0.03669092142521358\nR2 score is 0.9784023864302945\nRMSE is 0.1915487442538154\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232611181633.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-12.png\n",
  "history_begin_time" : 1701022587659,
  "history_end_time" : 1701022595273,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "UWAEJ6pDhN1K",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_1yr_cummulative_v2.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value', 'fsca', 'SWE',\n  'elevation',\t'slope',\t'curvature',\t'aspect',\t'eastness',\t'northness', 'air_temperature_tmmn',  \n  'potential_evapotranspiration', 'relative_humidity_rmax',   'northness'\n]\n\n\n\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n#hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-26\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_1yr_cummulative_v2.csv\n         date       lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-03  37.19236  ...               6.800000                0\n1  2019-11-10  37.19236  ...              81.100001              500\n2  2019-10-19  37.19236  ...              40.800000              250\n3  2019-12-12  37.19236  ...             145.200002             4862\n4  2019-12-06  37.19236  ...             132.800002             3913\n[5 rows x 34 columns]\nget swe statistics\ncount    305624.000000\nmean          0.813043\nstd           1.305277\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           1.325000\nmax           3.400000\nName: swe_value, dtype: float64\nrequired features: Index(['fsca', 'SWE', 'elevation', 'slope', 'curvature', 'aspect', 'eastness',\n       'northness', 'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    305624.000000\nmean          0.813043\nstd           1.305277\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           1.325000\nmax           3.400000\nName: swe_value, dtype: float64\ninput features and order:  Index(['fsca', 'SWE', 'elevation', 'slope', 'curvature', 'aspect', 'eastness',\n       'northness', 'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'northness'],\n      dtype='object')\ntraining data row number:  305624\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.023533105326521492\nMSE is 0.03058544270184114\nR2 score is 0.981996293724071\nRMSE is 0.1748869426282052\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232611181611.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-13.png\n",
  "history_begin_time" : 1701022565496,
  "history_end_time" : 1701022573152,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "zg6vDqRGls63",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_1yr_cummulative_v2.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value',\n  'cumulative_air_temperature_tmmn',\t'cumulative_potential_evapotranspiration',\t\t'cumulative_relative_humidity_rmax',\t'cumulative_relative_humidity_rmin',\t'cumulative_precipitation_amount',\t'cumulative_air_temperature_tmmx',\t'cumulative_wind_speed',\t\n  'elevation',\t'slope',\t'curvature',\t'aspect',\t'eastness',\t'northness', 'air_temperature_tmmn',  \n  'potential_evapotranspiration', 'relative_humidity_rmax',   'northness'\n]\n\n\n\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n#hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-26\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_1yr_cummulative_v2.csv\n         date       lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-03  37.19236  ...               6.800000                0\n1  2019-11-10  37.19236  ...              81.100001              500\n2  2019-10-19  37.19236  ...              40.800000              250\n3  2019-12-12  37.19236  ...             145.200002             4862\n4  2019-12-06  37.19236  ...             132.800002             3913\n[5 rows x 34 columns]\nget swe statistics\ncount    305624.000000\nmean          0.813043\nstd           1.305277\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           1.325000\nmax           3.400000\nName: swe_value, dtype: float64\nrequired features: Index(['cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    305624.000000\nmean          0.813043\nstd           1.305277\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           1.325000\nmax           3.400000\nName: swe_value, dtype: float64\ninput features and order:  Index(['cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'northness'],\n      dtype='object')\ntraining data row number:  305624\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.004268040899796581\nMSE is 0.0011290709856850685\nR2 score is 0.9993353876682738\nRMSE is 0.03360165153210581\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232611181528.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-18.png\n",
  "history_begin_time" : 1701022520564,
  "history_end_time" : 1701022529507,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "rNwn7EFDXa7l",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_1yr_cummulative_v2.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\nselected_columns = [\n  'SWE', \n                    'air_temperature_tmmn', \n                    #'cumulative_SWE', \n  'air_temperature_tmmx',\n                    #'cumulative_potential_evapotranspiration', \n  #'cumulative_mean_vapor_pressure_deficit',\n   #                 'cumulative_relative_humidity_rmax', \n    #                'cumulative_relative_humidity_rmin', \n     #               'cumulative_precipitation_amount',\n#                     'cumulative_Flag', \n                    'elevation',\n                    \n                    'potential_evapotranspiration',\n  'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount', 'wind_speed',\n  #'slope','aspect',\n  'northness',\n                    \n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n#  'wind_speed',  'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', \n# 'cumulative_Flag',  'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n                    \n                    'swe_value', 'fsca']\n\nselected_columns = [\n  'swe_value',\n  'cumulative_air_temperature_tmmn',\t'cumulative_potential_evapotranspiration',\t'cumulative_mean_vapor_pressure_deficit',\t'cumulative_relative_humidity_rmax',\t'cumulative_relative_humidity_rmin',\t'cumulative_precipitation_amount',\t'cumulative_air_temperature_tmmx',\t'cumulative_wind_speed',\t\n  'elevation',\t'slope',\t'curvature',\t'aspect',\t'eastness',\t'northness', 'air_temperature_tmmn',  \n  'potential_evapotranspiration', 'relative_humidity_rmax',   'northness'\n]\n\n\n\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n#hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-26\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_1yr_cummulative_v2.csv\n         date       lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-03  37.19236  ...               6.800000                0\n1  2019-11-10  37.19236  ...              81.100001              500\n2  2019-10-19  37.19236  ...              40.800000              250\n3  2019-12-12  37.19236  ...             145.200002             4862\n4  2019-12-06  37.19236  ...             132.800002             3913\n[5 rows x 34 columns]\nget swe statistics\ncount    305624.000000\nmean          0.813043\nstd           1.305277\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           1.325000\nmax           3.400000\nName: swe_value, dtype: float64\nrequired features: Index(['cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    305624.000000\nmean          0.813043\nstd           1.305277\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           1.325000\nmax           3.400000\nName: swe_value, dtype: float64\ninput features and order:  Index(['cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'northness'],\n      dtype='object')\ntraining data row number:  305624\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.0034810306748476903\nMSE is 0.0006503183640081787\nR2 score is 0.9996171989097695\nRMSE is 0.02550134043551787\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232611181408.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-19.png\n",
  "history_begin_time" : 1701022441043,
  "history_end_time" : 1701022449970,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "XD9kBQ2cwcCo",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_1yr_cummulative_v2.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\nselected_columns = [\n  'SWE', \n                    'air_temperature_tmmn', \n                    #'cumulative_SWE', \n  'air_temperature_tmmx',\n                    #'cumulative_potential_evapotranspiration', \n  #'cumulative_mean_vapor_pressure_deficit',\n   #                 'cumulative_relative_humidity_rmax', \n    #                'cumulative_relative_humidity_rmin', \n     #               'cumulative_precipitation_amount',\n#                     'cumulative_Flag', \n                    'elevation',\n                    \n                    'potential_evapotranspiration',\n  'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount', 'wind_speed',\n  #'slope','aspect',\n  'northness',\n                    \n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n#  'wind_speed',  'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', \n# 'cumulative_Flag',  'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n                    \n                    'swe_value', 'fsca']\n\nselected_columns = [\n  'swe_value',\n  'cumulative_air_temperature_tmmn',\t'cumulative_potential_evapotranspiration',\t'cumulative_mean_vapor_pressure_deficit',\t'cumulative_relative_humidity_rmax',\t'cumulative_relative_humidity_rmin',\t'cumulative_precipitation_amount',\t'cumulative_air_temperature_tmmx',\t'cumulative_wind_speed',\t\n  'elevation',\t'slope',\t'curvature',\t'aspect',\t'eastness',\t'northness', 'air_temperature_tmmn',  \n  'potential_evapotranspiration', 'relative_humidity_rmax',  'fsca', 'northness'\n]\n\n\n\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n#hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-26\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_1yr_cummulative_v2.csv\n         date       lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-03  37.19236  ...               6.800000                0\n1  2019-11-10  37.19236  ...              81.100001              500\n2  2019-10-19  37.19236  ...              40.800000              250\n3  2019-12-12  37.19236  ...             145.200002             4862\n4  2019-12-06  37.19236  ...             132.800002             3913\n[5 rows x 34 columns]\nget swe statistics\ncount    305624.000000\nmean          0.813043\nstd           1.305277\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           1.325000\nmax           3.400000\nName: swe_value, dtype: float64\nrequired features: Index(['cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'fsca', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    305624.000000\nmean          0.813043\nstd           1.305277\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           1.325000\nmax           3.400000\nName: swe_value, dtype: float64\ninput features and order:  Index(['cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'fsca', 'northness'],\n      dtype='object')\ntraining data row number:  305624\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.004366740286299636\nMSE is 0.0014307294396728\nR2 score is 0.9991578205081647\nRMSE is 0.03782498433142835\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232611181345.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-20.png\n",
  "history_begin_time" : 1701022417761,
  "history_end_time" : 1701022426658,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "UTZ044eHejJI",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_1yr_cummulative_v2.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\nselected_columns = [\n  'SWE', \n                    'air_temperature_tmmn', \n                    #'cumulative_SWE', \n  'air_temperature_tmmx',\n                    #'cumulative_potential_evapotranspiration', \n  #'cumulative_mean_vapor_pressure_deficit',\n   #                 'cumulative_relative_humidity_rmax', \n    #                'cumulative_relative_humidity_rmin', \n     #               'cumulative_precipitation_amount',\n#                     'cumulative_Flag', \n                    'elevation',\n                    \n                    'potential_evapotranspiration',\n  'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount', 'wind_speed',\n  #'slope','aspect',\n  'northness',\n                    \n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n#  'wind_speed',  'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', \n# 'cumulative_Flag',  'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n                    \n                    'swe_value', 'fsca']\n\nselected_columns = [\n  'swe_value',\n  'cumulative_SWE', 'cumulative_air_temperature_tmmn',\t'cumulative_potential_evapotranspiration',\t'cumulative_mean_vapor_pressure_deficit',\t'cumulative_relative_humidity_rmax',\t'cumulative_relative_humidity_rmin',\t'cumulative_precipitation_amount',\t'cumulative_air_temperature_tmmx',\t'cumulative_wind_speed',\t\n  'elevation',\t'slope',\t'curvature',\t'aspect',\t'eastness',\t'northness', 'air_temperature_tmmn',  \n  'potential_evapotranspiration', 'relative_humidity_rmax',  'fsca', 'northness'\n]\n\n\n\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n#hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-26\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_1yr_cummulative_v2.csv\n         date       lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-03  37.19236  ...               6.800000                0\n1  2019-11-10  37.19236  ...              81.100001              500\n2  2019-10-19  37.19236  ...              40.800000              250\n3  2019-12-12  37.19236  ...             145.200002             4862\n4  2019-12-06  37.19236  ...             132.800002             3913\n[5 rows x 34 columns]\nget swe statistics\ncount    305624.000000\nmean          0.813043\nstd           1.305277\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           1.325000\nmax           3.400000\nName: swe_value, dtype: float64\nrequired features: Index(['cumulative_SWE', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'fsca', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    305624.000000\nmean          0.813043\nstd           1.305277\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           1.325000\nmax           3.400000\nName: swe_value, dtype: float64\ninput features and order:  Index(['cumulative_SWE', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'relative_humidity_rmax', 'fsca', 'northness'],\n      dtype='object')\ntraining data row number:  305624\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.003885873210635021\nMSE is 0.0011409367689161538\nR2 score is 0.9993284030358097\nRMSE is 0.033777755534022\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232611175927.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-21.png\n",
  "history_begin_time" : 1701021559822,
  "history_end_time" : 1701021569455,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "p4iNfAYGgclX",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_1yr_cummulative_v2.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\nselected_columns = [\n  'SWE', \n                    'air_temperature_tmmn', \n                    #'cumulative_SWE', \n  'air_temperature_tmmx',\n                    #'cumulative_potential_evapotranspiration', \n  #'cumulative_mean_vapor_pressure_deficit',\n   #                 'cumulative_relative_humidity_rmax', \n    #                'cumulative_relative_humidity_rmin', \n     #               'cumulative_precipitation_amount',\n#                     'cumulative_Flag', \n                    'elevation',\n                    \n                    'potential_evapotranspiration',\n  'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount', 'wind_speed',\n  #'slope','aspect',\n  'northness',\n                    \n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n#  'wind_speed',  'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', \n# 'cumulative_Flag',  'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n                    \n                    'swe_value', 'fsca']\n\nselected_columns = [\n  'swe_value',\n  'cumulative_SWE', 'cumulative_air_temperature_tmmn',\t'cumulative_potential_evapotranspiration',\t'cumulative_mean_vapor_pressure_deficit',\t'cumulative_relative_humidity_rmax',\t'cumulative_relative_humidity_rmin',\t'cumulative_precipitation_amount',\t'cumulative_air_temperature_tmmx',\t'cumulative_wind_speed',\t'cumulative_fsca',\n  'elevation',\t'slope',\t'curvature',\t'aspect',\t'eastness',\t'northness', 'SWE', 'air_temperature_tmmn',  \n  'potential_evapotranspiration', 'relative_humidity_rmax',  'fsca', 'northness'\n]\n\n\n\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n#hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-26\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_1yr_cummulative_v2.csv\n         date       lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-03  37.19236  ...               6.800000                0\n1  2019-11-10  37.19236  ...              81.100001              500\n2  2019-10-19  37.19236  ...              40.800000              250\n3  2019-12-12  37.19236  ...             145.200002             4862\n4  2019-12-06  37.19236  ...             132.800002             3913\n[5 rows x 34 columns]\nget swe statistics\ncount    305624.000000\nmean          0.813043\nstd           1.305277\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           1.325000\nmax           3.400000\nName: swe_value, dtype: float64\nrequired features: Index(['cumulative_SWE', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n       'cumulative_fsca', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness', 'SWE', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'fsca',\n       'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    305624.000000\nmean          0.813043\nstd           1.305277\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           1.325000\nmax           3.400000\nName: swe_value, dtype: float64\ninput features and order:  Index(['cumulative_SWE', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n       'cumulative_fsca', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness', 'SWE', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'fsca',\n       'northness'],\n      dtype='object')\ntraining data row number:  305624\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.004756957055215822\nMSE is 0.0016950879509202428\nR2 score is 0.9990022094537675\nRMSE is 0.04117144582013416\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232611174647.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-23.png\n",
  "history_begin_time" : 1701020798382,
  "history_end_time" : 1701020808381,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "w245NB7fgX6M",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_1yr_cummulative_v2.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\nselected_columns = [\n  'SWE', \n                    'air_temperature_tmmn', \n                    #'cumulative_SWE', \n  'air_temperature_tmmx',\n                    #'cumulative_potential_evapotranspiration', \n  #'cumulative_mean_vapor_pressure_deficit',\n   #                 'cumulative_relative_humidity_rmax', \n    #                'cumulative_relative_humidity_rmin', \n     #               'cumulative_precipitation_amount',\n#                     'cumulative_Flag', \n                    'elevation',\n                    \n                    'potential_evapotranspiration',\n  'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount', 'wind_speed',\n  #'slope','aspect',\n  'northness',\n                    \n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n#  'wind_speed',  'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', \n# 'cumulative_Flag',  'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n                    \n                    'swe_value', 'fsca']\n\nselected_columns = [\n  'water_year', 'swe_value',\n  'cumulative_SWE', 'cumulative_air_temperature_tmmn',\t'cumulative_potential_evapotranspiration',\t'cumulative_mean_vapor_pressure_deficit',\t'cumulative_relative_humidity_rmax',\t'cumulative_relative_humidity_rmin',\t'cumulative_precipitation_amount',\t'cumulative_air_temperature_tmmx',\t'cumulative_wind_speed',\t'cumulative_fsca',\n  'elevation',\t'slope',\t'curvature',\t'aspect',\t'eastness',\t'northness', 'SWE', 'air_temperature_tmmn',  'air_temperature_tmmx',\n  'potential_evapotranspiration', 'mean_vapor_pressure_deficit', 'relative_humidity_rmax', 'relative_humidity_rmin', 'precipitation_amount', 'fsca', 'wind_speed', 'northness'\n]\n\n\n\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n#hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-26\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_1yr_cummulative_v2.csv\n         date       lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-03  37.19236  ...               6.800000                0\n1  2019-11-10  37.19236  ...              81.100001              500\n2  2019-10-19  37.19236  ...              40.800000              250\n3  2019-12-12  37.19236  ...             145.200002             4862\n4  2019-12-06  37.19236  ...             132.800002             3913\n[5 rows x 34 columns]\nget swe statistics\ncount    305624.000000\nmean          0.813043\nstd           1.305277\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           1.325000\nmax           3.400000\nName: swe_value, dtype: float64\nrequired features: Index(['water_year', 'cumulative_SWE', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n       'cumulative_fsca', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness', 'SWE', 'air_temperature_tmmn',\n       'air_temperature_tmmx', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount', 'fsca', 'wind_speed',\n       'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    305624.000000\nmean          0.813043\nstd           1.305277\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           1.325000\nmax           3.400000\nName: swe_value, dtype: float64\ninput features and order:  Index(['water_year', 'cumulative_SWE', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n       'cumulative_fsca', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness', 'SWE', 'air_temperature_tmmn',\n       'air_temperature_tmmx', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount', 'fsca', 'wind_speed',\n       'northness'],\n      dtype='object')\ntraining data row number:  305624\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.006442192229039945\nMSE is 0.0027089233701431434\nR2 score is 0.9984054289762783\nRMSE is 0.052047318568233115\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232611172659.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-29.png\n",
  "history_begin_time" : 1701019603730,
  "history_end_time" : 1701019620346,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "aduexgfemi8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701013937399,
  "history_end_time" : 1701015920036,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "uj9fxr8pivj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700974688686,
  "history_end_time" : 1700974688686,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "hh9hxbap3en",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700885116806,
  "history_end_time" : 1700885116806,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "gkG7kXapjPxL",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_1yr_cummulative_v2.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\nselected_columns = [\n  'SWE', \n                    'air_temperature_tmmn', \n                    #'cumulative_SWE', \n  'air_temperature_tmmx',\n                    #'cumulative_potential_evapotranspiration', \n  #'cumulative_mean_vapor_pressure_deficit',\n   #                 'cumulative_relative_humidity_rmax', \n    #                'cumulative_relative_humidity_rmin', \n     #               'cumulative_precipitation_amount',\n#                     'cumulative_Flag', \n                    'elevation',\n                    \n                    'potential_evapotranspiration',\n  'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount', 'wind_speed',\n  #'slope','aspect',\n  'northness',\n                    \n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n#  'wind_speed',  'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', \n# 'cumulative_Flag',  'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n                    \n                    'swe_value', 'fsca']\n\nselected_columns = [\n  'water_year', 'swe_value',\n  'cumulative_SWE', 'cumulative_air_temperature_tmmn',\t'cumulative_potential_evapotranspiration',\t'cumulative_mean_vapor_pressure_deficit',\t'cumulative_relative_humidity_rmax',\t'cumulative_relative_humidity_rmin',\t'cumulative_precipitation_amount',\t'cumulative_air_temperature_tmmx',\t'cumulative_wind_speed',\t'cumulative_fsca',\n  'elevation',\t'slope',\t'curvature',\t'aspect',\t'eastness',\t'northness', 'SWE', 'air_temperature_tmmn',  'air_temperature_tmmx',\n  'potential_evapotranspiration', 'mean_vapor_pressure_deficit', 'relative_humidity_rmax', 'relative_humidity_rmin', 'precipitation_amount', 'fsca', 'wind_speed', 'northness'\n]\n\n\n\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n#hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-25\ntest start date:  2022-10-05\ntest end date:  2023-10-07\n/home/chetana\n2022275\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_1yr_cummulative_v2.csv\n         date       lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-03  37.19236  ...               6.800000                0\n1  2019-11-10  37.19236  ...              81.100001              500\n2  2019-10-19  37.19236  ...              40.800000              250\n3  2019-12-12  37.19236  ...             145.200002             4862\n4  2019-12-06  37.19236  ...             132.800002             3913\n[5 rows x 34 columns]\nget swe statistics\ncount    305624.000000\nmean          0.813043\nstd           1.305277\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           1.325000\nmax           3.400000\nName: swe_value, dtype: float64\nrequired features: Index(['water_year', 'cumulative_SWE', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n       'cumulative_fsca', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness', 'SWE', 'air_temperature_tmmn',\n       'air_temperature_tmmx', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount', 'fsca', 'wind_speed',\n       'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    305624.000000\nmean          0.813043\nstd           1.305277\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           1.325000\nmax           3.400000\nName: swe_value, dtype: float64\ninput features and order:  Index(['water_year', 'cumulative_SWE', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n       'cumulative_fsca', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness', 'SWE', 'air_temperature_tmmn',\n       'air_temperature_tmmx', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount', 'fsca', 'wind_speed',\n       'northness'],\n      dtype='object')\ntraining data row number:  305624\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.006442192229039946\nMSE is 0.002708923370143144\nR2 score is 0.9984054289762783\nRMSE is 0.052047318568233115\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232511034749.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-29.png\n",
  "history_begin_time" : 1700884058044,
  "history_end_time" : 1700884071035,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "StDLKVniFRyb",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_1yr_cummulative_v2.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\nselected_columns = [\n  'SWE', \n                    'air_temperature_tmmn', \n                    #'cumulative_SWE', \n  'air_temperature_tmmx',\n                    #'cumulative_potential_evapotranspiration', \n  #'cumulative_mean_vapor_pressure_deficit',\n   #                 'cumulative_relative_humidity_rmax', \n    #                'cumulative_relative_humidity_rmin', \n     #               'cumulative_precipitation_amount',\n#                     'cumulative_Flag', \n                    'elevation',\n                    \n                    'potential_evapotranspiration',\n  'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount', 'wind_speed',\n  #'slope','aspect',\n  'northness',\n                    \n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n#  'wind_speed',  'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', \n# 'cumulative_Flag',  'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n                    \n                    'swe_value', 'fsca']\n\nselected_columns = [\n  'lat','lon', 'water_year', 'swe_value',\n  'cumulative_SWE',\t'cumulative_Flag',\t'cumulative_air_temperature_tmmn',\t'cumulative_potential_evapotranspiration',\t'cumulative_mean_vapor_pressure_deficit',\t'cumulative_relative_humidity_rmax',\t'cumulative_relative_humidity_rmin',\t'cumulative_precipitation_amount',\t'cumulative_air_temperature_tmmx',\t'cumulative_wind_speed',\t'cumulative_fsca',\n  'elevation',\t'slope',\t'curvature',\t'aspect',\t'eastness',\t'northness', 'SWE', 'air_temperature_tmmn',  'air_temperature_tmmx',\n  'potential_evapotranspiration', 'mean_vapor_pressure_deficit', 'relative_humidity_rmax', 'relative_humidity_rmin', 'precipitation_amount', 'fsca', 'wind_speed', 'northness'\n]\n\n\n\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n#hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-22\ntest start date:  2022-12-15\ntest end date:  2023-10-07\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_1yr_cummulative_v2.csv\n         date       lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-03  37.19236  ...               6.800000                0\n1  2019-11-10  37.19236  ...              81.100001              500\n2  2019-10-19  37.19236  ...              40.800000              250\n3  2019-12-12  37.19236  ...             145.200002             4862\n4  2019-12-06  37.19236  ...             132.800002             3913\n[5 rows x 34 columns]\nget swe statistics\ncount    305624.000000\nmean          0.813043\nstd           1.305277\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           1.325000\nmax           3.400000\nName: swe_value, dtype: float64\nrequired features: Index(['lat', 'lon', 'water_year', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n       'cumulative_fsca', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness', 'SWE', 'air_temperature_tmmn',\n       'air_temperature_tmmx', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount', 'fsca', 'wind_speed',\n       'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    305624.000000\nmean          0.813043\nstd           1.305277\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           1.325000\nmax           3.400000\nName: swe_value, dtype: float64\ninput features and order:  Index(['lat', 'lon', 'water_year', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n       'cumulative_fsca', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness', 'SWE', 'air_temperature_tmmn',\n       'air_temperature_tmmx', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount', 'fsca', 'wind_speed',\n       'northness'],\n      dtype='object')\ntraining data row number:  305624\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.004633505112475533\nMSE is 0.0016560354519427382\nR2 score is 0.9990251971779534\nRMSE is 0.04069441548840256\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232211173109.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-32.png\n",
  "history_begin_time" : 1700674257124,
  "history_end_time" : 1700674271050,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "G2msheLG35B2",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_1yr_cummulative_v2.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\nselected_columns = [\n  'SWE', \n                    'air_temperature_tmmn', \n                    #'cumulative_SWE', \n  'air_temperature_tmmx',\n                    #'cumulative_potential_evapotranspiration', \n  #'cumulative_mean_vapor_pressure_deficit',\n   #                 'cumulative_relative_humidity_rmax', \n    #                'cumulative_relative_humidity_rmin', \n     #               'cumulative_precipitation_amount',\n#                     'cumulative_Flag', \n                    'elevation',\n                    \n                    'potential_evapotranspiration',\n  'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount', 'wind_speed',\n  #'slope','aspect',\n  'northness',\n                    \n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n#  'wind_speed',  'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', \n# 'cumulative_Flag',  'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n                    \n                    'swe_value', 'fsca']\n\nselected_columns = [\n  'lat','lon', 'water_year', 'swe_value',\n  'cumulative_SWE',\t'cumulative_Flag',\t'cumulative_air_temperature_tmmn',\t'cumulative_potential_evapotranspiration',\t'cumulative_mean_vapor_pressure_deficit',\t'cumulative_relative_humidity_rmax',\t'cumulative_relative_humidity_rmin',\t'cumulative_precipitation_amount',\t'cumulative_air_temperature_tmmx',\t'cumulative_wind_speed',\t'cumulative_fsca',\n  'elevation',\t'slope',\t'curvature',\t'aspect',\t'eastness',\t'northness', 'SWE', 'air_temperature_tmmn',  'air_temperature_tmmx',\n  'potential_evapotranspiration', 'mean_vapor_pressure_deficit', 'relative_humidity_rmax', 'relative_humidity_rmin', 'precipitation_amount', 'wind_speed', 'northness'\n]\n\n\n\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n#hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-22\ntest start date:  2022-12-15\ntest end date:  2023-10-07\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_1yr_cummulative_v2.csv\n         date       lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-03  37.19236  ...               6.800000                0\n1  2019-11-10  37.19236  ...              81.100001              500\n2  2019-10-19  37.19236  ...              40.800000              250\n3  2019-12-12  37.19236  ...             145.200002             4862\n4  2019-12-06  37.19236  ...             132.800002             3913\n[5 rows x 34 columns]\nget swe statistics\ncount    305624.000000\nmean          0.813043\nstd           1.305277\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           1.325000\nmax           3.400000\nName: swe_value, dtype: float64\nrequired features: Index(['lat', 'lon', 'water_year', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n       'cumulative_fsca', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness', 'SWE', 'air_temperature_tmmn',\n       'air_temperature_tmmx', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount', 'wind_speed',\n       'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    305624.000000\nmean          0.813043\nstd           1.305277\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           1.325000\nmax           3.400000\nName: swe_value, dtype: float64\ninput features and order:  Index(['lat', 'lon', 'water_year', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n       'cumulative_fsca', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness', 'SWE', 'air_temperature_tmmn',\n       'air_temperature_tmmx', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount', 'wind_speed',\n       'northness'],\n      dtype='object')\ntraining data row number:  305624\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.003928229038855899\nMSE is 0.0009016131697341498\nR2 score is 0.9994692776285554\nRMSE is 0.030026874125259024\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232211170216.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-31.png\n",
  "history_begin_time" : 1700672524884,
  "history_end_time" : 1700672537379,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "rmMmDvafbNHB",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_1yr_cummulative_v2.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\nselected_columns = [\n  'SWE', \n                    'air_temperature_tmmn', \n                    #'cumulative_SWE', \n  'air_temperature_tmmx',\n                    #'cumulative_potential_evapotranspiration', \n  #'cumulative_mean_vapor_pressure_deficit',\n   #                 'cumulative_relative_humidity_rmax', \n    #                'cumulative_relative_humidity_rmin', \n     #               'cumulative_precipitation_amount',\n#                     'cumulative_Flag', \n                    'elevation',\n                    \n                    'potential_evapotranspiration',\n  'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount', 'wind_speed',\n  #'slope','aspect',\n  'northness',\n                    \n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n#  'wind_speed',  'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', \n# 'cumulative_Flag',  'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n                    \n                    'swe_value', 'fsca']\n\nselected_columns = [\n  'lat','lon', 'water_year', 'swe_value',\n  'cumulative_SWE',\t'cumulative_Flag',\t'cumulative_air_temperature_tmmn',\t'cumulative_potential_evapotranspiration',\t'cumulative_mean_vapor_pressure_deficit',\t'cumulative_relative_humidity_rmax',\t'cumulative_relative_humidity_rmin',\t'cumulative_precipitation_amount',\t'cumulative_air_temperature_tmmx',\t'cumulative_wind_speed',\t'cumulative_fsca',\n  'elevation',\t'slope',\t'curvature',\t'aspect',\t'eastness',\t'northness'\n\n]\n\n\n\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n#hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-22\ntest start date:  2022-12-15\ntest end date:  2023-10-07\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_1yr_cummulative_v2.csv\n         date       lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-03  37.19236  ...               6.800000                0\n1  2019-11-10  37.19236  ...              81.100001              500\n2  2019-10-19  37.19236  ...              40.800000              250\n3  2019-12-12  37.19236  ...             145.200002             4862\n4  2019-12-06  37.19236  ...             132.800002             3913\n[5 rows x 34 columns]\nget swe statistics\ncount    305624.000000\nmean          0.813043\nstd           1.305277\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           1.325000\nmax           3.400000\nName: swe_value, dtype: float64\nrequired features: Index(['lat', 'lon', 'water_year', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n       'cumulative_fsca', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    305624.000000\nmean          0.813043\nstd           1.305277\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           1.325000\nmax           3.400000\nName: swe_value, dtype: float64\ninput features and order:  Index(['lat', 'lon', 'water_year', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n       'cumulative_fsca', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  305624\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.0031614233128845312\nMSE is 0.0007490535787321064\nR2 score is 0.9995590797639292\nRMSE is 0.0273688432114349\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232211165242.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-21.png\n",
  "history_begin_time" : 1700671954736,
  "history_end_time" : 1700671963582,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "IkzBzPxjuxOc",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_1yr_cummulative_v2.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\nselected_columns = [\n  'SWE', \n                    'air_temperature_tmmn', \n                    #'cumulative_SWE', \n  'air_temperature_tmmx',\n                    #'cumulative_potential_evapotranspiration', \n  #'cumulative_mean_vapor_pressure_deficit',\n   #                 'cumulative_relative_humidity_rmax', \n    #                'cumulative_relative_humidity_rmin', \n     #               'cumulative_precipitation_amount',\n#                     'cumulative_Flag', \n                    'elevation',\n                    \n                    'potential_evapotranspiration',\n  'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount', 'wind_speed',\n  #'slope','aspect',\n  'northness',\n                    \n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n#  'wind_speed',  'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', \n# 'cumulative_Flag',  'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n                    \n                    'swe_value', 'fsca']\n\nselected_columns = [\n  'lat','lon', 'water_year', 'swe_value',\n  'cumulative_SWE',\t'cumulative_Flag',\t'cumulative_air_temperature_tmmn',\t'cumulative_potential_evapotranspiration',\t'cumulative_mean_vapor_pressure_deficit',\t'cumulative_relative_humidity_rmax',\t'cumulative_relative_humidity_rmin',\t'cumulative_precipitation_amount',\t'cumulative_air_temperature_tmmx',\t'cumulative_wind_speed',\t'cumulative_fsca'\n]\n\n\n\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n#hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-22\ntest start date:  2022-12-15\ntest end date:  2023-10-07\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_1yr_cummulative_v2.csv\n         date       lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-03  37.19236  ...               6.800000                0\n1  2019-11-10  37.19236  ...              81.100001              500\n2  2019-10-19  37.19236  ...              40.800000              250\n3  2019-12-12  37.19236  ...             145.200002             4862\n4  2019-12-06  37.19236  ...             132.800002             3913\n[5 rows x 34 columns]\nget swe statistics\ncount    305624.000000\nmean          0.813043\nstd           1.305277\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           1.325000\nmax           3.400000\nName: swe_value, dtype: float64\nrequired features: Index(['lat', 'lon', 'water_year', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n       'cumulative_fsca'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    305624.000000\nmean          0.813043\nstd           1.305277\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           1.325000\nmax           3.400000\nName: swe_value, dtype: float64\ninput features and order:  Index(['lat', 'lon', 'water_year', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n       'cumulative_fsca'],\n      dtype='object')\ntraining data row number:  305624\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.003304310838446924\nMSE is 0.0007393122944785277\nR2 score is 0.9995648138388668\nRMSE is 0.027190297800475223\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232211165010.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-15.png\n",
  "history_begin_time" : 1700671804186,
  "history_end_time" : 1700671811180,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "NVpaVxA7Xcug",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_1yr_cummulative_v2.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\nselected_columns = [\n  'SWE', \n                    'air_temperature_tmmn', \n                    #'cumulative_SWE', \n  'air_temperature_tmmx',\n                    #'cumulative_potential_evapotranspiration', \n  #'cumulative_mean_vapor_pressure_deficit',\n   #                 'cumulative_relative_humidity_rmax', \n    #                'cumulative_relative_humidity_rmin', \n     #               'cumulative_precipitation_amount',\n#                     'cumulative_Flag', \n                    'elevation',\n                    \n                    'potential_evapotranspiration',\n  'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount', 'wind_speed',\n  #'slope','aspect',\n  'northness',\n                    \n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n#  'wind_speed',  'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', \n# 'cumulative_Flag',  'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n                    \n                    'swe_value', 'fsca']\n\nselected_columns = [\n  'lat','lon', 'water_year', 'swe_value',\n  'cumulative_SWE',\t'cumulative_Flag',\t'cumulative_air_temperature_tmmn',\t'cumulative_potential_evapotranspiration',\t'cumulative_mean_vapor_pressure_deficit',\t'cumulative_relative_humidity_rmax',\t'cumulative_relative_humidity_rmin',\t'cumulative_precipitation_amount',\t'cumulative_air_temperature_tmmx',\t'cumulative_wind_speed',\t'cumulative_fsca', 'date'\n]\n\n\n\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n#hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-22\ntest start date:  2022-12-15\ntest end date:  2023-10-07\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_1yr_cummulative_v2.csv\n         date       lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-03  37.19236  ...               6.800000                0\n1  2019-11-10  37.19236  ...              81.100001              500\n2  2019-10-19  37.19236  ...              40.800000              250\n3  2019-12-12  37.19236  ...             145.200002             4862\n4  2019-12-06  37.19236  ...             132.800002             3913\n[5 rows x 34 columns]\nget swe statistics\ncount    305624.000000\nmean          0.813043\nstd           1.305277\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           1.325000\nmax           3.400000\nName: swe_value, dtype: float64\nrequired features: Index(['lat', 'lon', 'water_year', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n       'cumulative_fsca', 'date'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    305624.000000\nmean          0.813043\nstd           1.305277\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           1.325000\nmax           3.400000\nName: swe_value, dtype: float64\ninput features and order:  Index(['lat', 'lon', 'water_year', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n       'cumulative_fsca', 'date'],\n      dtype='object')\ntraining data row number:  305624\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.0023696359918211262\nMSE is 0.0005457093824130883\nR2 score is 0.9996787755688626\nRMSE is 0.023360423421100233\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232211164913.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-16.png\n",
  "history_begin_time" : 1700671747537,
  "history_end_time" : 1700671755452,
  "history_notes" : "from this run, the swe statistics is wrong. The max is 3.4, definitely lose some values",
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "0sVgwk8znc5Z",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_1yr_cummulative_v2.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\nselected_columns = [\n  'SWE', \n                    'air_temperature_tmmn', \n                    #'cumulative_SWE', \n  'air_temperature_tmmx',\n                    #'cumulative_potential_evapotranspiration', \n  #'cumulative_mean_vapor_pressure_deficit',\n   #                 'cumulative_relative_humidity_rmax', \n    #                'cumulative_relative_humidity_rmin', \n     #               'cumulative_precipitation_amount',\n#                     'cumulative_Flag', \n                    'elevation',\n                    \n                    'potential_evapotranspiration',\n  'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount', 'wind_speed',\n  #'slope','aspect',\n  'northness',\n                    \n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n#  'wind_speed',  'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', \n# 'cumulative_Flag',  'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n                    \n                    'swe_value', 'fsca']\n\nselected_columns = [\n  'lat','lon', 'water_year',\n  'cumulative_SWE',\t'cumulative_Flag',\t'cumulative_air_temperature_tmmn',\t'cumulative_potential_evapotranspiration',\t'cumulative_mean_vapor_pressure_deficit',\t'cumulative_relative_humidity_rmax',\t'cumulative_relative_humidity_rmin',\t'cumulative_precipitation_amount',\t'cumulative_air_temperature_tmmx',\t'cumulative_wind_speed',\t'cumulative_fsca', 'date'\n]\n\n\n\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n#hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-22\ntest start date:  2022-12-15\ntest end date:  2023-10-07\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_1yr_cummulative_v2.csv\n         date       lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-03  37.19236  ...               6.800000                0\n1  2019-11-10  37.19236  ...              81.100001              500\n2  2019-10-19  37.19236  ...              40.800000              250\n3  2019-12-12  37.19236  ...             145.200002             4862\n4  2019-12-06  37.19236  ...             132.800002             3913\n[5 rows x 34 columns]\nget swe statistics\ncount    305624.000000\nmean          0.813043\nstd           1.305277\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           1.325000\nmax           3.400000\nName: swe_value, dtype: float64\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/0sVgwk8znc5Z/model_creation_et.py\", line 256, in <module>\n    hole.preprocessing(chosen_columns = selected_columns)\n  File \"/home/chetana/gw-workspace/0sVgwk8znc5Z/model_creation_et.py\", line 130, in preprocessing\n    X = data.drop('swe_value', axis=1)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\", line 5399, in drop\n    return super().drop(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\", line 4505, in drop\n    obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\", line 4546, in _drop_axis\n    new_axis = axis.drop(labels, errors=errors)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 6934, in drop\n    raise KeyError(f\"{list(labels[mask])} not found in axis\")\nKeyError: \"['swe_value'] not found in axis\"\n",
  "history_begin_time" : 1700671727443,
  "history_end_time" : 1700671731035,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "2OAdqac2RmXZ",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_1yr_cummulative_v2.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\nselected_columns = [\n  'SWE', \n                    'air_temperature_tmmn', \n                    #'cumulative_SWE', \n  'air_temperature_tmmx',\n                    #'cumulative_potential_evapotranspiration', \n  #'cumulative_mean_vapor_pressure_deficit',\n   #                 'cumulative_relative_humidity_rmax', \n    #                'cumulative_relative_humidity_rmin', \n     #               'cumulative_precipitation_amount',\n#                     'cumulative_Flag', \n                    'elevation',\n                    \n                    'potential_evapotranspiration',\n  'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount', 'wind_speed',\n  #'slope','aspect',\n  'northness',\n                    \n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n#  'wind_speed',  'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', \n# 'cumulative_Flag',  'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n                    \n                    'swe_value', 'fsca']\n\nselected_columns = [\n  'lat','lon','SWE Flag', 'water_year',\n  'cumulative_SWE',\t'cumulative_Flag',\t'cumulative_air_temperature_tmmn',\t'cumulative_potential_evapotranspiration',\t'cumulative_mean_vapor_pressure_deficit',\t'cumulative_relative_humidity_rmax',\t'cumulative_relative_humidity_rmin',\t'cumulative_precipitation_amount',\t'cumulative_air_temperature_tmmx',\t'cumulative_wind_speed',\t'cumulative_fsca', 'date'\n]\n\n\n\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n#hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-22\ntest start date:  2022-12-15\ntest end date:  2023-10-07\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_1yr_cummulative_v2.csv\n         date       lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-03  37.19236  ...               6.800000                0\n1  2019-11-10  37.19236  ...              81.100001              500\n2  2019-10-19  37.19236  ...              40.800000              250\n3  2019-12-12  37.19236  ...             145.200002             4862\n4  2019-12-06  37.19236  ...             132.800002             3913\n[5 rows x 34 columns]\nget swe statistics\ncount    305624.000000\nmean          0.813043\nstd           1.305277\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           1.325000\nmax           3.400000\nName: swe_value, dtype: float64\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/2OAdqac2RmXZ/model_creation_et.py\", line 256, in <module>\n    hole.preprocessing(chosen_columns = selected_columns)\n  File \"/home/chetana/gw-workspace/2OAdqac2RmXZ/model_creation_et.py\", line 122, in preprocessing\n    data = data[chosen_columns]\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\", line 3813, in __getitem__\n    indexer = self.columns._get_indexer_strict(key, \"columns\")[1]\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 6070, in _get_indexer_strict\n    self._raise_if_missing(keyarr, indexer, axis_name)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 6133, in _raise_if_missing\n    raise KeyError(f\"{not_found} not in index\")\nKeyError: \"['SWE Flag'] not in index\"\n",
  "history_begin_time" : 1700671701081,
  "history_end_time" : 1700671704813,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "MObvvtE4xdDG",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/final_merged_data_1yr_cummulative_v2.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\nselected_columns = [\n  'SWE', \n                    'air_temperature_tmmn', \n                    #'cumulative_SWE', \n  'air_temperature_tmmx',\n                    #'cumulative_potential_evapotranspiration', \n  #'cumulative_mean_vapor_pressure_deficit',\n   #                 'cumulative_relative_humidity_rmax', \n    #                'cumulative_relative_humidity_rmin', \n     #               'cumulative_precipitation_amount',\n#                     'cumulative_Flag', \n                    'elevation',\n                    \n                    'potential_evapotranspiration',\n  'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount', 'wind_speed',\n  #'slope','aspect',\n  'northness',\n                    \n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n#  'wind_speed',  'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', \n# 'cumulative_Flag',  'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n                    \n                    'swe_value', 'fsca']\n\nselected_columns = [\n  'lat','lon','SWE Flag', 'water_year',\n  'cumulative_SWE',\t'cumulative_Flag',\t'cumulative_air_temperature_tmmn',\t'cumulative_potential_evapotranspiration',\t'cumulative_mean_vapor_pressure_deficit',\t'cumulative_relative_humidity_rmax',\t'cumulative_relative_humidity_rmin',\t'cumulative_precipitation_amount',\t'cumulative_air_temperature_tmmx',\t'cumulative_wind_speed',\t'cumulative_fsca'\n]\n\n\n\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n#hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-22\ntest start date:  2022-12-15\ntest end date:  2023-10-07\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_1yr_cummulative_v2.csv\n         date       lat  ...  cumulative_wind_speed  cumulative_fsca\n0  2019-10-03  37.19236  ...               6.800000                0\n1  2019-11-10  37.19236  ...              81.100001              500\n2  2019-10-19  37.19236  ...              40.800000              250\n3  2019-12-12  37.19236  ...             145.200002             4862\n4  2019-12-06  37.19236  ...             132.800002             3913\n[5 rows x 34 columns]\nget swe statistics\ncount    305624.000000\nmean          0.813043\nstd           1.305277\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           1.325000\nmax           3.400000\nName: swe_value, dtype: float64\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/MObvvtE4xdDG/model_creation_et.py\", line 256, in <module>\n    hole.preprocessing(chosen_columns = selected_columns)\n  File \"/home/chetana/gw-workspace/MObvvtE4xdDG/model_creation_et.py\", line 122, in preprocessing\n    data = data[chosen_columns]\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\", line 3813, in __getitem__\n    indexer = self.columns._get_indexer_strict(key, \"columns\")[1]\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 6070, in _get_indexer_strict\n    self._raise_if_missing(keyarr, indexer, axis_name)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 6133, in _raise_if_missing\n    raise KeyError(f\"{not_found} not in index\")\nKeyError: \"['SWE Flag'] not in index\"\n",
  "history_begin_time" : 1700671535234,
  "history_end_time" : 1700671549478,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "x6PI4sy09kTS",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n\t\ttraining_data_path = f\"{working_dir}/final_merged_data_1yr_cummulative_v2.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\nselected_columns = [\n  'SWE', \n                    'air_temperature_tmmn', \n                    #'cumulative_SWE', \n  'air_temperature_tmmx',\n                    #'cumulative_potential_evapotranspiration', \n  #'cumulative_mean_vapor_pressure_deficit',\n   #                 'cumulative_relative_humidity_rmax', \n    #                'cumulative_relative_humidity_rmin', \n     #               'cumulative_precipitation_amount',\n#                     'cumulative_Flag', \n                    'elevation',\n                    \n                    'potential_evapotranspiration',\n  'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount', 'wind_speed',\n  #'slope','aspect',\n  'northness',\n                    \n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n#  'wind_speed',  'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', \n# 'cumulative_Flag',  'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n                    \n                    'swe_value']\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n#hole.post_processing()\n\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/x6PI4sy09kTS/model_creation_et.py\", line 93\n    training_data_path = f\"{working_dir}/final_merged_data_1yr_cummulative_v2.csv\"\nTabError: inconsistent use of tabs and spaces in indentation\n",
  "history_begin_time" : 1700671230723,
  "history_end_time" : 1700671230787,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "4wtax1alfei",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700471590179,
  "history_end_time" : 1700471590179,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rgjfz4g4zmp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700468936407,
  "history_end_time" : 1700468936407,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "udMtPnqcqdMf",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\nselected_columns = [\n  'SWE', \n                    'air_temperature_tmmn', \n                    #'cumulative_SWE', \n  'air_temperature_tmmx',\n                    #'cumulative_potential_evapotranspiration', \n  #'cumulative_mean_vapor_pressure_deficit',\n   #                 'cumulative_relative_humidity_rmax', \n    #                'cumulative_relative_humidity_rmin', \n     #               'cumulative_precipitation_amount',\n#                     'cumulative_Flag', \n                    'elevation',\n                    \n                    'potential_evapotranspiration',\n  'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount', 'wind_speed',\n  #'slope','aspect',\n  'northness',\n                    \n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n#  'wind_speed',  'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', \n# 'cumulative_Flag',  'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n                    \n                    'swe_value']\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n#hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-20\ntest start date:  2022-12-15\ntest end date:  2023-10-07\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv\n         date       lat  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  2020-11-25  37.19236  ...                          15589.2             208.000003\n1  2020-10-18  37.19236  ...                           5001.6              72.700001\n2  2020-09-21  37.19236  ...                         102150.4             977.200008\n3  2022-09-01  37.19236  ...                          94640.4             811.300009\n4  2019-10-03  37.19236  ...                            880.5               6.800000\n[5 rows x 32 columns]\nget swe statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nrequired features: Index(['SWE', 'air_temperature_tmmn', 'air_temperature_tmmx', 'elevation',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'wind_speed', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'air_temperature_tmmn', 'air_temperature_tmmx', 'elevation',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'wind_speed', 'northness'],\n      dtype='object')\ntraining data row number:  766500\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 1.0890393052837575\nMSE is 3.5943912487622307\nR2 score is 0.7734803585071587\nRMSE is 1.895887984233834\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232011075038.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-12.png\n",
  "history_begin_time" : 1700466614779,
  "history_end_time" : 1700466649833,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "h2c6wxi851b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700461922979,
  "history_end_time" : 1700462913674,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "rjeghrpkjip",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500132,
  "history_end_time" : 1700448500132,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "b9ve9q2479l",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700447319840,
  "history_end_time" : 1700447319840,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0636e8e06lg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700230067237,
  "history_end_time" : 1700230067237,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "cjn5f65nnxs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700229012361,
  "history_end_time" : 1700229012361,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "jntozy1nsf1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700210213804,
  "history_end_time" : 1700210213804,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "54cgyq4dseb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700209780156,
  "history_end_time" : 1700209780156,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "esu73otfdmw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700209729242,
  "history_end_time" : 1700209729242,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0x8856hlon5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700203478642,
  "history_end_time" : 1700204245681,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "oGZqT9t5PCBf",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\nselected_columns = [\n  'SWE', \n                    'air_temperature_tmmn', \n                    #'cumulative_SWE', \n  'air_temperature_tmmx',\n                    #'cumulative_potential_evapotranspiration', \n  #'cumulative_mean_vapor_pressure_deficit',\n   #                 'cumulative_relative_humidity_rmax', \n    #                'cumulative_relative_humidity_rmin', \n     #               'cumulative_precipitation_amount',\n#                     'cumulative_Flag', \n                    'elevation',\n                    \n                    'potential_evapotranspiration',\n  'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount', 'wind_speed','slope','aspect','northness',\n                    \n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n#  'wind_speed',  'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', \n# 'cumulative_Flag',  'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n                    \n                    'swe_value']\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n#hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-17\ntest start date:  2022-10-04\ntest end date:  2023-10-07\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv\n         date       lat  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  2020-11-25  37.19236  ...                          15589.2             208.000003\n1  2020-10-18  37.19236  ...                           5001.6              72.700001\n2  2020-09-21  37.19236  ...                         102150.4             977.200008\n3  2022-09-01  37.19236  ...                          94640.4             811.300009\n4  2019-10-03  37.19236  ...                            880.5               6.800000\n[5 rows x 32 columns]\nget swe statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nrequired features: Index(['SWE', 'air_temperature_tmmn', 'air_temperature_tmmx', 'elevation',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'wind_speed', 'slope', 'aspect', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'air_temperature_tmmn', 'air_temperature_tmmx', 'elevation',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'wind_speed', 'slope', 'aspect', 'northness'],\n      dtype='object')\ntraining data row number:  766500\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 1.0347834866275278\nMSE is 3.354269166609589\nR2 score is 0.7886129259432889\nRMSE is 1.831466397892571\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231711064143.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-14.png\n",
  "history_begin_time" : 1700203281576,
  "history_end_time" : 1700203312333,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "soKBlfdc6lpu",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\nselected_columns = [\n  'SWE', \n                    'air_temperature_tmmn', \n                    #'cumulative_SWE', \n  'air_temperature_tmmx',\n                    #'cumulative_potential_evapotranspiration', \n  #'cumulative_mean_vapor_pressure_deficit',\n   #                 'cumulative_relative_humidity_rmax', \n    #                'cumulative_relative_humidity_rmin', \n     #               'cumulative_precipitation_amount',\n#                     'cumulative_Flag', \n                    'elevation',\n                    \n                    'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n                    \n#                     'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n#  'wind_speed',  'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', \n# 'cumulative_Flag',  'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n                    \n                    'swe_value']\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n#hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-17\ntest start date:  2022-10-04\ntest end date:  2023-10-07\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv\n         date       lat  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  2020-11-25  37.19236  ...                          15589.2             208.000003\n1  2020-10-18  37.19236  ...                           5001.6              72.700001\n2  2020-09-21  37.19236  ...                         102150.4             977.200008\n3  2022-09-01  37.19236  ...                          94640.4             811.300009\n4  2019-10-03  37.19236  ...                            880.5               6.800000\n[5 rows x 32 columns]\nget swe statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nrequired features: Index(['SWE', 'air_temperature_tmmn', 'air_temperature_tmmx', 'elevation',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'air_temperature_tmmn', 'air_temperature_tmmx', 'elevation',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount'],\n      dtype='object')\ntraining data row number:  766500\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 1.1579772994129158\nMSE is 3.972565899657534\nR2 score is 0.7496476757484345\nRMSE is 1.9931296745715101\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231711063329.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-10.png\n",
  "history_begin_time" : 1700202794026,
  "history_end_time" : 1700202821217,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "I2vsbmHx56UJ",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\nselected_columns = [\n  #'SWE', \n                    #'air_temperature_tmmn', \n                    'cumulative_SWE', \n  #'air_temperature_tmmx',\n                    'cumulative_potential_evapotranspiration', \n  'cumulative_mean_vapor_pressure_deficit',\n                    'cumulative_relative_humidity_rmax', \n                    'cumulative_relative_humidity_rmin', \n                    'cumulative_precipitation_amount',\n#                     'cumulative_Flag', \n                    'elevation',\n                    \n#                     'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n#  'wind_speed',  'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', \n# 'cumulative_Flag',  'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n                    \n                    'swe_value']\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n#hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-17\ntest start date:  2022-10-04\ntest end date:  2023-10-07\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv\n         date       lat  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  2020-11-25  37.19236  ...                          15589.2             208.000003\n1  2020-10-18  37.19236  ...                           5001.6              72.700001\n2  2020-09-21  37.19236  ...                         102150.4             977.200008\n3  2022-09-01  37.19236  ...                          94640.4             811.300009\n4  2019-10-03  37.19236  ...                            880.5               6.800000\n[5 rows x 32 columns]\nget swe statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nrequired features: Index(['cumulative_SWE', 'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'elevation'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['cumulative_SWE', 'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'elevation'],\n      dtype='object')\ntraining data row number:  766500\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.04493501630789414\nMSE is 0.015011281343770383\nR2 score is 0.9990539844349137\nRMSE is 0.12252053437595832\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231711063135.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-8.png\n",
  "history_begin_time" : 1700202682367,
  "history_end_time" : 1700202700615,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "W8HM1BRA2rww",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\nselected_columns = [\n  'SWE', \n                    'air_temperature_tmmn', \n                    'cumulative_SWE', \n  'air_temperature_tmmx',\n                    'cumulative_potential_evapotranspiration', \n  'cumulative_mean_vapor_pressure_deficit',\n#                     'cumulative_relative_humidity_rmax', \n#                     'cumulative_relative_humidity_rmin', \n#                     'cumulative_precipitation_amount',\n#                     'cumulative_Flag', \n                    'elevation',\n                    \n#                     'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n#  'wind_speed',  'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', \n# 'cumulative_Flag',  'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n                    \n                    'swe_value']\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n#hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-17\ntest start date:  2022-10-04\ntest end date:  2023-10-07\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv\n         date       lat  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  2020-11-25  37.19236  ...                          15589.2             208.000003\n1  2020-10-18  37.19236  ...                           5001.6              72.700001\n2  2020-09-21  37.19236  ...                         102150.4             977.200008\n3  2022-09-01  37.19236  ...                          94640.4             811.300009\n4  2019-10-03  37.19236  ...                            880.5               6.800000\n[5 rows x 32 columns]\nget swe statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nrequired features: Index(['SWE', 'air_temperature_tmmn', 'cumulative_SWE', 'air_temperature_tmmx',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit', 'elevation'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'air_temperature_tmmn', 'cumulative_SWE', 'air_temperature_tmmx',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit', 'elevation'],\n      dtype='object')\ntraining data row number:  766500\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.2937835746901508\nMSE is 0.6532451697586432\nR2 score is 0.9588322885797104\nRMSE is 0.8082358379573645\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231711062854.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-8.png\n",
  "history_begin_time" : 1700202521550,
  "history_end_time" : 1700202540707,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "VF4flevLtJo2",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\nselected_columns = [\n  'SWE', \n                    'air_temperature_tmmn', \n#                     'cumulative_SWE', \n#                     'cumulative_potential_evapotranspiration', \n#                     'cumulative_relative_humidity_rmax', \n#                     'cumulative_relative_humidity_rmin', \n#                     'cumulative_precipitation_amount',\n#                     'cumulative_Flag', \n                    'elevation',\n                    \n#                     'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed',  'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', \n# 'cumulative_Flag', 'cumulative_mean_vapor_pressure_deficit', 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n                    \n                    'swe_value']\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n#hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-17\ntest start date:  2022-10-04\ntest end date:  2023-10-07\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv\n         date       lat  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  2020-11-25  37.19236  ...                          15589.2             208.000003\n1  2020-10-18  37.19236  ...                           5001.6              72.700001\n2  2020-09-21  37.19236  ...                         102150.4             977.200008\n3  2022-09-01  37.19236  ...                          94640.4             811.300009\n4  2019-10-03  37.19236  ...                            880.5               6.800000\n[5 rows x 32 columns]\nget swe statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nrequired features: Index(['SWE', 'air_temperature_tmmn', 'elevation'], dtype='object')\ndescribe the statistics of swe_value:  count    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'air_temperature_tmmn', 'elevation'], dtype='object')\ntraining data row number:  766500\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 1.273628376872022\nMSE is 5.999757014968014\nR2 score is 0.6218934684579139\nRMSE is 2.4494401431690496\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231711062231.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-4.png\n",
  "history_begin_time" : 1700202141610,
  "history_end_time" : 1700202155422,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "dj0awzplrt1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700201828258,
  "history_end_time" : 1700201828258,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6tc61eziyt5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700200332850,
  "history_end_time" : 1700200332850,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "m8IGlWpooCAw",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\nselected_columns = ['SWE', \n                    'air_temperature_tmmn', \n                    'cumulative_SWE', \n                    'cumulative_potential_evapotranspiration', \n                    'cumulative_relative_humidity_rmax', \n                    'cumulative_relative_humidity_rmin', \n                    'cumulative_precipitation_amount',\n                    'cumulative_Flag', \n                    'elevation',\n                    \n#                     'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed',  'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', \n# 'cumulative_Flag', 'cumulative_mean_vapor_pressure_deficit', 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n                    \n                    'swe_value']\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n#hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-17\ntest start date:  2023-03-10\ntest end date:  2023-10-07\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv\n         date       lat  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  2020-11-25  37.19236  ...                          15589.2             208.000003\n1  2020-10-18  37.19236  ...                           5001.6              72.700001\n2  2020-09-21  37.19236  ...                         102150.4             977.200008\n3  2022-09-01  37.19236  ...                          94640.4             811.300009\n4  2019-10-03  37.19236  ...                            880.5               6.800000\n[5 rows x 32 columns]\nget swe statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nrequired features: Index(['SWE', 'air_temperature_tmmn', 'cumulative_SWE',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_Flag', 'elevation'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'air_temperature_tmmn', 'cumulative_SWE',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_Flag', 'elevation'],\n      dtype='object')\ntraining data row number:  766500\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.08758276581865732\nMSE is 0.10766284490541424\nR2 score is 0.9932150544161068\nRMSE is 0.3281201683917254\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231711053948.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-10.png\n",
  "history_begin_time" : 1700199554901,
  "history_end_time" : 1700199595013,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "bluw5zdnaoh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700145667866,
  "history_end_time" : 1700145667866,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0gh3jr0r7pz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700143295301,
  "history_end_time" : 1700143295301,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "33exmqhu848",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700141615808,
  "history_end_time" : 1700141615808,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "pc6b6amokrj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700134126832,
  "history_end_time" : 1700134126832,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "cm4ir2ntim7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700133783698,
  "history_end_time" : 1700133783698,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "grj2f1kugol",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699992839753,
  "history_end_time" : 1699992839753,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "zlw38r7ryzz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699982145397,
  "history_end_time" : 1699982145397,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "e2k0xiqarg4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699941614796,
  "history_end_time" : 1699941614796,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "2ak8377vkiw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699939440544,
  "history_end_time" : 1699939440544,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "s9exdwl5mj6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699937910461,
  "history_end_time" : 1699937910461,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "TxuGZrsF4bgY",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\nselected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness', 'cumulative_SWE',\n'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n'cumulative_potential_evapotranspiration',\n'cumulative_mean_vapor_pressure_deficit',\n'cumulative_relative_humidity_rmax',\n'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n#hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-13\ntest start date:  2022-12-16\ntest end date:  2023-10-07\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv\n         date       lat  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  2020-11-25  37.19236  ...                          15589.2             208.000003\n1  2020-10-18  37.19236  ...                           5001.6              72.700001\n2  2020-09-21  37.19236  ...                         102150.4             977.200008\n3  2022-09-01  37.19236  ...                          94640.4             811.300009\n4  2019-10-03  37.19236  ...                            880.5               6.800000\n[5 rows x 32 columns]\nget swe statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nrequired features: Index(['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\ntraining data row number:  766500\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.1895680104370518\nMSE is 0.22226302906718853\nR2 score is 0.9859929155795852\nRMSE is 0.4714478009994198\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231311133309.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-27.png\n",
  "history_begin_time" : 1699882352261,
  "history_end_time" : 1699882396442,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "sRw6vmitfp5R",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\nselected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness', 'cumulative_SWE',\n'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n'cumulative_potential_evapotranspiration',\n'cumulative_mean_vapor_pressure_deficit',\n'cumulative_relative_humidity_rmax',\n'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n#hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-13\ntest start date:  2022-12-15\ntest end date:  2023-10-07\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv\n         date       lat  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  2020-11-25  37.19236  ...                          15589.2             208.000003\n1  2020-10-18  37.19236  ...                           5001.6              72.700001\n2  2020-09-21  37.19236  ...                         102150.4             977.200008\n3  2022-09-01  37.19236  ...                          94640.4             811.300009\n4  2019-10-03  37.19236  ...                            880.5               6.800000\n[5 rows x 32 columns]\nget swe statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nrequired features: Index(['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\ntraining data row number:  766500\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.2036167514677106\nMSE is 0.24819366189171557\nR2 score is 0.9843587591273301\nRMSE is 0.4981903871932051\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231311005620.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-27.png\n",
  "history_begin_time" : 1699836943432,
  "history_end_time" : 1699836984995,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ILJP3qkE4A2X",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\nselected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness', 'cumulative_SWE',\n'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n'cumulative_potential_evapotranspiration',\n'cumulative_mean_vapor_pressure_deficit',\n'cumulative_relative_humidity_rmax',\n'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n#hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-12\ntest start date:  2022-12-15\ntest end date:  2023-10-07\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv\n         date       lat  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  2020-11-25  37.19236  ...                          15589.2             208.000003\n1  2020-10-18  37.19236  ...                           5001.6              72.700001\n2  2020-09-21  37.19236  ...                         102150.4             977.200008\n3  2022-09-01  37.19236  ...                          94640.4             811.300009\n4  2019-10-03  37.19236  ...                            880.5               6.800000\n[5 rows x 32 columns]\nget swe statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nrequired features: Index(['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\ntraining data row number:  766500\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.2036167514677106\nMSE is 0.24819366189171557\nR2 score is 0.9843587591273301\nRMSE is 0.4981903871932051\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231211194200.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-27.png\n",
  "history_begin_time" : 1699818085394,
  "history_end_time" : 1699818126365,
  "history_notes" : "train without water year and use all columns without time series",
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "6vKUeWn30diz",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\nselected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness', 'water_year', 'cumulative_SWE',\n'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n'cumulative_potential_evapotranspiration',\n'cumulative_mean_vapor_pressure_deficit',\n'cumulative_relative_humidity_rmax',\n'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n#hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-12\ntest start date:  2022-12-15\ntest end date:  2023-10-07\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv\n         date       lat  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  2020-11-25  37.19236  ...                          15589.2             208.000003\n1  2020-10-18  37.19236  ...                           5001.6              72.700001\n2  2020-09-21  37.19236  ...                         102150.4             977.200008\n3  2022-09-01  37.19236  ...                          94640.4             811.300009\n4  2019-10-03  37.19236  ...                            880.5               6.800000\n[5 rows x 32 columns]\nget swe statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nrequired features: Index(['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'water_year', 'cumulative_SWE',\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'water_year', 'cumulative_SWE',\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\ntraining data row number:  766500\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.1590012850619704\nMSE is 0.17872357374429226\nR2 score is 0.9887367854390315\nRMSE is 0.4227571096318692\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231211190341.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-28.png\n",
  "history_begin_time" : 1699815786654,
  "history_end_time" : 1699815826539,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ngjKwDacx3yu",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\nselected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness', 'water_year', 'cumulative_SWE',\n'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n'cumulative_potential_evapotranspiration',\n'cumulative_mean_vapor_pressure_deficit',\n'cumulative_relative_humidity_rmax',\n'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n'cumulative_air_temperature_tmmx', 'cumulative_wind_speed']\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n# hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n#hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-12\ntest start date:  2022-12-15\ntest end date:  2023-10-07\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv\n         date       lat  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  2020-11-25  37.19236  ...                          15589.2             208.000003\n1  2020-10-18  37.19236  ...                           5001.6              72.700001\n2  2020-09-21  37.19236  ...                         102150.4             977.200008\n3  2022-09-01  37.19236  ...                          94640.4             811.300009\n4  2019-10-03  37.19236  ...                            880.5               6.800000\n[5 rows x 32 columns]\nget swe statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/ngjKwDacx3yu/model_creation_et.py\", line 219, in <module>\n    hole.preprocessing(chosen_columns = selected_columns)\n  File \"/home/chetana/gw-workspace/ngjKwDacx3yu/model_creation_et.py\", line 129, in preprocessing\n    X = data.drop('swe_value', axis=1)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\", line 5399, in drop\n    return super().drop(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\", line 4505, in drop\n    obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\", line 4546, in _drop_axis\n    new_axis = axis.drop(labels, errors=errors)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 6934, in drop\n    raise KeyError(f\"{list(labels[mask])} not found in axis\")\nKeyError: \"['swe_value'] not found in axis\"\n",
  "history_begin_time" : 1699815761891,
  "history_end_time" : 1699815767734,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "6a4APU9vj1Qh",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\nhole = ETHole()\n# hole.preprocessing(chosen_columns = selected_columns)\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n# hole.post_processing(chosen_columns = selected_columns)\nhole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-12\ntest start date:  2022-12-15\ntest end date:  2023-10-07\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv\n         date       lat  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  2020-11-25  37.19236  ...                          15589.2             208.000003\n1  2020-10-18  37.19236  ...                           5001.6              72.700001\n2  2020-09-21  37.19236  ...                         102150.4             977.200008\n3  2022-09-01  37.19236  ...                          94640.4             811.300009\n4  2019-10-03  37.19236  ...                            880.5               6.800000\n[5 rows x 32 columns]\nget swe statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nrequired features: Index(['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'water_year', 'cumulative_SWE',\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'water_year', 'cumulative_SWE',\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\ntraining data row number:  766500\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.1590012850619704\nMSE is 0.17872357374429226\nR2 score is 0.9887367854390315\nRMSE is 0.4227571096318692\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231211185959.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1699815564087,
  "history_end_time" : 1699815605358,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "atihfz63ugn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699805634610,
  "history_end_time" : 1699806085192,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "2jLyHSJ48Oy5",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\nselected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n#hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n#hole.post_processing()\n\n",
  "history_output" : "today date = 2023-11-12\ntest start date:  2022-10-16\ntest end date:  2023-10-07\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv\n         date       lat  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  2020-11-25  37.19236  ...                          15589.2             208.000003\n1  2020-10-18  37.19236  ...                           5001.6              72.700001\n2  2020-09-21  37.19236  ...                         102150.4             977.200008\n3  2022-09-01  37.19236  ...                          94640.4             811.300009\n4  2019-10-03  37.19236  ...                            880.5               6.800000\n[5 rows x 32 columns]\nget swe statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nrequired features: Index(['lat', 'lon', 'elevation', 'slope', 'curvature', 'aspect', 'eastness',\n       'northness', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['lat', 'lon', 'elevation', 'slope', 'curvature', 'aspect', 'eastness',\n       'northness', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\ntraining data row number:  766500\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.03488951076321063\nMSE is 0.009789568662752775\nR2 score is 0.9993830583733421\nRMSE is 0.09894224912924092\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231211155625.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-19.png\n",
  "history_begin_time" : 1699804559524,
  "history_end_time" : 1699804590847,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "XVwHfGwOtL8R",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\nselected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"water_year\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n#hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n\n",
  "history_output" : "today date = 2023-11-12\ntest start date:  2022-10-16\ntest end date:  2023-10-07\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv\n         date       lat  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  2020-11-25  37.19236  ...                          15589.2             208.000003\n1  2020-10-18  37.19236  ...                           5001.6              72.700001\n2  2020-09-21  37.19236  ...                         102150.4             977.200008\n3  2022-09-01  37.19236  ...                          94640.4             811.300009\n4  2019-10-03  37.19236  ...                            880.5               6.800000\n[5 rows x 32 columns]\nget swe statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nrequired features: Index(['lat', 'lon', 'elevation', 'slope', 'curvature', 'aspect', 'eastness',\n       'northness', 'water_year', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['lat', 'lon', 'elevation', 'slope', 'curvature', 'aspect', 'eastness',\n       'northness', 'water_year', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\ntraining data row number:  766500\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.031811011089368436\nMSE is 0.008403574964122638\nR2 score is 0.999470404122315\nRMSE is 0.09167101485269288\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231211145953.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-20.png\n",
  "history_begin_time" : 1699801158704,
  "history_end_time" : 1699801199288,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "bs3eiiy2i49",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699684154056,
  "history_end_time" : 1699684154056,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "pu4345vtiti",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699681071334,
  "history_end_time" : 1699681071334,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "yV6gojPWWRtg",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\nselected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"water_year\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n#hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n\n",
  "history_output" : "today date = 2023-11-11\ntest start date:  2022-10-03\ntest end date:  2022-10-07\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv\n         date  level_0  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  2019-10-01      819  ...                            299.2                    2.2\n1  2019-10-01      820  ...                            594.5                    5.8\n2  2019-10-01      821  ...                            875.5                    8.1\n3  2019-10-02      822  ...                           1173.3                   10.6\n4  2019-10-02      823  ...                           1472.7                   13.2\n[5 rows x 104 columns]\nget swe statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nrequired features: Index(['lat', 'lon', 'elevation', 'slope', 'curvature', 'aspect', 'eastness',\n       'northness', 'water_year', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['lat', 'lon', 'elevation', 'slope', 'curvature', 'aspect', 'eastness',\n       'northness', 'water_year', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\ntraining data row number:  766500\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.031705688193086654\nMSE is 0.00830564322243966\nR2 score is 0.9994793748427564\nRMSE is 0.09113530173560441\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231111050224.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-20.png\n",
  "history_begin_time" : 1699678899599,
  "history_end_time" : 1699678949518,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "mlBqRq1hRoVm",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\nselected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"water_year\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\nhole = ETHole()\nhole.preprocessing(chosen_columns = selected_columns)\n#hole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = selected_columns)\n\n",
  "history_output" : "today date = 2023-11-10\ntest start date:  2022-10-03\ntest end date:  2022-10-07\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv\n         date  level_0  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  2019-10-01      819  ...                            299.2                    2.2\n1  2019-10-01      820  ...                            594.5                    5.8\n2  2019-10-01      821  ...                            875.5                    8.1\n3  2019-10-02      822  ...                           1173.3                   10.6\n4  2019-10-02      823  ...                           1472.7                   13.2\n[5 rows x 104 columns]\nget swe statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nrequired features: Index(['lat', 'lon', 'elevation', 'slope', 'curvature', 'aspect', 'eastness',\n       'northness', 'water_year', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['lat', 'lon', 'elevation', 'slope', 'curvature', 'aspect', 'eastness',\n       'northness', 'water_year', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\ntraining data row number:  766500\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.031705688193086654\nMSE is 0.00830564322243966\nR2 score is 0.9994793748427564\nRMSE is 0.09113530173560441\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231011083541.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-20.png\n",
  "history_begin_time" : 1699605299528,
  "history_end_time" : 1699605347853,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "yCsoFMY8dCKV",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\nselected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"water_year\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\nhole = ETHole()\n#hole.preprocessing(chosen_columns = all_used_columns)\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n\n",
  "history_output" : "",
  "history_begin_time" : 1699605280503,
  "history_end_time" : 1699605283491,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "HZA9UtDgJtQw",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\nselected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"water_year\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\nhole = ETHole()\n#hole.preprocessing(chosen_columns = all_used_columns)\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n#hole.post_processing(chosen_columns = all_used_columns)\n\nhole.post_processing()\n",
  "history_output" : "today date = 2023-11-10\ntest start date:  2022-10-03\ntest end date:  2022-10-07\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv\n         date  level_0  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  2019-10-01      819  ...                            299.2                    2.2\n1  2019-10-01      820  ...                            594.5                    5.8\n2  2019-10-01      821  ...                            875.5                    8.1\n3  2019-10-02      822  ...                           1173.3                   10.6\n4  2019-10-02      823  ...                           1472.7                   13.2\n[5 rows x 104 columns]\nget swe statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nrequired features: Index(['level_0', 'index', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'SWE_1', 'Flag_1', 'air_temperature_tmmn_1',\n       'potential_evapotranspiration_1', 'mean_vapor_pressure_deficit_1',\n       'relative_humidity_rmax_1', 'relative_humidity_rmin_1',\n       'precipitation_amount_1', 'air_temperature_tmmx_1', 'wind_speed_1',\n       'SWE_2', 'Flag_2', 'air_temperature_tmmn_2',\n       'potential_evapotranspiration_2', 'mean_vapor_pressure_deficit_2',\n       'relative_humidity_rmax_2', 'relative_humidity_rmin_2',\n       'precipitation_amount_2', 'air_temperature_tmmx_2', 'wind_speed_2',\n       'SWE_3', 'Flag_3', 'air_temperature_tmmn_3',\n       'potential_evapotranspiration_3', 'mean_vapor_pressure_deficit_3',\n       'relative_humidity_rmax_3', 'relative_humidity_rmin_3',\n       'precipitation_amount_3', 'air_temperature_tmmx_3', 'wind_speed_3',\n       'SWE_4', 'Flag_4', 'air_temperature_tmmn_4',\n       'potential_evapotranspiration_4', 'mean_vapor_pressure_deficit_4',\n       'relative_humidity_rmax_4', 'relative_humidity_rmin_4',\n       'precipitation_amount_4', 'air_temperature_tmmx_4', 'wind_speed_4',\n       'SWE_5', 'Flag_5', 'air_temperature_tmmn_5',\n       'potential_evapotranspiration_5', 'mean_vapor_pressure_deficit_5',\n       'relative_humidity_rmax_5', 'relative_humidity_rmin_5',\n       'precipitation_amount_5', 'air_temperature_tmmx_5', 'wind_speed_5',\n       'SWE_6', 'Flag_6', 'air_temperature_tmmn_6',\n       'potential_evapotranspiration_6', 'mean_vapor_pressure_deficit_6',\n       'relative_humidity_rmax_6', 'relative_humidity_rmin_6',\n       'precipitation_amount_6', 'air_temperature_tmmx_6', 'wind_speed_6',\n       'SWE_7', 'Flag_7', 'air_temperature_tmmn_7',\n       'potential_evapotranspiration_7', 'mean_vapor_pressure_deficit_7',\n       'relative_humidity_rmax_7', 'relative_humidity_rmin_7',\n       'precipitation_amount_7', 'air_temperature_tmmx_7', 'wind_speed_7',\n       'water_year', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['level_0', 'index', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'SWE_1', 'Flag_1', 'air_temperature_tmmn_1',\n       'potential_evapotranspiration_1', 'mean_vapor_pressure_deficit_1',\n       'relative_humidity_rmax_1', 'relative_humidity_rmin_1',\n       'precipitation_amount_1', 'air_temperature_tmmx_1', 'wind_speed_1',\n       'SWE_2', 'Flag_2', 'air_temperature_tmmn_2',\n       'potential_evapotranspiration_2', 'mean_vapor_pressure_deficit_2',\n       'relative_humidity_rmax_2', 'relative_humidity_rmin_2',\n       'precipitation_amount_2', 'air_temperature_tmmx_2', 'wind_speed_2',\n       'SWE_3', 'Flag_3', 'air_temperature_tmmn_3',\n       'potential_evapotranspiration_3', 'mean_vapor_pressure_deficit_3',\n       'relative_humidity_rmax_3', 'relative_humidity_rmin_3',\n       'precipitation_amount_3', 'air_temperature_tmmx_3', 'wind_speed_3',\n       'SWE_4', 'Flag_4', 'air_temperature_tmmn_4',\n       'potential_evapotranspiration_4', 'mean_vapor_pressure_deficit_4',\n       'relative_humidity_rmax_4', 'relative_humidity_rmin_4',\n       'precipitation_amount_4', 'air_temperature_tmmx_4', 'wind_speed_4',\n       'SWE_5', 'Flag_5', 'air_temperature_tmmn_5',\n       'potential_evapotranspiration_5', 'mean_vapor_pressure_deficit_5',\n       'relative_humidity_rmax_5', 'relative_humidity_rmin_5',\n       'precipitation_amount_5', 'air_temperature_tmmx_5', 'wind_speed_5',\n       'SWE_6', 'Flag_6', 'air_temperature_tmmn_6',\n       'potential_evapotranspiration_6', 'mean_vapor_pressure_deficit_6',\n       'relative_humidity_rmax_6', 'relative_humidity_rmin_6',\n       'precipitation_amount_6', 'air_temperature_tmmx_6', 'wind_speed_6',\n       'SWE_7', 'Flag_7', 'air_temperature_tmmn_7',\n       'potential_evapotranspiration_7', 'mean_vapor_pressure_deficit_7',\n       'relative_humidity_rmax_7', 'relative_humidity_rmin_7',\n       'precipitation_amount_7', 'air_temperature_tmmx_7', 'wind_speed_7',\n       'water_year', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\ntraining data row number:  766500\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.4068918069145467\nMSE is 0.6524817643052837\nR2 score is 0.9591002873537522\nRMSE is 0.8077634333796522\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231011083006.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1699604834787,
  "history_end_time" : 1699605020358,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "TKgMEEOh7A8n",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\nselected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"water_year\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\nhole = ETHole()\n#hole.preprocessing(chosen_columns = all_used_columns)\nhole.preprocessing(selected_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n#hole.post_processing(chosen_columns = all_used_columns)\n\nhole.post_processing(chosen_columns=selected_columns)\n",
  "history_output" : "today date = 2023-11-09\ntest start date:  2018-01-01\ntest end date:  2023-11-09\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv\n         date  level_0  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  2019-10-01      819  ...                            299.2                    2.2\n1  2019-10-01      820  ...                            594.5                    5.8\n2  2019-10-01      821  ...                            875.5                    8.1\n3  2019-10-02      822  ...                           1173.3                   10.6\n4  2019-10-02      823  ...                           1472.7                   13.2\n[5 rows x 104 columns]\nget swe statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nrequired features: Index(['lat', 'lon', 'elevation', 'slope', 'curvature', 'aspect', 'eastness',\n       'northness', 'water_year', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['lat', 'lon', 'elevation', 'slope', 'curvature', 'aspect', 'eastness',\n       'northness', 'water_year', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\ntraining data row number:  766500\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.031705688193086654\nMSE is 0.00830564322243966\nR2 score is 0.9994793748427564\nRMSE is 0.09113530173560441\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230911053349.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-20.png\n",
  "history_begin_time" : 1699507993282,
  "history_end_time" : 1699508036463,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "EFw8ZC6pWnNx",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\nselected_columns = [\"date\",\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"water_year\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\nhole = ETHole()\n#hole.preprocessing(chosen_columns = all_used_columns)\nhole.preprocessing(selected_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n#hole.post_processing(chosen_columns = all_used_columns)\n\nhole.post_processing(chosen_columns=selected_columns)\n",
  "history_output" : "today date = 2023-11-09\ntest start date:  2018-01-01\ntest end date:  2023-11-09\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv\n         date  level_0  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  2019-10-01      819  ...                            299.2                    2.2\n1  2019-10-01      820  ...                            594.5                    5.8\n2  2019-10-01      821  ...                            875.5                    8.1\n3  2019-10-02      822  ...                           1173.3                   10.6\n4  2019-10-02      823  ...                           1472.7                   13.2\n[5 rows x 104 columns]\nget swe statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness', 'water_year', 'cumulative_SWE',\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['date', 'lat', 'lon', 'elevation', 'slope', 'curvature', 'aspect',\n       'eastness', 'northness', 'water_year', 'cumulative_SWE',\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\ntraining data row number:  766500\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.026672837573386555\nMSE is 0.007077188623613821\nR2 score is 0.9995563784355609\nRMSE is 0.08412602821727543\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230911053145.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-21.png\n",
  "history_begin_time" : 1699507874080,
  "history_end_time" : 1699507911168,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "8zd8EwJxGoHF",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\nselected_columns = [\"date\",\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"water_year\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\"]\nhole = ETHole()\n#hole.preprocessing(chosen_columns = all_used_columns)\nhole.preprocessing(selected_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n#hole.post_processing(chosen_columns = all_used_columns)\n\nhole.post_processing(chosen_columns=selected_columns)\n",
  "history_output" : "today date = 2023-11-09\ntest start date:  2018-01-01\ntest end date:  2023-11-09\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv\n         date  level_0  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  2019-10-01      819  ...                            299.2                    2.2\n1  2019-10-01      820  ...                            594.5                    5.8\n2  2019-10-01      821  ...                            875.5                    8.1\n3  2019-10-02      822  ...                           1173.3                   10.6\n4  2019-10-02      823  ...                           1472.7                   13.2\n[5 rows x 104 columns]\nget swe statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/8zd8EwJxGoHF/model_creation_et.py\", line 207, in <module>\n    hole.preprocessing(selected_columns)\n  File \"/home/chetana/gw-workspace/8zd8EwJxGoHF/model_creation_et.py\", line 129, in preprocessing\n    X = data.drop('swe_value', axis=1)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\", line 5399, in drop\n    return super().drop(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\", line 4505, in drop\n    obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\", line 4546, in _drop_axis\n    new_axis = axis.drop(labels, errors=errors)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 6934, in drop\n    raise KeyError(f\"{list(labels[mask])} not found in axis\")\nKeyError: \"['swe_value'] not found in axis\"\n",
  "history_begin_time" : 1699507829340,
  "history_end_time" : 1699507848148,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "KKcCtS3c5iKO",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n\nhole = ETHole()\n#hole.preprocessing(chosen_columns = all_used_columns)\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n#hole.post_processing(chosen_columns = all_used_columns)\nselected_columns = [\"date\",\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"water_year\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\"]\nhole.post_processing(chosen_columns=selected_columns)\n",
  "history_output" : "today date = 2023-11-09\ntest start date:  2018-01-01\ntest end date:  2023-11-09\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv\n         date  level_0  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  2019-10-01      819  ...                            299.2                    2.2\n1  2019-10-01      820  ...                            594.5                    5.8\n2  2019-10-01      821  ...                            875.5                    8.1\n3  2019-10-02      822  ...                           1173.3                   10.6\n4  2019-10-02      823  ...                           1472.7                   13.2\n[5 rows x 104 columns]\nget swe statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nrequired features: Index(['level_0', 'index', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'SWE_1', 'Flag_1', 'air_temperature_tmmn_1',\n       'potential_evapotranspiration_1', 'mean_vapor_pressure_deficit_1',\n       'relative_humidity_rmax_1', 'relative_humidity_rmin_1',\n       'precipitation_amount_1', 'air_temperature_tmmx_1', 'wind_speed_1',\n       'SWE_2', 'Flag_2', 'air_temperature_tmmn_2',\n       'potential_evapotranspiration_2', 'mean_vapor_pressure_deficit_2',\n       'relative_humidity_rmax_2', 'relative_humidity_rmin_2',\n       'precipitation_amount_2', 'air_temperature_tmmx_2', 'wind_speed_2',\n       'SWE_3', 'Flag_3', 'air_temperature_tmmn_3',\n       'potential_evapotranspiration_3', 'mean_vapor_pressure_deficit_3',\n       'relative_humidity_rmax_3', 'relative_humidity_rmin_3',\n       'precipitation_amount_3', 'air_temperature_tmmx_3', 'wind_speed_3',\n       'SWE_4', 'Flag_4', 'air_temperature_tmmn_4',\n       'potential_evapotranspiration_4', 'mean_vapor_pressure_deficit_4',\n       'relative_humidity_rmax_4', 'relative_humidity_rmin_4',\n       'precipitation_amount_4', 'air_temperature_tmmx_4', 'wind_speed_4',\n       'SWE_5', 'Flag_5', 'air_temperature_tmmn_5',\n       'potential_evapotranspiration_5', 'mean_vapor_pressure_deficit_5',\n       'relative_humidity_rmax_5', 'relative_humidity_rmin_5',\n       'precipitation_amount_5', 'air_temperature_tmmx_5', 'wind_speed_5',\n       'SWE_6', 'Flag_6', 'air_temperature_tmmn_6',\n       'potential_evapotranspiration_6', 'mean_vapor_pressure_deficit_6',\n       'relative_humidity_rmax_6', 'relative_humidity_rmin_6',\n       'precipitation_amount_6', 'air_temperature_tmmx_6', 'wind_speed_6',\n       'SWE_7', 'Flag_7', 'air_temperature_tmmn_7',\n       'potential_evapotranspiration_7', 'mean_vapor_pressure_deficit_7',\n       'relative_humidity_rmax_7', 'relative_humidity_rmin_7',\n       'precipitation_amount_7', 'air_temperature_tmmx_7', 'wind_speed_7',\n       'water_year', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['level_0', 'index', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'SWE_1', 'Flag_1', 'air_temperature_tmmn_1',\n       'potential_evapotranspiration_1', 'mean_vapor_pressure_deficit_1',\n       'relative_humidity_rmax_1', 'relative_humidity_rmin_1',\n       'precipitation_amount_1', 'air_temperature_tmmx_1', 'wind_speed_1',\n       'SWE_2', 'Flag_2', 'air_temperature_tmmn_2',\n       'potential_evapotranspiration_2', 'mean_vapor_pressure_deficit_2',\n       'relative_humidity_rmax_2', 'relative_humidity_rmin_2',\n       'precipitation_amount_2', 'air_temperature_tmmx_2', 'wind_speed_2',\n       'SWE_3', 'Flag_3', 'air_temperature_tmmn_3',\n       'potential_evapotranspiration_3', 'mean_vapor_pressure_deficit_3',\n       'relative_humidity_rmax_3', 'relative_humidity_rmin_3',\n       'precipitation_amount_3', 'air_temperature_tmmx_3', 'wind_speed_3',\n       'SWE_4', 'Flag_4', 'air_temperature_tmmn_4',\n       'potential_evapotranspiration_4', 'mean_vapor_pressure_deficit_4',\n       'relative_humidity_rmax_4', 'relative_humidity_rmin_4',\n       'precipitation_amount_4', 'air_temperature_tmmx_4', 'wind_speed_4',\n       'SWE_5', 'Flag_5', 'air_temperature_tmmn_5',\n       'potential_evapotranspiration_5', 'mean_vapor_pressure_deficit_5',\n       'relative_humidity_rmax_5', 'relative_humidity_rmin_5',\n       'precipitation_amount_5', 'air_temperature_tmmx_5', 'wind_speed_5',\n       'SWE_6', 'Flag_6', 'air_temperature_tmmn_6',\n       'potential_evapotranspiration_6', 'mean_vapor_pressure_deficit_6',\n       'relative_humidity_rmax_6', 'relative_humidity_rmin_6',\n       'precipitation_amount_6', 'air_temperature_tmmx_6', 'wind_speed_6',\n       'SWE_7', 'Flag_7', 'air_temperature_tmmn_7',\n       'potential_evapotranspiration_7', 'mean_vapor_pressure_deficit_7',\n       'relative_humidity_rmax_7', 'relative_humidity_rmin_7',\n       'precipitation_amount_7', 'air_temperature_tmmx_7', 'wind_speed_7',\n       'water_year', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\ntraining data row number:  766500\n",
  "history_begin_time" : 1699507760894,
  "history_end_time" : 1699507810706,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "2RgHpZ9ScWkb",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n\nhole = ETHole()\n#hole.preprocessing(chosen_columns = all_used_columns)\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n#hole.post_processing(chosen_columns = all_used_columns)\nhole.post_processing()\n",
  "history_output" : "today date = 2023-11-08\ntest start date:  2018-01-01\ntest end date:  2023-11-08\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv\n         date  level_0  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  2019-10-01      819  ...                            299.2                    2.2\n1  2019-10-01      820  ...                            594.5                    5.8\n2  2019-10-01      821  ...                            875.5                    8.1\n3  2019-10-02      822  ...                           1173.3                   10.6\n4  2019-10-02      823  ...                           1472.7                   13.2\n[5 rows x 104 columns]\nget swe statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nrequired features: Index(['level_0', 'index', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'SWE_1', 'Flag_1', 'air_temperature_tmmn_1',\n       'potential_evapotranspiration_1', 'mean_vapor_pressure_deficit_1',\n       'relative_humidity_rmax_1', 'relative_humidity_rmin_1',\n       'precipitation_amount_1', 'air_temperature_tmmx_1', 'wind_speed_1',\n       'SWE_2', 'Flag_2', 'air_temperature_tmmn_2',\n       'potential_evapotranspiration_2', 'mean_vapor_pressure_deficit_2',\n       'relative_humidity_rmax_2', 'relative_humidity_rmin_2',\n       'precipitation_amount_2', 'air_temperature_tmmx_2', 'wind_speed_2',\n       'SWE_3', 'Flag_3', 'air_temperature_tmmn_3',\n       'potential_evapotranspiration_3', 'mean_vapor_pressure_deficit_3',\n       'relative_humidity_rmax_3', 'relative_humidity_rmin_3',\n       'precipitation_amount_3', 'air_temperature_tmmx_3', 'wind_speed_3',\n       'SWE_4', 'Flag_4', 'air_temperature_tmmn_4',\n       'potential_evapotranspiration_4', 'mean_vapor_pressure_deficit_4',\n       'relative_humidity_rmax_4', 'relative_humidity_rmin_4',\n       'precipitation_amount_4', 'air_temperature_tmmx_4', 'wind_speed_4',\n       'SWE_5', 'Flag_5', 'air_temperature_tmmn_5',\n       'potential_evapotranspiration_5', 'mean_vapor_pressure_deficit_5',\n       'relative_humidity_rmax_5', 'relative_humidity_rmin_5',\n       'precipitation_amount_5', 'air_temperature_tmmx_5', 'wind_speed_5',\n       'SWE_6', 'Flag_6', 'air_temperature_tmmn_6',\n       'potential_evapotranspiration_6', 'mean_vapor_pressure_deficit_6',\n       'relative_humidity_rmax_6', 'relative_humidity_rmin_6',\n       'precipitation_amount_6', 'air_temperature_tmmx_6', 'wind_speed_6',\n       'SWE_7', 'Flag_7', 'air_temperature_tmmn_7',\n       'potential_evapotranspiration_7', 'mean_vapor_pressure_deficit_7',\n       'relative_humidity_rmax_7', 'relative_humidity_rmin_7',\n       'precipitation_amount_7', 'air_temperature_tmmx_7', 'wind_speed_7',\n       'water_year', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['level_0', 'index', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'SWE_1', 'Flag_1', 'air_temperature_tmmn_1',\n       'potential_evapotranspiration_1', 'mean_vapor_pressure_deficit_1',\n       'relative_humidity_rmax_1', 'relative_humidity_rmin_1',\n       'precipitation_amount_1', 'air_temperature_tmmx_1', 'wind_speed_1',\n       'SWE_2', 'Flag_2', 'air_temperature_tmmn_2',\n       'potential_evapotranspiration_2', 'mean_vapor_pressure_deficit_2',\n       'relative_humidity_rmax_2', 'relative_humidity_rmin_2',\n       'precipitation_amount_2', 'air_temperature_tmmx_2', 'wind_speed_2',\n       'SWE_3', 'Flag_3', 'air_temperature_tmmn_3',\n       'potential_evapotranspiration_3', 'mean_vapor_pressure_deficit_3',\n       'relative_humidity_rmax_3', 'relative_humidity_rmin_3',\n       'precipitation_amount_3', 'air_temperature_tmmx_3', 'wind_speed_3',\n       'SWE_4', 'Flag_4', 'air_temperature_tmmn_4',\n       'potential_evapotranspiration_4', 'mean_vapor_pressure_deficit_4',\n       'relative_humidity_rmax_4', 'relative_humidity_rmin_4',\n       'precipitation_amount_4', 'air_temperature_tmmx_4', 'wind_speed_4',\n       'SWE_5', 'Flag_5', 'air_temperature_tmmn_5',\n       'potential_evapotranspiration_5', 'mean_vapor_pressure_deficit_5',\n       'relative_humidity_rmax_5', 'relative_humidity_rmin_5',\n       'precipitation_amount_5', 'air_temperature_tmmx_5', 'wind_speed_5',\n       'SWE_6', 'Flag_6', 'air_temperature_tmmn_6',\n       'potential_evapotranspiration_6', 'mean_vapor_pressure_deficit_6',\n       'relative_humidity_rmax_6', 'relative_humidity_rmin_6',\n       'precipitation_amount_6', 'air_temperature_tmmx_6', 'wind_speed_6',\n       'SWE_7', 'Flag_7', 'air_temperature_tmmn_7',\n       'potential_evapotranspiration_7', 'mean_vapor_pressure_deficit_7',\n       'relative_humidity_rmax_7', 'relative_humidity_rmin_7',\n       'precipitation_amount_7', 'air_temperature_tmmx_7', 'wind_speed_7',\n       'water_year', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\ntraining data row number:  766500\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.4068918069145467\nMSE is 0.6524817643052837\nR2 score is 0.9591002873537522\nRMSE is 0.8077634333796522\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230811160251.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1699459181279,
  "history_end_time" : 1699459379920,
  "history_notes" : "first successful run on cumulative vars",
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "8Mpt9auHqHp8",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n\nhole = ETHole()\n#hole.preprocessing(chosen_columns = all_used_columns)\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-11-08\ntest start date:  2018-01-01\ntest end date:  2023-11-08\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv\n         date  level_0  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  2019-10-01      819  ...                            299.2                    2.2\n1  2019-10-01      820  ...                            594.5                    5.8\n2  2019-10-01      821  ...                            875.5                    8.1\n3  2019-10-02      822  ...                           1173.3                   10.6\n4  2019-10-02      823  ...                           1472.7                   13.2\n[5 rows x 104 columns]\nget swe statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nrequired features: Index(['level_0', 'index', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'SWE_1', 'Flag_1', 'air_temperature_tmmn_1',\n       'potential_evapotranspiration_1', 'mean_vapor_pressure_deficit_1',\n       'relative_humidity_rmax_1', 'relative_humidity_rmin_1',\n       'precipitation_amount_1', 'air_temperature_tmmx_1', 'wind_speed_1',\n       'SWE_2', 'Flag_2', 'air_temperature_tmmn_2',\n       'potential_evapotranspiration_2', 'mean_vapor_pressure_deficit_2',\n       'relative_humidity_rmax_2', 'relative_humidity_rmin_2',\n       'precipitation_amount_2', 'air_temperature_tmmx_2', 'wind_speed_2',\n       'SWE_3', 'Flag_3', 'air_temperature_tmmn_3',\n       'potential_evapotranspiration_3', 'mean_vapor_pressure_deficit_3',\n       'relative_humidity_rmax_3', 'relative_humidity_rmin_3',\n       'precipitation_amount_3', 'air_temperature_tmmx_3', 'wind_speed_3',\n       'SWE_4', 'Flag_4', 'air_temperature_tmmn_4',\n       'potential_evapotranspiration_4', 'mean_vapor_pressure_deficit_4',\n       'relative_humidity_rmax_4', 'relative_humidity_rmin_4',\n       'precipitation_amount_4', 'air_temperature_tmmx_4', 'wind_speed_4',\n       'SWE_5', 'Flag_5', 'air_temperature_tmmn_5',\n       'potential_evapotranspiration_5', 'mean_vapor_pressure_deficit_5',\n       'relative_humidity_rmax_5', 'relative_humidity_rmin_5',\n       'precipitation_amount_5', 'air_temperature_tmmx_5', 'wind_speed_5',\n       'SWE_6', 'Flag_6', 'air_temperature_tmmn_6',\n       'potential_evapotranspiration_6', 'mean_vapor_pressure_deficit_6',\n       'relative_humidity_rmax_6', 'relative_humidity_rmin_6',\n       'precipitation_amount_6', 'air_temperature_tmmx_6', 'wind_speed_6',\n       'SWE_7', 'Flag_7', 'air_temperature_tmmn_7',\n       'potential_evapotranspiration_7', 'mean_vapor_pressure_deficit_7',\n       'relative_humidity_rmax_7', 'relative_humidity_rmin_7',\n       'precipitation_amount_7', 'air_temperature_tmmx_7', 'wind_speed_7',\n       'water_year', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\ninput features and order:  Index(['level_0', 'index', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'SWE_1', 'Flag_1', 'air_temperature_tmmn_1',\n       'potential_evapotranspiration_1', 'mean_vapor_pressure_deficit_1',\n       'relative_humidity_rmax_1', 'relative_humidity_rmin_1',\n       'precipitation_amount_1', 'air_temperature_tmmx_1', 'wind_speed_1',\n       'SWE_2', 'Flag_2', 'air_temperature_tmmn_2',\n       'potential_evapotranspiration_2', 'mean_vapor_pressure_deficit_2',\n       'relative_humidity_rmax_2', 'relative_humidity_rmin_2',\n       'precipitation_amount_2', 'air_temperature_tmmx_2', 'wind_speed_2',\n       'SWE_3', 'Flag_3', 'air_temperature_tmmn_3',\n       'potential_evapotranspiration_3', 'mean_vapor_pressure_deficit_3',\n       'relative_humidity_rmax_3', 'relative_humidity_rmin_3',\n       'precipitation_amount_3', 'air_temperature_tmmx_3', 'wind_speed_3',\n       'SWE_4', 'Flag_4', 'air_temperature_tmmn_4',\n       'potential_evapotranspiration_4', 'mean_vapor_pressure_deficit_4',\n       'relative_humidity_rmax_4', 'relative_humidity_rmin_4',\n       'precipitation_amount_4', 'air_temperature_tmmx_4', 'wind_speed_4',\n       'SWE_5', 'Flag_5', 'air_temperature_tmmn_5',\n       'potential_evapotranspiration_5', 'mean_vapor_pressure_deficit_5',\n       'relative_humidity_rmax_5', 'relative_humidity_rmin_5',\n       'precipitation_amount_5', 'air_temperature_tmmx_5', 'wind_speed_5',\n       'SWE_6', 'Flag_6', 'air_temperature_tmmn_6',\n       'potential_evapotranspiration_6', 'mean_vapor_pressure_deficit_6',\n       'relative_humidity_rmax_6', 'relative_humidity_rmin_6',\n       'precipitation_amount_6', 'air_temperature_tmmx_6', 'wind_speed_6',\n       'SWE_7', 'Flag_7', 'air_temperature_tmmn_7',\n       'potential_evapotranspiration_7', 'mean_vapor_pressure_deficit_7',\n       'relative_humidity_rmax_7', 'relative_humidity_rmin_7',\n       'precipitation_amount_7', 'air_temperature_tmmx_7', 'wind_speed_7',\n       'water_year', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\ntraining data row number:  766500\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.4068918069145467\nMSE is 0.6524817643052837\nR2 score is 0.9591002873537522\nRMSE is 0.8077634333796522\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230811155849.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/8Mpt9auHqHp8/model_creation_et.py\", line 212, in <module>\n    hole.post_processing(chosen_columns = all_used_columns)\nNameError: name 'all_used_columns' is not defined\n",
  "history_begin_time" : 1699458940112,
  "history_end_time" : 1699459139219,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "frkxrl29xyt4",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nall_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-11-08\ntest start date:  2018-01-01\ntest end date:  2023-11-08\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv\n         date  level_0  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  2019-10-01      819  ...                            299.2                    2.2\n1  2019-10-01      820  ...                            594.5                    5.8\n2  2019-10-01      821  ...                            875.5                    8.1\n3  2019-10-02      822  ...                           1173.3                   10.6\n4  2019-10-02      823  ...                           1472.7                   13.2\n[5 rows x 104 columns]\nget swe statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/frkxrl29xyt4/model_creation_et.py\", line 206, in <module>\n    hole.preprocessing(chosen_columns = all_used_columns)\n  File \"/home/chetana/gw-workspace/frkxrl29xyt4/model_creation_et.py\", line 121, in preprocessing\n    data = data[chosen_columns]\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\", line 3813, in __getitem__\n    indexer = self.columns._get_indexer_strict(key, \"columns\")[1]\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 6070, in _get_indexer_strict\n    self._raise_if_missing(keyarr, indexer, axis_name)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 6133, in _raise_if_missing\n    raise KeyError(f\"{not_found} not in index\")\nKeyError: \"['cumulative_pr', 'station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr', 'cumulative_rmin', 'cumulative_vpd'] not in index\"\n",
  "history_begin_time" : 1699458887063,
  "history_end_time" : 1699458909264,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "7im4LAFXqV0b",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nall_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-11-04\ntest start date:  2018-01-01\ntest end date:  2023-11-04\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['cumulative_pr', 'station_elevation', 'cumulative_tmmn',\n       'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax',\n       'cumulative_etr', 'aspect', 'cumulative_rmin', 'elevation',\n       'cumulative_vpd'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['cumulative_pr', 'station_elevation', 'cumulative_tmmn',\n       'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax',\n       'cumulative_etr', 'aspect', 'cumulative_rmin', 'elevation',\n       'cumulative_vpd'],\n      dtype='object')\ntraining data row number:  5480\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.5958120437956205\nMSE is 3.8484738138686128\nR2 score is 0.9957062796307135\nRMSE is 1.9617527402475097\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230411154457.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-13.png\n",
  "history_begin_time" : 1699112677580,
  "history_end_time" : 1699112698957,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "0ylH5HD6wZwm",
  "history_input" : "\nprint(\"this is my experiment\")\n",
  "history_output" : "this is my experiment\n",
  "history_begin_time" : 1699112635156,
  "history_end_time" : 1699112635980,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "jfo3umypysh",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nall_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-31\ntest start date:  2018-01-01\ntest end date:  2023-10-31\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['cumulative_pr', 'station_elevation', 'cumulative_tmmn',\n       'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax',\n       'cumulative_etr', 'aspect', 'cumulative_rmin', 'elevation',\n       'cumulative_vpd'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['cumulative_pr', 'station_elevation', 'cumulative_tmmn',\n       'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax',\n       'cumulative_etr', 'aspect', 'cumulative_rmin', 'elevation',\n       'cumulative_vpd'],\n      dtype='object')\ntraining data row number:  5480\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.5958120437956205\nMSE is 3.8484738138686128\nR2 score is 0.9957062796307135\nRMSE is 1.9617527402475097\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20233110143122.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-13.png\n",
  "history_begin_time" : 1698762680288,
  "history_end_time" : 1698762683128,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "69u0kvx5m1n",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nall_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-31\ntest start date:  2018-01-01\ntest end date:  2023-10-31\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['cumulative_pr', 'station_elevation', 'cumulative_tmmn',\n       'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax',\n       'cumulative_etr', 'aspect', 'cumulative_rmin', 'elevation',\n       'cumulative_vpd'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['cumulative_pr', 'station_elevation', 'cumulative_tmmn',\n       'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax',\n       'cumulative_etr', 'aspect', 'cumulative_rmin', 'elevation',\n       'cumulative_vpd'],\n      dtype='object')\ntraining data row number:  5480\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.5958120437956205\nMSE is 3.8484738138686128\nR2 score is 0.9957062796307135\nRMSE is 1.9617527402475097\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20233110143049.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-13.png\n",
  "history_begin_time" : 1698762640590,
  "history_end_time" : 1698762650741,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "RCV3DK21pFbn",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nall_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-30\ntest start date:  2018-01-01\ntest end date:  2023-10-30\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['cumulative_pr', 'station_elevation', 'cumulative_tmmn',\n       'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax',\n       'cumulative_etr', 'aspect', 'cumulative_rmin', 'elevation',\n       'cumulative_vpd'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['cumulative_pr', 'station_elevation', 'cumulative_tmmn',\n       'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax',\n       'cumulative_etr', 'aspect', 'cumulative_rmin', 'elevation',\n       'cumulative_vpd'],\n      dtype='object')\ntraining data row number:  5480\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.5958120437956205\nMSE is 3.8484738138686128\nR2 score is 0.9957062796307135\nRMSE is 1.9617527402475097\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20233010224514.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-13.png\n",
  "history_begin_time" : 1698705897975,
  "history_end_time" : 1698705915971,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "WSydsAmZoibn",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nall_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-27\ntest start date:  2018-01-01\ntest end date:  2023-10-27\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['cumulative_pr', 'station_elevation', 'cumulative_tmmn',\n       'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax',\n       'cumulative_etr', 'aspect', 'cumulative_rmin', 'elevation',\n       'cumulative_vpd'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['cumulative_pr', 'station_elevation', 'cumulative_tmmn',\n       'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax',\n       'cumulative_etr', 'aspect', 'cumulative_rmin', 'elevation',\n       'cumulative_vpd'],\n      dtype='object')\ntraining data row number:  5480\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.5958120437956205\nMSE is 3.8484738138686128\nR2 score is 0.9957062796307135\nRMSE is 1.9617527402475097\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232710163846.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-13.png\n",
  "history_begin_time" : 1698424725476,
  "history_end_time" : 1698424727228,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "b1AW53AbicwB",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n         self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nall_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "  File \"/home/ubuntu/gw-workspace/b1AW53AbicwB/model_creation_et.py\", line 165\n    self.classifier.fit(self.train_x, self.train_y)\nIndentationError: unexpected indent\n",
  "history_begin_time" : 1698424708140,
  "history_end_time" : 1698424708157,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "vr3J9F1xrBQf",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n          self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nall_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "  File \"/home/ubuntu/gw-workspace/vr3J9F1xrBQf/model_creation_et.py\", line 165\n    self.classifier.fit(self.train_x, self.train_y)\nIndentationError: unexpected indent\n",
  "history_begin_time" : 1698424694707,
  "history_end_time" : 1698424694724,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "jiIANB1rM9E0",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nall_used_columns = ['cumulative_pr','station_elevation','swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-27\ntest start date:  2018-01-01\ntest end date:  2023-10-27\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['cumulative_pr', 'station_elevation'], dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['cumulative_pr', 'station_elevation'], dtype='object')\ntraining data row number:  5480\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 1.898709992639774\nMSE is 32.723844651108195\nR2 score is 0.9634901924410942\nRMSE is 5.720475911242717\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232710163630.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-3.png\n",
  "history_begin_time" : 1698424588952,
  "history_end_time" : 1698424590767,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "eaheOHmUrz4K",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n        weights = np.zeros_like(self.train_y, dtype=float)\n\n        # Set weight to 1 if the target variable is 0\n        weights[self.train_y == 0] = 10.0\n\n        # Calculate weights for non-zero target values\n        non_zero_indices = self.train_y != 0\n        weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nall_used_columns = ['cumulative_pr','station_elevation','swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-27\ntest start date:  2018-01-01\ntest end date:  2023-10-27\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['cumulative_pr', 'station_elevation'], dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['cumulative_pr', 'station_elevation'], dtype='object')\ntraining data row number:  5480\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 1.864818688280028\nMSE is 41.24452130519504\nR2 score is 0.9539837219077844\nRMSE is 6.422189759357399\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232710162137.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-3.png\n",
  "history_begin_time" : 1698423696711,
  "history_end_time" : 1698423698356,
  "history_notes" : "only two variables",
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "uUSV91y7antu",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n        weights = np.zeros_like(self.train_y, dtype=float)\n\n        # Set weight to 1 if the target variable is 0\n        weights[self.train_y == 0] = 10.0\n\n        # Calculate weights for non-zero target values\n        non_zero_indices = self.train_y != 0\n        weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nall_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-27\ntest start date:  2018-01-01\ntest end date:  2023-10-27\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['cumulative_pr', 'station_elevation', 'cumulative_tmmn',\n       'cumulative_tmmx', 'northness'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['cumulative_pr', 'station_elevation', 'cumulative_tmmn',\n       'cumulative_tmmx', 'northness'],\n      dtype='object')\ntraining data row number:  5480\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.5312043795620438\nMSE is 2.981110766423358\nR2 score is 0.996673991654883\nRMSE is 1.7265893450451262\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232710161757.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-6.png\n",
  "history_begin_time" : 1698423476116,
  "history_end_time" : 1698423477899,
  "history_notes" : "only five most important variables",
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "nj1Q4qteadF6",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n        weights = np.zeros_like(self.train_y, dtype=float)\n\n        # Set weight to 1 if the target variable is 0\n        weights[self.train_y == 0] = 10.0\n\n        # Calculate weights for non-zero target values\n        non_zero_indices = self.train_y != 0\n        weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nall_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-27\ntest start date:  2018-01-01\ntest end date:  2023-10-27\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['cumulative_pr', 'station_elevation', 'cumulative_tmmn',\n       'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax',\n       'cumulative_etr', 'aspect', 'cumulative_rmin', 'elevation',\n       'cumulative_vpd'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['cumulative_pr', 'station_elevation', 'cumulative_tmmn',\n       'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax',\n       'cumulative_etr', 'aspect', 'cumulative_rmin', 'elevation',\n       'cumulative_vpd'],\n      dtype='object')\ntraining data row number:  5480\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.6018430656934306\nMSE is 4.38097609489051\nR2 score is 0.9951121698611535\nRMSE is 2.0930781387445885\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232710155807.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-13.png\n",
  "history_begin_time" : 1698422286637,
  "history_end_time" : 1698422288411,
  "history_notes" : "12 variables remove all vars after eastness",
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "goHQj0nPmXZp",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n        weights = np.zeros_like(self.train_y, dtype=float)\n\n        # Set weight to 1 if the target variable is 0\n        weights[self.train_y == 0] = 10.0\n\n        # Calculate weights for non-zero target values\n        non_zero_indices = self.train_y != 0\n        weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nall_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd', 'eastness', 'slope', 'tmmn', 'fSCA','curvature', \n 'etr', 'pr','vs', 'rmax', 'rmin', 'vpd', 'tmmx',\n 'lc_code', 'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-27\ntest start date:  2018-01-01\ntest end date:  2023-10-27\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['cumulative_pr', 'station_elevation', 'cumulative_tmmn',\n       'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax',\n       'cumulative_etr', 'aspect', 'cumulative_rmin', 'elevation',\n       'cumulative_vpd', 'eastness', 'slope', 'tmmn', 'fSCA', 'curvature',\n       'etr', 'pr', 'vs', 'rmax', 'rmin', 'vpd', 'tmmx', 'lc_code'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['cumulative_pr', 'station_elevation', 'cumulative_tmmn',\n       'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax',\n       'cumulative_etr', 'aspect', 'cumulative_rmin', 'elevation',\n       'cumulative_vpd', 'eastness', 'slope', 'tmmn', 'fSCA', 'curvature',\n       'etr', 'pr', 'vs', 'rmax', 'rmin', 'vpd', 'tmmx', 'lc_code'],\n      dtype='object')\ntraining data row number:  5480\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.6501094890510949\nMSE is 2.793492518248176\nR2 score is 0.9968833162684315\nRMSE is 1.671374439869228\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232710155530.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-26.png\n",
  "history_begin_time" : 1698422129313,
  "history_end_time" : 1698422131172,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "92gYidhs7Tbc",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n        weights = np.zeros_like(self.train_y, dtype=float)\n\n        # Set weight to 1 if the target variable is 0\n        weights[self.train_y == 0] = 10.0\n\n        # Calculate weights for non-zero target values\n        non_zero_indices = self.train_y != 0\n        weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\nall_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-27\ntest start date:  2018-01-01\ntest end date:  2023-10-27\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ntraining data row number:  5480\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.6669069343065694\nMSE is 2.897364872262773\nR2 score is 0.9967674264732013\nRMSE is 1.7021647606100807\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232710151924.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-26.png\n",
  "history_begin_time" : 1698419963528,
  "history_end_time" : 1698419965301,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "tc8tHBYPyOTy",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n        weights = np.zeros_like(self.train_y, dtype=float)\n\n        # Set weight to 1 if the target variable is 0\n        weights[self.train_y == 0] = 1.0\n\n        # Calculate weights for non-zero target values\n        non_zero_indices = self.train_y != 0\n        weights[non_zero_indices] = 1.0 / np.abs(self.train_y[non_zero_indices])\n\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\nall_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-27\ntest start date:  2018-01-01\ntest end date:  2023-10-27\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ntraining data row number:  5480\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.5551551094890512\nMSE is 2.2287597627737235\nR2 score is 0.9975133853952229\nRMSE is 1.4929031324147335\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232710143314.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-26.png\n",
  "history_begin_time" : 1698417193288,
  "history_end_time" : 1698417195052,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "BV6v9ybgU22h",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n        weights = np.zeros_like(self.train_y, dtype=float)\n\n        # Set weight to 1 if the target variable is 0\n        weights[self.train_y == 0] = 1.0\n\n        # Calculate weights for non-zero target values\n        non_zero_indices = self.train_y != 0\n        weights[non_zero_indices] = 1000.0 / np.abs(self.train_y[non_zero_indices])\n\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\nall_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-27\ntest start date:  2018-01-01\ntest end date:  2023-10-27\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ntraining data row number:  5480\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.646742700729927\nMSE is 2.4170019160583935\nR2 score is 0.9973033646942885\nRMSE is 1.5546709992980488\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232710143223.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-26.png\n",
  "history_begin_time" : 1698417142691,
  "history_end_time" : 1698417144419,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "T5QTeXRTyWLp",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n        weights = np.zeros_like(self.train_y, dtype=float)\n\n        # Set weight to 1 if the target variable is 0\n        weights[self.train_y == 0] = 1.0\n\n        # Calculate weights for non-zero target values\n        non_zero_indices = self.train_y != 0\n        weights[non_zero_indices] = 1000.0 / np.abs(self.train_y[non_zero_indices])\n\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\nall_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-27\ntest start date:  2018-01-01\ntest end date:  2023-10-27\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ntraining data row number:  5480\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 8.960456204379561\nMSE is 170.69047244525547\nR2 score is 0.8095616096593365\nRMSE is 13.064856388236935\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232710143141.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-26.png\n",
  "history_begin_time" : 1698417100596,
  "history_end_time" : 1698417102319,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "E6n5r4W95BPB",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n        weights = np.zeros_like(self.train_y, dtype=float)\n\n        # Set weight to 1 if the target variable is 0\n        weights[self.train_y == 0] = 1.0\n\n        # Calculate weights for non-zero target values\n        non_zero_indices = self.train_y != 0\n        weights[non_zero_indices] = 1.0 / np.abs(self.train_y[non_zero_indices])\n\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\nall_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-27\ntest start date:  2018-01-01\ntest end date:  2023-10-27\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ntraining data row number:  5480\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 8.960456204379561\nMSE is 170.69047244525547\nR2 score is 0.8095616096593365\nRMSE is 13.064856388236935\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232710142954.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-26.png\n",
  "history_begin_time" : 1698416993607,
  "history_end_time" : 1698416995358,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "MqNljicO7aHL",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        self.weights = 1+self.train_y # You can adjust this formula as needed\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\nall_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-27\ntest start date:  2018-01-01\ntest end date:  2023-10-27\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ntraining data row number:  5480\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.6031204379562044\nMSE is 2.915271350364963\nR2 score is 0.9967474483173171\nRMSE is 1.7074165720072425\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232710140906.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-26.png\n",
  "history_begin_time" : 1698415745479,
  "history_end_time" : 1698415747231,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "RGeItBiuo6wj",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n        self.classifier.fit(self.train_x, self.train_y)\n        errors = abs(self.train_y - self.classifier.predict(self.train_x))\n        print(errors)\n        self.weights = 1/(1+self.train_y) # You can adjust this formula as needed\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\nall_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-27\ntest start date:  2018-01-01\ntest end date:  2023-10-27\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ntraining data row number:  5480\n[0. 0. 0. ... 0. 0. 0.]\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.5451277372262774\nMSE is 2.183161313868613\nR2 score is 0.9975642593256019\nRMSE is 1.477552474150618\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232710140736.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-26.png\n",
  "history_begin_time" : 1698415655449,
  "history_end_time" : 1698415657352,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Uo44eIjs0XNl",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n        self.classifier.fit(self.train_x, self.train_y)\n        errors = abs(self.train_y - self.classifier.predict(self.train_x))\n        print(errors)\n        self.weights = 1 + 100*errors  # You can adjust this formula as needed\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\nall_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-27\ntest start date:  2018-01-01\ntest end date:  2023-10-27\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ntraining data row number:  5480\n[0. 0. 0. ... 0. 0. 0.]\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.4903923357664234\nMSE is 1.7466015510948907\nR2 score is 0.9980513265726434\nRMSE is 1.3215905383646218\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232710140606.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-26.png\n",
  "history_begin_time" : 1698415565215,
  "history_end_time" : 1698415567109,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "AMXkEk00ZinS",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n        self.classifier.fit(self.train_x, self.train_y)\n        errors = abs(self.train_y - self.classifier.predict(self.train_x))\n        print(errors)\n        self.weights = 1 + 10*errors  # You can adjust this formula as needed\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\nall_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-27\ntest start date:  2018-01-01\ntest end date:  2023-10-27\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ntraining data row number:  5480\n[0. 0. 0. ... 0. 0. 0.]\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.4903923357664234\nMSE is 1.7466015510948907\nR2 score is 0.9980513265726434\nRMSE is 1.3215905383646218\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232710140556.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-26.png\n",
  "history_begin_time" : 1698415554891,
  "history_end_time" : 1698415556775,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "svqIsZnIK5WR",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n        self.classifier.fit(self.train_x, self.train_y)\n        errors = abs(self.train_y - self.classifier.predict(self.train_x))\n        print(errors)\n        self.weights = 1+errors  # You can adjust this formula as needed\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\nall_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-27\ntest start date:  2018-01-01\ntest end date:  2023-10-27\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ntraining data row number:  5480\n[0. 0. 0. ... 0. 0. 0.]\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.4903923357664234\nMSE is 1.7466015510948907\nR2 score is 0.9980513265726434\nRMSE is 1.3215905383646218\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232710140508.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-26.png\n",
  "history_begin_time" : 1698415507366,
  "history_end_time" : 1698415509206,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "jb34AaZnIdr4",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n        self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n#         self.weights = errors  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\nall_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-27\ntest start date:  2018-01-01\ntest end date:  2023-10-27\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ntraining data row number:  5480\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.4903923357664234\nMSE is 1.7466015510948907\nR2 score is 0.9980513265726434\nRMSE is 1.3215905383646218\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232710140443.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-26.png\n",
  "history_begin_time" : 1698415481831,
  "history_end_time" : 1698415483559,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "qyRnOPvdmcJY",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n        self.classifier.fit(self.train_x, self.train_y)\n        errors = abs(self.train_y - self.classifier.predict(self.train_x))\n        self.weights = errors  # You can adjust this formula as needed\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\nall_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-27\ntest start date:  2018-01-01\ntest end date:  2023-10-27\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ntraining data row number:  5480\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/qyRnOPvdmcJY/model_creation_et.py\", line 194, in <module>\n    hole.evaluate()\n  File \"/home/ubuntu/gw-workspace/qyRnOPvdmcJY/model_creation_rf.py\", line 65, in evaluate\n    mae = metrics.mean_absolute_error(self.test_y, self.test_y_results)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py\", line 196, in mean_absolute_error\n    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n                                          ^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py\", line 102, in _check_reg_targets\n    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 921, in check_array\n    _assert_all_finite(\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 161, in _assert_all_finite\n    raise ValueError(msg_err)\nValueError: Input contains NaN.\n",
  "history_begin_time" : 1698415366907,
  "history_end_time" : 1698415368459,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "vh22kMttCamE",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n        self.classifier.fit(self.train_x, self.train_y)\n        errors = abs(self.train_y - self.classifier.predict(self.train_x))\n        self.weights = 10*errors  # You can adjust this formula as needed\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\nall_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\n#hole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-27\ntest start date:  2018-01-01\ntest end date:  2023-10-27\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ntraining data row number:  5480\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232710135932.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-26.png\n",
  "history_begin_time" : 1698415171562,
  "history_end_time" : 1698415173308,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "eUdkim1prQgF",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        return []\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n        self.classifier.fit(self.train_x, self.train_y)\n        errors = abs(self.train_y - self.classifier.predict(self.train_x))\n        self.weights = 10*errors  # You can adjust this formula as needed\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\nall_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-27\ntest start date:  2018-01-01\ntest end date:  2023-10-27\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ntraining data row number:  5480\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/eUdkim1prQgF/model_creation_et.py\", line 195, in <module>\n    hole.evaluate()\n  File \"/home/ubuntu/gw-workspace/eUdkim1prQgF/model_creation_rf.py\", line 65, in evaluate\n    mae = metrics.mean_absolute_error(self.test_y, self.test_y_results)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py\", line 196, in mean_absolute_error\n    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n                                          ^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py\", line 102, in _check_reg_targets\n    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 921, in check_array\n    _assert_all_finite(\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 161, in _assert_all_finite\n    raise ValueError(msg_err)\nValueError: Input contains NaN.\n",
  "history_begin_time" : 1698415070355,
  "history_end_time" : 1698415072028,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "JDwe3HLFWB3U",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n        self.classifier.fit(self.train_x, self.train_y)\n        errors = abs(self.train_y - self.classifier.predict(self.train_x))\n        self.weights = 10*errors  # You can adjust this formula as needed\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\nall_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-27\ntest start date:  2018-01-01\ntest end date:  2023-10-27\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ntraining data row number:  5480\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/JDwe3HLFWB3U/model_creation_et.py\", line 194, in <module>\n    hole.evaluate()\n  File \"/home/ubuntu/gw-workspace/JDwe3HLFWB3U/model_creation_rf.py\", line 65, in evaluate\n    mae = metrics.mean_absolute_error(self.test_y, self.test_y_results)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py\", line 196, in mean_absolute_error\n    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n                                          ^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py\", line 102, in _check_reg_targets\n    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 921, in check_array\n    _assert_all_finite(\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 161, in _assert_all_finite\n    raise ValueError(msg_err)\nValueError: Input contains NaN.\n",
  "history_begin_time" : 1698415006264,
  "history_end_time" : 1698415007849,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "A9H5awg8kC5h",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n        self.classifier.fit(self.train_x, self.train_y)\n        errors = abs(self.train_y - self.classifier.predict(self.train_x))\n        self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\nall_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-27\ntest start date:  2018-01-01\ntest end date:  2023-10-27\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ntraining data row number:  5480\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.4903923357664234\nMSE is 1.7466015510948907\nR2 score is 0.9980513265726434\nRMSE is 1.3215905383646218\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232710135408.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-26.png\n",
  "history_begin_time" : 1698414846560,
  "history_end_time" : 1698414848530,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "8M3hZyyR6pUs",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n        self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\nall_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-27\ntest start date:  2018-01-01\ntest end date:  2023-10-27\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ntraining data row number:  5480\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.4903923357664234\nMSE is 1.7466015510948907\nR2 score is 0.9980513265726434\nRMSE is 1.3215905383646218\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232710135340.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-26.png\n",
  "history_begin_time" : 1698414818844,
  "history_end_time" : 1698414820554,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "g8lPNnXSHFnT",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\nall_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-27\ntest start date:  2018-01-01\ntest end date:  2023-10-27\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ntraining data row number:  5480\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 8.960456204379561\nMSE is 170.69047244525547\nR2 score is 0.8095616096593365\nRMSE is 13.064856388236935\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232710135334.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-26.png\n",
  "history_begin_time" : 1698414812780,
  "history_end_time" : 1698414814565,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "OpoUMOGo9bTN",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\nall_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-27\ntest start date:  2018-01-01\ntest end date:  2023-10-27\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ntraining data row number:  5480\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 8.960456204379561\nMSE is 170.69047244525547\nR2 score is 0.8095616096593365\nRMSE is 13.064856388236935\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232710135040.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-26.png\n",
  "history_begin_time" : 1698414639320,
  "history_end_time" : 1698414641084,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "vb75l3Hhae1D",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n        self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\nall_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-27\ntest start date:  2018-01-01\ntest end date:  2023-10-27\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ntraining data row number:  5480\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.4903923357664234\nMSE is 1.7466015510948907\nR2 score is 0.9980513265726434\nRMSE is 1.3215905383646218\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232710134707.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-26.png\n",
  "history_begin_time" : 1698414425826,
  "history_end_time" : 1698414427639,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "TCTXWWV58CsX",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n        self.classifier.fit(self.train_x, self.train_y)\n        errors = abs(self.train_y - self.classifier.predict(self.train_x))\n        self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n        errors = abs(self.train_y - self.classifier.predict(self.train_x))\n        self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\nall_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-27\ntest start date:  2018-01-01\ntest end date:  2023-10-27\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ntraining data row number:  5480\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.4903923357664234\nMSE is 1.7466015510948907\nR2 score is 0.9980513265726434\nRMSE is 1.3215905383646218\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232710133438.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-26.png\n",
  "history_begin_time" : 1698413677275,
  "history_end_time" : 1698413679408,
  "history_notes" : "do two iteration weighting",
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "MJ7KkCoJmCjT",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n        self.classifier.fit(self.train_x, self.train_y)\n        errors = abs(self.train_y - self.classifier.predict(self.train_x))\n        self.weights = 100 / (1 + errors)  # You can adjust this formula as needed\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\nall_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-27\ntest start date:  2018-01-01\ntest end date:  2023-10-27\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ntraining data row number:  5480\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.4876003649635036\nMSE is 1.6893774635036491\nR2 score is 0.9981151711620541\nRMSE is 1.2997605408319062\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232710132808.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-26.png\n",
  "history_begin_time" : 1698413286741,
  "history_end_time" : 1698413288694,
  "history_notes" : "100 times weight",
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "vnRjrLX4Z3T1",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n        self.classifier.fit(self.train_x, self.train_y)\n        errors = abs(self.train_y - self.classifier.predict(self.train_x))\n        self.weights = 10 / (1 + errors)  # You can adjust this formula as needed\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\nall_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-27\ntest start date:  2018-01-01\ntest end date:  2023-10-27\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ntraining data row number:  5480\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.47189781021897814\nMSE is 1.5735505474452554\nR2 score is 0.9982443985942375\nRMSE is 1.2544124311586105\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232710132147.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-26.png\n",
  "history_begin_time" : 1698412905979,
  "history_end_time" : 1698412907876,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "C91tUEmrsfzO",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n        self.classifier.fit(self.train_x, self.train_y)\n        errors = abs(self.train_y - self.classifier.predict(self.train_x))\n        self.weights = 0.1 / (1 + errors)  # You can adjust this formula as needed\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\nall_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-27\ntest start date:  2018-01-01\ntest end date:  2023-10-27\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ntraining data row number:  5480\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.5017335766423359\nMSE is 1.8649527372262777\nR2 score is 0.997919282826681\nRMSE is 1.3656327241342299\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232710131449.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-26.png\n",
  "history_begin_time" : 1698412487951,
  "history_end_time" : 1698412489916,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "q0ht4sMpsu7S",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n        self.classifier.fit(self.train_x, self.train_y)\n        errors = abs(self.train_y - self.classifier.predict(self.train_x))\n        self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\nall_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-27\ntest start date:  2018-01-01\ntest end date:  2023-10-27\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ntraining data row number:  5480\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.4903923357664234\nMSE is 1.7466015510948907\nR2 score is 0.9980513265726434\nRMSE is 1.3215905383646218\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232710001557.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-26.png\n",
  "history_begin_time" : 1698365755956,
  "history_end_time" : 1698365757575,
  "history_notes" : "this is first weighted model and very good performance, rmse is 1.32",
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "TwIDEOcP6moy",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n        errors = abs(self.train_y - self.classifier.predict(self.train_x))\n        self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\nall_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-27\ntest start date:  2018-01-01\ntest end date:  2023-10-27\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ntraining data row number:  5480\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/TwIDEOcP6moy/model_creation_et.py\", line 188, in <module>\n    hole.train()\n  File \"/home/ubuntu/gw-workspace/TwIDEOcP6moy/model_creation_et.py\", line 150, in train\n    errors = abs(self.train_y - self.classifier.predict(self.train_x))\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 979, in predict\n    check_is_fitted(self)\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 1390, in check_is_fitted\n    raise NotFittedError(msg % {\"name\": type(estimator).__name__})\nsklearn.exceptions.NotFittedError: This ExtraTreesRegressor instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
  "history_begin_time" : 1698365734071,
  "history_end_time" : 1698365735217,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "yXjIAvdopPIM",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n        errors = abs(self.train_y - model.predict(self.train_x))\n        self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\nall_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-27\ntest start date:  2018-01-01\ntest end date:  2023-10-27\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ntraining data row number:  5480\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/yXjIAvdopPIM/model_creation_et.py\", line 188, in <module>\n    hole.train()\n  File \"/home/ubuntu/gw-workspace/yXjIAvdopPIM/model_creation_et.py\", line 150, in train\n    errors = abs(self.train_y - model.predict(self.train_x))\n                                ^^^^^\nNameError: name 'model' is not defined\n",
  "history_begin_time" : 1698365722849,
  "history_end_time" : 1698365723996,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "tVVxyVBKAaqi",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\nall_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-27\ntest start date:  2018-01-01\ntest end date:  2023-10-27\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ndescribe the statistics of swe_value:  count    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ntraining data row number:  5480\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 8.960456204379561\nMSE is 170.69047244525547\nR2 score is 0.8095616096593365\nRMSE is 13.064856388236935\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232710000752.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-26.png\n",
  "history_begin_time" : 1698365270969,
  "history_end_time" : 1698365272476,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "VRp1tkRFLcjO",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\nall_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ntraining data row number:  5480\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 8.960456204379561\nMSE is 170.69047244525547\nR2 score is 0.8095616096593365\nRMSE is 13.064856388236935\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232610231204.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-26.png\n",
  "history_begin_time" : 1698361923586,
  "history_end_time" : 1698361925145,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "f4KgFz6qicfi",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\nall_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0  ... cumulative_vs  cumulative_pr\n0             0         273  ...           4.2            0.0\n1             1         274  ...           8.3            0.0\n2             2         275  ...          10.9            2.0\n3             3         276  ...          13.2            2.0\n4             4         277  ...          15.7            2.0\n[5 rows x 31 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ntraining data row number:  5480\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 8.960456204379561\nMSE is 170.69047244525547\nR2 score is 0.8095616096593365\nRMSE is 13.064856388236935\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232610231153.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-26.png\n",
  "history_begin_time" : 1698361912032,
  "history_end_time" : 1698361913566,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "A100W1wZZwdc",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n        training_data_path = f'{working_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv'\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\nall_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n   Unnamed: 0.1  Unnamed: 0        date       lat  ...   vpd   vs  lc_code    fSCA\n0             0         273  2017-10-01  37.89748  ...  0.73  4.2    142.0  0.3508\n1             1         274  2017-10-02  37.89748  ...  0.50  4.1    142.0  0.1638\n2             2         275  2017-10-03  37.89748  ...  0.27  2.6    142.0  0.0042\n3             3         276  2017-10-04  37.89748  ...  0.46  2.3    142.0  0.0624\n4             4         277  2017-10-05  37.89748  ...  0.69  2.5    142.0  0.0398\n[5 rows x 23 columns]\nget swe statistics\ncount    5480.000000\nmean       13.840693\nstd        27.672438\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%        14.000000\nmax       155.000000\nName: swe_value, dtype: float64\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/A100W1wZZwdc/model_creation_et.py\", line 183, in <module>\n    hole.preprocessing(chosen_columns = all_used_columns)\n  File \"/home/ubuntu/gw-workspace/A100W1wZZwdc/model_creation_et.py\", line 121, in preprocessing\n    data = data[chosen_columns]\n           ~~~~^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 3767, in __getitem__\n    indexer = self.columns._get_indexer_strict(key, \"columns\")[1]\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 5876, in _get_indexer_strict\n    self._raise_if_missing(keyarr, indexer, axis_name)\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 5938, in _raise_if_missing\n    raise KeyError(f\"{not_found} not in index\")\nKeyError: \"['cumulative_etr', 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'] not in index\"\n",
  "history_begin_time" : 1698361835817,
  "history_end_time" : 1698361837287,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "MynidFVqsPhe",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\nall_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_cum_water_year_winter_month_only.csv\n   Unnamed: 0        date  ...  cumulative_pr  cumulative_fSCA\n0         273  2017-10-01  ...            0.0           0.3508\n1         274  2017-10-02  ...            0.0           0.5146\n2         275  2017-10-03  ...            2.0           0.5188\n3         276  2017-10-04  ...            2.0           0.5812\n4         277  2017-10-05  ...            2.0           0.6210\n[5 rows x 33 columns]\nget swe statistics\ncount    2740.000000\nmean       27.681387\nstd        33.890083\nmin         0.000000\n25%         0.000000\n50%        14.000000\n75%        40.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'cumulative_etr', 'cumulative_rmax',\n       'cumulative_rmin', 'cumulative_tmmn', 'cumulative_tmmx',\n       'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ntraining data row number:  2740\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 2.054653284671533\nMSE is 10.179326094890511\nR2 score is 0.9924346316755358\nRMSE is 3.1905056174359747\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232610214102.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-26.png\n",
  "history_begin_time" : 1698356461643,
  "history_end_time" : 1698356463134,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "asCxX8IpIzZ9",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\nall_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code', 'pmv', 'fSCA', 'SnowClass', 'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = all_used_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = all_used_columns)\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_cum_water_year_winter_month_only.csv\n   Unnamed: 0        date  ...  cumulative_pr  cumulative_fSCA\n0         273  2017-10-01  ...            0.0           0.3508\n1         274  2017-10-02  ...            0.0           0.5146\n2         275  2017-10-03  ...            2.0           0.5188\n3         276  2017-10-04  ...            2.0           0.5812\n4         277  2017-10-05  ...            2.0           0.6210\n[5 rows x 33 columns]\nget swe statistics\ncount    2740.000000\nmean       27.681387\nstd        33.890083\nmin         0.000000\n25%         0.000000\n50%        14.000000\n75%        40.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'pmv', 'fSCA', 'SnowClass', 'cumulative_etr',\n       'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n       'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'pmv', 'fSCA', 'SnowClass', 'cumulative_etr',\n       'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n       'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ntraining data row number:  2740\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 1.965638686131387\nMSE is 9.293888503649635\nR2 score is 0.9930926969977014\nRMSE is 3.04858795242152\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232610211503.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-28.png\n",
  "history_begin_time" : 1698354902185,
  "history_end_time" : 1698354903699,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "mt9McBuGdgQV",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\ninput_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code', 'pmv', 'fSCA', 'SnowClass', 'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr']\n\nhole = ETHole()\nhole.preprocessing(chosen_columns = input_columns)\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing(chosen_columns = input_columns)\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_cum_water_year_winter_month_only.csv\n   Unnamed: 0        date  ...  cumulative_pr  cumulative_fSCA\n0         273  2017-10-01  ...            0.0           0.3508\n1         274  2017-10-02  ...            0.0           0.5146\n2         275  2017-10-03  ...            2.0           0.5188\n3         276  2017-10-04  ...            2.0           0.5812\n4         277  2017-10-05  ...            2.0           0.6210\n[5 rows x 33 columns]\nget swe statistics\ncount    2740.000000\nmean       27.681387\nstd        33.890083\nmin         0.000000\n25%         0.000000\n50%        14.000000\n75%        40.000000\nmax       155.000000\nName: swe_value, dtype: float64\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/mt9McBuGdgQV/model_creation_et.py\", line 181, in <module>\n    hole.preprocessing(chosen_columns = input_columns)\n  File \"/home/ubuntu/gw-workspace/mt9McBuGdgQV/model_creation_et.py\", line 127, in preprocessing\n    X = data.drop('swe_value', axis=1)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 5258, in drop\n    return super().drop(\n           ^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 4549, in drop\n    obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 4591, in _drop_axis\n    new_axis = axis.drop(labels, errors=errors)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 6696, in drop\n    raise KeyError(f\"{list(labels[mask])} not found in axis\")\nKeyError: \"['swe_value'] not found in axis\"\n",
  "history_begin_time" : 1698354878081,
  "history_end_time" : 1698354879215,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "5kQv55hQlG5G",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\ninput_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code', 'pmv', 'fSCA', 'SnowClass', 'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr']\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_cum_water_year_winter_month_only.csv\n   Unnamed: 0        date  ...  cumulative_pr  cumulative_fSCA\n0         273  2017-10-01  ...            0.0           0.3508\n1         274  2017-10-02  ...            0.0           0.5146\n2         275  2017-10-03  ...            2.0           0.5188\n3         276  2017-10-04  ...            2.0           0.5812\n4         277  2017-10-05  ...            2.0           0.6210\n[5 rows x 33 columns]\nget swe statistics\ncount    2740.000000\nmean       27.681387\nstd        33.890083\nmin         0.000000\n25%         0.000000\n50%        14.000000\n75%        40.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'pmv', 'fSCA', 'SnowClass', 'cumulative_etr',\n       'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n       'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr',\n       'cumulative_fSCA'],\n      dtype='object')\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'pmv', 'fSCA', 'SnowClass', 'cumulative_etr',\n       'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n       'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr',\n       'cumulative_fSCA'],\n      dtype='object')\ntraining data row number:  2740\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 1.9960401459854014\nMSE is 9.529181569343066\nR2 score is 0.9929178250376552\nRMSE is 3.086937247393129\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232610211344.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1698354823115,
  "history_end_time" : 1698354824638,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "cNqr0BgXsklQ",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\ninput_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code', 'pmv', 'fSCA', 'SnowClass', 'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr']\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_cum_water_year_winter_month_only.csv\n   Unnamed: 0        date  ...  cumulative_pr  cumulative_fSCA\n0         273  2017-10-01  ...            0.0           0.3508\n1         274  2017-10-02  ...            0.0           0.5146\n2         275  2017-10-03  ...            2.0           0.5188\n3         276  2017-10-04  ...            2.0           0.5812\n4         277  2017-10-05  ...            2.0           0.6210\n[5 rows x 33 columns]\nget swe statistics\ncount    2740.000000\nmean       27.681387\nstd        33.890083\nmin         0.000000\n25%         0.000000\n50%        14.000000\n75%        40.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'pmv', 'fSCA', 'SnowClass', 'cumulative_etr',\n       'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n       'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr',\n       'cumulative_fSCA'],\n      dtype='object')\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'pmv', 'fSCA', 'SnowClass', 'cumulative_etr',\n       'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n       'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr',\n       'cumulative_fSCA'],\n      dtype='object')\ntraining data row number:  2740\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 1.9960401459854014\nMSE is 9.529181569343066\nR2 score is 0.9929178250376552\nRMSE is 3.086937247393129\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232610210353.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1698354232700,
  "history_end_time" : 1698354234230,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "oFozDU9Sim8g",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\ninput_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code', 'pmv', 'fSCA', 'SnowClass', 'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr']\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_cum_water_year_winter_month_only.csv\n   Unnamed: 0        date  ...  cumulative_pr  cumulative_fSCA\n0         273  2017-10-01  ...            0.0           0.3508\n1         274  2017-10-02  ...            0.0           0.5146\n2         275  2017-10-03  ...            2.0           0.5188\n3         276  2017-10-04  ...            2.0           0.5812\n4         277  2017-10-05  ...            2.0           0.6210\n[5 rows x 33 columns]\nget swe statistics\ncount    2740.000000\nmean       27.681387\nstd        33.890083\nmin         0.000000\n25%         0.000000\n50%        14.000000\n75%        40.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'pmv', 'fSCA', 'SnowClass', 'cumulative_etr',\n       'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n       'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr',\n       'cumulative_fSCA'],\n      dtype='object')\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'pmv', 'fSCA', 'SnowClass', 'cumulative_etr',\n       'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n       'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr',\n       'cumulative_fSCA'],\n      dtype='object')\ntraining data row number:  2740\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 1.9960401459854014\nMSE is 9.529181569343066\nR2 score is 0.9929178250376552\nRMSE is 3.086937247393129\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232610205132.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1698353491342,
  "history_end_time" : 1698353492859,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "wjvIloldggdg",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\ninput_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code', 'pmv', 'fSCA', 'SnowClass', 'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr']\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_cum_water_year_winter_month_only.csv\n   Unnamed: 0        date  ...  cumulative_pr  cumulative_fSCA\n0         273  2017-10-01  ...            0.0           0.3508\n1         274  2017-10-02  ...            0.0           0.5146\n2         275  2017-10-03  ...            2.0           0.5188\n3         276  2017-10-04  ...            2.0           0.5812\n4         277  2017-10-05  ...            2.0           0.6210\n[5 rows x 33 columns]\nget swe statistics\ncount    2740.000000\nmean       27.681387\nstd        33.890083\nmin         0.000000\n25%         0.000000\n50%        14.000000\n75%        40.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'pmv', 'fSCA', 'SnowClass', 'cumulative_etr',\n       'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n       'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr',\n       'cumulative_fSCA'],\n      dtype='object')\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'pmv', 'fSCA', 'SnowClass', 'cumulative_etr',\n       'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n       'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr',\n       'cumulative_fSCA'],\n      dtype='object')\ntraining data row number:  2740\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 1.9960401459854014\nMSE is 9.529181569343066\nR2 score is 0.9929178250376552\nRMSE is 3.086937247393129\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232610151518.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1698333317692,
  "history_end_time" : 1698333319659,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "RSSoH9OHQCgZ",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\ninput_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code', 'pmv', 'fSCA', 'SnowClass', 'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr']\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\n         date       lat         lon  ...     aspect  eastness  northness\n0  2020-11-25  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n1  2020-10-18  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n2  2020-09-21  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n3  2022-09-01  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n4  2019-10-03  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n[5 rows x 21 columns]\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  1008700\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 5.122912189541815\nMSE is 36.462691478007415\nR2 score is -0.32271066180979235\nRMSE is 6.038434522126361\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232610151508.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1698333298879,
  "history_end_time" : 1698333319650,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "exqgwweteqz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698276496953,
  "history_end_time" : 1698276496953,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "SnEMKvBDzwe6",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\ninput_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code', 'pmv', 'fSCA', 'SnowClass', 'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr']\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-03-02\ntest end date:  2023-10-25\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\n         date       lat         lon  ...     aspect  eastness  northness\n0  2020-11-25  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n1  2020-10-18  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n2  2020-09-21  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n3  2022-09-01  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n4  2019-10-03  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n[5 rows x 21 columns]\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  1008700\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 5.122912189541815\nMSE is 36.462691478007415\nR2 score is -0.32271066180979235\nRMSE is 6.038434522126361\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232510230426.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1698275055720,
  "history_end_time" : 1698275090003,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "AtHrFa6S2o6F",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        if chosen_columns == None:\n          data = data.drop('Unnamed: 0', axis=1)\n          #data = data.drop('level_0', axis=1)\n#           data = data.drop(['date'], axis=1)\n#           data = data.drop(['lat'], axis=1)\n#           data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\ninput_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n'vpd', 'vs', 'lc_code', 'pmv', 'fSCA', 'SnowClass', 'cumulative_etr',\n'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr']\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-03-02\ntest end date:  2023-10-25\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\n         date       lat         lon  ...     aspect  eastness  northness\n0  2020-11-25  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n1  2020-10-18  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n2  2020-09-21  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n3  2022-09-01  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n4  2019-10-03  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n[5 rows x 21 columns]\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  1008700\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 3.139633208895659\nMSE is 18.680649876817494\nR2 score is 0.32234583460451205\nRMSE is 4.322111738122638\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232510230349.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1698275017847,
  "history_end_time" : 1698275049552,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Z3n4vaf5RytF",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop('level_0', axis=1)\n        data = data.drop(['date'], axis=1)\n        data = data.drop(['lat'], axis=1)\n        data = data.drop(['lon'], axis=1)\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-03-02\ntest end date:  2023-10-25\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_cum_water_year_winter_month_only.csv\n   Unnamed: 0        date  ...  cumulative_vs  cumulative_pr\n0         273  2017-10-01  ...            4.2            0.0\n1         274  2017-10-02  ...            8.3            0.0\n2         275  2017-10-03  ...           10.9            2.0\n3         276  2017-10-04  ...           13.2            2.0\n4         277  2017-10-05  ...           15.7            2.0\n[5 rows x 32 columns]\nget swe statistics\ncount    2740.000000\nmean       27.681387\nstd        33.890083\nmin         0.000000\n25%         0.000000\n50%        14.000000\n75%        40.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'pmv', 'fSCA', 'SnowClass', 'cumulative_etr',\n       'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n       'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ninput features and order:  Index(['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n       'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'pmv', 'fSCA', 'SnowClass', 'cumulative_etr',\n       'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n       'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr'],\n      dtype='object')\ntraining data row number:  2740\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 1.965638686131387\nMSE is 9.293888503649635\nR2 score is 0.9930926969977014\nRMSE is 3.04858795242152\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232510225255.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1698274374399,
  "history_end_time" : 1698274376212,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "KcczzfIehxBN",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop('level_0', axis=1)\n        data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-03-02\ntest end date:  2023-10-25\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_cum_water_year_winter_month_only.csv\n   Unnamed: 0        date  ...  cumulative_vs  cumulative_pr\n0         273  2017-10-01  ...            4.2            0.0\n1         274  2017-10-02  ...            8.3            0.0\n2         275  2017-10-03  ...           10.9            2.0\n3         276  2017-10-04  ...           13.2            2.0\n4         277  2017-10-05  ...           15.7            2.0\n[5 rows x 32 columns]\nget swe statistics\ncount    2740.000000\nmean       27.681387\nstd        33.890083\nmin         0.000000\n25%         0.000000\n50%        14.000000\n75%        40.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['lat', 'lon', 'station_elevation', 'elevation', 'aspect', 'curvature',\n       'slope', 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn',\n       'tmmx', 'vpd', 'vs', 'lc_code', 'pmv', 'fSCA', 'SnowClass',\n       'cumulative_etr', 'cumulative_rmax', 'cumulative_rmin',\n       'cumulative_tmmn', 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs',\n       'cumulative_pr'],\n      dtype='object')\ninput features and order:  Index(['lat', 'lon', 'station_elevation', 'elevation', 'aspect', 'curvature',\n       'slope', 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn',\n       'tmmx', 'vpd', 'vs', 'lc_code', 'pmv', 'fSCA', 'SnowClass',\n       'cumulative_etr', 'cumulative_rmax', 'cumulative_rmin',\n       'cumulative_tmmn', 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs',\n       'cumulative_pr'],\n      dtype='object')\ntraining data row number:  2740\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 1.9625547445255476\nMSE is 9.254154379562044\nR2 score is 0.9931222277624073\nRMSE is 3.0420641642743242\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232510224909.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1698274148277,
  "history_end_time" : 1698274150023,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "yZJYJgWtdW8R",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop('level_0', axis=1)\n        data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-03-02\ntest end date:  2023-10-25\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_cum_water_year_winter_month_only.csv\n   Unnamed: 0        date  ...  cumulative_vs  cumulative_pr\n0         273  2017-10-01  ...            4.2            0.0\n1         274  2017-10-02  ...            8.3            0.0\n2         275  2017-10-03  ...           10.9            2.0\n3         276  2017-10-04  ...           13.2            2.0\n4         277  2017-10-05  ...           15.7            2.0\n[5 rows x 32 columns]\nget swe statistics\ncount    2740.000000\nmean       27.681387\nstd        33.890083\nmin         0.000000\n25%         0.000000\n50%        14.000000\n75%        40.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['Unnamed: 0', 'lat', 'lon', 'station_elevation', 'elevation', 'aspect',\n       'curvature', 'slope', 'eastness', 'northness', 'etr', 'pr', 'rmax',\n       'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'lc_code', 'pmv', 'fSCA',\n       'SnowClass', 'cumulative_etr', 'cumulative_rmax', 'cumulative_rmin',\n       'cumulative_tmmn', 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs',\n       'cumulative_pr'],\n      dtype='object')\ninput features and order:  Index(['Unnamed: 0', 'lat', 'lon', 'station_elevation', 'elevation', 'aspect',\n       'curvature', 'slope', 'eastness', 'northness', 'etr', 'pr', 'rmax',\n       'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'lc_code', 'pmv', 'fSCA',\n       'SnowClass', 'cumulative_etr', 'cumulative_rmax', 'cumulative_rmin',\n       'cumulative_tmmn', 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs',\n       'cumulative_pr'],\n      dtype='object')\ntraining data row number:  2740\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 2.038759124087591\nMSE is 10.137885401459855\nR2 score is 0.9924654307781976\nRMSE is 3.1840046170600718\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232510224551.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1698273950057,
  "history_end_time" : 1698273951833,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "p88VmgHrFEvn",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop('level_0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-03-02\ntest end date:  2023-10-25\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_cum_water_year_winter_month_only.csv\n   Unnamed: 0        date  ...  cumulative_vs  cumulative_pr\n0         273  2017-10-01  ...            4.2            0.0\n1         274  2017-10-02  ...            8.3            0.0\n2         275  2017-10-03  ...           10.9            2.0\n3         276  2017-10-04  ...           13.2            2.0\n4         277  2017-10-05  ...           15.7            2.0\n[5 rows x 32 columns]\nget swe statistics\ncount    2740.000000\nmean       27.681387\nstd        33.890083\nmin         0.000000\n25%         0.000000\n50%        14.000000\n75%        40.000000\nmax       155.000000\nName: swe_value, dtype: float64\nrequired features: Index(['Unnamed: 0', 'date', 'lat', 'lon', 'station_elevation', 'elevation',\n       'aspect', 'curvature', 'slope', 'eastness', 'northness', 'etr', 'pr',\n       'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'lc_code', 'pmv', 'fSCA',\n       'SnowClass', 'cumulative_etr', 'cumulative_rmax', 'cumulative_rmin',\n       'cumulative_tmmn', 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs',\n       'cumulative_pr'],\n      dtype='object')\ninput features and order:  Index(['Unnamed: 0', 'date', 'lat', 'lon', 'station_elevation', 'elevation',\n       'aspect', 'curvature', 'slope', 'eastness', 'northness', 'etr', 'pr',\n       'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'lc_code', 'pmv', 'fSCA',\n       'SnowClass', 'cumulative_etr', 'cumulative_rmax', 'cumulative_rmin',\n       'cumulative_tmmn', 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs',\n       'cumulative_pr'],\n      dtype='object')\ntraining data row number:  2740\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 2.357536496350365\nMSE is 14.90236587591241\nR2 score is 0.9889244252805895\nRMSE is 3.860358257456477\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232510223620.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1698273379444,
  "history_end_time" : 1698273381212,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "P46VqfiXKpVw",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        training_data_path = f'{working_dir}/all_merged_training_cum.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop('level_0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-03-02\ntest end date:  2023-10-25\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_cum.csv\n         date       lat  ...  cumulative_vs  cumulative_pr\n0  2017-01-01  37.89748  ...            6.9            0.0\n1  2017-01-02  37.89748  ...           18.7            1.7\n2  2017-01-03  37.89748  ...           29.3           34.7\n3  2017-01-04  37.89748  ...           42.7          105.3\n4  2017-01-05  37.89748  ...           51.7          112.8\n[5 rows x 31 columns]\nget swe statistics\ncount    7300.000000\nmean       39.572877\nstd        60.408404\nmin         0.000000\n25%         0.000000\n50%        12.000000\n75%        62.000000\nmax       280.000000\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'station_elevation', 'elevation', 'aspect',\n       'curvature', 'slope', 'eastness', 'northness', 'etr', 'pr', 'rmax',\n       'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'lc_code', 'pmv', 'fSCA',\n       'SnowClass', 'cumulative_etr', 'cumulative_rmax', 'cumulative_rmin',\n       'cumulative_tmmn', 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs',\n       'cumulative_pr'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'station_elevation', 'elevation', 'aspect',\n       'curvature', 'slope', 'eastness', 'northness', 'etr', 'pr', 'rmax',\n       'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'lc_code', 'pmv', 'fSCA',\n       'SnowClass', 'cumulative_etr', 'cumulative_rmax', 'cumulative_rmin',\n       'cumulative_tmmn', 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs',\n       'cumulative_pr'],\n      dtype='object')\ntraining data row number:  7300\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 3.9314863013698633\nMSE is 42.762413219178086\nR2 score is 0.9878357833109352\nRMSE is 6.539297609008026\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232510212654.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1698269213394,
  "history_end_time" : 1698269215437,
  "history_notes" : "this is very good result with 0.98 R2 using cumulative meteo",
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "FRpHa1KKIRNm",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        training_data_path = f'{working_dir}/all_merged_training_cum.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop('level_0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-03-02\ntest end date:  2023-10-25\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training_cum.csv\n         date       lat  ...  cumulative_vs  cumulative_pr\n0  2017-01-01  37.89748  ...            6.9            0.0\n1  2017-01-02  37.89748  ...           18.7            1.7\n2  2017-01-03  37.89748  ...           29.3           34.7\n3  2017-01-04  37.89748  ...           42.7          105.3\n4  2017-01-05  37.89748  ...           51.7          112.8\n[5 rows x 31 columns]\nget swe statistics\ncount    7300.000000\nmean       39.572877\nstd        60.408404\nmin         0.000000\n25%         0.000000\n50%        12.000000\n75%        62.000000\nmax       280.000000\nName: swe_value, dtype: float64\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/FRpHa1KKIRNm/model_creation_et.py\", line 167, in <module>\n    hole.preprocessing()\n  File \"/home/ubuntu/gw-workspace/FRpHa1KKIRNm/model_creation_et.py\", line 112, in preprocessing\n    data = data.drop('Unnamed: 0', axis=1)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 5258, in drop\n    return super().drop(\n           ^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 4549, in drop\n    obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 4591, in _drop_axis\n    new_axis = axis.drop(labels, errors=errors)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 6696, in drop\n    raise KeyError(f\"{list(labels[mask])} not found in axis\")\nKeyError: \"['Unnamed: 0'] not found in axis\"\n",
  "history_begin_time" : 1698269181104,
  "history_end_time" : 1698269182512,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "bhv5zjrzacl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698252277357,
  "history_end_time" : 1698252277357,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xOaFReY7UWyS",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop('level_0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/final_merged_data_3yrs_cleaned_v3.csv\n         date       lat         lon  ...     aspect  eastness  northness\n0  2020-11-25  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n1  2020-10-18  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n2  2020-09-21  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n3  2022-09-01  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n4  2019-10-03  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n[5 rows x 21 columns]\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  1008700\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 3.1470146678745006\nMSE is 18.757062081259313\nR2 score is 0.31957392629465664\nRMSE is 4.330942401055377\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232510164222.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1698252132789,
  "history_end_time" : 1698252162231,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "DynpsBcD7n02",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop('level_0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/final_merged_data_3yrs_cleaned_v3.csv\n         date       lat         lon  ...     aspect  eastness  northness\n0  2020-11-25  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n1  2020-10-18  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n2  2020-09-21  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n3  2022-09-01  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n4  2019-10-03  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n[5 rows x 21 columns]\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  1008700\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 3.1470146678745006\nMSE is 18.757062081259313\nR2 score is 0.31957392629465664\nRMSE is 4.330942401055377\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232510164154.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1698252104147,
  "history_end_time" : 1698252132627,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "5UDeA83BQFVW",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n\t\treturn ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop('level_0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "  File \"/home/ubuntu/gw-workspace/5UDeA83BQFVW/model_creation_et.py\", line 62\n    return ExtraTreesRegressor(n_jobs=-1, random_state=123)\nTabError: inconsistent use of tabs and spaces in indentation\n",
  "history_begin_time" : 1698252085997,
  "history_end_time" : 1698252086013,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "y73264d596g",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698251392459,
  "history_end_time" : 1698251392459,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "iVcqLssC4TmO",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_v1.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop('level_0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-26\ntest end date:  2023-10-25\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\n         date       lat         lon  ...     aspect  eastness  northness\n0  2020-11-25  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n1  2020-10-18  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n2  2020-09-21  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n3  2022-09-01  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n4  2019-10-03  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n[5 rows x 21 columns]\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 3.209594825555586\nMSE is 19.344912573622306\nR2 score is 0.29824922199333936\nRMSE is 4.398285185572021\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232510100842.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1698228403759,
  "history_end_time" : 1698228547398,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "OinGFNieV0iY",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_v1.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        data = data.drop('level_0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-26\ntest end date:  2023-10-25\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\n         date       lat         lon  ...     aspect  eastness  northness\n0  2020-11-25  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n1  2020-10-18  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n2  2020-09-21  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n3  2022-09-01  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n4  2019-10-03  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n[5 rows x 21 columns]\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/OinGFNieV0iY/model_creation_et.py\", line 164, in <module>\n    hole.preprocessing()\n  File \"/home/chetana/gw-workspace/OinGFNieV0iY/model_creation_et.py\", line 111, in preprocessing\n    data = data.drop('level_0', axis=1)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\", line 5399, in drop\n    return super().drop(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\", line 4505, in drop\n    obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\", line 4546, in _drop_axis\n    new_axis = axis.drop(labels, errors=errors)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 6934, in drop\n    raise KeyError(f\"{list(labels[mask])} not found in axis\")\nKeyError: \"['level_0'] not found in axis\"\n",
  "history_begin_time" : 1698228377959,
  "history_end_time" : 1698228383620,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "bdpsgz50xke",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698228210943,
  "history_end_time" : 1698228210943,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "56xhr4318y3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698227897132,
  "history_end_time" : 1698227897132,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "RNEW1I1TbB9X",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop('level_0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/final_merged_data_3yrs_cleaned_v3.csv\n         date       lat         lon  ...     aspect  eastness  northness\n0  2020-11-25  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n1  2020-10-18  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n2  2020-09-21  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n3  2022-09-01  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n4  2019-10-03  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n[5 rows x 21 columns]\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ntraining data row number:  1008700\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 3.2070603851911663\nMSE is 19.298011027419108\nR2 score is 0.29995060970509824\nRMSE is 4.392950150800611\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232510061108.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1698214207790,
  "history_end_time" : 1698214322573,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "twI5F3VX2kTD",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop('level_0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/final_merged_data_3yrs_cleaned_v3.csv\n         date       lat         lon  ...     aspect  eastness  northness\n0  2020-11-25  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n1  2020-10-18  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n2  2020-09-21  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n3  2022-09-01  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n4  2019-10-03  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n[5 rows x 21 columns]\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 3.2070603851911663\nMSE is 19.298011027419108\nR2 score is 0.29995060970509824\nRMSE is 4.392950150800611\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232510060630.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1698213930228,
  "history_end_time" : 1698214043571,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "t7vjSYvMyRum",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        training_data_path = f\"{working_dir}/all_merged_training.csv\"\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        # careful this is filling all the n/a values with interpolation\n        #data.fillna(method='ffill', inplace=True)\n        #print(data.head())\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop('level_0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n      \t\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training.csv\n         date       lat         lon  swe_value  ...  lc_code  pmv    fSCA  SnowClass\n0  2017-01-01  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n1  2017-01-02  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n2  2017-01-03  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n3  2017-01-04  37.89748 -119.262434       23.0  ...    142.0  NaN  0.9954        NaN\n4  2017-01-05  37.89748 -119.262434       43.0  ...    142.0  NaN  0.9954        NaN\n[5 rows x 23 columns]\nget swe statistics\ncount    7300.000000\nmean       39.572877\nstd        60.408404\nmin         0.000000\n25%         0.000000\n50%        12.000000\n75%        62.000000\nmax       280.000000\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'station_elevation', 'elevation', 'aspect',\n       'curvature', 'slope', 'eastness', 'northness', 'etr', 'pr', 'rmax',\n       'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'lc_code', 'pmv', 'fSCA',\n       'SnowClass'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'station_elevation', 'elevation', 'aspect',\n       'curvature', 'slope', 'eastness', 'northness', 'etr', 'pr', 'rmax',\n       'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'lc_code', 'pmv', 'fSCA',\n       'SnowClass'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 17.70900342465754\nMSE is 825.7799517636986\nR2 score is 0.7650982366394121\nRMSE is 28.736387242722397\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232510045229.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1698209548130,
  "history_end_time" : 1698209550188,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "2tmge8tMGFVo",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        training_data_path = f\"{working_dir}/all_merged_training.csv\"\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        # careful this is filling all the n/a values with interpolation\n        #data.fillna(method='ffill', inplace=True)\n        #print(data.head())\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop('level_0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n      \t\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training.csv\n         date       lat         lon  swe_value  ...  lc_code  pmv    fSCA  SnowClass\n0  2017-01-01  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n1  2017-01-02  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n2  2017-01-03  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n3  2017-01-04  37.89748 -119.262434       23.0  ...    142.0  NaN  0.9954        NaN\n4  2017-01-05  37.89748 -119.262434       43.0  ...    142.0  NaN  0.9954        NaN\n[5 rows x 23 columns]\nget swe statistics\ncount    7300.000000\nmean       39.572877\nstd        60.408404\nmin         0.000000\n25%         0.000000\n50%        12.000000\n75%        62.000000\nmax       280.000000\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'station_elevation', 'elevation', 'aspect',\n       'curvature', 'slope', 'eastness', 'northness', 'etr', 'pr', 'rmax',\n       'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'lc_code', 'pmv', 'fSCA',\n       'SnowClass'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'station_elevation', 'elevation', 'aspect',\n       'curvature', 'slope', 'eastness', 'northness', 'etr', 'pr', 'rmax',\n       'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'lc_code', 'pmv', 'fSCA',\n       'SnowClass'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 17.70900342465754\nMSE is 825.7799517636986\nR2 score is 0.7650982366394121\nRMSE is 28.736387242722397\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232510043326.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1698208405324,
  "history_end_time" : 1698208407384,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "bTbhE19fTDik",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        training_data_path = f\"{working_dir}/all_merged_training.csv\"\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        #data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        # careful this is filling all the n/a values with interpolation\n        data.fillna(method='ffill', inplace=True)\n        print(data.head())\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop('level_0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n      \n        \n        \n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training.csv\n         date       lat         lon  swe_value  ...  lc_code  pmv    fSCA  SnowClass\n0  2017-01-01  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n1  2017-01-02  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n2  2017-01-03  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n3  2017-01-04  37.89748 -119.262434       23.0  ...    142.0  NaN  0.9954        NaN\n4  2017-01-05  37.89748 -119.262434       43.0  ...    142.0  NaN  0.9954        NaN\n[5 rows x 23 columns]\n   date       lat         lon  swe_value  ...  lc_code  pmv    fSCA  SnowClass\n0     4  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n1     4  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n2     4  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n3     4  37.89748 -119.262434       23.0  ...    142.0  NaN  0.9954        NaN\n4     4  37.89748 -119.262434       43.0  ...    142.0  NaN  0.9954        NaN\n[5 rows x 23 columns]\nget swe statistics\ncount    7300.000000\nmean       39.572877\nstd        60.408404\nmin         0.000000\n25%         0.000000\n50%        12.000000\n75%        62.000000\nmax       280.000000\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'station_elevation', 'elevation', 'aspect',\n       'curvature', 'slope', 'eastness', 'northness', 'etr', 'pr', 'rmax',\n       'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'lc_code', 'pmv', 'fSCA',\n       'SnowClass'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'station_elevation', 'elevation', 'aspect',\n       'curvature', 'slope', 'eastness', 'northness', 'etr', 'pr', 'rmax',\n       'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'lc_code', 'pmv', 'fSCA',\n       'SnowClass'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/bTbhE19fTDik/model_creation_et.py\", line 173, in <module>\n    hole.train()\n  File \"/home/ubuntu/gw-workspace/bTbhE19fTDik/model_creation_et.py\", line 147, in train\n    self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 348, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/base.py\", line 621, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 1147, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 959, in check_array\n    _assert_all_finite(\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 124, in _assert_all_finite\n    _assert_all_finite_element_wise(\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 173, in _assert_all_finite_element_wise\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nExtraTreesRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
  "history_begin_time" : 1698208142825,
  "history_end_time" : 1698208143982,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "LKZ0yoTqpTMi",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        training_data_path = f\"{working_dir}/all_merged_training.csv\"\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        #data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        \n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop('level_0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n      \n        # careful this is filling all the n/a values with interpolation\n        X.fillna(method='ffill', inplace=True)\n        print(X.head())\n        \n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training.csv\n         date       lat         lon  swe_value  ...  lc_code  pmv    fSCA  SnowClass\n0  2017-01-01  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n1  2017-01-02  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n2  2017-01-03  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n3  2017-01-04  37.89748 -119.262434       23.0  ...    142.0  NaN  0.9954        NaN\n4  2017-01-05  37.89748 -119.262434       43.0  ...    142.0  NaN  0.9954        NaN\n[5 rows x 23 columns]\nget swe statistics\ncount    7300.000000\nmean       39.572877\nstd        60.408404\nmin         0.000000\n25%         0.000000\n50%        12.000000\n75%        62.000000\nmax       280.000000\nName: swe_value, dtype: float64\n   date       lat         lon  ...  pmv    fSCA  SnowClass\n0     4  37.89748 -119.262434  ...  NaN  0.9954        NaN\n1     4  37.89748 -119.262434  ...  NaN  0.9954        NaN\n2     4  37.89748 -119.262434  ...  NaN  0.9954        NaN\n3     4  37.89748 -119.262434  ...  NaN  0.9954        NaN\n4     4  37.89748 -119.262434  ...  NaN  0.9954        NaN\n[5 rows x 22 columns]\nrequired features: Index(['date', 'lat', 'lon', 'station_elevation', 'elevation', 'aspect',\n       'curvature', 'slope', 'eastness', 'northness', 'etr', 'pr', 'rmax',\n       'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'lc_code', 'pmv', 'fSCA',\n       'SnowClass'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'station_elevation', 'elevation', 'aspect',\n       'curvature', 'slope', 'eastness', 'northness', 'etr', 'pr', 'rmax',\n       'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'lc_code', 'pmv', 'fSCA',\n       'SnowClass'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/LKZ0yoTqpTMi/model_creation_et.py\", line 173, in <module>\n    hole.train()\n  File \"/home/ubuntu/gw-workspace/LKZ0yoTqpTMi/model_creation_et.py\", line 147, in train\n    self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 348, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/base.py\", line 621, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 1147, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 959, in check_array\n    _assert_all_finite(\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 124, in _assert_all_finite\n    _assert_all_finite_element_wise(\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 173, in _assert_all_finite_element_wise\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nExtraTreesRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
  "history_begin_time" : 1698208112216,
  "history_end_time" : 1698208113381,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "8Q36T0KCMiM1",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        training_data_path = f\"{working_dir}/all_merged_training.csv\"\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        #data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        \n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop('level_0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n      \n        # careful this is filling all the n/a values with interpolation\n        X.interpolate(method='linear', inplace=True)\n        print(X.head())\n        \n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training.csv\n         date       lat         lon  swe_value  ...  lc_code  pmv    fSCA  SnowClass\n0  2017-01-01  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n1  2017-01-02  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n2  2017-01-03  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n3  2017-01-04  37.89748 -119.262434       23.0  ...    142.0  NaN  0.9954        NaN\n4  2017-01-05  37.89748 -119.262434       43.0  ...    142.0  NaN  0.9954        NaN\n[5 rows x 23 columns]\nget swe statistics\ncount    7300.000000\nmean       39.572877\nstd        60.408404\nmin         0.000000\n25%         0.000000\n50%        12.000000\n75%        62.000000\nmax       280.000000\nName: swe_value, dtype: float64\n   date       lat         lon  ...  pmv    fSCA  SnowClass\n0     4  37.89748 -119.262434  ...  NaN  0.9954        NaN\n1     4  37.89748 -119.262434  ...  NaN  0.9954        NaN\n2     4  37.89748 -119.262434  ...  NaN  0.9954        NaN\n3     4  37.89748 -119.262434  ...  NaN  0.9954        NaN\n4     4  37.89748 -119.262434  ...  NaN  0.9954        NaN\n[5 rows x 22 columns]\nrequired features: Index(['date', 'lat', 'lon', 'station_elevation', 'elevation', 'aspect',\n       'curvature', 'slope', 'eastness', 'northness', 'etr', 'pr', 'rmax',\n       'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'lc_code', 'pmv', 'fSCA',\n       'SnowClass'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'station_elevation', 'elevation', 'aspect',\n       'curvature', 'slope', 'eastness', 'northness', 'etr', 'pr', 'rmax',\n       'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'lc_code', 'pmv', 'fSCA',\n       'SnowClass'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/8Q36T0KCMiM1/model_creation_et.py\", line 173, in <module>\n    hole.train()\n  File \"/home/ubuntu/gw-workspace/8Q36T0KCMiM1/model_creation_et.py\", line 147, in train\n    self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 348, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/base.py\", line 621, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 1147, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 959, in check_array\n    _assert_all_finite(\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 124, in _assert_all_finite\n    _assert_all_finite_element_wise(\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 173, in _assert_all_finite_element_wise\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nExtraTreesRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
  "history_begin_time" : 1698208002298,
  "history_end_time" : 1698208003465,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "0R6v8GtLB9Qg",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        training_data_path = f\"{working_dir}/all_merged_training.csv\"\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        #data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        \n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop('level_0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n      \n        # careful this is filling all the n/a values with interpolation\n        X.interpolate(method='linear', inplace=True)\n        \n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training.csv\n         date       lat         lon  swe_value  ...  lc_code  pmv    fSCA  SnowClass\n0  2017-01-01  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n1  2017-01-02  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n2  2017-01-03  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n3  2017-01-04  37.89748 -119.262434       23.0  ...    142.0  NaN  0.9954        NaN\n4  2017-01-05  37.89748 -119.262434       43.0  ...    142.0  NaN  0.9954        NaN\n[5 rows x 23 columns]\nget swe statistics\ncount    7300.000000\nmean       39.572877\nstd        60.408404\nmin         0.000000\n25%         0.000000\n50%        12.000000\n75%        62.000000\nmax       280.000000\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'station_elevation', 'elevation', 'aspect',\n       'curvature', 'slope', 'eastness', 'northness', 'etr', 'pr', 'rmax',\n       'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'lc_code', 'pmv', 'fSCA',\n       'SnowClass'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'station_elevation', 'elevation', 'aspect',\n       'curvature', 'slope', 'eastness', 'northness', 'etr', 'pr', 'rmax',\n       'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'lc_code', 'pmv', 'fSCA',\n       'SnowClass'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/0R6v8GtLB9Qg/model_creation_et.py\", line 172, in <module>\n    hole.train()\n  File \"/home/ubuntu/gw-workspace/0R6v8GtLB9Qg/model_creation_et.py\", line 146, in train\n    self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 348, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/base.py\", line 621, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 1147, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 959, in check_array\n    _assert_all_finite(\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 124, in _assert_all_finite\n    _assert_all_finite_element_wise(\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 173, in _assert_all_finite_element_wise\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nExtraTreesRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
  "history_begin_time" : 1698207911704,
  "history_end_time" : 1698207912853,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "KdXJr9eeYsAS",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        training_data_path = f\"{working_dir}/all_merged_training.csv\"\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop('level_0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training.csv\n         date       lat         lon  swe_value  ...  lc_code  pmv    fSCA  SnowClass\n0  2017-01-01  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n1  2017-01-02  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n2  2017-01-03  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n3  2017-01-04  37.89748 -119.262434       23.0  ...    142.0  NaN  0.9954        NaN\n4  2017-01-05  37.89748 -119.262434       43.0  ...    142.0  NaN  0.9954        NaN\n[5 rows x 23 columns]\nget swe statistics\ncount    7300.000000\nmean       39.572877\nstd        60.408404\nmin         0.000000\n25%         0.000000\n50%        12.000000\n75%        62.000000\nmax       280.000000\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'station_elevation', 'elevation', 'aspect',\n       'curvature', 'slope', 'eastness', 'northness', 'etr', 'pr', 'rmax',\n       'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'lc_code', 'pmv', 'fSCA',\n       'SnowClass'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'station_elevation', 'elevation', 'aspect',\n       'curvature', 'slope', 'eastness', 'northness', 'etr', 'pr', 'rmax',\n       'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'lc_code', 'pmv', 'fSCA',\n       'SnowClass'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 17.70900342465754\nMSE is 825.7799517636986\nR2 score is 0.7650982366394121\nRMSE is 28.736387242722397\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232510040922.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1698206960819,
  "history_end_time" : 1698206963103,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "7ebgZFeDXa2g",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        training_data_path = f\"{working_dir}/all_merged_training.csv\"\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop('level_0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/all_merged_training.csv\n         date       lat         lon  swe_value  ...  lc_code  pmv    fSCA  SnowClass\n0  2017-01-01  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n1  2017-01-02  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n2  2017-01-03  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n3  2017-01-04  37.89748 -119.262434       23.0  ...    142.0  NaN  0.9954        NaN\n4  2017-01-05  37.89748 -119.262434       43.0  ...    142.0  NaN  0.9954        NaN\n[5 rows x 23 columns]\nget swe statistics\ncount    7300.000000\nmean       39.572877\nstd        60.408404\nmin         0.000000\n25%         0.000000\n50%        12.000000\n75%        62.000000\nmax       280.000000\nName: swe_value, dtype: float64\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/7ebgZFeDXa2g/model_creation_et.py\", line 165, in <module>\n    hole.preprocessing()\n  File \"/home/ubuntu/gw-workspace/7ebgZFeDXa2g/model_creation_et.py\", line 111, in preprocessing\n    data = data.drop('Unnamed: 0', axis=1)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 5258, in drop\n    return super().drop(\n           ^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 4549, in drop\n    obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 4591, in _drop_axis\n    new_axis = axis.drop(labels, errors=errors)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 6696, in drop\n    raise KeyError(f\"{list(labels[mask])} not found in axis\")\nKeyError: \"['Unnamed: 0'] not found in axis\"\n",
  "history_begin_time" : 1698206948041,
  "history_end_time" : 1698206949386,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "3qDdVAb8m4rs",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop('level_0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-24\ntest start date:  2023-05-29\ntest end date:  2023-10-24\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/final_merged_data_3yrs_cleaned_v3.csv\n         date       lat         lon  ...     aspect  eastness  northness\n0  2020-11-25  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n1  2020-10-18  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n2  2020-09-21  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n3  2022-09-01  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n4  2019-10-03  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n[5 rows x 21 columns]\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 3.2070603851911663\nMSE is 19.298011027419108\nR2 score is 0.29995060970509824\nRMSE is 4.392950150800611\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232410185325.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1698173537574,
  "history_end_time" : 1698173655961,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "6tjrpiyb8ov",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698163737286,
  "history_end_time" : 1698163737286,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "43cp2iu4183",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698163445820,
  "history_end_time" : 1698163445820,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "z2cedmz3pfm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698163121532,
  "history_end_time" : 1698163121532,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "KCEZ4mRq4vik",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop('level_0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-24\ntest start date:  2023-05-25\ntest end date:  2023-10-24\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/final_merged_data_3yrs_cleaned_v3.csv\n         date       lat         lon  ...     aspect  eastness  northness\n0  2020-11-25  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n1  2020-10-18  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n2  2020-09-21  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n3  2022-09-01  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n4  2019-10-03  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n[5 rows x 21 columns]\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 3.2070603851911663\nMSE is 19.298011027419108\nR2 score is 0.29995060970509824\nRMSE is 4.392950150800611\nSaving model to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232410154811.joblib\na copy of the model is saved to /home/ubuntu/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/ubuntu/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1698162423392,
  "history_end_time" : 1698162516554,
  "history_notes" : "this is messed up",
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "yycNh7AtUPOz",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        data = data.drop('level_0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-24\ntest start date:  2023-05-25\ntest end date:  2023-10-24\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/final_merged_data_3yrs_cleaned_v3.csv\n         date       lat         lon  ...     aspect  eastness  northness\n0  2020-11-25  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n1  2020-10-18  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n2  2020-09-21  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n3  2022-09-01  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n4  2019-10-03  37.19236 -118.939041  ...  24.605782  0.394541   0.737872\n[5 rows x 21 columns]\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/yycNh7AtUPOz/model_creation_et.py\", line 164, in <module>\n    hole.preprocessing()\n  File \"/home/ubuntu/gw-workspace/yycNh7AtUPOz/model_creation_et.py\", line 111, in preprocessing\n    data = data.drop('level_0', axis=1)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 5258, in drop\n    return super().drop(\n           ^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 4549, in drop\n    obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 4591, in _drop_axis\n    new_axis = axis.drop(labels, errors=errors)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 6696, in drop\n    raise KeyError(f\"{list(labels[mask])} not found in axis\")\nKeyError: \"['level_0'] not found in axis\"\n",
  "history_begin_time" : 1698162404912,
  "history_end_time" : 1698162407991,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "9UCYUuLK5b2k",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_v1.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        data = data.drop('level_0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-24\ntest start date:  2023-05-25\ntest end date:  2023-10-24\n/home/ubuntu\npreparing training data from csv:  /home/ubuntu/gridmet_test_run/final_merged_data_3yrs_cleaned_v3_time_series_v1.csv\n         date  level_0  ...  air_temperature_tmmx_7  wind_speed_7\n0  2019-01-01        0  ...                     NaN           NaN\n1  2019-01-01        1  ...                     NaN           NaN\n2  2019-01-01        2  ...                     NaN           NaN\n3  2019-01-02        3  ...                     NaN           NaN\n4  2019-01-02        4  ...                     NaN           NaN\n[5 rows x 93 columns]\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'index', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'SWE_1', 'Flag_1', 'air_temperature_tmmn_1',\n       'potential_evapotranspiration_1', 'mean_vapor_pressure_deficit_1',\n       'relative_humidity_rmax_1', 'relative_humidity_rmin_1',\n       'precipitation_amount_1', 'air_temperature_tmmx_1', 'wind_speed_1',\n       'SWE_2', 'Flag_2', 'air_temperature_tmmn_2',\n       'potential_evapotranspiration_2', 'mean_vapor_pressure_deficit_2',\n       'relative_humidity_rmax_2', 'relative_humidity_rmin_2',\n       'precipitation_amount_2', 'air_temperature_tmmx_2', 'wind_speed_2',\n       'SWE_3', 'Flag_3', 'air_temperature_tmmn_3',\n       'potential_evapotranspiration_3', 'mean_vapor_pressure_deficit_3',\n       'relative_humidity_rmax_3', 'relative_humidity_rmin_3',\n       'precipitation_amount_3', 'air_temperature_tmmx_3', 'wind_speed_3',\n       'SWE_4', 'Flag_4', 'air_temperature_tmmn_4',\n       'potential_evapotranspiration_4', 'mean_vapor_pressure_deficit_4',\n       'relative_humidity_rmax_4', 'relative_humidity_rmin_4',\n       'precipitation_amount_4', 'air_temperature_tmmx_4', 'wind_speed_4',\n       'SWE_5', 'Flag_5', 'air_temperature_tmmn_5',\n       'potential_evapotranspiration_5', 'mean_vapor_pressure_deficit_5',\n       'relative_humidity_rmax_5', 'relative_humidity_rmin_5',\n       'precipitation_amount_5', 'air_temperature_tmmx_5', 'wind_speed_5',\n       'SWE_6', 'Flag_6', 'air_temperature_tmmn_6',\n       'potential_evapotranspiration_6', 'mean_vapor_pressure_deficit_6',\n       'relative_humidity_rmax_6', 'relative_humidity_rmin_6',\n       'precipitation_amount_6', 'air_temperature_tmmx_6', 'wind_speed_6',\n       'SWE_7', 'Flag_7', 'air_temperature_tmmn_7',\n       'potential_evapotranspiration_7', 'mean_vapor_pressure_deficit_7',\n       'relative_humidity_rmax_7', 'relative_humidity_rmin_7',\n       'precipitation_amount_7', 'air_temperature_tmmx_7', 'wind_speed_7'],\n      dtype='object')\ninput features and order:  Index(['date', 'index', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'SWE_1', 'Flag_1', 'air_temperature_tmmn_1',\n       'potential_evapotranspiration_1', 'mean_vapor_pressure_deficit_1',\n       'relative_humidity_rmax_1', 'relative_humidity_rmin_1',\n       'precipitation_amount_1', 'air_temperature_tmmx_1', 'wind_speed_1',\n       'SWE_2', 'Flag_2', 'air_temperature_tmmn_2',\n       'potential_evapotranspiration_2', 'mean_vapor_pressure_deficit_2',\n       'relative_humidity_rmax_2', 'relative_humidity_rmin_2',\n       'precipitation_amount_2', 'air_temperature_tmmx_2', 'wind_speed_2',\n       'SWE_3', 'Flag_3', 'air_temperature_tmmn_3',\n       'potential_evapotranspiration_3', 'mean_vapor_pressure_deficit_3',\n       'relative_humidity_rmax_3', 'relative_humidity_rmin_3',\n       'precipitation_amount_3', 'air_temperature_tmmx_3', 'wind_speed_3',\n       'SWE_4', 'Flag_4', 'air_temperature_tmmn_4',\n       'potential_evapotranspiration_4', 'mean_vapor_pressure_deficit_4',\n       'relative_humidity_rmax_4', 'relative_humidity_rmin_4',\n       'precipitation_amount_4', 'air_temperature_tmmx_4', 'wind_speed_4',\n       'SWE_5', 'Flag_5', 'air_temperature_tmmn_5',\n       'potential_evapotranspiration_5', 'mean_vapor_pressure_deficit_5',\n       'relative_humidity_rmax_5', 'relative_humidity_rmin_5',\n       'precipitation_amount_5', 'air_temperature_tmmx_5', 'wind_speed_5',\n       'SWE_6', 'Flag_6', 'air_temperature_tmmn_6',\n       'potential_evapotranspiration_6', 'mean_vapor_pressure_deficit_6',\n       'relative_humidity_rmax_6', 'relative_humidity_rmin_6',\n       'precipitation_amount_6', 'air_temperature_tmmx_6', 'wind_speed_6',\n       'SWE_7', 'Flag_7', 'air_temperature_tmmn_7',\n       'potential_evapotranspiration_7', 'mean_vapor_pressure_deficit_7',\n       'relative_humidity_rmax_7', 'relative_humidity_rmin_7',\n       'precipitation_amount_7', 'air_temperature_tmmx_7', 'wind_speed_7'],\n      dtype='object')\n",
  "history_begin_time" : 1698162283490,
  "history_end_time" : 1698162401821,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "SHKjqKGc2Yfw",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_v1.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        data = data.drop('level_0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/SHKjqKGc2Yfw/model_creation_et.py\", line 25, in <module>\n    from model_creation_rf import RandomForestHole\n  File \"/home/ubuntu/gw-workspace/SHKjqKGc2Yfw/model_creation_rf.py\", line 26, in <module>\n    import geojson\nModuleNotFoundError: No module named 'geojson'\n",
  "history_begin_time" : 1698162268564,
  "history_end_time" : 1698162269818,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "N75yIFW5cEGm",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_v1.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        data = data.drop('level_0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/N75yIFW5cEGm/model_creation_et.py\", line 25, in <module>\n    from model_creation_rf import RandomForestHole\n  File \"/home/ubuntu/gw-workspace/N75yIFW5cEGm/model_creation_rf.py\", line 25, in <module>\n    import geopandas as gpd\nModuleNotFoundError: No module named 'geopandas'\n",
  "history_begin_time" : 1698162130895,
  "history_end_time" : 1698162132034,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "t5k81xr89el",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698160809358,
  "history_end_time" : 1698160809358,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "cm4x3ft1dmu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698157805168,
  "history_end_time" : 1698157805168,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "hb5ibaz3sx9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698152099733,
  "history_end_time" : 1698152099733,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "mlzf17fo8n7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698095495727,
  "history_end_time" : 1698095495727,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tk4u2cayrqv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698075453564,
  "history_end_time" : 1698075453564,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "lCNI6ws3A7W6",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_v1.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-18\ntest start date:  2023-05-17\ntest end date:  2023-10-18\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v3_time_series_v1.csv\n         date  level_0  ...  air_temperature_tmmx_7  wind_speed_7\n0  2019-01-01        0  ...                     NaN           NaN\n1  2019-01-01        1  ...                     NaN           NaN\n2  2019-01-01        2  ...                     NaN           NaN\n3  2019-01-02        3  ...                     NaN           NaN\n4  2019-01-02        4  ...                     NaN           NaN\n[5 rows x 93 columns]\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'level_0', 'index', 'lat', 'lon', 'SWE', 'Flag',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'SWE_1', 'Flag_1',\n       'air_temperature_tmmn_1', 'potential_evapotranspiration_1',\n       'mean_vapor_pressure_deficit_1', 'relative_humidity_rmax_1',\n       'relative_humidity_rmin_1', 'precipitation_amount_1',\n       'air_temperature_tmmx_1', 'wind_speed_1', 'SWE_2', 'Flag_2',\n       'air_temperature_tmmn_2', 'potential_evapotranspiration_2',\n       'mean_vapor_pressure_deficit_2', 'relative_humidity_rmax_2',\n       'relative_humidity_rmin_2', 'precipitation_amount_2',\n       'air_temperature_tmmx_2', 'wind_speed_2', 'SWE_3', 'Flag_3',\n       'air_temperature_tmmn_3', 'potential_evapotranspiration_3',\n       'mean_vapor_pressure_deficit_3', 'relative_humidity_rmax_3',\n       'relative_humidity_rmin_3', 'precipitation_amount_3',\n       'air_temperature_tmmx_3', 'wind_speed_3', 'SWE_4', 'Flag_4',\n       'air_temperature_tmmn_4', 'potential_evapotranspiration_4',\n       'mean_vapor_pressure_deficit_4', 'relative_humidity_rmax_4',\n       'relative_humidity_rmin_4', 'precipitation_amount_4',\n       'air_temperature_tmmx_4', 'wind_speed_4', 'SWE_5', 'Flag_5',\n       'air_temperature_tmmn_5', 'potential_evapotranspiration_5',\n       'mean_vapor_pressure_deficit_5', 'relative_humidity_rmax_5',\n       'relative_humidity_rmin_5', 'precipitation_amount_5',\n       'air_temperature_tmmx_5', 'wind_speed_5', 'SWE_6', 'Flag_6',\n       'air_temperature_tmmn_6', 'potential_evapotranspiration_6',\n       'mean_vapor_pressure_deficit_6', 'relative_humidity_rmax_6',\n       'relative_humidity_rmin_6', 'precipitation_amount_6',\n       'air_temperature_tmmx_6', 'wind_speed_6', 'SWE_7', 'Flag_7',\n       'air_temperature_tmmn_7', 'potential_evapotranspiration_7',\n       'mean_vapor_pressure_deficit_7', 'relative_humidity_rmax_7',\n       'relative_humidity_rmin_7', 'precipitation_amount_7',\n       'air_temperature_tmmx_7', 'wind_speed_7'],\n      dtype='object')\ninput features and order:  Index(['date', 'level_0', 'index', 'lat', 'lon', 'SWE', 'Flag',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'SWE_1', 'Flag_1',\n       'air_temperature_tmmn_1', 'potential_evapotranspiration_1',\n       'mean_vapor_pressure_deficit_1', 'relative_humidity_rmax_1',\n       'relative_humidity_rmin_1', 'precipitation_amount_1',\n       'air_temperature_tmmx_1', 'wind_speed_1', 'SWE_2', 'Flag_2',\n       'air_temperature_tmmn_2', 'potential_evapotranspiration_2',\n       'mean_vapor_pressure_deficit_2', 'relative_humidity_rmax_2',\n       'relative_humidity_rmin_2', 'precipitation_amount_2',\n       'air_temperature_tmmx_2', 'wind_speed_2', 'SWE_3', 'Flag_3',\n       'air_temperature_tmmn_3', 'potential_evapotranspiration_3',\n       'mean_vapor_pressure_deficit_3', 'relative_humidity_rmax_3',\n       'relative_humidity_rmin_3', 'precipitation_amount_3',\n       'air_temperature_tmmx_3', 'wind_speed_3', 'SWE_4', 'Flag_4',\n       'air_temperature_tmmn_4', 'potential_evapotranspiration_4',\n       'mean_vapor_pressure_deficit_4', 'relative_humidity_rmax_4',\n       'relative_humidity_rmin_4', 'precipitation_amount_4',\n       'air_temperature_tmmx_4', 'wind_speed_4', 'SWE_5', 'Flag_5',\n       'air_temperature_tmmn_5', 'potential_evapotranspiration_5',\n       'mean_vapor_pressure_deficit_5', 'relative_humidity_rmax_5',\n       'relative_humidity_rmin_5', 'precipitation_amount_5',\n       'air_temperature_tmmx_5', 'wind_speed_5', 'SWE_6', 'Flag_6',\n       'air_temperature_tmmn_6', 'potential_evapotranspiration_6',\n       'mean_vapor_pressure_deficit_6', 'relative_humidity_rmax_6',\n       'relative_humidity_rmin_6', 'precipitation_amount_6',\n       'air_temperature_tmmx_6', 'wind_speed_6', 'SWE_7', 'Flag_7',\n       'air_temperature_tmmn_7', 'potential_evapotranspiration_7',\n       'mean_vapor_pressure_deficit_7', 'relative_humidity_rmax_7',\n       'relative_humidity_rmin_7', 'precipitation_amount_7',\n       'air_temperature_tmmx_7', 'wind_speed_7'],\n      dtype='object')\nThe random forest model performance for testing set\nMAE is 2.8853143204124123\nMSE is 15.13187745076089\nR2 score is 0.45053164141899604\nRMSE is 3.889971394594167\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231810041621.joblib\n",
  "history_begin_time" : 1697601936758,
  "history_end_time" : 1697602582165,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "vFOMn2viykEt",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_v1.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "Running",
  "history_begin_time" : 1697601875549,
  "history_end_time" : 1697601916834,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "GxGaSGvzBTls",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_v1.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "Running",
  "history_begin_time" : 1697601622287,
  "history_end_time" : 1697601915284,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "SRgO0gqfXNJS",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_v1.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-16\ntest start date:  2023-05-17\ntest end date:  2023-10-16\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v3_time_series_v1.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['level_0', 'index', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'SWE_1', 'Flag_1', 'air_temperature_tmmn_1',\n       'potential_evapotranspiration_1', 'mean_vapor_pressure_deficit_1',\n       'relative_humidity_rmax_1', 'relative_humidity_rmin_1',\n       'precipitation_amount_1', 'air_temperature_tmmx_1', 'wind_speed_1',\n       'SWE_2', 'Flag_2', 'air_temperature_tmmn_2',\n       'potential_evapotranspiration_2', 'mean_vapor_pressure_deficit_2',\n       'relative_humidity_rmax_2', 'relative_humidity_rmin_2',\n       'precipitation_amount_2', 'air_temperature_tmmx_2', 'wind_speed_2',\n       'SWE_3', 'Flag_3', 'air_temperature_tmmn_3',\n       'potential_evapotranspiration_3', 'mean_vapor_pressure_deficit_3',\n       'relative_humidity_rmax_3', 'relative_humidity_rmin_3',\n       'precipitation_amount_3', 'air_temperature_tmmx_3', 'wind_speed_3',\n       'SWE_4', 'Flag_4', 'air_temperature_tmmn_4',\n       'potential_evapotranspiration_4', 'mean_vapor_pressure_deficit_4',\n       'relative_humidity_rmax_4', 'relative_humidity_rmin_4',\n       'precipitation_amount_4', 'air_temperature_tmmx_4', 'wind_speed_4',\n       'SWE_5', 'Flag_5', 'air_temperature_tmmn_5',\n       'potential_evapotranspiration_5', 'mean_vapor_pressure_deficit_5',\n       'relative_humidity_rmax_5', 'relative_humidity_rmin_5',\n       'precipitation_amount_5', 'air_temperature_tmmx_5', 'wind_speed_5',\n       'SWE_6', 'Flag_6', 'air_temperature_tmmn_6',\n       'potential_evapotranspiration_6', 'mean_vapor_pressure_deficit_6',\n       'relative_humidity_rmax_6', 'relative_humidity_rmin_6',\n       'precipitation_amount_6', 'air_temperature_tmmx_6', 'wind_speed_6',\n       'SWE_7', 'Flag_7', 'air_temperature_tmmn_7',\n       'potential_evapotranspiration_7', 'mean_vapor_pressure_deficit_7',\n       'relative_humidity_rmax_7', 'relative_humidity_rmin_7',\n       'precipitation_amount_7', 'air_temperature_tmmx_7', 'wind_speed_7'],\n      dtype='object')\ninput features and order:  Index(['level_0', 'index', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'SWE_1', 'Flag_1', 'air_temperature_tmmn_1',\n       'potential_evapotranspiration_1', 'mean_vapor_pressure_deficit_1',\n       'relative_humidity_rmax_1', 'relative_humidity_rmin_1',\n       'precipitation_amount_1', 'air_temperature_tmmx_1', 'wind_speed_1',\n       'SWE_2', 'Flag_2', 'air_temperature_tmmn_2',\n       'potential_evapotranspiration_2', 'mean_vapor_pressure_deficit_2',\n       'relative_humidity_rmax_2', 'relative_humidity_rmin_2',\n       'precipitation_amount_2', 'air_temperature_tmmx_2', 'wind_speed_2',\n       'SWE_3', 'Flag_3', 'air_temperature_tmmn_3',\n       'potential_evapotranspiration_3', 'mean_vapor_pressure_deficit_3',\n       'relative_humidity_rmax_3', 'relative_humidity_rmin_3',\n       'precipitation_amount_3', 'air_temperature_tmmx_3', 'wind_speed_3',\n       'SWE_4', 'Flag_4', 'air_temperature_tmmn_4',\n       'potential_evapotranspiration_4', 'mean_vapor_pressure_deficit_4',\n       'relative_humidity_rmax_4', 'relative_humidity_rmin_4',\n       'precipitation_amount_4', 'air_temperature_tmmx_4', 'wind_speed_4',\n       'SWE_5', 'Flag_5', 'air_temperature_tmmn_5',\n       'potential_evapotranspiration_5', 'mean_vapor_pressure_deficit_5',\n       'relative_humidity_rmax_5', 'relative_humidity_rmin_5',\n       'precipitation_amount_5', 'air_temperature_tmmx_5', 'wind_speed_5',\n       'SWE_6', 'Flag_6', 'air_temperature_tmmn_6',\n       'potential_evapotranspiration_6', 'mean_vapor_pressure_deficit_6',\n       'relative_humidity_rmax_6', 'relative_humidity_rmin_6',\n       'precipitation_amount_6', 'air_temperature_tmmx_6', 'wind_speed_6',\n       'SWE_7', 'Flag_7', 'air_temperature_tmmn_7',\n       'potential_evapotranspiration_7', 'mean_vapor_pressure_deficit_7',\n       'relative_humidity_rmax_7', 'relative_humidity_rmin_7',\n       'precipitation_amount_7', 'air_temperature_tmmx_7', 'wind_speed_7'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 4.970466226330921\nMSE is 32.76086451715204\nR2 score is -0.1896116995732755\nRMSE is 5.723710729688568\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231610034335.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1697427280075,
  "history_end_time" : 1697427846836,
  "history_notes" : "drop date",
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "C0uZocwEWQ1i",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_v1.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-16\ntest start date:  2023-05-17\ntest end date:  2023-10-16\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v3_time_series_v1.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'level_0', 'index', 'lat', 'lon', 'SWE', 'Flag',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'SWE_1', 'Flag_1',\n       'air_temperature_tmmn_1', 'potential_evapotranspiration_1',\n       'mean_vapor_pressure_deficit_1', 'relative_humidity_rmax_1',\n       'relative_humidity_rmin_1', 'precipitation_amount_1',\n       'air_temperature_tmmx_1', 'wind_speed_1', 'SWE_2', 'Flag_2',\n       'air_temperature_tmmn_2', 'potential_evapotranspiration_2',\n       'mean_vapor_pressure_deficit_2', 'relative_humidity_rmax_2',\n       'relative_humidity_rmin_2', 'precipitation_amount_2',\n       'air_temperature_tmmx_2', 'wind_speed_2', 'SWE_3', 'Flag_3',\n       'air_temperature_tmmn_3', 'potential_evapotranspiration_3',\n       'mean_vapor_pressure_deficit_3', 'relative_humidity_rmax_3',\n       'relative_humidity_rmin_3', 'precipitation_amount_3',\n       'air_temperature_tmmx_3', 'wind_speed_3', 'SWE_4', 'Flag_4',\n       'air_temperature_tmmn_4', 'potential_evapotranspiration_4',\n       'mean_vapor_pressure_deficit_4', 'relative_humidity_rmax_4',\n       'relative_humidity_rmin_4', 'precipitation_amount_4',\n       'air_temperature_tmmx_4', 'wind_speed_4', 'SWE_5', 'Flag_5',\n       'air_temperature_tmmn_5', 'potential_evapotranspiration_5',\n       'mean_vapor_pressure_deficit_5', 'relative_humidity_rmax_5',\n       'relative_humidity_rmin_5', 'precipitation_amount_5',\n       'air_temperature_tmmx_5', 'wind_speed_5', 'SWE_6', 'Flag_6',\n       'air_temperature_tmmn_6', 'potential_evapotranspiration_6',\n       'mean_vapor_pressure_deficit_6', 'relative_humidity_rmax_6',\n       'relative_humidity_rmin_6', 'precipitation_amount_6',\n       'air_temperature_tmmx_6', 'wind_speed_6', 'SWE_7', 'Flag_7',\n       'air_temperature_tmmn_7', 'potential_evapotranspiration_7',\n       'mean_vapor_pressure_deficit_7', 'relative_humidity_rmax_7',\n       'relative_humidity_rmin_7', 'precipitation_amount_7',\n       'air_temperature_tmmx_7', 'wind_speed_7'],\n      dtype='object')\ninput features and order:  Index(['date', 'level_0', 'index', 'lat', 'lon', 'SWE', 'Flag',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'SWE_1', 'Flag_1',\n       'air_temperature_tmmn_1', 'potential_evapotranspiration_1',\n       'mean_vapor_pressure_deficit_1', 'relative_humidity_rmax_1',\n       'relative_humidity_rmin_1', 'precipitation_amount_1',\n       'air_temperature_tmmx_1', 'wind_speed_1', 'SWE_2', 'Flag_2',\n       'air_temperature_tmmn_2', 'potential_evapotranspiration_2',\n       'mean_vapor_pressure_deficit_2', 'relative_humidity_rmax_2',\n       'relative_humidity_rmin_2', 'precipitation_amount_2',\n       'air_temperature_tmmx_2', 'wind_speed_2', 'SWE_3', 'Flag_3',\n       'air_temperature_tmmn_3', 'potential_evapotranspiration_3',\n       'mean_vapor_pressure_deficit_3', 'relative_humidity_rmax_3',\n       'relative_humidity_rmin_3', 'precipitation_amount_3',\n       'air_temperature_tmmx_3', 'wind_speed_3', 'SWE_4', 'Flag_4',\n       'air_temperature_tmmn_4', 'potential_evapotranspiration_4',\n       'mean_vapor_pressure_deficit_4', 'relative_humidity_rmax_4',\n       'relative_humidity_rmin_4', 'precipitation_amount_4',\n       'air_temperature_tmmx_4', 'wind_speed_4', 'SWE_5', 'Flag_5',\n       'air_temperature_tmmn_5', 'potential_evapotranspiration_5',\n       'mean_vapor_pressure_deficit_5', 'relative_humidity_rmax_5',\n       'relative_humidity_rmin_5', 'precipitation_amount_5',\n       'air_temperature_tmmx_5', 'wind_speed_5', 'SWE_6', 'Flag_6',\n       'air_temperature_tmmn_6', 'potential_evapotranspiration_6',\n       'mean_vapor_pressure_deficit_6', 'relative_humidity_rmax_6',\n       'relative_humidity_rmin_6', 'precipitation_amount_6',\n       'air_temperature_tmmx_6', 'wind_speed_6', 'SWE_7', 'Flag_7',\n       'air_temperature_tmmn_7', 'potential_evapotranspiration_7',\n       'mean_vapor_pressure_deficit_7', 'relative_humidity_rmax_7',\n       'relative_humidity_rmin_7', 'precipitation_amount_7',\n       'air_temperature_tmmx_7', 'wind_speed_7'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 2.8853143204124123\nMSE is 15.13187745076089\nR2 score is 0.45053164141899604\nRMSE is 3.889971394594167\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231610024007.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1697423455360,
  "history_end_time" : 1697424039154,
  "history_notes" : "use the filled data using inter polation",
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "abWkqlk8Zbn7",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-15\ntest start date:  2023-05-17\ntest end date:  2023-10-15\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v3_time_series.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'SWE_1', 'Flag_1', 'air_temperature_tmmn_1',\n       'potential_evapotranspiration_1', 'mean_vapor_pressure_deficit_1',\n       'relative_humidity_rmax_1', 'relative_humidity_rmin_1',\n       'precipitation_amount_1', 'air_temperature_tmmx_1', 'wind_speed_1',\n       'SWE_2', 'Flag_2', 'air_temperature_tmmn_2',\n       'potential_evapotranspiration_2', 'mean_vapor_pressure_deficit_2',\n       'relative_humidity_rmax_2', 'relative_humidity_rmin_2',\n       'precipitation_amount_2', 'air_temperature_tmmx_2', 'wind_speed_2',\n       'SWE_3', 'Flag_3', 'air_temperature_tmmn_3',\n       'potential_evapotranspiration_3', 'mean_vapor_pressure_deficit_3',\n       'relative_humidity_rmax_3', 'relative_humidity_rmin_3',\n       'precipitation_amount_3', 'air_temperature_tmmx_3', 'wind_speed_3',\n       'SWE_4', 'Flag_4', 'air_temperature_tmmn_4',\n       'potential_evapotranspiration_4', 'mean_vapor_pressure_deficit_4',\n       'relative_humidity_rmax_4', 'relative_humidity_rmin_4',\n       'precipitation_amount_4', 'air_temperature_tmmx_4', 'wind_speed_4',\n       'SWE_5', 'Flag_5', 'air_temperature_tmmn_5',\n       'potential_evapotranspiration_5', 'mean_vapor_pressure_deficit_5',\n       'relative_humidity_rmax_5', 'relative_humidity_rmin_5',\n       'precipitation_amount_5', 'air_temperature_tmmx_5', 'wind_speed_5',\n       'SWE_6', 'Flag_6', 'air_temperature_tmmn_6',\n       'potential_evapotranspiration_6', 'mean_vapor_pressure_deficit_6',\n       'relative_humidity_rmax_6', 'relative_humidity_rmin_6',\n       'precipitation_amount_6', 'air_temperature_tmmx_6', 'wind_speed_6',\n       'SWE_7', 'Flag_7', 'air_temperature_tmmn_7',\n       'potential_evapotranspiration_7', 'mean_vapor_pressure_deficit_7',\n       'relative_humidity_rmax_7', 'relative_humidity_rmin_7',\n       'precipitation_amount_7', 'air_temperature_tmmx_7', 'wind_speed_7'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'SWE_1', 'Flag_1', 'air_temperature_tmmn_1',\n       'potential_evapotranspiration_1', 'mean_vapor_pressure_deficit_1',\n       'relative_humidity_rmax_1', 'relative_humidity_rmin_1',\n       'precipitation_amount_1', 'air_temperature_tmmx_1', 'wind_speed_1',\n       'SWE_2', 'Flag_2', 'air_temperature_tmmn_2',\n       'potential_evapotranspiration_2', 'mean_vapor_pressure_deficit_2',\n       'relative_humidity_rmax_2', 'relative_humidity_rmin_2',\n       'precipitation_amount_2', 'air_temperature_tmmx_2', 'wind_speed_2',\n       'SWE_3', 'Flag_3', 'air_temperature_tmmn_3',\n       'potential_evapotranspiration_3', 'mean_vapor_pressure_deficit_3',\n       'relative_humidity_rmax_3', 'relative_humidity_rmin_3',\n       'precipitation_amount_3', 'air_temperature_tmmx_3', 'wind_speed_3',\n       'SWE_4', 'Flag_4', 'air_temperature_tmmn_4',\n       'potential_evapotranspiration_4', 'mean_vapor_pressure_deficit_4',\n       'relative_humidity_rmax_4', 'relative_humidity_rmin_4',\n       'precipitation_amount_4', 'air_temperature_tmmx_4', 'wind_speed_4',\n       'SWE_5', 'Flag_5', 'air_temperature_tmmn_5',\n       'potential_evapotranspiration_5', 'mean_vapor_pressure_deficit_5',\n       'relative_humidity_rmax_5', 'relative_humidity_rmin_5',\n       'precipitation_amount_5', 'air_temperature_tmmx_5', 'wind_speed_5',\n       'SWE_6', 'Flag_6', 'air_temperature_tmmn_6',\n       'potential_evapotranspiration_6', 'mean_vapor_pressure_deficit_6',\n       'relative_humidity_rmax_6', 'relative_humidity_rmin_6',\n       'precipitation_amount_6', 'air_temperature_tmmx_6', 'wind_speed_6',\n       'SWE_7', 'Flag_7', 'air_temperature_tmmn_7',\n       'potential_evapotranspiration_7', 'mean_vapor_pressure_deficit_7',\n       'relative_humidity_rmax_7', 'relative_humidity_rmin_7',\n       'precipitation_amount_7', 'air_temperature_tmmx_7', 'wind_speed_7'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 2.9448414791315556\nMSE is 15.769573702042239\nR2 score is 0.42737563096325426\nRMSE is 3.9710922555440886\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231510194517.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1697398600471,
  "history_end_time" : 1697399145673,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "vcegmykpiii",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1697349529993,
  "history_end_time" : 1697349529993,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "Oc02rBIrYSUE",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-15\ntest start date:  2023-05-17\ntest end date:  2023-10-15\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 3.209594825555586\nMSE is 19.344912573622306\nR2 score is 0.29824922199333936\nRMSE is 4.398285185572021\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231510055804.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1697349371128,
  "history_end_time" : 1697349516952,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "czvH75k721ee",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-15\ntest start date:  2023-05-16\ntest end date:  2023-10-15\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/czvH75k721ee/model_creation_et.py\", line 161, in <module>\n    hole.train()\n  File \"/home/chetana/gw-workspace/czvH75k721ee/model_creation_et.py\", line 135, in train\n    self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 348, in fit\n    X, y = self._validate_data(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 621, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1147, in check_X_y\n    X = check_array(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 917, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/_array_api.py\", line 380, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\nValueError: could not convert string to float: 'Spring'\n",
  "history_begin_time" : 1697349289756,
  "history_end_time" : 1697349295811,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "d81dj9qmx30",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1697348852260,
  "history_end_time" : 1697348852260,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "r3EMja2CVXTq",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        data['date'] = data['date'].dt.month\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-15\ntest start date:  2023-03-16\ntest end date:  2023-10-15\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 1.52289608065037\nMSE is 5.124031621676623\nR2 score is 0.8141220249325224\nRMSE is 2.263632395438054\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231510054041.joblib\na copy of the model is saved to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1697348329942,
  "history_end_time" : 1697348456390,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "273apekHj3xH",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        data['date'] = data['date'].dt.month\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-15\ntest start date:  2023-03-16\ntest end date:  2023-10-15\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 1.52289608065037\nMSE is 5.124031621676623\nR2 score is 0.8141220249325224\nRMSE is 2.263632395438054\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231510053705.joblib\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/273apekHj3xH/model_creation_et.py\", line 164, in <module>\n    hole.save()\n  File \"/home/chetana/gw-workspace/273apekHj3xH/base_hole.py\", line 63, in save\n    shutil.copy(self.save_file,\nTypeError: copy() missing 1 required positional argument: 'dst'\n",
  "history_begin_time" : 1697348111055,
  "history_end_time" : 1697348237725,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "YD7esnyUEt0z",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        data['date'] = data['date'].dt.month\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-15\ntest start date:  2023-03-16\ntest end date:  2023-10-15\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 1.52289608065037\nMSE is 5.124031621676623\nR2 score is 0.8141220249325224\nRMSE is 2.263632395438054\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231510053436.joblib\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/YD7esnyUEt0z/model_creation_et.py\", line 163, in <module>\n    hole.save()\n  File \"/home/chetana/gw-workspace/YD7esnyUEt0z/base_hole.py\", line 61, in save\n    shutil.copy(self.save_file,\nNameError: name 'shutil' is not defined\n",
  "history_begin_time" : 1697347961250,
  "history_end_time" : 1697348089308,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "dVnawa7UxqqQ",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        data['date'] = data['date'].dt.month\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-15\ntest start date:  2023-03-16\ntest end date:  2023-10-15\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 1.52289608065037\nMSE is 5.124031621676623\nR2 score is 0.8141220249325224\nRMSE is 2.263632395438054\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231510051348.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1697346715948,
  "history_end_time" : 1697346844493,
  "history_notes" : "change date to month and use all columns",
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Y6E95gCNWml7",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        data['month'] = data['date'].dt.month\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-15\ntest start date:  2023-03-16\ntest end date:  2023-10-15\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'month'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'month'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/Y6E95gCNWml7/model_creation_et.py\", line 160, in <module>\n    hole.train()\n  File \"/home/chetana/gw-workspace/Y6E95gCNWml7/model_creation_et.py\", line 134, in train\n    self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 348, in fit\n    X, y = self._validate_data(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 621, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1147, in check_X_y\n    X = check_array(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 917, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/_array_api.py\", line 380, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\nTypeError: float() argument must be a string or a number, not 'Timestamp'\n",
  "history_begin_time" : 1697346648099,
  "history_end_time" : 1697346656353,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "dmVsEBH11Nsn",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        data['date'] = data['date'].dt.strftime('%B')\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-15\ntest start date:  2023-03-16\ntest end date:  2023-10-15\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/dmVsEBH11Nsn/model_creation_et.py\", line 160, in <module>\n    hole.train()\n  File \"/home/chetana/gw-workspace/dmVsEBH11Nsn/model_creation_et.py\", line 134, in train\n    self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 348, in fit\n    X, y = self._validate_data(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 621, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1147, in check_X_y\n    X = check_array(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 917, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/_array_api.py\", line 380, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\nValueError: could not convert string to float: 'April'\n",
  "history_begin_time" : 1697346567314,
  "history_end_time" : 1697346579163,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "1qZwo68h6sP0",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        data['date'] = data['date'].dt.strftime('%j').astype(int)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-15\ntest start date:  2023-03-16\n/home/chetana\nget swe statistics\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\nMSE is 2.18864148919649\nR2 score is 0.9206054376324482\nRMSE is 1.479405789226367\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231510050833.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1697346408349,
  "history_end_time" : 1697346519132,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "f2YCIYv8e0Qm",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        data['date'] = data['date'].dt.strftime('%j').astype(int)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "Running",
  "history_begin_time" : 1697346323471,
  "history_end_time" : null,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Running"
},{
  "history_id" : "n81lvawxmmx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1697189923545,
  "history_end_time" : 1697189923545,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "SYyuaJuLNDks",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        data['date'] = data['date'].dt.strftime('%j').astype(int)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-13\ntest start date:  2023-03-16\ntest end date:  2023-10-13\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['lat', 'lon', 'air_temperature_tmmn', 'precipitation_amount',\n       'wind_speed', 'elevation', 'slope', 'curvature', 'aspect', 'eastness',\n       'northness'],\n      dtype='object')\ninput features and order:  Index(['lat', 'lon', 'air_temperature_tmmn', 'precipitation_amount',\n       'wind_speed', 'elevation', 'slope', 'curvature', 'aspect', 'eastness',\n       'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 5.2817950705082515\nMSE is 44.500127831774286\nR2 score is -0.6142745131825664\nRMSE is 6.670841613452855\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231310093755.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1697189804943,
  "history_end_time" : 1697189884464,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "0SMzZnQiGIBk",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        data['date'] = data['date'].dt.strftime('%j').astype(int)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-13\ntest start date:  2023-03-16\ntest end date:  2023-10-13\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['lat', 'lon', 'air_temperature_tmmn', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['lat', 'lon', 'air_temperature_tmmn', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 5.551871589561307\nMSE is 46.04209184332093\nR2 score is -0.6702103795579251\nRMSE is 6.785432325454358\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231310093628.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1697189703397,
  "history_end_time" : 1697189797193,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "vsgwdGeTqNBc",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=3,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        data['date'] = data['date'].dt.strftime('%j').astype(int)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        data = data.drop(['date', 'SWE', 'Flag'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-13\ntest start date:  2023-03-16\ntest end date:  2023-10-13\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['lat', 'lon', 'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['lat', 'lon', 'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 6.712385043778777\nMSE is 61.00606358369445\nR2 score is -1.2130393414833138\nRMSE is 7.810637847429264\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231310093054.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1697189363016,
  "history_end_time" : 1697189458799,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "vxc52ubft2y",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1697188523286,
  "history_end_time" : 1697188523286,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "yEOBGkg4q6bH",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        data['date'] = data['date'].dt.strftime('%j').astype(int)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        data = data.drop(['date', 'SWE', 'Flag'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-13\ntest start date:  2023-07-15\ntest end date:  2023-10-13\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['lat', 'lon', 'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['lat', 'lon', 'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 5.4137124957478955\nMSE is 40.392940592950566\nR2 score is -0.46528330790859607\nRMSE is 6.355544083156891\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231310091310.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1697188274867,
  "history_end_time" : 1697188404695,
  "history_notes" : "drop date swe flag",
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "pgbuctqezqd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1697187892251,
  "history_end_time" : 1697187892251,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "t5v88f8xmy1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1697187367956,
  "history_end_time" : 1697187367956,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "iuGRNX3wTpNx",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        data['date'] = data['date'].dt.strftime('%j').astype(int)\n        \n        \n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        data = data.drop(['date', 'lat', 'lon'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-13\ntest start date:  2023-09-16\ntest end date:  2023-10-13\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 5.1199242354959855\nMSE is 36.40607793350674\nR2 score is -0.3206569642938244\nRMSE is 6.033744934409039\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231310084310.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1697186476343,
  "history_end_time" : 1697186604633,
  "history_notes" : "drop date lat and lon, but R2 is -0.32",
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "XQY0jVYmRcJu",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        data['date'] = data['date'].dt.strftime('%j').astype(int)\n        \n        \n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        data = data.drop('date', axis=1)\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-13\ntest start date:  2023-09-16\ntest end date:  2023-10-13\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 5.062777971786551\nMSE is 36.16279019039616\nR2 score is -0.31183152440839823\nRMSE is 6.01355054775431\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231310082603.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1697185423061,
  "history_end_time" : 1697185578526,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "dflrj55i04a",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696863953215,
  "history_end_time" : 1696863953215,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "y81h47ikh5a",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696862402939,
  "history_end_time" : 1696862402939,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "zyxjbtdxg8w",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696832263662,
  "history_end_time" : 1696832263662,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "d6obxzxzsq4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696831867366,
  "history_end_time" : 1696831867366,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "m1v68kv1z6n",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696830174321,
  "history_end_time" : 1696830174321,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "IHKu4kBgvo0U",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        data['date'] = data['date'].dt.strftime('%j').astype(int)\n        \n        \n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        data = data.drop('date', axis=1)\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-09\ntest start date:  2023-02-10\ntest end date:  2023-10-09\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 5.062777971786551\nMSE is 36.16279019039616\nR2 score is -0.31183152440839823\nRMSE is 6.01355054775431\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1696826919077,
  "history_end_time" : 1696827053499,
  "history_notes" : "remove date from input wormhole_ ETHole_ 20230910045039.joblib",
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "rg261zqcmpy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696787541901,
  "history_end_time" : 1696787541901,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "yigrpm4yl31",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696786838184,
  "history_end_time" : 1696786838184,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "kK1jJ5DLAk7F",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        data['date'] = data['date'].dt.strftime('%j').astype(int)\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.8386623327054622\nMSE is 2.18864148919649\nR2 score is 0.9206054376324482\nRMSE is 1.479405789226367\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230810173220.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1696786235515,
  "history_end_time" : 1696786345801,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "manOEK7LHDg5",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        df['date'] = df['date'].dt.strftime('%j').astype(int)\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/manOEK7LHDg5/model_creation_et.py\", line 149, in <module>\n    hole.preprocessing()\n  File \"/home/chetana/gw-workspace/manOEK7LHDg5/model_creation_et.py\", line 95, in preprocessing\n    df['date'] = df['date'].dt.strftime('%j').astype(int)\nNameError: name 'df' is not defined\n",
  "history_begin_time" : 1696786216000,
  "history_end_time" : 1696786221218,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "vxFeDxtb9BaI",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.2418702240507661\nMSE is 0.1308238310622578\nR2 score is 0.9952542703472885\nRMSE is 0.3616957714188235\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230810172628.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1696785920176,
  "history_end_time" : 1696785989973,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "EVLYlRYUnlK1",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/testing_output/et-model-feature-importance-latest.png')\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.2418702240507661\nMSE is 0.1308238310622578\nR2 score is 0.9952542703472885\nRMSE is 0.3616957714188235\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230810172411.joblib\n",
  "history_begin_time" : 1696785786641,
  "history_end_time" : 1696785852836,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "axHOn5CgQbpD",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features\n        feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n        default_weight = 1.0\n\n        # Create an array of sample weights based on feature_weights\n        sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/testing_output/et-model-feature-importance-latest.png')\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n#hole.post_processing()\n",
  "history_output" : "today date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/axHOn5CgQbpD/model_creation_et.py\", line 145, in <module>\n    hole.train()\n  File \"/home/chetana/gw-workspace/axHOn5CgQbpD/model_creation_et.py\", line 121, in train\n    self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 352, in fit\n    sample_weight = _check_sample_weight(sample_weight, X)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1851, in _check_sample_weight\n    raise ValueError(\nValueError: sample_weight.shape == (19,), expected (806960,)!\n",
  "history_begin_time" : 1696784706722,
  "history_end_time" : 1696784711688,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "rqBGxRZ1IwOR",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features\n        feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n\n        # Create an array of sample weights based on feature_weights\n        sample_weights = np.array([feature_weights[feature] for feature in columns])\n        return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/testing_output/et-model-feature-importance-latest.png')\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n#hole.post_processing()\n",
  "history_output" : "today date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/rqBGxRZ1IwOR/model_creation_et.py\", line 143, in <module>\n    hole.preprocessing()\n  File \"/home/chetana/gw-workspace/rqBGxRZ1IwOR/model_creation_et.py\", line 111, in preprocessing\n    self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n  File \"/home/chetana/gw-workspace/rqBGxRZ1IwOR/model_creation_et.py\", line 78, in create_sample_weights\n    sample_weights = np.array([feature_weights[feature] for feature in columns])\n  File \"/home/chetana/gw-workspace/rqBGxRZ1IwOR/model_creation_et.py\", line 78, in <listcomp>\n    sample_weights = np.array([feature_weights[feature] for feature in columns])\nKeyError: 'lat'\n",
  "history_begin_time" : 1696784580005,
  "history_end_time" : 1696784584877,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "51WHRddf4VBx",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns=[]):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features\n        feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n\n        # Create an array of sample weights based on feature_weights\n        sample_weights = np.array([feature_weights[feature] for feature in columns])\n        return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/testing_output/et-model-feature-importance-latest.png')\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/51WHRddf4VBx/model_creation_et.py\", line 111\n    self.weights = self.create_sample_weights(y_train, scale_factor=30.0, X.columns)\n                                                                                   ^\nSyntaxError: positional argument follows keyword argument\n",
  "history_begin_time" : 1696784499922,
  "history_end_time" : 1696784499972,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "z4hy4LPbzGM6",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/testing_output/et-model-feature-importance-latest.png')\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.2418702240507661\nMSE is 0.1308238310622578\nR2 score is 0.9952542703472885\nRMSE is 0.3616957714188235\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230810165814.joblib\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/z4hy4LPbzGM6/model_creation_et.py\", line 141, in <module>\n    hole.post_processing()\n  File \"/home/chetana/gw-workspace/z4hy4LPbzGM6/model_creation_et.py\", line 120, in post_processing\n    feature_names = self.test_x.columns\nAttributeError: 'numpy.ndarray' object has no attribute 'columns'\n",
  "history_begin_time" : 1696784228244,
  "history_end_time" : 1696784295611,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "s6fivoz18uu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696771780875,
  "history_end_time" : 1696771780875,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "fm7x3xxjwzp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696602943931,
  "history_end_time" : 1696602943931,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "c0lqdaQM1lFT",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-10-06\ntest start date:  2023-01-18\ntest end date:  2023-10-06\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.2418702240507661\nMSE is 0.1308238310622578\nR2 score is 0.9952542703472885\nRMSE is 0.3616957714188235\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230610142049.joblib\n",
  "history_begin_time" : 1696601981398,
  "history_end_time" : 1696602050424,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "vqa8bl7n5MsU",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-10-06\ntest start date:  2023-01-18\ntest end date:  2023-10-06\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.2418702240507661\nMSE is 0.1308238310622578\nR2 score is 0.9952542703472885\nRMSE is 0.3616957714188235\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230610043724.joblib\n",
  "history_begin_time" : 1696566980416,
  "history_end_time" : 1696567045454,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Cw6XSX9G0roP",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    # Define a custom loss function that penalizes errors for values >10 more\n    def custom_loss(y_true, y_pred):\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split = 2,\n                                   min_samples_leaf = 1,\n                                   n_jobs=5\n                                  )\n\n    # Create sample weights based on target values\n    def create_sample_weights(self, y, scale_factor):\n        # Here, we'll use a linear scaling of weights based on target values\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n      \n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        #data.dropna(inplace=True)\n        \n#         for column in data.columns:\n#           data[column] = pd.to_numeric(data[column], errors='coerce')\n#           print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        \n    def train(self):\n    \tself.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-10-06\ntest start date:  2023-01-18\ntest end date:  2023-10-06\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.2418702240507661\nMSE is 0.1308238310622578\nR2 score is 0.9952542703472885\nRMSE is 0.3616957714188235\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230610043115.joblib\n",
  "history_begin_time" : 1696566602468,
  "history_end_time" : 1696566676744,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Sb3AmZigUsjd",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used to train and evaluate an Extra Trees Regression model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regression model for hole analysis.\n\nFunctions:\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    custom_loss(y_true, y_pred): Defines a custom loss function that penalizes errors for values greater than 10.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values.\n    preprocessing(): Performs data preprocessing, including reading and cleaning the training data.\n    train(): Trains the Extra Trees Regression model.\n    post_processing(): Generates a feature importance plot for the trained model.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n    \n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n        \n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        Defines a custom loss function that penalizes errors for values greater than 10.\n        \n        Args:\n            y_true (array-like): True target values.\n            y_pred (array-like): Predicted values.\n\n        Returns:\n            array-like: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def create_sample_weights(self, y, scale_factor):\n        \"\"\"\n        Creates sample weights based on target values.\n\n        Args:\n            y (array-like): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            array-like: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n\n    def preprocessing(self):\n        \"\"\"\n        Performs data preprocessing, including reading and cleaning the training data.\n        \"\"\"\n        # ... (The rest of the preprocessing code)\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regression model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Generates a feature importance plot for the trained model.\n        \"\"\"\n        # ... (Feature importance plotting code)\n\nif __name__ == \"__main__\":\n    hole = ETHole()\n    hole.preprocessing()\n    hole.train()\n    hole.test()\n    hole.evaluate()\n    hole.save()\n",
  "history_output" : "today date = 2023-10-06\ntest start date:  2023-01-18\ntest end date:  2023-10-06\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/Sb3AmZigUsjd/model_creation_et.py\", line 96, in <module>\n    hole.train()\n  File \"/home/chetana/gw-workspace/Sb3AmZigUsjd/model_creation_et.py\", line 85, in train\n    self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\nAttributeError: 'ETHole' object has no attribute 'weights'\n",
  "history_begin_time" : 1696566191001,
  "history_end_time" : 1696566198513,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "uedj6totfkn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696432484314,
  "history_end_time" : 1696432484314,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "efl8praw053",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696432299750,
  "history_end_time" : 1696432482232,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "fnkhes8u7m1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695827991081,
  "history_end_time" : 1695827991081,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "99jqmk8pwas",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695827889170,
  "history_end_time" : 1695827964214,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "0wb2mv0ju2q",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695827855637,
  "history_end_time" : 1695827867006,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ksynwlbrskg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695696616109,
  "history_end_time" : 1695696616109,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "p8kgbjxxtlt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695694257321,
  "history_end_time" : 1695694257321,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "e4zli9f8mgt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695693585740,
  "history_end_time" : 1695693585740,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "99nd0nw7x8j",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695693149359,
  "history_end_time" : 1695693149359,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "K881SY9UeMNq",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    # Define a custom loss function that penalizes errors for values >10 more\n    def custom_loss(y_true, y_pred):\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split = 2,\n                                   min_samples_leaf = 1,\n                                   n_jobs=5\n                                  )\n\n    # Create sample weights based on target values\n    def create_sample_weights(self, y, scale_factor):\n        # Here, we'll use a linear scaling of weights based on target values\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n      \n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        #data.dropna(inplace=True)\n        \n#         for column in data.columns:\n#           data[column] = pd.to_numeric(data[column], errors='coerce')\n#           print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        \n    def train(self):\n    \tself.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-24\ntest start date:  2022-01-16\ntest end date:  2023-09-24\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.2418702240507661\nMSE is 0.1308238310622578\nR2 score is 0.9952542703472885\nRMSE is 0.3616957714188235\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib\n",
  "history_begin_time" : 1695584738226,
  "history_end_time" : 1695584804213,
  "history_notes" : "continuous weights",
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "GmBc52mKQWw1",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    # Define a custom loss function that penalizes errors for values >10 more\n    def custom_loss(y_true, y_pred):\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split = 2,\n                                   min_samples_leaf = 1,\n                                   n_jobs=5\n                                  )\n\n    # Create sample weights based on target values\n    def create_sample_weights(y, scale_factor):\n        # Here, we'll use a linear scaling of weights based on target values\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n      \n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        #data.dropna(inplace=True)\n        \n#         for column in data.columns:\n#           data[column] = pd.to_numeric(data[column], errors='coerce')\n#           print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        \n    def train(self):\n    \tself.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-24\ntest start date:  2022-01-16\ntest end date:  2023-09-24\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/GmBc52mKQWw1/model_creation_et.py\", line 97, in <module>\n    hole.preprocessing()\n  File \"/home/chetana/gw-workspace/GmBc52mKQWw1/model_creation_et.py\", line 72, in preprocessing\n    self.weights = self.create_sample_weights(y_train, scale_factor=30.0)\nTypeError: create_sample_weights() got multiple values for argument 'scale_factor'\n",
  "history_begin_time" : 1695584709170,
  "history_end_time" : 1695584714057,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "pPhvEQHNr2uY",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    # Define a custom loss function that penalizes errors for values >10 more\n    def custom_loss(y_true, y_pred):\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split = 2,\n                                   min_samples_leaf = 1,\n                                   n_jobs=5\n                                  )\n\n    # Create sample weights based on target values\n    def create_sample_weights(y, scale_factor):\n        # Here, we'll use a linear scaling of weights based on target values\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n      \n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        #data.dropna(inplace=True)\n        \n#         for column in data.columns:\n#           data[column] = pd.to_numeric(data[column], errors='coerce')\n#           print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = create_sample_weights(y_train, scale_factor=30.0)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        \n    def train(self):\n    \tself.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-24\ntest start date:  2022-01-16\ntest end date:  2023-09-24\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/pPhvEQHNr2uY/model_creation_et.py\", line 97, in <module>\n    hole.preprocessing()\n  File \"/home/chetana/gw-workspace/pPhvEQHNr2uY/model_creation_et.py\", line 72, in preprocessing\n    self.weights = create_sample_weights(y_train, scale_factor=30.0)\nNameError: name 'create_sample_weights' is not defined\n",
  "history_begin_time" : 1695584671507,
  "history_end_time" : 1695584676278,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "9uK9jG4Kv5p9",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    # Define a custom loss function that penalizes errors for values >10 more\n    def custom_loss(y_true, y_pred):\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split = 2,\n                                   min_samples_leaf = 1,\n                                   n_jobs=5\n                                  )\n\n    # Create sample weights based on target values\n\tdef create_sample_weights(y, scale_factor):\n        # Here, we'll use a linear scaling of weights based on target values\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n      \n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        #data.dropna(inplace=True)\n        \n#         for column in data.columns:\n#           data[column] = pd.to_numeric(data[column], errors='coerce')\n#           print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = create_sample_weights(y_train, scale_factor=30.0)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        \n    def train(self):\n    \tself.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/9uK9jG4Kv5p9/model_creation_et.py\", line 33\n    def create_sample_weights(y, scale_factor):\nTabError: inconsistent use of tabs and spaces in indentation\n",
  "history_begin_time" : 1695584647798,
  "history_end_time" : 1695584647849,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "e88i8iluz95",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695580915840,
  "history_end_time" : 1695580915840,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0fx1MwweWvJY",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    # Define a custom loss function that penalizes errors for values >10 more\n    def custom_loss(y_true, y_pred):\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split = 2,\n                                   min_samples_leaf = 1,\n                                   n_jobs=5\n                                  )\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        #data.dropna(inplace=True)\n        \n#         for column in data.columns:\n#           data[column] = pd.to_numeric(data[column], errors='coerce')\n#           print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = np.where(y_train > 10, 2, 1) # Assign a weight of 2 for values >10, 1 otherwise\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        \n    def train(self):\n    \tself.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-24\ntest start date:  2022-10-16\ntest end date:  2023-09-24\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.026495932883911957\nMSE is 0.00951323829310003\nR2 score is 0.9996549003595576\nRMSE is 0.09753583081668003\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409183401.joblib\n",
  "history_begin_time" : 1695580296667,
  "history_end_time" : 1695580443099,
  "history_notes" : "first weighted model exciting",
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "IXyR50Xve2wz",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    # Define a custom loss function that penalizes errors for values >10 more\n    def custom_loss(y_true, y_pred):\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split = 2,\n                                   min_samples_leaf = 1,\n                                   n_jobs=5\n                                  )\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        #data.dropna(inplace=True)\n        \n#         for column in data.columns:\n#           data[column] = pd.to_numeric(data[column], errors='coerce')\n#           print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        weights = np.where(y_train > 10, 2, 1) # Assign a weight of 2 for values >10, 1 otherwise\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, sample_weight=weights)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-24\ntest start date:  2022-10-16\ntest end date:  2023-09-24\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/IXyR50Xve2wz/model_creation_et.py\", line 89, in <module>\n    hole.preprocessing()\n  File \"/home/chetana/gw-workspace/IXyR50Xve2wz/model_creation_et.py\", line 65, in preprocessing\n    weights = np.where(y_train > 10, 2, 1) # Assign a weight of 2 for values >10, 1 otherwise\nUnboundLocalError: local variable 'y_train' referenced before assignment\n",
  "history_begin_time" : 1695578209913,
  "history_end_time" : 1695578214419,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "4WnrVFRd7aqx",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    # Define a custom loss function that penalizes errors for values >10 more\n    def custom_loss(y_true, y_pred):\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split = 2,\n                                   min_samples_leaf = 1,\n                                   n_jobs=5\n                                  )\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        #data.dropna(inplace=True)\n        \n#         for column in data.columns:\n#           data[column] = pd.to_numeric(data[column], errors='coerce')\n#           print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,\n                                   criterion=self.custom_loss  # Use the custom loss function\n                                                           )\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-24\ntest start date:  2022-10-16\ntest end date:  2023-09-24\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/4WnrVFRd7aqx/model_creation_et.py\", line 89, in <module>\n    hole.preprocessing()\n  File \"/home/chetana/gw-workspace/4WnrVFRd7aqx/model_creation_et.py\", line 65, in preprocessing\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\", line 189, in wrapper\n    params = func_sig.bind(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/inspect.py\", line 3045, in bind\n    return self._bind(args, kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/inspect.py\", line 3034, in _bind\n    raise TypeError(\nTypeError: got an unexpected keyword argument 'criterion'\n",
  "history_begin_time" : 1695578130634,
  "history_end_time" : 1695578135221,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "I7rL698ObX2L",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    # Define a custom loss function that penalizes errors for values >10 more\n    def custom_loss(y_true, y_pred):\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split = 2,\n                                   min_samples_leaf = 1,\n                                   n_jobs=5,\n                                   criterion=self.custom_loss  # Use the custom loss function\n                                  )\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        #data.dropna(inplace=True)\n        \n#         for column in data.columns:\n#           data[column] = pd.to_numeric(data[column], errors='coerce')\n#           print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-24\ntest start date:  2022-10-16\ntest end date:  2023-09-24\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/I7rL698ObX2L/model_creation_et.py\", line 89, in <module>\n    hole.train()\n  File \"/home/chetana/gw-workspace/I7rL698ObX2L/base_hole.py\", line 47, in train\n    self.classifier.fit(self.train_x, self.train_y)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 1144, in wrapper\n    estimator._validate_params()\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 637, in _validate_params\n    validate_parameter_constraints(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'criterion' parameter of ExtraTreesRegressor must be a str among {'squared_error', 'poisson', 'absolute_error', 'friedman_mse'}. Got <bound method ETHole.custom_loss of <__main__.ETHole object at 0x7fa51bd1f970>> instead.\n",
  "history_begin_time" : 1695578054284,
  "history_end_time" : 1695578059133,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "kKYW1aG3rxLD",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    # Define a custom loss function that penalizes errors for values >10 more\n    def custom_loss(y_true, y_pred):\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split = 2,\n                                   min_samples_leaf = 1,\n                                   n_jobs=5,\n                                   criterion=self.custom_loss  # Use the custom loss function)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        #data.dropna(inplace=True)\n        \n#         for column in data.columns:\n#           data[column] = pd.to_numeric(data[column], errors='coerce')\n#           print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/kKYW1aG3rxLD/model_creation_et.py\", line 32\n    def preprocessing(self):\n    ^\nSyntaxError: invalid syntax\n",
  "history_begin_time" : 1695578043313,
  "history_end_time" : 1695578043362,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "YMVpuBus5IJn",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split = 2,\n                                   min_samples_leaf = 1,\n                                   n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        #data.dropna(inplace=True)\n        \n#         for column in data.columns:\n#           data[column] = pd.to_numeric(data[column], errors='coerce')\n#           print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-24\ntest start date:  2022-10-16\ntest end date:  2023-09-24\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.03996316546049505\nMSE is 0.01770245434222269\nR2 score is 0.9993578305893085\nRMSE is 0.1330505706196809\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409175058.joblib\n",
  "history_begin_time" : 1695577722326,
  "history_end_time" : 1695577860442,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "xNy9FfuRqxcH",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split = 2,\n                                   min_samples_leaf = 1,\n                                   n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        #data.dropna(inplace=True)\n        \n#         for column in data.columns:\n#           data[column] = pd.to_numeric(data[column], errors='coerce')\n#           print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        # data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "",
  "history_begin_time" : 1695577667666,
  "history_end_time" : 1695577681011,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "7fcSe5JOxed2",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split = 2,\n                                   min_samples_leaf = 1,\n                                   n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        #data.dropna(inplace=True)\n        \n#         for column in data.columns:\n#           data[column] = pd.to_numeric(data[column], errors='coerce')\n#           print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-24\ntest start date:  2022-10-16\ntest end date:  2023-09-24\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.03996316546049504\nMSE is 0.01770245434222269\nR2 score is 0.9993578305893085\nRMSE is 0.1330505706196809\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409173829.joblib\n",
  "history_begin_time" : 1695576970319,
  "history_end_time" : 1695577111965,
  "history_notes" : "train model with filled value -999, et hyper parameters are changed to 200 estimates",
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "lNzQ4e7EtxLO",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   max_features='auto',\n                                   random_state=42, \n                                   min_samples_split = 2,\n                                   min_samples_leaf = 1,\n                                   n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        #data.dropna(inplace=True)\n        \n#         for column in data.columns:\n#           data[column] = pd.to_numeric(data[column], errors='coerce')\n#           print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-24\ntest start date:  2022-10-16\ntest end date:  2023-09-24\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/lNzQ4e7EtxLO/model_creation_et.py\", line 82, in <module>\n    hole.train()\n  File \"/home/chetana/gw-workspace/lNzQ4e7EtxLO/base_hole.py\", line 47, in train\n    self.classifier.fit(self.train_x, self.train_y)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 1144, in wrapper\n    estimator._validate_params()\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 637, in _validate_params\n    validate_parameter_constraints(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of ExtraTreesRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
  "history_begin_time" : 1695576953790,
  "history_end_time" : 1695576958964,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "SZlLQu8dajwb",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   max_features='auto',\n                                   random_state=42, \n                                   min_samples_split = 2,\n                                   min_samples_leaf = 1,\n                                   n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        #data.dropna(inplace=True)\n        \n        for column in data.columns:\n          data[column] = pd.to_numeric(data[column], errors='coerce')\n          print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-24\ntest start date:  2022-10-16\ntest end date:  2023-09-24\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\ncount    1.022700e+06\nmean     4.419400e+04\nstd      4.217545e+02\nmin      4.346400e+04\n25%      4.382900e+04\n50%      4.419400e+04\n75%      4.455900e+04\nmax      4.492400e+04\nName: date, dtype: float64\ncount    1.022700e+06\nmean     4.165838e+01\nstd      3.632464e+00\nmin      3.335825e+01\n25%      3.891814e+01\n50%      4.107190e+01\n75%      4.461398e+01\nmax      4.897107e+01\nName: lat, dtype: float64\ncount    1.022700e+06\nmean    -1.141534e+02\nstd      5.423376e+00\nmin     -1.234486e+02\n25%     -1.197812e+02\n50%     -1.137737e+02\n75%     -1.100367e+02\nmax     -1.051948e+02\nName: lon, dtype: float64\ncount    1.022700e+06\nmean     5.046323e+01\nstd      9.547185e+01\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      3.500000e+01\nmax      2.550000e+02\nName: SWE, dtype: float64\ncount    1.022700e+06\nmean     2.434436e+02\nstd      5.313797e+00\nmin      2.410000e+02\n25%      2.410000e+02\n50%      2.410000e+02\n75%      2.410000e+02\nmax      2.550000e+02\nName: Flag, dtype: float64\ncount    1.022700e+06\nmean    -1.038741e+01\nstd      1.165852e+02\nmin     -9.990000e+02\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\ncount    1.022700e+06\nmean     5.113495e+05\nstd      2.952282e+05\nmin      0.000000e+00\n25%      2.556748e+05\n50%      5.113495e+05\n75%      7.670242e+05\nmax      1.022699e+06\nName: Unnamed: 0, dtype: float64\ncount    1.022700e+06\nmean     2.720187e+02\nstd      8.384982e+00\nmin      2.328000e+02\n25%      2.663000e+02\n50%      2.722000e+02\n75%      2.784000e+02\nmax      2.983000e+02\nName: air_temperature_tmmn, dtype: float64\ncount    1.022700e+06\nmean     3.943372e+00\nstd      2.478664e+00\nmin      0.000000e+00\n25%      1.900000e+00\n50%      3.500000e+00\n75%      5.800000e+00\nmax      1.510000e+01\nName: potential_evapotranspiration, dtype: float64\ncount    1.022700e+06\nmean     6.258550e-01\nstd      5.233133e-01\nmin      0.000000e+00\n25%      2.100000e-01\n50%      4.700000e-01\n75%      9.300000e-01\nmax      3.870000e+00\nName: mean_vapor_pressure_deficit, dtype: float64\ncount    1.022700e+06\nmean     7.123571e+01\nstd      2.030150e+01\nmin      8.200000e+00\n25%      5.560000e+01\n50%      7.240000e+01\n75%      8.820001e+01\nmax      1.000000e+02\nName: relative_humidity_rmax, dtype: float64\ncount    1.022700e+06\nmean     3.578010e+01\nstd      1.888091e+01\nmin      1.000000e+00\n25%      2.130000e+01\n50%      3.160000e+01\n75%      4.740000e+01\nmax      1.000000e+02\nName: relative_humidity_rmin, dtype: float64\ncount    1.022700e+06\nmean     2.689152e+00\nstd      7.456402e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      2.200000e+00\nmax      5.042000e+02\nName: precipitation_amount, dtype: float64\ncount    1.022700e+06\nmean     2.837481e+02\nstd      9.912702e+00\nmin      2.444000e+02\n25%      2.759000e+02\n50%      2.831000e+02\n75%      2.920000e+02\nmax      3.144000e+02\nName: air_temperature_tmmx, dtype: float64\ncount    1.022700e+06\nmean     4.214964e+00\nstd      2.043559e+00\nmin      5.000000e-01\n25%      2.800000e+00\n50%      3.700000e+00\n75%      5.200000e+00\nmax      1.860000e+01\nName: wind_speed, dtype: float64\ncount    1.022700e+06\nmean     2.397170e+03\nstd      6.647262e+02\nmin      7.584437e+02\n25%      1.948688e+03\n50%      2.481006e+03\n75%      2.895791e+03\nmax      3.823385e+03\nName: elevation, dtype: float64\ncount    1.022700e+06\nmean     8.998188e+01\nstd      2.818171e-02\nmin      8.939863e+01\n25%      8.998083e+01\n50%      8.998801e+01\n75%      8.999234e+01\nmax      8.999674e+01\nName: slope, dtype: float64\ncount    1.022700e+06\nmean    -3.909783e+03\nstd      4.385982e+03\nmin     -1.917768e+04\n25%     -6.479865e+03\n50%     -3.710969e+03\n75%     -5.754078e+02\nmax      9.069726e+03\nName: curvature, dtype: float64\ncount    1.022700e+06\nmean     1.722213e+02\nstd      1.031039e+02\nmin      4.159291e-01\n25%      9.217068e+01\n50%      1.691301e+02\n75%      2.498582e+02\nmax      3.586563e+02\nName: aspect, dtype: float64\ncount    1.022700e+06\nmean     6.965357e-02\nstd      5.171924e-01\nmin     -7.853438e-01\n25%     -3.907275e-01\n50%      1.334633e-01\n75%      5.767317e-01\nmax      7.853980e-01\nName: eastness, dtype: float64\ncount    1.022700e+06\nmean    -4.509867e-02\nstd      6.412035e-01\nmin     -7.853694e-01\n25%     -7.356204e-01\n50%     -1.913248e-01\n75%      6.495535e-01\nmax      7.853850e-01\nName: northness, dtype: float64\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/SZlLQu8dajwb/model_creation_et.py\", line 82, in <module>\n    hole.train()\n  File \"/home/chetana/gw-workspace/SZlLQu8dajwb/base_hole.py\", line 47, in train\n    self.classifier.fit(self.train_x, self.train_y)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 1144, in wrapper\n    estimator._validate_params()\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 637, in _validate_params\n    validate_parameter_constraints(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of ExtraTreesRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
  "history_begin_time" : 1695576932758,
  "history_end_time" : 1695576939620,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "saFXiljrZtWk",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   max_features='auto'\n                                   random_state=42, \n                                   min_samples_split = 2,\n                                   min_samples_leaf = 1,\n                                   n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        #data.dropna(inplace=True)\n        \n        for column in data.columns:\n          data[column] = pd.to_numeric(data[column], errors='coerce')\n          print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/saFXiljrZtWk/model_creation_et.py\", line 21\n    random_state=42, \n    ^\nSyntaxError: invalid syntax\n",
  "history_begin_time" : 1695576924525,
  "history_end_time" : 1695576924581,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "7y63qodifmo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695576291648,
  "history_end_time" : 1695576291648,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "334xzj03og9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695575931006,
  "history_end_time" : 1695575931006,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "entvyyon2fm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695535769205,
  "history_end_time" : 1695535769205,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vom430oimh9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695535478679,
  "history_end_time" : 1695535478679,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rmyzq7a5njs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695535214017,
  "history_end_time" : 1695535214017,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wcr6vv8vlmj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695534943581,
  "history_end_time" : 1695534943581,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "hdbp7vizsnt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695534671821,
  "history_end_time" : 1695534671821,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "im9qgpmuk2t",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695533024089,
  "history_end_time" : 1695533024089,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "088synr3ttf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695529187858,
  "history_end_time" : 1695529187858,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "92dtcihps8j",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695528505176,
  "history_end_time" : 1695528505176,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "iOWJ0XFoLPt4",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        \n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        #data.fillna(-999, inplace=True)\n        data.dropna(inplace=True)\n        \n        for column in data.columns:\n          data[column] = pd.to_numeric(data[column], errors='coerce')\n          print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-24\ntest start date:  2022-08-22\ntest end date:  2023-09-24\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\ncount    1.008700e+06\nmean     4.418437e+04\nstd      4.163835e+02\nmin      4.346400e+04\n25%      4.382400e+04\n50%      4.418400e+04\n75%      4.454500e+04\nmax      4.490500e+04\nName: date, dtype: float64\ncount    1.008700e+06\nmean     4.165838e+01\nstd      3.632464e+00\nmin      3.335825e+01\n25%      3.891814e+01\n50%      4.107190e+01\n75%      4.461398e+01\nmax      4.897107e+01\nName: lat, dtype: float64\ncount    1.008700e+06\nmean    -1.141534e+02\nstd      5.423377e+00\nmin     -1.234486e+02\n25%     -1.197812e+02\n50%     -1.137737e+02\n75%     -1.100367e+02\nmax     -1.051948e+02\nName: lon, dtype: float64\ncount    1.008700e+06\nmean     5.049100e+01\nstd      9.551420e+01\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      3.600000e+01\nmax      2.550000e+02\nName: SWE, dtype: float64\ncount    1.008700e+06\nmean     2.434455e+02\nstd      5.315359e+00\nmin      2.410000e+02\n25%      2.410000e+02\n50%      2.410000e+02\n75%      2.410000e+02\nmax      2.550000e+02\nName: Flag, dtype: float64\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\ncount    1.008700e+06\nmean     5.164940e+05\nstd      2.937986e+05\nmin      0.000000e+00\n25%      2.655018e+05\n50%      5.183495e+05\n75%      7.705242e+05\nmax      1.022699e+06\nName: Unnamed: 0, dtype: float64\ncount    1.008700e+06\nmean     2.721254e+02\nstd      8.333993e+00\nmin      2.380000e+02\n25%      2.664000e+02\n50%      2.723000e+02\n75%      2.785000e+02\nmax      2.983000e+02\nName: air_temperature_tmmn, dtype: float64\ncount    1.008700e+06\nmean     3.937009e+00\nstd      2.476735e+00\nmin      0.000000e+00\n25%      1.800000e+00\n50%      3.500000e+00\n75%      5.800000e+00\nmax      1.510000e+01\nName: potential_evapotranspiration, dtype: float64\ncount    1.008700e+06\nmean     6.318230e-01\nstd      5.238681e-01\nmin      0.000000e+00\n25%      2.100000e-01\n50%      4.800000e-01\n75%      9.400000e-01\nmax      3.870000e+00\nName: mean_vapor_pressure_deficit, dtype: float64\ncount    1.008700e+06\nmean     7.107944e+01\nstd      2.027642e+01\nmin      8.200000e+00\n25%      5.540000e+01\n50%      7.220001e+01\n75%      8.800000e+01\nmax      1.000000e+02\nName: relative_humidity_rmax, dtype: float64\ncount    1.008700e+06\nmean     3.555765e+01\nstd      1.877470e+01\nmin      1.000000e+00\n25%      2.120000e+01\n50%      3.150000e+01\n75%      4.690000e+01\nmax      1.000000e+02\nName: relative_humidity_rmin, dtype: float64\ncount    1.008700e+06\nmean     2.606782e+00\nstd      7.298533e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      2.100000e+00\nmax      5.042000e+02\nName: precipitation_amount, dtype: float64\ncount    1.008700e+06\nmean     2.838897e+02\nstd      9.873665e+00\nmin      2.444000e+02\n25%      2.760000e+02\n50%      2.833000e+02\n75%      2.921000e+02\nmax      3.144000e+02\nName: air_temperature_tmmx, dtype: float64\ncount    1.008700e+06\nmean     4.192890e+00\nstd      2.023231e+00\nmin      5.000000e-01\n25%      2.700000e+00\n50%      3.700000e+00\n75%      5.200000e+00\nmax      1.860000e+01\nName: wind_speed, dtype: float64\ncount    1.008700e+06\nmean     2.397170e+03\nstd      6.647262e+02\nmin      7.584437e+02\n25%      1.948688e+03\n50%      2.481006e+03\n75%      2.895791e+03\nmax      3.823385e+03\nName: elevation, dtype: float64\ncount    1.008700e+06\nmean     8.998188e+01\nstd      2.818171e-02\nmin      8.939863e+01\n25%      8.998083e+01\n50%      8.998801e+01\n75%      8.999234e+01\nmax      8.999674e+01\nName: slope, dtype: float64\ncount    1.008700e+06\nmean    -3.909783e+03\nstd      4.385982e+03\nmin     -1.917768e+04\n25%     -6.479865e+03\n50%     -3.710969e+03\n75%     -5.754078e+02\nmax      9.069726e+03\nName: curvature, dtype: float64\ncount    1.008700e+06\nmean     1.722213e+02\nstd      1.031039e+02\nmin      4.159291e-01\n25%      9.217068e+01\n50%      1.691301e+02\n75%      2.498582e+02\nmax      3.586563e+02\nName: aspect, dtype: float64\ncount    1.008700e+06\nmean     6.965357e-02\nstd      5.171924e-01\nmin     -7.853438e-01\n25%     -3.907275e-01\n50%      1.334633e-01\n75%      5.767317e-01\nmax      7.853980e-01\nName: eastness, dtype: float64\ncount    1.008700e+06\nmean    -4.509867e-02\nstd      6.412035e-01\nmin     -7.853694e-01\n25%     -7.356204e-01\n50%     -1.913248e-01\n75%      6.495535e-01\nmax      7.853850e-01\nName: northness, dtype: float64\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.04118521859819647\nMSE is 0.01923175954198464\nR2 score is 0.9993023539305406\nRMSE is 0.138678619628206\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409005655.joblib\n",
  "history_begin_time" : 1695516939134,
  "history_end_time" : 1695517016699,
  "history_notes" : "this is a good run with single time input",
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "o3zxlj2ipb2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695515862387,
  "history_end_time" : 1695515862387,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "OcbdyL9bmuhQ",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        \n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        \n        \n        for column in data.columns:\n          data[column] = pd.to_numeric(data[column], errors='coerce')\n          print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-23\ntest start date:  2022-08-22\ntest end date:  2023-09-23\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\ncount    1.022700e+06\nmean     4.419400e+04\nstd      4.217545e+02\nmin      4.346400e+04\n25%      4.382900e+04\n50%      4.419400e+04\n75%      4.455900e+04\nmax      4.492400e+04\nName: date, dtype: float64\ncount    1.022700e+06\nmean     4.165838e+01\nstd      3.632464e+00\nmin      3.335825e+01\n25%      3.891814e+01\n50%      4.107190e+01\n75%      4.461398e+01\nmax      4.897107e+01\nName: lat, dtype: float64\ncount    1.022700e+06\nmean    -1.141534e+02\nstd      5.423376e+00\nmin     -1.234486e+02\n25%     -1.197812e+02\n50%     -1.137737e+02\n75%     -1.100367e+02\nmax     -1.051948e+02\nName: lon, dtype: float64\ncount    1.022700e+06\nmean     5.046323e+01\nstd      9.547185e+01\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      3.500000e+01\nmax      2.550000e+02\nName: SWE, dtype: float64\ncount    1.022700e+06\nmean     2.434436e+02\nstd      5.313797e+00\nmin      2.410000e+02\n25%      2.410000e+02\n50%      2.410000e+02\n75%      2.410000e+02\nmax      2.550000e+02\nName: Flag, dtype: float64\ncount    1.022700e+06\nmean    -1.038741e+01\nstd      1.165852e+02\nmin     -9.990000e+02\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\ncount    1.022700e+06\nmean     5.113495e+05\nstd      2.952282e+05\nmin      0.000000e+00\n25%      2.556748e+05\n50%      5.113495e+05\n75%      7.670242e+05\nmax      1.022699e+06\nName: Unnamed: 0, dtype: float64\ncount    1.022700e+06\nmean     2.720187e+02\nstd      8.384982e+00\nmin      2.328000e+02\n25%      2.663000e+02\n50%      2.722000e+02\n75%      2.784000e+02\nmax      2.983000e+02\nName: air_temperature_tmmn, dtype: float64\ncount    1.022700e+06\nmean     3.943372e+00\nstd      2.478664e+00\nmin      0.000000e+00\n25%      1.900000e+00\n50%      3.500000e+00\n75%      5.800000e+00\nmax      1.510000e+01\nName: potential_evapotranspiration, dtype: float64\ncount    1.022700e+06\nmean     6.258550e-01\nstd      5.233133e-01\nmin      0.000000e+00\n25%      2.100000e-01\n50%      4.700000e-01\n75%      9.300000e-01\nmax      3.870000e+00\nName: mean_vapor_pressure_deficit, dtype: float64\ncount    1.022700e+06\nmean     7.123571e+01\nstd      2.030150e+01\nmin      8.200000e+00\n25%      5.560000e+01\n50%      7.240000e+01\n75%      8.820001e+01\nmax      1.000000e+02\nName: relative_humidity_rmax, dtype: float64\ncount    1.022700e+06\nmean     3.578010e+01\nstd      1.888091e+01\nmin      1.000000e+00\n25%      2.130000e+01\n50%      3.160000e+01\n75%      4.740000e+01\nmax      1.000000e+02\nName: relative_humidity_rmin, dtype: float64\ncount    1.022700e+06\nmean     2.689152e+00\nstd      7.456402e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      2.200000e+00\nmax      5.042000e+02\nName: precipitation_amount, dtype: float64\ncount    1.022700e+06\nmean     2.837481e+02\nstd      9.912702e+00\nmin      2.444000e+02\n25%      2.759000e+02\n50%      2.831000e+02\n75%      2.920000e+02\nmax      3.144000e+02\nName: air_temperature_tmmx, dtype: float64\ncount    1.022700e+06\nmean     4.214964e+00\nstd      2.043559e+00\nmin      5.000000e-01\n25%      2.800000e+00\n50%      3.700000e+00\n75%      5.200000e+00\nmax      1.860000e+01\nName: wind_speed, dtype: float64\ncount    1.022700e+06\nmean     2.397170e+03\nstd      6.647262e+02\nmin      7.584437e+02\n25%      1.948688e+03\n50%      2.481006e+03\n75%      2.895791e+03\nmax      3.823385e+03\nName: elevation, dtype: float64\ncount    1.022700e+06\nmean     8.998188e+01\nstd      2.818171e-02\nmin      8.939863e+01\n25%      8.998083e+01\n50%      8.998801e+01\n75%      8.999234e+01\nmax      8.999674e+01\nName: slope, dtype: float64\ncount    1.022700e+06\nmean    -3.909783e+03\nstd      4.385982e+03\nmin     -1.917768e+04\n25%     -6.479865e+03\n50%     -3.710969e+03\n75%     -5.754078e+02\nmax      9.069726e+03\nName: curvature, dtype: float64\ncount    1.022700e+06\nmean     1.722213e+02\nstd      1.031039e+02\nmin      4.159291e-01\n25%      9.217068e+01\n50%      1.691301e+02\n75%      2.498582e+02\nmax      3.586563e+02\nName: aspect, dtype: float64\ncount    1.022700e+06\nmean     6.965357e-02\nstd      5.171924e-01\nmin     -7.853438e-01\n25%     -3.907275e-01\n50%      1.334633e-01\n75%      5.767317e-01\nmax      7.853980e-01\nName: eastness, dtype: float64\ncount    1.022700e+06\nmean    -4.509867e-02\nstd      6.412035e-01\nmin     -7.853694e-01\n25%     -7.356204e-01\n50%     -1.913248e-01\n75%      6.495535e-01\nmax      7.853850e-01\nName: northness, dtype: float64\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.04118521859819647\nMSE is 0.019231759541984646\nR2 score is 0.9993023539305406\nRMSE is 0.13867861962820602\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232309230152.joblib\n",
  "history_begin_time" : 1695510037968,
  "history_end_time" : 1695510114132,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "OJB1gOLpNHvG",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        \n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        \n        \n        for column in df.columns:\n          data[column] = pd.to_numeric(data[column], errors='coerce')\n          print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-23\ntest start date:  2022-08-22\ntest end date:  2023-09-23\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/OJB1gOLpNHvG/model_creation_et.py\", line 78, in <module>\n    hole.preprocessing()\n  File \"/home/chetana/gw-workspace/OJB1gOLpNHvG/model_creation_et.py\", line 36, in preprocessing\n    for column in df.columns:\nNameError: name 'df' is not defined\n",
  "history_begin_time" : 1695509883508,
  "history_end_time" : 1695509887821,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "NZW00kwn82tk",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        \n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        \n        \n        for column in df.columns:\n          data[column] = pd.to_numeric(data[column], errors='coerce')\n\t\t  print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/NZW00kwn82tk/model_creation_et.py\", line 38\n    print(data[column].describe())\nTabError: inconsistent use of tabs and spaces in indentation\n",
  "history_begin_time" : 1695509871186,
  "history_end_time" : 1695509871238,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "nRq1unqaokFS",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-23\ntest start date:  2022-08-22\ntest end date:  2023-09-23\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.04118521859819647\nMSE is 0.019231759541984646\nR2 score is 0.9993023539305406\nRMSE is 0.13867861962820602\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232309222136.joblib\n",
  "history_begin_time" : 1695507621632,
  "history_end_time" : 1695507697311,
  "history_notes" : "filter out all the rows with snotel values are -999",
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "g31ZYj3L6VHU",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/g31ZYj3L6VHU/model_creation_et.py\", line 35\n    print(\"get swe statistics\")\n    ^\nSyntaxError: invalid syntax\n",
  "history_begin_time" : 1695507608820,
  "history_end_time" : 1695507608873,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "q4NMFmJmxJV2",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-23\ntest start date:  2022-08-22\ntest end date:  2023-09-23\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.022700e+06\nmean    -1.038741e+01\nstd      1.165852e+02\nmin     -9.990000e+02\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.16053394446074246\nMSE is 12.979812945726998\nR2 score is 0.9990612195729837\nRMSE is 3.6027507470996376\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232309221830.joblib\n",
  "history_begin_time" : 1695507442850,
  "history_end_time" : 1695507511043,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "rQMkWCTFD3ef",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        print(\"get swe statistics\")\n        data[\"swe_value\"].describe()\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-23\ntest start date:  2022-08-22\ntest end date:  2023-09-23\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.16053394446074246\nMSE is 12.979812945726998\nR2 score is 0.9990612195729837\nRMSE is 3.6027507470996376\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232309221708.joblib\n",
  "history_begin_time" : 1695507361018,
  "history_end_time" : 1695507429955,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "zT13CRVkykEQ",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data[\"swe_value\"].describe()\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-23\ntest start date:  2022-08-22\ntest end date:  2023-09-23\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.16053394446074246\nMSE is 12.979812945726998\nR2 score is 0.9990612195729837\nRMSE is 3.6027507470996376\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232309221403.joblib\n",
  "history_begin_time" : 1695507177204,
  "history_end_time" : 1695507244480,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "cehkpk8gmm2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695506423837,
  "history_end_time" : 1695506423837,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "nDlh0wcDHzOB",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-23\ntest start date:  2022-08-21\ntest end date:  2023-09-23\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.16053394446074246\nMSE is 12.979812945726998\nR2 score is 0.9990612195729837\nRMSE is 3.6027507470996376\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232309213724.joblib\n",
  "history_begin_time" : 1695504978037,
  "history_end_time" : 1695505045071,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ZVRHEHpWD8gw",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-1, inplace=True)\n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-23\ntest start date:  2022-08-21\ntest end date:  2023-09-23\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.03204651901828608\nMSE is 0.012338813175906847\nR2 score is 0.9995505484963834\nRMSE is 0.11108021055033541\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232309161426.joblib\n",
  "history_begin_time" : 1695485595285,
  "history_end_time" : 1695485667151,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "WdjAbq1VfvzF",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-1, inplace=True)\n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        print(\"output feature: \", y.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-23\ntest start date:  2022-08-21\ntest end date:  2023-09-23\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/WdjAbq1VfvzF/model_creation_et.py\", line 64, in <module>\n    hole.preprocessing()\n  File \"/home/chetana/gw-workspace/WdjAbq1VfvzF/model_creation_et.py\", line 40, in preprocessing\n    print(\"output feature: \", y.columns)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\", line 5902, in __getattr__\n    return object.__getattribute__(self, name)\nAttributeError: 'Series' object has no attribute 'columns'\n",
  "history_begin_time" : 1695485570009,
  "history_end_time" : 1695485575241,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "1p61q591oys",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695418741332,
  "history_end_time" : 1695418741332,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "76J4jahYIYCp",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-1, inplace=True)\n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-22\ntest start date:  2022-03-21\ntest end date:  2023-09-22\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.03204651901828608\nMSE is 0.012338813175906847\nR2 score is 0.9995505484963834\nRMSE is 0.11108021055033541\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232209213743.joblib\n",
  "history_begin_time" : 1695418592123,
  "history_end_time" : 1695418664191,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "6vltonehnnu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695417619670,
  "history_end_time" : 1695417619670,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "mq68zgagfl1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695417171272,
  "history_end_time" : 1695417171272,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ejhkftcb2kw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695417052726,
  "history_end_time" : 1695417052726,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8tv9prsrjqa",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695416916014,
  "history_end_time" : 1695416916014,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "RqcaX3dt8P2y",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-1, inplace=True)\n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-20\ntest start date:  2022-03-19\ntest end date:  2023-09-20\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.032046519018286077\nMSE is 0.012338813175906847\nR2 score is 0.9995505484963834\nRMSE is 0.11108021055033541\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232009042228.joblib\n",
  "history_begin_time" : 1695183676880,
  "history_end_time" : 1695183749089,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "qHjYM16c9SeC",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-1, inplace=True)\n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-20\ntest start date:  2022-03-19\ntest end date:  2023-09-20\n/home/chetana\ndownloading http://www.northwestknowledge.net/metdata/data/tmmn_2019.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/tmmn_2019.nc\ndownloading http://www.northwestknowledge.net/metdata/data/tmmn_2020.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/tmmn_2020.nc\ndownloading http://www.northwestknowledge.net/metdata/data/tmmn_2021.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/tmmn_2021.nc\ndownloading http://www.northwestknowledge.net/metdata/data/tmmn_2022.nc\ndownloading http://www.northwestknowledge.net/metdata/data/tmmx_2019.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/tmmx_2019.nc\ndownloading http://www.northwestknowledge.net/metdata/data/tmmx_2020.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/tmmx_2020.nc\ndownloading http://www.northwestknowledge.net/metdata/data/tmmx_2021.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/tmmx_2021.nc\ndownloading http://www.northwestknowledge.net/metdata/data/tmmx_2022.nc\ndownloading http://www.northwestknowledge.net/metdata/data/pr_2019.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/pr_2019.nc\ndownloading http://www.northwestknowledge.net/metdata/data/pr_2020.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/pr_2020.nc\ndownloading http://www.northwestknowledge.net/metdata/data/pr_2021.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/pr_2021.nc\ndownloading http://www.northwestknowledge.net/metdata/data/pr_2022.nc\ndownloading http://www.northwestknowledge.net/metdata/data/vpd_2019.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/vpd_2019.nc\ndownloading http://www.northwestknowledge.net/metdata/data/vpd_2020.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/vpd_2020.nc\ndownloading http://www.northwestknowledge.net/metdata/data/vpd_2021.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/vpd_2021.nc\ndownloading http://www.northwestknowledge.net/metdata/data/vpd_2022.nc\ndownloading http://www.northwestknowledge.net/metdata/data/etr_2019.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/etr_2019.nc\ndownloading http://www.northwestknowledge.net/metdata/data/etr_2020.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/etr_2020.nc\ndownloading http://www.northwestknowledge.net/metdata/data/etr_2021.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/etr_2021.nc\ndownloading http://www.northwestknowledge.net/metdata/data/etr_2022.nc\ndownloading http://www.northwestknowledge.net/metdata/data/rmax_2019.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/rmax_2019.nc\ndownloading http://www.northwestknowledge.net/metdata/data/rmax_2020.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/rmax_2020.nc\ndownloading http://www.northwestknowledge.net/metdata/data/rmax_2021.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/rmax_2021.nc\ndownloading http://www.northwestknowledge.net/metdata/data/rmax_2022.nc\ndownloading http://www.northwestknowledge.net/metdata/data/rmin_2019.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/rmin_2019.nc\ndownloading http://www.northwestknowledge.net/metdata/data/rmin_2020.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/rmin_2020.nc\ndownloading http://www.northwestknowledge.net/metdata/data/rmin_2021.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/rmin_2021.nc\ndownloading http://www.northwestknowledge.net/metdata/data/rmin_2022.nc\ndownloading http://www.northwestknowledge.net/metdata/data/vs_2019.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/vs_2019.nc\ndownloading http://www.northwestknowledge.net/metdata/data/vs_2020.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/vs_2020.nc\ndownloading http://www.northwestknowledge.net/metdata/data/vs_2021.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/vs_2021.nc\ndownloading http://www.northwestknowledge.net/metdata/data/vs_2022.nc\nreading values from /home/chetana/gridmet_test_run/gridmet_climatology/rmin_2020.nc\n['relative_humidity']\ngot result_df :           day        lat         lon  relative_humidity\n0 2020-01-01  41.993149 -120.178715          68.400002\n1 2020-01-02  41.993149 -120.178715          55.700001\n2 2020-01-03  41.993149 -120.178715          30.300001\n3 2020-01-04  41.993149 -120.178715          59.600002\n4 2020-01-05  41.993149 -120.178715          74.300003\ncompleted extracting data for /home/chetana/gridmet_test_run/gridmet_climatology/rmin_2020.nc\nreading values from /home/chetana/gridmet_test_run/gridmet_climatology/rmin_2021.nc\n['relative_humidity']\ngot result_df :           day        lat         lon  relative_humidity\n0 2021-01-01  41.993149 -120.178715          39.000000\n1 2021-01-02  41.993149 -120.178715          74.800003\n2 2021-01-03  41.993149 -120.178715          70.400002\n3 2021-01-04  41.993149 -120.178715          65.599998\n4 2021-01-05  41.993149 -120.178715          51.299999\ncompleted extracting data for /home/chetana/gridmet_test_run/gridmet_climatology/rmin_2021.nc\nreading values from /home/chetana/gridmet_test_run/gridmet_climatology/tmmx_2019.nc\n['air_temperature']\ngot result_df :           day        lat         lon  air_temperature\n0 2019-01-01  41.993149 -120.178715       271.299988\n1 2019-01-02  41.993149 -120.178715       276.100006\n2 2019-01-03  41.993149 -120.178715       274.399994\n3 2019-01-04  41.993149 -120.178715       277.600006\n4 2019-01-05  41.993149 -120.178715       273.000000\ncompleted extracting data for /home/chetana/gridmet_test_run/gridmet_climatology/tmmx_2019.nc\nreading values from /home/chetana/gridmet_test_run/gridmet_climatology/vpd_2021.nc\n['mean_vapor_pressure_deficit']\ngot result_df :           day        lat         lon  mean_vapor_pressure_deficit\n0 2021-01-01  41.993149 -120.178715                         0.25\n1 2021-01-02  41.993149 -120.178715                         0.10\n2 2021-01-03  41.993149 -120.178715                         0.14\n3 2021-01-04  41.993149 -120.178715                         0.12\n4 2021-01-05  41.993149 -120.178715                         0.17\ncompleted extracting data for /home/chetana/gridmet_test_run/gridmet_climatology/vpd_2021.nc\nreading values from /home/chetana/gridmet_test_run/gridmet_climatology/tmmx_2021.nc\n['air_temperature']\ngot result_df :           day        lat         lon  air_temperature\n0 2021-01-01  41.993149 -120.178715       275.299988\n1 2021-01-02  41.993149 -120.178715       270.299988\n2 2021-01-03  41.993149 -120.178715       272.000000\n3 2021-01-04  41.993149 -120.178715       273.100006\n4 2021-01-05  41.993149 -120.178715       271.299988\ncompleted extracting data for /home/chetana/gridmet_test_run/gridmet_climatology/tmmx_2021.nc\nreading values from /home/chetana/gridmet_test_run/gridmet_climatology/tmmn_2021.nc\n['air_temperature']\ngot result_df :           day        lat         lon  air_temperature\n\nStream closed",
  "history_begin_time" : 1695181505166,
  "history_end_time" : 1695181862957,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "3lii5cmbrsp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695106488972,
  "history_end_time" : 1695106488972,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wua7sjfad6u",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695106316193,
  "history_end_time" : 1695106316193,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "P0Df13PW3ECJ",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.fillna(-1, inplace=True)\n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-19\ntest start date:  2022-11-08\ntest end date:  2023-09-19\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.03204651901828608\nMSE is 0.012338813175906847\nR2 score is 0.9995505484963834\nRMSE is 0.11108021055033541\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231909064713.joblib\n",
  "history_begin_time" : 1695105962305,
  "history_end_time" : 1695106034703,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "AWi7EyZNrRPC",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path, index=False)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.fillna(-1, inplace=True)\n        #data = data.dropna(subset=['swe_value'])\n        #data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-19\ntest start date:  2022-11-08\ntest end date:  2023-09-19\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/AWi7EyZNrRPC/model_creation_et.py\", line 60, in <module>\n    hole.preprocessing()\n  File \"/home/chetana/gw-workspace/AWi7EyZNrRPC/model_creation_et.py\", line 23, in preprocessing\n    data = pd.read_csv(training_data_path, index=False)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 211, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\nTypeError: read_csv() got an unexpected keyword argument 'index'\n",
  "history_begin_time" : 1695105938017,
  "history_end_time" : 1695105941435,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "t7x6j2oe4gg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695054045020,
  "history_end_time" : 1695054045020,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "oa76rrum9qe",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695054019753,
  "history_end_time" : 1695054032321,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "kwvy6u7x98t",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695053979884,
  "history_end_time" : 1695054019272,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "jubu7gk4fiy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695053793399,
  "history_end_time" : 1695053793399,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "bhbmmsvk33k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695053733401,
  "history_end_time" : 1695053733401,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "j6dq4311j4v",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694971144816,
  "history_end_time" : 1694972839686,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "kw1wdamj6c7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970707918,
  "history_end_time" : 1694970707918,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rgafsfd0xef",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594755,
  "history_end_time" : 1694970594755,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "p7aevsltpml",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970131540,
  "history_end_time" : 1694970131540,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "lbb12p0cjoo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694969350047,
  "history_end_time" : 1694969350047,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "g2iY6d7LVeBd",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-17\ntest start date:  2023-06-05\ntest end date:  2023-09-17\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.04118521859819645\nMSE is 0.01923175954198464\nR2 score is 0.9993023539305406\nRMSE is 0.138678619628206\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231709164339.joblib\n",
  "history_begin_time" : 1694968944152,
  "history_end_time" : 1694969020522,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "5r2MpZFJV2mo",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/5r2MpZFJV2mo/model_creation_et.py\", line 10, in <module>\n    from model_creation_rf import RandomForestHole\n  File \"/home/chetana/gw-workspace/5r2MpZFJV2mo/model_creation_rf.py\", line 4, in <module>\n    import seaborn as sns\nModuleNotFoundError: No module named 'seaborn'\n",
  "history_begin_time" : 1694968924075,
  "history_end_time" : 1694968925498,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "t4sjhnV98O2k",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/t4sjhnV98O2k/model_creation_et.py\", line 3, in <module>\n    import sklearn\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/__init__.py\", line 83, in <module>\n    from .base import clone\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 19, in <module>\n    from .utils import _IS_32BIT\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/__init__.py\", line 19, in <module>\n    from . import _joblib, metadata_routing\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/_joblib.py\", line 8, in <module>\n    from joblib import (\nImportError: cannot import name 'Memory' from 'joblib' (unknown location)\n",
  "history_begin_time" : 1694968314546,
  "history_end_time" : 1694968315106,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "HSwESbXlH0YB",
  "history_input" : "import pandas as pd\nimport joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/HSwESbXlH0YB/model_creation_et.py\", line 3, in <module>\n    from sklearn.model_selection import train_test_split\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/__init__.py\", line 82, in <module>\n    from .base import clone\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 17, in <module>\n    from .utils import _IS_32BIT\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/__init__.py\", line 24, in <module>\n    from . import _joblib\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/_joblib.py\", line 8, in <module>\n    from joblib import logger\nImportError: cannot import name 'logger' from 'joblib' (unknown location)\n",
  "history_begin_time" : 1694968197183,
  "history_end_time" : 1694968198209,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "qcjvMJHXphni",
  "history_input" : "import pandas as pd\nimport joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/qcjvMJHXphni/model_creation_et.py\", line 3, in <module>\n    from sklearn.model_selection import train_test_split\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/__init__.py\", line 82, in <module>\n    from .base import clone\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 17, in <module>\n    from .utils import _IS_32BIT\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/__init__.py\", line 21, in <module>\n    from . import _joblib\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/_joblib.py\", line 8, in <module>\n    from joblib import logger\nImportError: cannot import name 'logger' from 'joblib' (unknown location)\n",
  "history_begin_time" : 1694968168274,
  "history_end_time" : 1694968168862,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "PXvxxAukg96a",
  "history_input" : "import pandas as pd\nimport joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/PXvxxAukg96a/model_creation_et.py\", line 3, in <module>\n    from sklearn.model_selection import train_test_split\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/__init__.py\", line 83, in <module>\n    from .base import clone\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 19, in <module>\n    from .utils import _IS_32BIT\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/__init__.py\", line 19, in <module>\n    from . import _joblib, metadata_routing\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/_joblib.py\", line 8, in <module>\n    from joblib import (\nImportError: cannot import name 'Memory' from 'joblib' (unknown location)\n",
  "history_begin_time" : 1694967862951,
  "history_end_time" : 1694967863495,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "ywvUzqzJyDbs",
  "history_input" : "import pandas as pd\nimport joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/ywvUzqzJyDbs/model_creation_et.py\", line 3, in <module>\n    from sklearn.model_selection import train_test_split\nModuleNotFoundError: No module named 'sklearn'\n",
  "history_begin_time" : 1694967838508,
  "history_end_time" : 1694967838953,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "Xqd1x3zMb3S1",
  "history_input" : "import pandas as pd\nimport joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/Xqd1x3zMb3S1/model_creation_et.py\", line 3, in <module>\n    from sklearn.model_selection import train_test_split\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/__init__.py\", line 83, in <module>\n    from .base import clone\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 19, in <module>\n    from .utils import _IS_32BIT\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/__init__.py\", line 19, in <module>\n    from . import _joblib, metadata_routing\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/_joblib.py\", line 8, in <module>\n    from joblib import (\nImportError: cannot import name 'Memory' from 'joblib' (unknown location)\n",
  "history_begin_time" : 1694967807802,
  "history_end_time" : 1694967808407,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "ST2ZkEFB7NuM",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/ST2ZkEFB7NuM/model_creation_et.py\", line 2, in <module>\n    from sklearn.model_selection import train_test_split\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/__init__.py\", line 83, in <module>\n    from .base import clone\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 19, in <module>\n    from .utils import _IS_32BIT\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/__init__.py\", line 19, in <module>\n    from . import _joblib, metadata_routing\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/_joblib.py\", line 8, in <module>\n    from joblib import (\nImportError: cannot import name 'Memory' from 'joblib' (unknown location)\n",
  "history_begin_time" : 1694967314258,
  "history_end_time" : 1694967314842,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "JwAYVOM0ojNv",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        data = pd.read_csv(f'{working_dir}/final_merged_data_3yrs_cleaned.csv')\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-17\ntest start date:  2023-06-02\ntest end date:  2023-09-17\n/home/chetana\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.04054373450976585\nMSE is 0.019460380355903556\nR2 score is 0.9992940605441825\nRMSE is 0.13950046722467835\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231709073529.joblib\n",
  "history_begin_time" : 1694936050368,
  "history_end_time" : 1694936130379,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "2IL60E8cCSty",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        data = pd.read_csv(f'{working_dir}/final_merged_data_3yrs_cleaned.csv')\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-17\n\nStream closed",
  "history_begin_time" : 1694933333485,
  "history_end_time" : 1694934368281,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "PnMVuDEuXl5O",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        data = pd.read_csv(f'{working_dir}/final_merged_data_3yrs_cleaned.csv')\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/PnMVuDEuXl5O/model_creation_et.py\", line 9, in <module>\n    from model_creation_rf import RandomForestHole\n  File \"/home/chetana/gw-workspace/PnMVuDEuXl5O/model_creation_rf.py\", line 14, in <module>\n    import geopandas as gpd\nModuleNotFoundError: No module named 'geopandas'\n",
  "history_begin_time" : 1694932508760,
  "history_end_time" : 1694932510906,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "5uxpYGi1Zce8",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        data = pd.read_csv(f'{working_dir}/final_merged_data_3yrs_cleaned.csv')\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/5uxpYGi1Zce8/model_creation_et.py\", line 9, in <module>\n    from model_creation_rf import RandomForestHole\n  File \"/home/chetana/gw-workspace/5uxpYGi1Zce8/model_creation_rf.py\", line 14, in <module>\n    import geopandas as gpd\nModuleNotFoundError: No module named 'geopandas'\n",
  "history_begin_time" : 1694932415610,
  "history_end_time" : 1694932420903,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "ko43z5o93uu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694905307654,
  "history_end_time" : 1694905307654,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9jrqt33jlvc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887122,
  "history_end_time" : 1694897887122,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "uf9QlGjBqJdg",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom base_hole import BaseHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(BaseHole):\n    def __init__(self):\n        super().__init__()\n        self.classifier = self.get_model()\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        data = pd.read_csv(f'{working_dir}/final_merged_data_3yrs_cleaned.csv')\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def train(self):\n        self.classifier.fit(self.train_x, self.train_y)\n\n    def test(self):\n        self.test_y_results = self.classifier.predict(self.test_x)\n        print(self.test_y_results)\n        return self.test_y_results\n\n    def evaluate(self):\n        y_pred = self.test_y_results\n        mse = mean_squared_error(self.test_y, y_pred)\n        rmse = np.sqrt(mse)\n        mae = mean_absolute_error(self.test_y, y_pred)\n        r2 = r2_score(self.test_y, y_pred)\n\n        print(\"Mean Squared Error (MSE):\", mse)\n        print(\"Root Mean Squared Error (RMSE):\", rmse)\n        print(\"Mean Absolute Error (MAE):\", mae)\n        print(\"R-squared (R2):\", r2)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-11\n/home/chetana\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n[ 0.     0.068 18.1   ...  9.998  0.     0.   ]\nMean Squared Error (MSE): 0.01923175954198464\nRoot Mean Squared Error (RMSE): 0.138678619628206\nMean Absolute Error (MAE): 0.04118521859819645\nR-squared (R2): 0.9993023539305406\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231109162144.joblib\n",
  "history_begin_time" : 1694449231456,
  "history_end_time" : 1694449305690,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "dvxriBVTdSQM",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom base_hole import BaseHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(BaseHole):\n    def __init__(self):\n        super().__init__()\n        self.classifier = self.get_model()\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        data = pd.read_csv(f'{working_dir}/final_merged_data_3yrs_cleaned.csv')\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data = data.dropna(subset=['swe_value'])\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def train(self):\n        self.classifier.fit(self.train_x, self.train_y)\n\n    def test(self):\n        self.test_y_results = self.classifier.predict(self.test_x)\n        print(self.test_y_results)\n        return self.test_y_results\n\n    def evaluate(self):\n        y_pred = self.test_y_results\n        mse = mean_squared_error(self.test_y, y_pred)\n        rmse = np.sqrt(mse)\n        mae = mean_absolute_error(self.test_y, y_pred)\n        r2 = r2_score(self.test_y, y_pred)\n\n        print(\"Mean Squared Error (MSE):\", mse)\n        print(\"Root Mean Squared Error (RMSE):\", rmse)\n        print(\"Mean Absolute Error (MAE):\", mae)\n        print(\"R-squared (R2):\", r2)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-11\n/home/chetana\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'Unnamed: 0',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness'],\n      dtype='object')\n[ 0.     0.    17.959 ... 10.     0.194  0.   ]\nMean Squared Error (MSE): 0.009067688708238262\nRoot Mean Squared Error (RMSE): 0.09522441235438664\nMean Absolute Error (MAE): 0.02579866164370087\nR-squared (R2): 0.9996710629949083\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231109161531.joblib\n",
  "history_begin_time" : 1694448843878,
  "history_end_time" : 1694448932500,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "VIkVpdpAgKE5",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom base_hole import BaseHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(BaseHole):\n    def __init__(self):\n        super().__init__()\n        self.classifier = self.get_model()\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        data = pd.read_csv(f'{working_dir}/final_merged_data_3yrs_cleaned.csv')\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n\n        data = data.dropna(subset=['swe_value'])\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def train(self):\n        self.classifier.fit(self.train_x, self.train_y)\n\n    def test(self):\n        self.test_y_results = self.classifier.predict(self.test_x)\n        print(self.test_y_results)\n        return self.test_y_results\n\n    def evaluate(self):\n        y_pred = self.test_y_results\n        mse = mean_squared_error(self.test_y, y_pred)\n        rmse = np.sqrt(mse)\n        mae = mean_absolute_error(self.test_y, y_pred)\n        r2 = r2_score(self.test_y, y_pred)\n\n        print(\"Mean Squared Error (MSE):\", mse)\n        print(\"Root Mean Squared Error (RMSE):\", rmse)\n        print(\"Mean Absolute Error (MAE):\", mae)\n        print(\"R-squared (R2):\", r2)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-11\n/home/chetana\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'Unnamed: 0.2', 'Unnamed: 0.1',\n       'Unnamed: 0', 'air_temperature_tmmn', 'air_temperature_tmmn.1',\n       'air_temperature_tmmn.2', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmax.2', 'relative_humidity_rmax.1',\n       'relative_humidity_rmin', 'relative_humidity_rmin.2',\n       'relative_humidity_rmin.1', 'precipitation_amount',\n       'air_temperature_tmmx', 'air_temperature_tmmx.2',\n       'air_temperature_tmmx.1', 'wind_speed', 'elevation', 'slope',\n       'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/VIkVpdpAgKE5/model_creation_et.py\", line 80, in <module>\n    hole.train()\n  File \"/home/chetana/gw-workspace/VIkVpdpAgKE5/model_creation_et.py\", line 43, in train\n    self.classifier.fit(self.train_x, self.train_y)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 345, in fit\n    X, y = self._validate_data(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/base.py\", line 584, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1106, in check_X_y\n    X = check_array(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 921, in check_array\n    _assert_all_finite(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 161, in _assert_all_finite\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nExtraTreesRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
  "history_begin_time" : 1694410397580,
  "history_end_time" : 1694410420225,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "o3b7XcBttSLF",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom base_hole import BaseHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(BaseHole):\n    def __init__(self):\n        super().__init__()\n        self.classifier = self.get_model()\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        data = pd.read_csv(f'{working_dir}/final_merged_data_3_yrs.csv')\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n\n        data = data.dropna(subset=['swe_value'])\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def train(self):\n        self.classifier.fit(self.train_x, self.train_y)\n\n    def test(self):\n        self.test_y_results = self.classifier.predict(self.test_x)\n        print(self.test_y_results)\n        return self.test_y_results\n\n    def evaluate(self):\n        y_pred = self.test_y_results\n        mse = mean_squared_error(self.test_y, y_pred)\n        rmse = np.sqrt(mse)\n        mae = mean_absolute_error(self.test_y, y_pred)\n        r2 = r2_score(self.test_y, y_pred)\n\n        print(\"Mean Squared Error (MSE):\", mse)\n        print(\"Root Mean Squared Error (RMSE):\", rmse)\n        print(\"Mean Absolute Error (MAE):\", mae)\n        print(\"R-squared (R2):\", r2)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-11\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/o3b7XcBttSLF/model_creation_et.py\", line 79, in <module>\n    hole.preprocessing()\n  File \"/home/chetana/gw-workspace/o3b7XcBttSLF/model_creation_et.py\", line 23, in preprocessing\n    data = pd.read_csv(f'{working_dir}/final_merged_data_3_yrs.csv')\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 211, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 950, in read_csv\n    return _read(filepath_or_buffer, kwds)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 611, in _read\n    return parser.read(nrows)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1778, in read\n    ) = self._engine.read(  # type: ignore[attr-defined]\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 232, in read\n    data = _concatenate_chunks(chunks)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 402, in _concatenate_chunks\n    result[name] = np.concatenate(arrs)  # type: ignore[arg-type]\n  File \"<__array_function__ internals>\", line 180, in concatenate\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 1.94 GiB for an array with shape (260876568,) and data type float64\n",
  "history_begin_time" : 1694402136494,
  "history_end_time" : 1694402937425,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "axdRlsiN5IU4",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom base_hole import BaseHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(BaseHole):\n    def __init__(self):\n        super().__init__()\n        self.classifier = self.get_model()\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        data = pd.read_csv(f'{working_dir}/final_merged_data_3_yrs.csv')\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n\n        data = data.dropna(subset=['swe_value'])\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def train(self):\n        self.classifier.fit(self.train_x, self.train_y)\n\n    def test(self):\n        self.test_y_results = self.classifier.predict(self.test_x)\n        print(self.test_y_results)\n        return self.test_y_results\n\n    def evaluate(self):\n        y_pred = self.test_y_results\n        mse = mean_squared_error(self.test_y, y_pred)\n        rmse = np.sqrt(mse)\n        mae = mean_absolute_error(self.test_y, y_pred)\n        r2 = r2_score(self.test_y, y_pred)\n\n        print(\"Mean Squared Error (MSE):\", mse)\n        print(\"Root Mean Squared Error (RMSE):\", rmse)\n        print(\"Mean Absolute Error (MAE):\", mae)\n        print(\"R-squared (R2):\", r2)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-11\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/axdRlsiN5IU4/model_creation_et.py\", line 79, in <module>\n    hole.preprocessing()\n  File \"/home/chetana/gw-workspace/axdRlsiN5IU4/model_creation_et.py\", line 23, in preprocessing\n    data = pd.read_csv(f'{working_dir}/final_merged_data_3_yrs.csv')\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 211, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 950, in read_csv\n    return _read(filepath_or_buffer, kwds)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 611, in _read\n    return parser.read(nrows)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1778, in read\n    ) = self._engine.read(  # type: ignore[attr-defined]\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 232, in read\n    data = _concatenate_chunks(chunks)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 402, in _concatenate_chunks\n    result[name] = np.concatenate(arrs)  # type: ignore[arg-type]\n  File \"<__array_function__ internals>\", line 180, in concatenate\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 1.94 GiB for an array with shape (260876568,) and data type float64\n",
  "history_begin_time" : 1694391223370,
  "history_end_time" : 1694391960461,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "S8O3DzqO9YLd",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom base_hole import BaseHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(BaseHole):\n    def __init__(self):\n        super().__init__()\n        self.classifier = self.get_model()\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        data = pd.read_csv(f'{working_dir}/final_merged_data_3_yrs.csv')\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n\n        data = data.dropna(subset=['swe_value'])\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def train(self):\n        self.classifier.fit(self.train_x, self.train_y)\n\n    def test(self):\n        self.test_y_results = self.classifier.predict(self.test_x)\n        print(self.test_y_results)\n        return self.test_y_results\n\n    def evaluate(self):\n        y_pred = self.test_y_results\n        mse = mean_squared_error(self.test_y, y_pred)\n        rmse = np.sqrt(mse)\n        mae = mean_absolute_error(self.test_y, y_pred)\n        r2 = r2_score(self.test_y, y_pred)\n\n        print(\"Mean Squared Error (MSE):\", mse)\n        print(\"Root Mean Squared Error (RMSE):\", rmse)\n        print(\"Mean Absolute Error (MAE):\", mae)\n        print(\"R-squared (R2):\", r2)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/S8O3DzqO9YLd/model_creation_et.py\", line 79, in <module>\n    hole.preprocessing()\n  File \"/home/chetana/gw-workspace/S8O3DzqO9YLd/model_creation_et.py\", line 23, in preprocessing\n    data = pd.read_csv(f'{working_dir}/final_merged_data_3_yrs.csv')\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 211, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 950, in read_csv\n    return _read(filepath_or_buffer, kwds)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 611, in _read\n    return parser.read(nrows)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1795, in read\n    df = DataFrame(col_dict, columns=columns, index=index)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/frame.py\", line 664, in __init__\n    mgr = dict_to_mgr(data, index, columns, dtype=dtype, copy=copy, typ=manager)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/construction.py\", line 493, in dict_to_mgr\n    return arrays_to_mgr(arrays, columns, index, dtype=dtype, typ=typ, consolidate=copy)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/construction.py\", line 154, in arrays_to_mgr\n    return create_block_manager_from_column_arrays(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/managers.py\", line 2199, in create_block_manager_from_column_arrays\n    blocks = _form_blocks(arrays, consolidate)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/managers.py\", line 2273, in _form_blocks\n    values, placement = _stack_arrays(list(tup_block), dtype)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/managers.py\", line 2312, in _stack_arrays\n    stacked = np.empty(shape, dtype=dtype)\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 25.0 GiB for an array with shape (13, 257808468) and data type float64\n",
  "history_begin_time" : 1692212520391,
  "history_end_time" : 1692213100724,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "R9u67QWwzhLb",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom base_hole import BaseHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(BaseHole):\n    def __init__(self):\n        super().__init__()\n        self.classifier = self.get_model()\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        data = pd.read_csv(f'{working_dir}/final_merged_data.csv')\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n\n        data = data.dropna(subset=['swe_value'])\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def train(self):\n        self.classifier.fit(self.train_x, self.train_y)\n\n    def test(self):\n        self.test_y_results = self.classifier.predict(self.test_x)\n        print(self.test_y_results)\n        return self.test_y_results\n\n    def evaluate(self):\n        y_pred = self.test_y_results\n        mse = mean_squared_error(self.test_y, y_pred)\n        rmse = np.sqrt(mse)\n        mae = mean_absolute_error(self.test_y, y_pred)\n        r2 = r2_score(self.test_y, y_pred)\n\n        print(\"Mean Squared Error (MSE):\", mse)\n        print(\"Root Mean Squared Error (RMSE):\", rmse)\n        print(\"Mean Absolute Error (MAE):\", mae)\n        print(\"R-squared (R2):\", r2)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity', 'precipitation_amount', 'wind_speed', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n",
  "history_begin_time" : 1692207360402,
  "history_end_time" : 1692207407545,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "KFjNkIUbl4HW",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom base_hole import BaseHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(BaseHole):\n    def __init__(self):\n        super().__init__()\n        self.classifier = self.get_model()\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        data = pd.read_csv(f'{working_dir}/final_merged_data.csv')\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n\n        data = data.dropna(subset=['swe_value'])\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def train(self):\n        self.classifier.fit(self.train_x, self.train_y)\n\n    def test(self):\n        self.test_y_results = self.classifier.predict(self.test_x)\n        print(self.test_y_results)\n        return self.test_y_results\n\n    def evaluate(self):\n        y_pred = self.test_y_results\n        mse = mean_squared_error(self.test_y, y_pred)\n        rmse = np.sqrt(mse)\n        mae = mean_absolute_error(self.test_y, y_pred)\n        r2 = r2_score(self.test_y, y_pred)\n\n        print(\"Mean Squared Error (MSE):\", mse)\n        print(\"Root Mean Squared Error (RMSE):\", rmse)\n        print(\"Mean Absolute Error (MAE):\", mae)\n        print(\"R-squared (R2):\", r2)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\n#hole = ETHole()\n#hole.preprocessing()\n#hole.train()\n#hole.test()\n#hole.evaluate()\n#hole.save()\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\n",
  "history_begin_time" : 1692207343906,
  "history_end_time" : 1692207346178,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "J25fAku3u3mU",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom base_hole import BaseHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(BaseHole):\n    def __init__(self):\n        super().__init__()\n        self.classifier = self.get_model()\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        data = pd.read_csv(f'{working_dir}/final_merged_data.csv')\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n\n        data = data.dropna(subset=['swe_value'])\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def train(self):\n        self.classifier.fit(self.train_x, self.train_y)\n\n    def test(self):\n        self.test_y_results = self.classifier.predict(self.test_x)\n        print(self.test_y_results)\n        return self.test_y_results\n\n    def evaluate(self):\n        y_pred = self.test_y_results\n        mse = mean_squared_error(self.test_y, y_pred)\n        rmse = np.sqrt(mse)\n        mae = mean_absolute_error(self.test_y, y_pred)\n        r2 = r2_score(self.test_y, y_pred)\n\n        print(\"Mean Squared Error (MSE):\", mse)\n        print(\"Root Mean Squared Error (RMSE):\", rmse)\n        print(\"Mean Absolute Error (MAE):\", mae)\n        print(\"R-squared (R2):\", r2)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\n[0.    0.    0.    ... 0.    5.727 7.5  ]\nMean Squared Error (MSE): 0.012246736115475262\nRoot Mean Squared Error (RMSE): 0.11066497239630642\nMean Absolute Error (MAE): 0.028775089404619033\nR-squared (R2): 0.9995573145490274\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231608170807.joblib\n",
  "history_begin_time" : 1692205623853,
  "history_end_time" : 1692205688215,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "nYAyGBpoCdjS",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom base_hole import BaseHole\nfrom snowcast_utils import work_dir\n\nclass ETHole(BaseHole):\n    def __init__(self):\n        super().__init__()\n        self.classifier = self.get_model()\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        data = pd.read_csv(f'{working_dir}/final_merged_data.csv')\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n\n        data = data.dropna(subset=['swe_value'])\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def train(self):\n        self.classifier.fit(self.train_x, self.train_y)\n\n    def test(self):\n        self.test_y_results = self.classifier.predict(self.test_x)\n        print(self.test_y_results)\n        return self.test_y_results\n\n    def evaluate(self):\n        y_pred = self.test_y_results\n        mse = mean_squared_error(self.test_y, y_pred)\n        rmse = np.sqrt(mse)\n        mae = mean_absolute_error(self.test_y, y_pred)\n        r2 = r2_score(self.test_y, y_pred)\n\n        print(\"Mean Squared Error (MSE):\", mse)\n        print(\"Root Mean Squared Error (RMSE):\", rmse)\n        print(\"Mean Absolute Error (MAE):\", mae)\n        print(\"R-squared (R2):\", r2)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/nYAyGBpoCdjS/model_creation_et.py\", line 76, in <module>\n    hole.preprocessing()\n  File \"/home/chetana/gw-workspace/nYAyGBpoCdjS/model_creation_et.py\", line 21, in preprocessing\n    data = pd.read_csv(f'{working_dir}/final_merged_data.csv')\nNameError: name 'working_dir' is not defined\n",
  "history_begin_time" : 1692205604873,
  "history_end_time" : 1692205607185,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "MqqZMNs1OrsZ",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom base_hole import BaseHole\nfrom snowcast_utils import work_dir\n\nclass ETHole(BaseHole):\n    def __init__(self):\n        super().__init__()\n        self.classifier = self.get_model()\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        data = pd.read_csv(f'{working_dir}/final_merged_data.csv')\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n\n        data = data.dropna(subset=['swe_value'])\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def train(self):\n        self.classifier.fit(self.train_x, self.train_y)\n\n    def test(self):\n        self.test_y_results = self.classifier.predict(self.test_x)\n        print(self.test_y_results)\n        return self.test_y_results\n\n    def evaluate(self):\n        y_pred = self.test_y_results\n        mse = mean_squared_error(self.test_y, y_pred)\n        rmse = np.sqrt(mse)\n        mae = mean_absolute_error(self.test_y, y_pred)\n        r2 = r2_score(self.test_y, y_pred)\n\n        print(\"Mean Squared Error (MSE):\", mse)\n        print(\"Root Mean Squared Error (RMSE):\", rmse)\n        print(\"Mean Absolute Error (MAE):\", mae)\n        print(\"R-squared (R2):\", r2)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/MqqZMNs1OrsZ/model_creation_et.py\", line 9, in <module>\n    from base_hole import BaseHole\n  File \"/home/chetana/gw-workspace/MqqZMNs1OrsZ/base_hole.py\", line 44\n    train[['swe_value']].to_numpy().astype('float')\nIndentationError: unexpected indent\n",
  "history_begin_time" : 1692205546154,
  "history_end_time" : 1692205548509,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "sP5GUrvlsQpj",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom base_hole import BaseHole\nfrom snowcast_utils import work_dir\n\nclass ETHole(BaseHole):\n    def __init__(self):\n        super().__init__()\n        self.classifier = self.get_model()\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        data = pd.read_csv(f'{working_dir}/final_merged_data.csv')\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n\n        data = data.dropna(subset=['swe_value'])\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def train(self):\n        self.classifier.fit(self.train_x, self.train_y)\n\n    def test(self):\n        self.test_y_results = self.classifier.predict(self.test_x)\n        print(self.test_y_results)\n        return self.test_y_results\n\n    def evaluate(self):\n        y_pred = self.test_y_results\n        mse = mean_squared_error(self.test_y, y_pred)\n        rmse = np.sqrt(mse)\n        mae = mean_absolute_error(self.test_y, y_pred)\n        r2 = r2_score(self.test_y, y_pred)\n\n        print(\"Mean Squared Error (MSE):\", mse)\n        print(\"Root Mean Squared Error (RMSE):\", rmse)\n        print(\"Mean Absolute Error (MAE):\", mae)\n        print(\"R-squared (R2):\", r2)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/sP5GUrvlsQpj/model_creation_et.py\", line 9, in <module>\n    from base_hole import BaseHole\n  File \"/home/chetana/gw-workspace/sP5GUrvlsQpj/base_hole.py\", line 54\n    train[['swe_snotel']].to_numpy().astype('float')\nIndentationError: unexpected indent\n",
  "history_begin_time" : 1692205321568,
  "history_end_time" : 1692205338424,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "QrmY0DboAfF6",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom base_hole import BaseHole\nfrom snowcast_utils import work_dir\n\nclass ETHole(BaseHole):\n    def __init__(self):\n        super().__init__()\n        self.classifier = self.get_model()\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        data = pd.read_csv('/home/chetana/gridmet_test_run/training_ready_3_yr_data.csv')\n        data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n\n        data = data.dropna(subset=['SWE_x'])\n        data = data[data['SWE_x'] != 0]\n        \n        X = data.drop('SWE_x', axis=1)\n        y = data['SWE_x']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def train(self):\n        self.classifier.fit(self.train_x, self.train_y)\n\n    def test(self):\n        self.test_y_results = self.classifier.predict(self.test_x)\n        print(self.test_y_results)\n        return self.test_y_results\n\n    def evaluate(self):\n        y_pred = self.test_y_results\n        mse = mean_squared_error(self.test_y, y_pred)\n        rmse = np.sqrt(mse)\n        mae = mean_absolute_error(self.test_y, y_pred)\n        r2 = r2_score(self.test_y, y_pred)\n\n        print(\"Mean Squared Error (MSE):\", mse)\n        print(\"Root Mean Squared Error (RMSE):\", rmse)\n        print(\"Mean Absolute Error (MAE):\", mae)\n        print(\"R-squared (R2):\", r2)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-08-14\n/home/chetana\n[ 0.1   20.7    2.91  ... 42.1   15.801 23.299]\nMean Squared Error (MSE): 0.003595932019352251\nRoot Mean Squared Error (RMSE): 0.05996609057919527\nMean Absolute Error (MAE): 0.023765050305137387\nR-squared (R2): 0.9999695218580148\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231408051359.joblib\n",
  "history_begin_time" : 1691990027241,
  "history_end_time" : 1691990039476,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ZR85qwP7FFJZ",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom base_hole import BaseHole\nfrom snowcast_utils import work_dir\n\nclass ETHole(BaseHole):\n    def __init__(self):\n        super().__init__()\n        self.classifier = self.get_model()\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        data = pd.read_csv('/home/chetana/gridmet_test_run/training_ready_3_yr_data.csv')\n        data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n\n        data = data.dropna(subset=['SWE_x'])\n        data = data[data['SWE_x'] != 0]\n        \n        X = data.drop('SWE_x', axis=1)\n        y = data['SWE_x']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def train(self):\n        self.classifier.fit(self.train_x, self.train_y)\n\n    def test(self):\n        self.test_y_results = self.classifier.predict(self.test_x)\n        return self.test_y_results\n\n    def evaluate(self):\n        y_pred = self.test_y_results\n        mse = mean_squared_error(self.test_y, y_pred)\n        rmse = np.sqrt(mse)\n        mae = mean_absolute_error(self.test_y, y_pred)\n        r2 = r2_score(self.test_y, y_pred)\n\n        print(\"Mean Squared Error (MSE):\", mse)\n        print(\"Root Mean Squared Error (RMSE):\", rmse)\n        print(\"Mean Absolute Error (MAE):\", mae)\n        print(\"R-squared (R2):\", r2)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/ZR85qwP7FFJZ/model_creation_et.py\", line 9, in <module>\n    from base_hole import BaseHole\n  File \"/home/chetana/gw-workspace/ZR85qwP7FFJZ/base_hole.py\", line 10, in <module>\n    import seaborn as sns\nModuleNotFoundError: No module named 'seaborn'\n",
  "history_begin_time" : 1691989899567,
  "history_end_time" : 1691989901046,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "FK87yyg8OlrQ",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = pd.read_csv('/home/chetana/gridmet_test_run/training_ready_3_yr_data.csv')\n\ndata = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\nprint(\"data head: \", data.head())\n\ndata['date'] = pd.to_datetime(data['date'])\n# Calculate the number of days since the year 1900\nreference_date = pd.to_datetime('1900-01-01')\ndata['date'] = (data['date'] - reference_date).dt.days\n\nprint(\"data head after date change: \", data.head())\n\ndata = data.dropna(subset=['SWE_x'])\ncolumn_means = data.mean()\n#data = data.fillna(column_means) \n# if there are NA, remove the rows\n# add the drop rows code here\n\ndata = data[data['SWE_x'] != 0]\nX = data.drop('SWE_x', axis=1)\ny = data['SWE_x']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\nmodel.fit(X_train, y_train)\n\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et_3_yrs.pkl')\n\nfeature_importances = model.feature_importances_\nfeature_names = X.columns\nsorted_indices = np.argsort(feature_importances)[::-1]\nsorted_importances = feature_importances[sorted_indices]\nsorted_feature_names = feature_names[sorted_indices]\n\nplt.figure(figsize=(10, 6))\nplt.bar(range(X.shape[1]), sorted_importances, tick_label=sorted_feature_names)\nplt.xticks(rotation=90)\nplt.xlabel('Feature')\nplt.ylabel('Feature Importance')\nplt.title('Feature Importance Plot (ET model)')\nplt.tight_layout()\nplt.savefig('/home/chetana/gridmet_test_run/3_year_feat_importance.png')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "data head:           date  SWE_x        lat  ...  northness  AMSR_SWE  AMSR_Flag\n0  2019-01-01   10.9  41.993149  ...   0.649194        17        241\n1  2019-01-02   11.0  41.993149  ...   0.649194        14        241\n2  2019-01-03   11.0  41.993149  ...   0.649194        13        241\n3  2019-01-04   11.1  41.993149  ...   0.649194         0        241\n4  2019-01-05   11.1  41.993149  ...   0.649194       255        255\n[5 rows x 20 columns]\ndata head after date change:      date  SWE_x        lat  ...  northness  AMSR_SWE  AMSR_Flag\n0  43464   10.9  41.993149  ...   0.649194        17        241\n1  43465   11.0  41.993149  ...   0.649194        14        241\n2  43466   11.0  41.993149  ...   0.649194        13        241\n3  43467   11.1  41.993149  ...   0.649194         0        241\n4  43468   11.1  41.993149  ...   0.649194       255        255\n[5 rows x 20 columns]\nMean Squared Error (MSE): 0.003595932019352251\nRoot Mean Squared Error (RMSE): 0.05996609057919527\nMean Absolute Error (MAE): 0.023765050305137366\nR-squared (R2): 0.9999695218580148\n",
  "history_begin_time" : 1691874241459,
  "history_end_time" : 1691874252182,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "5qsrTBMRasQ6",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = pd.read_csv('/home/chetana/gridmet_test_run/training_ready_3_yr_data.csv')\n\ndata = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\nprint(\"data head: \", data.head())\n\ndata['date'] = pd.to_datetime(data['date'])\n# Calculate the number of days since the year 1900\nreference_date = pd.to_datetime('1900-01-01')\ndata['date'] = (data['date'] - reference_date).dt.days\n\nprint(\"data head after date change: \", data.head())\n\ndata = data.dropna(subset=['SWE_x'])\ncolumn_means = data.mean()\ndata = data.fillna(column_means)\ndata = data[data['SWE_x'] != 0]\nX = data.drop('SWE_x', axis=1)\ny = data['SWE_x']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\nmodel.fit(X_train, y_train)\n\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et_3_yrs.pkl')\n\nfeature_importances = model.feature_importances_\nfeature_names = X.columns\nsorted_indices = np.argsort(feature_importances)[::-1]\nsorted_importances = feature_importances[sorted_indices]\nsorted_feature_names = feature_names[sorted_indices]\n\nplt.figure(figsize=(10, 6))\nplt.bar(range(X.shape[1]), sorted_importances, tick_label=sorted_feature_names)\nplt.xticks(rotation=90)\nplt.xlabel('Feature')\nplt.ylabel('Feature Importance')\nplt.title('Feature Importance Plot (ET model)')\nplt.tight_layout()\nplt.savefig('/home/chetana/gridmet_test_run/3_year_feat_importance.png')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "data head:           date  SWE_x        lat  ...  northness  AMSR_SWE  AMSR_Flag\n0  2019-01-01   10.9  41.993149  ...   0.649194        17        241\n1  2019-01-02   11.0  41.993149  ...   0.649194        14        241\n2  2019-01-03   11.0  41.993149  ...   0.649194        13        241\n3  2019-01-04   11.1  41.993149  ...   0.649194         0        241\n4  2019-01-05   11.1  41.993149  ...   0.649194       255        255\n[5 rows x 20 columns]\ndata head after date change:      date  SWE_x        lat  ...  northness  AMSR_SWE  AMSR_Flag\n0  43464   10.9  41.993149  ...   0.649194        17        241\n1  43465   11.0  41.993149  ...   0.649194        14        241\n2  43466   11.0  41.993149  ...   0.649194        13        241\n3  43467   11.1  41.993149  ...   0.649194         0        241\n4  43468   11.1  41.993149  ...   0.649194       255        255\n[5 rows x 20 columns]\nMean Squared Error (MSE): 0.003595932019352253\nRoot Mean Squared Error (RMSE): 0.05996609057919528\nMean Absolute Error (MAE): 0.023765050305137404\nR-squared (R2): 0.9999695218580148\n",
  "history_begin_time" : 1691874187098,
  "history_end_time" : 1691874198086,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ExiCUGADgOn6",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = pd.read_csv('/home/chetana/gridmet_test_run/training_ready_3_yr_data.csv')\n\ndata = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\ndata['date'] = pd.to_datetime(data['date'])\n# Calculate the number of days since the year 1900\nreference_date = pd.to_datetime('1900-01-01')\ndata['date'] = (data['date'] - reference_date).dt.days\n\ndata = data.dropna(subset=['SWE_x'])\ncolumn_means = data.mean()\ndata = data.fillna(column_means)\ndata = data[data['SWE_x'] != 0]\nX = data.drop('SWE_x', axis=1)\ny = data['SWE_x']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\nmodel.fit(X_train, y_train)\n\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et_3_yrs.pkl')\n\nfeature_importances = model.feature_importances_\nfeature_names = X.columns\nsorted_indices = np.argsort(feature_importances)[::-1]\nsorted_importances = feature_importances[sorted_indices]\nsorted_feature_names = feature_names[sorted_indices]\n\nplt.figure(figsize=(10, 6))\nplt.bar(range(X.shape[1]), sorted_importances, tick_label=sorted_feature_names)\nplt.xticks(rotation=90)\nplt.xlabel('Feature')\nplt.ylabel('Feature Importance')\nplt.title('Feature Importance Plot (ET model)')\nplt.tight_layout()\nplt.savefig('/home/chetana/gridmet_test_run/3_year_feat_importance.png')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "Mean Squared Error (MSE): 0.003595932019352251\nRoot Mean Squared Error (RMSE): 0.05996609057919527\nMean Absolute Error (MAE): 0.023765050305137383\nR-squared (R2): 0.9999695218580148\n",
  "history_begin_time" : 1691712485585,
  "history_end_time" : 1691712497782,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "jC1MjUg1PHZi",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = pd.read_csv('/home/chetana/gridmet_test_run/training_ready_3_yr_data.csv')\n\ndata = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\ndata['date'] = pd.to_datetime(data['date'])\n# Calculate the number of days since the year 1900\nreference_date = pd.to_datetime('1900-01-01')\ndata['date'] = (data['date'] - reference_date).dt.days\n\ndata = data.dropna(subset=['SWE_x'])\ndata.drop([\"SWE_x\"], axis=1, inplace=True)\ncolumn_means = data.mean()\ndata = data.fillna(column_means)\ndata = data[data['SWE_x'] != 0]\nX = data.drop('SWE_x', axis=1)\ny = data['SWE_x']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\nmodel.fit(X_train, y_train)\n\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et_3_yrs.pkl')\n\nfeature_importances = model.feature_importances_\nfeature_names = X.columns\nsorted_indices = np.argsort(feature_importances)[::-1]\nsorted_importances = feature_importances[sorted_indices]\nsorted_feature_names = feature_names[sorted_indices]\n\nplt.figure(figsize=(10, 6))\nplt.bar(range(X.shape[1]), sorted_importances, tick_label=sorted_feature_names)\nplt.xticks(rotation=90)\nplt.xlabel('Feature')\nplt.ylabel('Feature Importance')\nplt.title('Feature Importance Plot (ET model)')\nplt.tight_layout()\nplt.savefig('/home/chetana/gridmet_test_run/3_year_feat_importance.png')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 3802, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 138, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 165, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5745, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5753, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'SWE_x'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/jC1MjUg1PHZi/model_creation_et.py\", line 24, in <module>\n    data = data[data['SWE_x'] != 0]\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/frame.py\", line 3807, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 3804, in get_loc\n    raise KeyError(key) from err\nKeyError: 'SWE_x'\n",
  "history_begin_time" : 1691705771543,
  "history_end_time" : 1691705774761,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "nni8OGXv1ont",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = pd.read_csv('/home/chetana/gridmet_test_run/training_ready_3_yr_data.csv')\n\ndata = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\ndata['date'] = pd.to_datetime(data['date'])\n# Calculate the number of days since the year 1900\nreference_date = pd.to_datetime('1900-01-01')\ndata['date'] = (data['date'] - reference_date).dt.days\n\ndata = data.dropna(subset=['SWE_x'])\ncolumn_means = data.mean()\ndata = data.fillna(column_means)\ndata = data[data['SWE_x'] != 0]\nX = data.drop('SWE_x', axis=1)\ny = data['SWE_x']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\nmodel.fit(X_train, y_train)\n\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et_3_yrs.pkl')\n\nfeature_importances = model.feature_importances_\nfeature_names = X.columns\nsorted_indices = np.argsort(feature_importances)[::-1]\nsorted_importances = feature_importances[sorted_indices]\nsorted_feature_names = feature_names[sorted_indices]\n\nplt.figure(figsize=(10, 6))\nplt.bar(range(X.shape[1]), sorted_importances, tick_label=sorted_feature_names)\nplt.xticks(rotation=90)\nplt.xlabel('Feature')\nplt.ylabel('Feature Importance')\nplt.title('Feature Importance Plot (ET model)')\nplt.tight_layout()\nplt.savefig('/home/chetana/gridmet_test_run/3_year_feat_importance.png')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "Mean Squared Error (MSE): 0.003595932019352253\nRoot Mean Squared Error (RMSE): 0.05996609057919528\nMean Absolute Error (MAE): 0.023765050305137394\nR-squared (R2): 0.9999695218580148\n",
  "history_begin_time" : 1691699066884,
  "history_end_time" : 1691699078819,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Rchm5ktH6KZC",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = pd.read_csv('/home/chetana/gridmet_test_run/training_ready_3_yr_data.csv')\n\ndata = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\ndata['date'] = pd.to_datetime(data['date'])\n# Calculate the number of days since the year 1900\nreference_date = pd.to_datetime('1900-01-01')\ndata['days_since_reference'] = (data['date'] - reference_date).dt.days\n\ndata = data.dropna(subset=['SWE_x'])\ncolumn_means = data.mean()\ndata = data.fillna(column_means)\n\n# Create a new feature 'scaled_date' by applying Min-Max scaling to 'days_since_reference'\nscaler = MinMaxScaler()\ndata['scaled_date'] = scaler.fit_transform(data['days_since_reference'].values.reshape(-1, 1))\n\n# Drop the original 'date' and 'days_since_reference' columns\ndata = data.drop(['date', 'days_since_reference'], axis=1)\ndata = data[data['SWE_x'] != 0]\nX = data.drop('SWE_x', axis=1)\ny = data['SWE_x']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\nmodel.fit(X_train, y_train)\n\nfeature_importances = model.feature_importances_\nfeature_names = X.columns\nsorted_indices = np.argsort(feature_importances)[::-1]\nsorted_importances = feature_importances[sorted_indices]\nsorted_feature_names = feature_names[sorted_indices]\n\nplt.figure(figsize=(10, 6))\nplt.bar(range(X.shape[1]), sorted_importances, tick_label=sorted_feature_names)\nplt.xticks(rotation=90)\nplt.xlabel('Feature')\nplt.ylabel('Feature Importance')\nplt.title('Feature Importance Plot (ET model)')\nplt.tight_layout()\nplt.savefig('/home/chetana/gridmet_test_run/3_year_feat_importance.png')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et_3_yrs.pkl')\n\n",
  "history_output" : "/home/chetana/gw-workspace/Rchm5ktH6KZC/model_creation_et.py:21: FutureWarning: DataFrame.mean and DataFrame.median with numeric_only=None will include datetime64 and datetime64tz columns in a future version.\n  column_means = data.mean()\nMean Squared Error (MSE): 0.004759203996921045\nRoot Mean Squared Error (RMSE): 0.06898698425732962\nMean Absolute Error (MAE): 0.030414838638743724\nR-squared (R2): 0.9999596622810515\n",
  "history_begin_time" : 1691698875001,
  "history_end_time" : 1691698887629,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "uXjUGKo5YuvJ",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = pd.read_csv('/home/chetana/gridmet_test_run/training_ready_3_yr_data.csv')\n\ndata = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\ndata['date'] = pd.to_datetime(data['date'])\n# Calculate the number of days since the year 1900\nreference_date = pd.to_datetime('1900-01-01')\ndata['days_since_reference'] = (data['date'] - reference_date).dt.days\n\ndata = data.dropna(subset=['SWE_x'])\ncolumn_means = data.mean()\ndata = data.fillna(column_means)\n\n# Create a new feature 'scaled_date' by applying Min-Max scaling to 'days_since_reference'\nscaler = MinMaxScaler()\ndata['scaled_date'] = scaler.fit_transform(data['days_since_reference'].values.reshape(-1, 1))\n\n# Drop the original 'date' and 'days_since_reference' columns\ndata = data.drop(['date', 'days_since_reference'], axis=1)\ndata = data[data['SWE_x'] != 0]\nX = data.drop('SWE_x', axis=1)\ny = data['SWE_x']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\nmodel.fit(X_train, y_train)\n\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et_3_yrs.pkl')\n\nfeature_importances = model.feature_importances_\nfeature_names = X.columns\nsorted_indices = np.argsort(feature_importances)[::-1]\nsorted_importances = feature_importances[sorted_indices]\nsorted_feature_names = feature_names[sorted_indices]\n\nplt.figure(figsize=(10, 6))\nplt.bar(range(X.shape[1]), sorted_importances, tick_label=sorted_feature_names)\nplt.xticks(rotation=90)\nplt.xlabel('Feature')\nplt.ylabel('Feature Importance')\nplt.title('Feature Importance Plot (ET model)')\nplt.tight_layout()\nplt.savefig('/home/chetana/gridmet_test_run/3_year_feat_importance.png')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "/home/chetana/gw-workspace/uXjUGKo5YuvJ/model_creation_et.py:21: FutureWarning: DataFrame.mean and DataFrame.median with numeric_only=None will include datetime64 and datetime64tz columns in a future version.\n  column_means = data.mean()\nMean Squared Error (MSE): 0.004759203996921045\nRoot Mean Squared Error (RMSE): 0.06898698425732962\nMean Absolute Error (MAE): 0.030414838638743755\nR-squared (R2): 0.9999596622810515\n",
  "history_begin_time" : 1691698571995,
  "history_end_time" : 1691698590209,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "92hc0mnajmz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531335784,
  "history_end_time" : 1691531335784,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "74y00fa0xz2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531292708,
  "history_end_time" : 1691531292708,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "8uezvdj2bcq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531254583,
  "history_end_time" : 1691531284898,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "81wb8wgbook",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531163795,
  "history_end_time" : 1691531163795,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "36mylr46r1x",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531120847,
  "history_end_time" : 1691531120847,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "otwietge3p9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531060879,
  "history_end_time" : 1691531060879,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "q5dot4n29tp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530848273,
  "history_end_time" : 1691530848273,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "02b17spfycw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717685,
  "history_end_time" : 1691530721103,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "snn2vy9r7fv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530690111,
  "history_end_time" : 1691530716747,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "x4smkvnd2m6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530621011,
  "history_end_time" : 1691530622437,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "yaxbmjcown7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530617158,
  "history_end_time" : 1691530617158,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "gfzu6kpsjik",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530599783,
  "history_end_time" : 1691530614282,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "5jOMOGL2oGsX",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = pd.read_csv('/home/chetana/gridmet_test_run/training_ready_3_yr_data.csv')\n\ndata = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\ndata['date'] = pd.to_datetime(data['date'])\n# Calculate the number of days since the year 1900\nreference_date = pd.to_datetime('1900-01-01')\ndata['days_since_reference'] = (data['date'] - reference_date).dt.days\n\ndata = data.dropna(subset=['SWE_x'])\ncolumn_means = data.mean()\ndata = data.fillna(column_means)\n\n# Create a new feature 'scaled_date' by applying Min-Max scaling to 'days_since_reference'\nscaler = MinMaxScaler()\ndata['scaled_date'] = scaler.fit_transform(data['days_since_reference'].values.reshape(-1, 1))\n\n# Drop the original 'date' and 'days_since_reference' columns\ndata = data.drop(['date', 'days_since_reference'], axis=1)\ndata = data[data['SWE_x'] != 0]\nX = data.drop('SWE_x', axis=1)\ny = data['SWE_x']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\nmodel.fit(X_train, y_train)\n\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et_3_yrs.pkl')\n\nfeature_importances = model.feature_importances_\nfeature_names = X.columns\nsorted_indices = np.argsort(feature_importances)[::-1]\nsorted_importances = feature_importances[sorted_indices]\nsorted_feature_names = feature_names[sorted_indices]\n\nplt.figure(figsize=(10, 6))\nplt.bar(range(X.shape[1]), sorted_importances, tick_label=sorted_feature_names)\nplt.xticks(rotation=90)\nplt.xlabel('Feature')\nplt.ylabel('Feature Importance')\nplt.title('Feature Importance Plot (ET model)')\nplt.tight_layout()\nplt.savefig('/home/chetana/gridmet_test_run/3_year_feat_importance.png')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "/home/chetana/gw-workspace/5jOMOGL2oGsX/model_creation_et.py:21: FutureWarning: DataFrame.mean and DataFrame.median with numeric_only=None will include datetime64 and datetime64tz columns in a future version.\n  column_means = data.mean()\nMean Squared Error (MSE): 0.004759203996921045\nRoot Mean Squared Error (RMSE): 0.06898698425732962\nMean Absolute Error (MAE): 0.030414838638743745\nR-squared (R2): 0.9999596622810515\n",
  "history_begin_time" : 1691419342566,
  "history_end_time" : 1691419354455,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "e9XLw96qFYGN",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = pd.read_csv('/home/chetana/gridmet_test_run/training_ready_3_yr_data.csv')\n\ndata = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\ndata['date'] = pd.to_datetime(data['date'])\n# Calculate the number of days since the year 1900\nreference_date = pd.to_datetime('1900-01-01')\ndata['days_since_reference'] = (data['date'] - reference_date).dt.days\n\ndata = data.dropna(subset=['SWE_x'])\ncolumn_means = data.mean()\ndata = data.fillna(column_means)\n\n# Create a new feature 'scaled_date' by applying Min-Max scaling to 'days_since_reference'\nscaler = MinMaxScaler()\ndata['scaled_date'] = scaler.fit_transform(data['days_since_reference'].values.reshape(-1, 1))\n\n# Drop the original 'date' and 'days_since_reference' columns\ndata = data.drop(['date', 'days_since_reference'], axis=1)\n\nX = data.drop('SWE_x', axis=1)\ny = data['SWE_x']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\nmodel.fit(X_train, y_train)\n\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et_3_yrs.pkl')\n\nfeature_importances = model.feature_importances_\nfeature_names = X.columns\nsorted_indices = np.argsort(feature_importances)[::-1]\nsorted_importances = feature_importances[sorted_indices]\nsorted_feature_names = feature_names[sorted_indices]\n\nplt.figure(figsize=(10, 6))\nplt.bar(range(X.shape[1]), sorted_importances, tick_label=sorted_feature_names)\nplt.xticks(rotation=90)\nplt.xlabel('Feature')\nplt.ylabel('Feature Importance')\nplt.title('Feature Importance Plot (ET model)')\nplt.tight_layout()\nplt.savefig('/home/chetana/gridmet_test_run/3_year_feat_importance.png')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "/home/chetana/gw-workspace/e9XLw96qFYGN/model_creation_et.py:21: FutureWarning: DataFrame.mean and DataFrame.median with numeric_only=None will include datetime64 and datetime64tz columns in a future version.\n  column_means = data.mean()\nMean Squared Error (MSE): 0.21048511653392757\nRoot Mean Squared Error (RMSE): 0.45878656969655024\nMean Absolute Error (MAE): 0.1828980567059585\nR-squared (R2): 0.998475849147189\n",
  "history_begin_time" : 1691394069688,
  "history_end_time" : 1691394091471,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "tgkCOi8hxVkp",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.feature_selection import SelectKBest, f_regression\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = pd.read_csv('/home/chetana/gridmet_test_run/training_ready_3_yr_data.csv')\n\n# Rename columns using the 'rename' method and reassign to 'data'\ndata = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\ndata['date'] = pd.to_datetime(data['date'])\n\n# Extract year, month, day, and day_of_week\ndata['year'] = data['date'].dt.year\ndata['month'] = data['date'].dt.month\ndata['day'] = data['date'].dt.day\ndata['day_of_week'] = data['date'].dt.dayofweek\n\n# Drop date-related columns and unnecessary ones\ndata = data.drop(['date', 'AMSR_SWE', 'AMSR_Flag'], axis=1)\n\n# Fill missing values with column means\ndata = data.dropna(subset=['SWE_x'])\ncolumn_means = data.mean()\ndata = data.fillna(column_means)\n\n# Separate numerical and categorical features\nX_numeric = data.drop(['SWE_x'], axis=1).select_dtypes(include=[np.number])\nX_categorical = data.drop(['SWE_x'], axis=1).select_dtypes(include=[object])\n\n# Normalize numerical features\nscaler = StandardScaler()\nX_numeric_scaled = scaler.fit_transform(X_numeric)\n\n# One-Hot encode categorical features\nencoder = OneHotEncoder()\nX_categorical_encoded = encoder.fit_transform(X_categorical).toarray()\n\n# Combine numerical and categorical features\nX = np.concatenate((X_numeric_scaled, X_categorical_encoded), axis=1)\n\ny = data['SWE_x']\n\n# Get feature names after one-hot encoding\nencoded_feature_names = encoder.get_feature_names(input_features=X_categorical.columns)\nfeature_names = np.concatenate((X_numeric.columns, encoded_feature_names))\n\n# Use SelectKBest for feature selection\nk = 10  # Choose the number of top features to select\nselector = SelectKBest(score_func=f_regression, k=k)\nX_selected = selector.fit_transform(X, y)\n\nX_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\nmodel.fit(X_train, y_train)\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et_3_yrs.pkl')\n\nfeature_importances = model.feature_importances_\nselected_feature_names = feature_names[selector.get_support()]\nsorted_indices = np.argsort(feature_importances)[::-1]\nsorted_importances = feature_importances[sorted_indices]\nsorted_feature_names = selected_feature_names[sorted_indices]\n\nplt.figure(figsize=(10, 6))\nplt.bar(range(X_selected.shape[1]), sorted_importances, tick_label=sorted_feature_names)\nplt.xticks(rotation=90)\nplt.xlabel('Feature')\nplt.ylabel('Feature Importance')\nplt.title('Feature Importance Plot (ET model)')\nplt.tight_layout()\nplt.savefig('/home/chetana/gridmet_test_run/3_year_feat_importance.png')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/tgkCOi8hxVkp/model_creation_et.py\", line 51, in <module>\n    encoded_feature_names = encoder.get_feature_names(input_features=X_categorical.columns)\nAttributeError: 'OneHotEncoder' object has no attribute 'get_feature_names'\n",
  "history_begin_time" : 1691391782065,
  "history_end_time" : 1691391786339,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "mrn5j0Is9AX0",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.feature_selection import SelectKBest, f_regression\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = pd.read_csv('/home/chetana/gridmet_test_run/training_ready_3_yr_data.csv')\n\n# Rename columns using the 'rename' method and reassign to 'data'\ndata = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\ndata['date'] = pd.to_datetime(data['date'])\n\n# Extract year, month, day, and day_of_week\ndata['year'] = data['date'].dt.year\ndata['month'] = data['date'].dt.month\ndata['day'] = data['date'].dt.day\ndata['day_of_week'] = data['date'].dt.dayofweek\n\n# Drop date-related columns and unnecessary ones\ndata = data.drop(['date', 'AMSR_SWE', 'AMSR_Flag'], axis=1)\n\n# Fill missing values with column means\ndata = data.dropna(subset=['SWE_x'])\ncolumn_means = data.mean()\ndata = data.fillna(column_means)\n\n# Separate numerical and categorical features\nX_numeric = data.drop(['SWE_x'], axis=1).select_dtypes(include=[np.number])\nX_categorical = data.drop(['SWE_x'], axis=1).select_dtypes(include=[object])\n\n# Normalize numerical features\nscaler = StandardScaler()\nX_numeric_scaled = scaler.fit_transform(X_numeric)\n\n# One-Hot encode categorical features\nencoder = OneHotEncoder()\nX_categorical_encoded = encoder.fit_transform(X_categorical).toarray()\n\n# Combine numerical and categorical features\nX = np.concatenate((X_numeric_scaled, X_categorical_encoded), axis=1)\n\ny = data['SWE_x']\n\n# Use SelectKBest for feature selection\nk = 10  # Choose the number of top features to select\nselector = SelectKBest(score_func=f_regression, k=k)\nX_selected = selector.fit_transform(X, y)\n\nX_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\nmodel.fit(X_train, y_train)\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et_3_yrs.pkl')\n\nfeature_importances = model.feature_importances_\nselected_feature_names = [feature_names[i] for i in selector.get_support()]\nsorted_indices = np.argsort(feature_importances)[::-1]\nsorted_importances = feature_importances[sorted_indices]\nsorted_feature_names = [selected_feature_names[i] for i in sorted_indices]\n\nplt.figure(figsize=(10, 6))\nplt.bar(range(X_selected.shape[1]), sorted_importances, tick_label=sorted_feature_names)\nplt.xticks(rotation=90)\nplt.xlabel('Feature')\nplt.ylabel('Feature Importance')\nplt.title('Feature Importance Plot (ET model)')\nplt.tight_layout()\nplt.savefig('/home/chetana/gridmet_test_run/3_year_feat_importance.png')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/mrn5j0Is9AX0/model_creation_et.py\", line 61, in <module>\n    selected_feature_names = [feature_names[i] for i in selector.get_support()]\n  File \"/home/chetana/gw-workspace/mrn5j0Is9AX0/model_creation_et.py\", line 61, in <listcomp>\n    selected_feature_names = [feature_names[i] for i in selector.get_support()]\nNameError: name 'feature_names' is not defined\n",
  "history_begin_time" : 1691391688470,
  "history_end_time" : 1691391710161,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "61kBYiJRNjio",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = pd.read_csv('/home/chetana/gridmet_test_run/training_ready_3_yr_data.csv')\n\ndata.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\ndata['date'] = pd.to_datetime(data['date'])\n\ndata['date'] = pd.to_datetime(data['date'])\n\ndata['year'] = data['date'].dt.year\ndata['month'] = data['date'].dt.month\ndata['day'] = data['date'].dt.day\ndata['day_of_week'] = data['date'].dt.dayofweek\ndata = data.drop('date', axis=1)\ndata = data.dropna(subset=['SWE_x'])\ncolumn_means = data.mean()\ndata = data.fillna(column_means)\n\nX = data.drop('SWE_x', axis=1)\ny = data['SWE_x']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\nmodel.fit(X_train, y_train)\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et_3_yrs.pkl')\n\nfeature_importances = model.feature_importances_\nfeature_names = X.columns\nsorted_indices = np.argsort(feature_importances)[::-1]\nsorted_importances = feature_importances[sorted_indices]\nsorted_feature_names = feature_names[sorted_indices]\n\nplt.figure(figsize=(10, 6))\nplt.bar(range(X.shape[1]), sorted_importances, tick_label=sorted_feature_names)\nplt.xticks(rotation=90)\nplt.xlabel('Feature')\nplt.ylabel('Feature Importance')\nplt.title('Feature Importance Plot (ET model)')\nplt.tight_layout()\nplt.savefig('/home/chetana/gridmet_test_run/3_year_feat_importance.png')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "Mean Squared Error (MSE): 0.0003353819369225782\nRoot Mean Squared Error (RMSE): 0.018313435967141124\nMean Absolute Error (MAE): 0.003012902198164983\nR-squared (R2): 0.9999975714545827\n",
  "history_begin_time" : 1691389986668,
  "history_end_time" : 1691390020138,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "0GPTEq69olB5",
  "history_input" : "import dask.dataframe as dd\nfrom dask_ml.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport joblib\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = dd.read_csv('/home/chetana/gridmet_test_run/model_training_cleaned.csv')\ndata['date'] = dd.to_datetime(data['date'])\n\ndata['date'] = dd.to_datetime(data['date'])\n\ndata['year'] = data['date'].dt.year\ndata['month'] = data['date'].dt.month\ndata['day'] = data['date'].dt.day\ndata['day_of_week'] = data['date'].dt.dayofweek\ndata = data.drop('date', axis=1)\ndata = data.dropna(subset=['swe_value'])\ncolumn_means = data.mean()\ndata = data.fillna(column_means)\n\nX = data.drop('swe_value', axis=1)\ny = data['swe_value']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1)\nmodel.fit(X_train, y_train)\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et.pkl')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask_ml/model_selection/_split.py:462: FutureWarning: The default value for 'shuffle' must be specified when splitting DataFrames. In the future DataFrames will automatically be shuffled within blocks prior to splitting. Specify 'shuffle=True' to adopt the future behavior now, or 'shuffle=False' to retain the previous behavior.\n  warnings.warn(\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/0GPTEq69olB5/model_creation_et.py\", line 27, in <module>\n    model.fit(X_train, y_train)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 345, in fit\n    X, y = self._validate_data(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/base.py\", line 584, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1106, in check_X_y\n    X = check_array(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/utils/_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/core.py\", line 599, in __array__\n    x = np.array(self._computed)\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 17.8 GiB for an array with shape (125800448, 19) and data type float64\n",
  "history_begin_time" : 1690608867593,
  "history_end_time" : 1690609370894,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "2oxKExkdXgAE",
  "history_input" : "import dask.dataframe as dd\nfrom dask_ml.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport joblib\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = dd.read_csv('/home/chetana/gridmet_test_run/model_training_cleaned.csv')\ndata['date'] = dd.to_datetime(data['date'])\n\ndata['year'] = data['date'].dt.year\ndata['month'] = data['date'].dt.month\ndata['day'] = data['date'].dt.day\ndata['day_of_week'] = data['date'].dt.dayofweek\ndata = data.drop('date', axis=1)\n\n# Split the data into features (X) and target variable (y)\nX = data.drop('swe_value', axis=1)\ny = data['swe_value']\n\n# Split the data into small chunks (adjust chunk_size based on available memory)\nchunk_size = 100000  # You may need to adjust this value based on your system's memory capacity\nX_chunks = [X.compute().iloc[i:i + chunk_size] for i in range(0, len(X), chunk_size)]\ny_chunks = [y.compute().iloc[i:i + chunk_size] for i in range(0, len(y), chunk_size)]\n\n# Create an instance of the ExtraTreesRegressor\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n\n# Train the model with the initial chunk\nmodel.fit(X_chunks[0], y_chunks[0])\n\n# Save the trained model after initial training\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et.pkl')\n\n# Perform incremental learning on the remaining chunks\nfor i in range(1, len(X_chunks)):\n    model.n_estimators += 10  # Increment n_estimators for each chunk\n    model.fit(X_chunks[i], y_chunks[i])\n\n# Save the trained model after incremental learning\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et.pkl')\n\n# Split the data into a separate test set\nX_test, y_test = train_test_split(X_chunks[-1], y_chunks[-1], test_size=0.2, random_state=42)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/2oxKExkdXgAE/model_creation_et.py\", line 23, in <module>\n    X_chunks = [X.compute().iloc[i:i + chunk_size] for i in range(0, len(X), chunk_size)]\n  File \"/home/chetana/gw-workspace/2oxKExkdXgAE/model_creation_et.py\", line 23, in <listcomp>\n    X_chunks = [X.compute().iloc[i:i + chunk_size] for i in range(0, len(X), chunk_size)]\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/dispatch.py\", line 68, in concat\n    return func(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/backends.py\", line 667, in concat_pandas\n    out = pd.concat(dfs3, join=join, sort=False)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"<__array_function__ internals>\", line 180, in concatenate\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 17.6 GiB for an array with shape (15, 157282561) and data type float64\n",
  "history_begin_time" : 1690602019835,
  "history_end_time" : 1690602894751,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "eF7GGgv9wMf3",
  "history_input" : "import dask.dataframe as dd\nfrom dask_ml.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport joblib\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = dd.read_csv('/home/chetana/gridmet_test_run/model_training_cleaned.csv')\ndata['date'] = dd.to_datetime(data['date'])\n\ndata['year'] = data['date'].dt.year\ndata['month'] = data['date'].dt.month\ndata['day'] = data['date'].dt.day\ndata['day_of_week'] = data['date'].dt.dayofweek\ndata = data.drop('date', axis=1)\n\n# Split the data into features (X) and target variable (y)\nX = data.drop('swe_value', axis=1)\ny = data['swe_value']\n\n# Split the data into training and test sets using Dask\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create an instance of the ExtraTreesRegressor\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n\n# Train the model with the initial training set\nmodel.fit(X_train, y_train)\n\n# Save the trained model after initial training\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et.pkl')\n\n# Load the model to perform incremental learning in batches\nmodel = joblib.load('/home/chetana/gridmet_test_run/model_creation_et.pkl')\n\n# Perform incremental learning on the remaining data\nbatch_size = 1000\nfor i in range(0, len(X_train), batch_size):\n    X_batch = X_train.iloc[i:i+batch_size]\n    y_batch = y_train.iloc[i:i+batch_size]\n    model.fit(X_batch, y_batch)\n\n# Save the trained model after incremental learning\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et.pkl')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask_ml/model_selection/_split.py:462: FutureWarning: The default value for 'shuffle' must be specified when splitting DataFrames. In the future DataFrames will automatically be shuffled within blocks prior to splitting. Specify 'shuffle=True' to adopt the future behavior now, or 'shuffle=False' to retain the previous behavior.\n  warnings.warn(\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/eF7GGgv9wMf3/model_creation_et.py\", line 28, in <module>\n    model.fit(X_train, y_train)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 345, in fit\n    X, y = self._validate_data(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/base.py\", line 584, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1106, in check_X_y\n    X = check_array(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/utils/_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/core.py\", line 599, in __array__\n    x = np.array(self._computed)\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 17.8 GiB for an array with shape (125834411, 19) and data type float64\n",
  "history_begin_time" : 1690601270988,
  "history_end_time" : 1690601582198,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "fVChoQSreHyt",
  "history_input" : "import dask.dataframe as dd\nfrom dask_ml.model_selection import train_test_split\nfrom dask_ml.wrappers import Incremental\nimport joblib\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = dd.read_csv('/home/chetana/gridmet_test_run/model_training_cleaned.csv')\ndata['date'] = dd.to_datetime(data['date'])\n\ndata['year'] = data['date'].dt.year\ndata['month'] = data['date'].dt.month\ndata['day'] = data['date'].dt.day\ndata['day_of_week'] = data['date'].dt.dayofweek\ndata = data.drop('date', axis=1)\n\n# Split the data into features (X) and target variable (y)\nX = data.drop('swe_value', axis=1)\ny = data['swe_value']\n\n# Split the data into a small initial training set and a larger pool\nX_train_init, X_pool, y_train_init, y_pool = train_test_split(X, y, test_size=0.9, random_state=42)\n\n# Create an instance of the ExtraTreesRegressor wrapped in Incremental\nmodel = Incremental(ExtraTreesRegressor(n_estimators=100, random_state=42))\n\n# Train the model with the initial training set\nmodel.fit(X_train_init, y_train_init)\n\n# The remaining data (X_pool, y_pool) can be used for incremental learning\n# For example, you can loop over the remaining data in smaller batches and update the model\nbatch_size = 1000\nfor i in range(0, len(X_pool), batch_size):\n    X_batch = X_pool.iloc[i:i+batch_size]\n    y_batch = y_pool.iloc[i:i+batch_size]\n    # Perform incremental learning\n    model.partial_fit(X_batch, y_batch)\n\n# Save the trained model after incremental learning\nmodel.steps[-1][1].n_jobs = 1  # Ensure n_jobs = 1 to avoid potential issues with joblib.dump\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et.pkl')\n\n# Split the data into a separate test set\nX_test, y_test = train_test_split(X_pool, y_pool, test_size=0.2, random_state=42)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask_ml/model_selection/_split.py:462: FutureWarning: The default value for 'shuffle' must be specified when splitting DataFrames. In the future DataFrames will automatically be shuffled within blocks prior to splitting. Specify 'shuffle=True' to adopt the future behavior now, or 'shuffle=False' to retain the previous behavior.\n  warnings.warn(\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/fVChoQSreHyt/model_creation_et.py\", line 29, in <module>\n    model.fit(X_train_init, y_train_init)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask_ml/wrappers.py\", line 579, in fit\n    self._fit_for_estimator(estimator, X, y, **fit_kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask_ml/wrappers.py\", line 563, in _fit_for_estimator\n    result = fit(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask_ml/_partial.py\", line 98, in fit\n    raise ValueError(msg.format(type(model)))\nValueError: The class '<class 'sklearn.ensemble._forest.ExtraTreesRegressor'>' does not implement 'partial_fit'.\n",
  "history_begin_time" : 1690601225728,
  "history_end_time" : 1690601233258,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ixqC9z45MoTa",
  "history_input" : "import dask.dataframe as dd\nfrom dask_ml.model_selection import train_test_split\nfrom dask_ml.wrappers import Incremental\nimport joblib\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = dd.read_csv('/home/chetana/gridmet_test_run/model_training_cleaned.csv')\ndata['date'] = dd.to_datetime(data['date'])\n\ndata['year'] = data['date'].dt.year\ndata['month'] = data['date'].dt.month\ndata['day'] = data['date'].dt.day\ndata['day_of_week'] = data['date'].dt.dayofweek\ndata = data.drop('date', axis=1)\n\n# Split the data into features (X) and target variable (y)\nX = data.drop('swe_value', axis=1)\ny = data['swe_value']\n\n# Split the data into a small initial training set and a larger pool\nX_train_init, X_pool, y_train_init, y_pool = train_test_split(X, y, test_size=0.9, random_state=42)\n\n# Create an instance of the ExtraTreesRegressor wrapped in Incremental\nmodel = Incremental(ExtraTreesRegressor(n_estimators=100, random_state=42))\n\n# Train the model with the initial training set\nmodel.fit(X_train_init, y_train_init)\n\n# The remaining data (X_pool, y_pool) can be used for incremental learning\n# For example, you can loop over the remaining data in smaller batches and update the model\nbatch_size = 1000\nfor i in range(0, len(X_pool), batch_size):\n    X_batch = X_pool.iloc[i:i+batch_size]\n    y_batch = y_pool.iloc[i:i+batch_size]\n    # Perform incremental learning\n    model.partial_fit(X_batch, y_batch)\n\n# Save the trained model after incremental learning\nmodel.steps[-1][1].n_jobs = 1  # Ensure n_jobs = 1 to avoid potential issues with joblib.dump\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et.pkl')\n\n# Split the data into a separate test set\nX_test, y_test = train_test_split(X_pool, y_pool, test_size=0.2, random_state=42)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/ixqC9z45MoTa/model_creation_et.py\", line 2, in <module>\n    from dask_ml.model_selection import train_test_split\nModuleNotFoundError: No module named 'dask_ml'\n",
  "history_begin_time" : 1690601139207,
  "history_end_time" : 1690601153621,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "jjIg5Pl53i1w",
  "history_input" : "import numpy as np\nimport pandas as pd\nimport dask.dataframe as dd\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport joblib\nfrom scipy import sparse\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = dd.read_csv('/home/chetana/gridmet_test_run/model_training_cleaned.csv')\ndata['date'] = dd.to_datetime(data['date'])\n\ndata['year'] = data['date'].dt.year\ndata['month'] = data['date'].dt.month\ndata['day'] = data['date'].dt.day\ndata['day_of_week'] = data['date'].dt.dayofweek\ndata = data.drop('date', axis=1)\n\n# Convert the Dask DataFrame to a Pandas DataFrame\ndata = data.compute()\n\nprint('feature order:', list(data.keys()))\n\n# Split the data into features (X) and target variable (y)\nX = data.drop('swe_value', axis=1)\ny = data['swe_value']\n\n# Convert X to a sparse matrix if it contains many zero values\nX_sparse = sparse.csr_matrix(X)  # Use sparse.csr_matrix for compressed sparse row format\n\n# Split the data into a small initial training set and a larger pool\nX_train_init, X_pool, y_train_init, y_pool = train_test_split(X_sparse, y, test_size=0.9, random_state=42)\n\n# Create an instance of the ExtraTreesRegressor\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n\n# Train the model with the initial training set\nmodel.fit(X_train_init, y_train_init)\n\n# The remaining data (X_pool, y_pool) can be used for incremental learning\n# For example, you can loop over the remaining data in smaller batches and update the model\nbatch_size = 1000\nfor i in range(0, len(X_pool), batch_size):\n    X_batch = X_pool[i:i+batch_size]\n    y_batch = y_pool.iloc[i:i+batch_size]\n    # Perform incremental learning with numpy instead of model.partial_fit\n    model.n_estimators += 10  # Increment n_estimators for each batch\n    model.fit(X_batch, y_batch)\n\n# Save the trained model after incremental learning\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et.pkl')\n\n# Split the data into a separate test set\nX_test, y_test = train_test_split(X_pool, y_pool, test_size=0.2, random_state=42)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "feature order: ['lat', 'lon', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'swe_value', 'slope', 'curvature', 'aspect', 'eastness', 'northness', 'year', 'month', 'day', 'day_of_week']\nTraceback (most recent call last):\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/scipy/sparse/_compressed.py\", line 79, in __init__\n    arg1 = np.asarray(arg1)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 2070, in __array__\n    return np.asarray(self._values, dtype=dtype)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/frame.py\", line 958, in _values\n    self._consolidate_inplace()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 5980, in _consolidate_inplace\n    self._protect_consolidate(f)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 5968, in _protect_consolidate\n    result = f()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 5978, in f\n    self._mgr = self._mgr.consolidate()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/managers.py\", line 686, in consolidate\n    bm._consolidate_inplace()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/managers.py\", line 1873, in _consolidate_inplace\n    self.blocks, self.refs = _consolidate_with_refs(self.blocks, self.refs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/managers.py\", line 2350, in _consolidate_with_refs\n    merged_blocks, consolidated = _merge_blocks(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/managers.py\", line 2388, in _merge_blocks\n    new_values = new_values[argsort]\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 4.69 GiB for an array with shape (4, 157282561) and data type int64\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/jjIg5Pl53i1w/model_creation_et.py\", line 30, in <module>\n    X_sparse = sparse.csr_matrix(X)  # Use sparse.csr_matrix for compressed sparse row format\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/scipy/sparse/_compressed.py\", line 81, in __init__\n    raise ValueError(\"unrecognized {}_matrix constructor usage\"\nValueError: unrecognized csr_matrix constructor usage\n",
  "history_begin_time" : 1690600623346,
  "history_end_time" : 1690601093473,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "bUprvU4W0AnJ",
  "history_input" : "import numpy as np\nimport pandas as pd\nimport dask.dataframe as dd\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport joblib\nfrom scipy import sparse\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = dd.read_csv('/home/chetana/gridmet_test_run/model_training_cleaned.csv')\ndata['date'] = dd.to_datetime(data['date'])\n\ndata['year'] = data['date'].dt.year\ndata['month'] = data['date'].dt.month\ndata['day'] = data['date'].dt.day\ndata['day_of_week'] = data['date'].dt.dayofweek\ndata = data.drop('date', axis=1)\n\n# Convert the Dask DataFrame to a Pandas DataFrame\ndata = data.compute()\n\nprint('feature order:', list(data.keys()))\n\n# Split the data into features (X) and target variable (y)\nX = data.drop('swe_value', axis=1)\ny = data['swe_value']\n\n# Split the data into a small initial training set and a larger pool\nX_train_init, X_pool, y_train_init, y_pool = train_test_split(X, y, test_size=0.9, random_state=42)\n\n# Create an instance of the ExtraTreesRegressor\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n\n# Train the model with the initial training set\nmodel.fit(X_train_init, y_train_init)\n\n# The remaining data (X_pool, y_pool) can be used for incremental learning\n# For example, you can loop over the remaining data in smaller batches and update the model\nbatch_size = 1000\nfor i in range(0, len(X_pool), batch_size):\n    X_batch = X_pool.iloc[i:i+batch_size]\n    y_batch = y_pool.iloc[i:i+batch_size]\n    model.partial_fit(X_batch, y_batch)\n\n# Save the trained model after incremental learning\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et.pkl')\n\n# Split the data into a separate test set\nX_test, y_test = train_test_split(X_pool, y_pool, test_size=0.2, random_state=42)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "feature order: ['lat', 'lon', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'swe_value', 'slope', 'curvature', 'aspect', 'eastness', 'northness', 'year', 'month', 'day', 'day_of_week']\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/bUprvU4W0AnJ/model_creation_et.py\", line 30, in <module>\n    X_train_init, X_pool, y_train_init, y_pool = train_test_split(X, y, test_size=0.9, random_state=42)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/model_selection/_split.py\", line 2585, in train_test_split\n    return list(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/model_selection/_split.py\", line 2587, in <genexpr>\n    (_safe_indexing(a, train), _safe_indexing(a, test)) for a in arrays\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/utils/__init__.py\", line 354, in _safe_indexing\n    return _pandas_indexing(X, indices, indices_dtype, axis=axis)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/utils/__init__.py\", line 196, in _pandas_indexing\n    return X.take(key, axis=axis)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 3871, in take\n    return self._take(indices, axis)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 3884, in _take\n    self._consolidate_inplace()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 5980, in _consolidate_inplace\n    self._protect_consolidate(f)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 5968, in _protect_consolidate\n    result = f()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 5978, in f\n    self._mgr = self._mgr.consolidate()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/managers.py\", line 686, in consolidate\n    bm._consolidate_inplace()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/managers.py\", line 1873, in _consolidate_inplace\n    self.blocks, self.refs = _consolidate_with_refs(self.blocks, self.refs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/managers.py\", line 2350, in _consolidate_with_refs\n    merged_blocks, consolidated = _merge_blocks(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/managers.py\", line 2388, in _merge_blocks\n    new_values = new_values[argsort]\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 4.69 GiB for an array with shape (4, 157282561) and data type int64\n",
  "history_begin_time" : 1690599663437,
  "history_end_time" : 1690600090410,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "xeDnisnyxmwF",
  "history_input" : "import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport joblib\n\n# Load your dataset into a pandas DataFrame (Assuming the target variable is named 'target')\ndata = pd.read_csv('/home/chetana/gridmet_test_run/model_training_cleaned.csv')\ndata['date'] = pd.to_datetime(data['date'])\n\ndata['year'] = data['date'].dt.year\ndata['month'] = data['date'].dt.month\ndata['day'] = data['date'].dt.day\ndata['day_of_week'] = data['date'].dt.dayofweek\ndata.drop('date', axis=1, inplace=True)\n\nprint('feature order:', list(data.keys()))\n\n# Split the data into features (X) and target variable (y)\nX = data.drop('swe_value', axis=1)\ny = data['swe_value']\n\n# Split the data into a small initial training set and a larger pool\nX_train_init, X_pool, y_train_init, y_pool = train_test_split(X, y, test_size=0.9, random_state=42)\n\n# Create an instance of the ExtraTreesRegressor\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n\n# Train the model with the initial training set\nmodel.fit(X_train_init, y_train_init)\n\n# The remaining data (X_pool, y_pool) can be used for incremental learning\n# For example, you can loop over the remaining data in smaller batches and update the model\nbatch_size = 10000\nfor i in range(0, len(X_pool), batch_size):\n    X_batch = X_pool[i:i+batch_size]\n    y_batch = y_pool[i:i+batch_size]\n    model.partial_fit(X_batch, y_batch)\n\n# Save the trained model after incremental learning\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et.pkl')\n\n# Split the data into a separate test set\nX_test, y_test = train_test_split(X_pool, y_pool, test_size=0.2, random_state=42)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "sh: line 1:  4596 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python model_creation_et.py\n",
  "history_begin_time" : 1690598943604,
  "history_end_time" : 1690599490481,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "T4yTfk1DDCOL",
  "history_input" : "import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport joblib\n\n# Load your dataset into a pandas DataFrame (Assuming the target variable is named 'target')\ndata = pd.read_csv('/home/chetana/gridmet_test_run/model_training_cleaned.csv')\ndata['date'] = pd.to_datetime(data['date'])\n\ndata['year'] = data['date'].dt.year\ndata['month'] = data['date'].dt.month\ndata['day'] = data['date'].dt.day\ndata['day_of_week'] = data['date'].dt.dayofweek\ndata.drop('date', axis=1, inplace=True)\n\nprint('feature order:', list(data.keys()))\n\n# Split the data into features (X) and target variable (y)\nX = data.drop('swe_value', axis=1)\ny = data['swe_value']\n\n# Split the data into a small initial training set and a larger pool\nX_train_init, X_pool, y_train_init, y_pool = train_test_split(X, y, test_size=0.9, random_state=42)\n\n# Create an instance of the ExtraTreesRegressor\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n\n# Train the model with the initial training set\nmodel.fit(X_train_init, y_train_init)\n\n# The remaining data (X_pool, y_pool) can be used for incremental learning\n# For example, you can loop over the remaining data in smaller batches and update the model\nbatch_size = 1000\nfor i in range(0, len(X_pool), batch_size):\n    X_batch = X_pool[i:i+batch_size]\n    y_batch = y_pool[i:i+batch_size]\n    model.partial_fit(X_batch, y_batch)\n\n# Save the trained model after incremental learning\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et.pkl')\n\n# Split the data into a separate test set\nX_test, y_test = train_test_split(X_pool, y_pool, test_size=0.2, random_state=42)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "",
  "history_begin_time" : 1690598882203,
  "history_end_time" : 1690598943609,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "aJb4B7Ivh3tl",
  "history_input" : "from sklearn.model_selection import train_test_split\n\n# Split the data into a small initial training set and a larger pool\nX_train_init, X_pool, y_train_init, y_pool = train_test_split(X, y, test_size=0.9, random_state=42)\n\n# Create an instance of the ExtraTreesRegressor\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n\n# Train the model with the initial training set\nmodel.fit(X_train_init, y_train_init)\n\n# The remaining data (X_pool, y_pool) can be used for incremental learning\n# For example, you can loop over the remaining data in smaller batches and update the model\nbatch_size = 100000\nfor i in range(0, len(X_pool), batch_size):\n    X_batch = X_pool[i:i+batch_size]\n    y_batch = y_pool[i:i+batch_size]\n    model.partial_fit(X_batch, y_batch)\n\n# Save the trained model after incremental learning\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et.pkl')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/aJb4B7Ivh3tl/model_creation_et.py\", line 4, in <module>\n    X_train_init, X_pool, y_train_init, y_pool = train_test_split(X, y, test_size=0.9, random_state=42)\nNameError: name 'X' is not defined\n",
  "history_begin_time" : 1690598724999,
  "history_end_time" : 1690598731191,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "exFtmqlKnhGn",
  "history_input" : "import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport joblib\n\n# Load your dataset into a pandas DataFrame (Assuming the target variable is named 'target')\ndata = pd.read_csv('/home/chetana/gridmet_test_run/model_training_cleaned.csv')\ndata['date'] = pd.to_datetime(data['date'])\n\ndata['year'] = data['date'].dt.year\ndata['month'] = data['date'].dt.month\ndata['day'] = data['date'].dt.day\ndata['day_of_week'] = data['date'].dt.dayofweek\ndata.drop('date', axis=1, inplace=True)\n# data.drop('station_elevation', inplace=True, errors='ignore')\n\nprint('feature order:', list(data.keys()))\n\n# Split the data into features (X) and target variable (y)\nX = data.drop('swe_value', axis=1)\ny = data['swe_value']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create an instance of the ExtraTreesRegressor\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Save the trained model\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et.pkl')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)",
  "history_output" : "feature order: ['lat', 'lon', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'swe_value', 'slope', 'curvature', 'aspect', 'eastness', 'northness', 'year', 'month', 'day', 'day_of_week']\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/exFtmqlKnhGn/model_creation_et.py\", line 26, in <module>\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/model_selection/_split.py\", line 2585, in train_test_split\n    return list(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/model_selection/_split.py\", line 2587, in <genexpr>\n    (_safe_indexing(a, train), _safe_indexing(a, test)) for a in arrays\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/utils/__init__.py\", line 354, in _safe_indexing\n    return _pandas_indexing(X, indices, indices_dtype, axis=axis)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/utils/__init__.py\", line 196, in _pandas_indexing\n    return X.take(key, axis=axis)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 3871, in take\n    return self._take(indices, axis)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 3884, in _take\n    self._consolidate_inplace()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 5980, in _consolidate_inplace\n    self._protect_consolidate(f)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 5968, in _protect_consolidate\n    result = f()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 5978, in f\n    self._mgr = self._mgr.consolidate()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/managers.py\", line 686, in consolidate\n    bm._consolidate_inplace()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/managers.py\", line 1873, in _consolidate_inplace\n    self.blocks, self.refs = _consolidate_with_refs(self.blocks, self.refs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/managers.py\", line 2350, in _consolidate_with_refs\n    merged_blocks, consolidated = _merge_blocks(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/managers.py\", line 2381, in _merge_blocks\n    new_values = np.vstack([b.values for b in blocks])  # type: ignore[misc]\n  File \"<__array_function__ internals>\", line 180, in vstack\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/numpy/core/shape_base.py\", line 282, in vstack\n    return _nx.concatenate(arrs, 0)\n  File \"<__array_function__ internals>\", line 180, in concatenate\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 4.69 GiB for an array with shape (4, 157282561) and data type int64\n",
  "history_begin_time" : 1690597078460,
  "history_end_time" : 1690597751471,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "4qHeW6At3bIM",
  "history_input" : "import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport joblib\n\n# Load your dataset into a pandas DataFrame (Assuming the target variable is named 'target')\ndata = pd.read_csv('/home/chetana/gridmet_test_run/final_training_data.csv')\ndata['date'] = pd.to_datetime(data['date'])\n\ndata['year'] = data['date'].dt.year\ndata['month'] = data['date'].dt.month\ndata['day'] = data['date'].dt.day\ndata['day_of_week'] = data['date'].dt.dayofweek\ndata.drop('date', axis=1, inplace=True)\ndata.drop('station_elevation', inplace=True, errors='ignore')\n\n# Split the data into features (X) and target variable (y)\nX = data.drop('swe_value', axis=1)\ny = data['swe_value']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create an instance of the ExtraTreesRegressor\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Save the trained model\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et.pkl')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)",
  "history_output" : "Mean Squared Error (MSE): 0.273139660484067\nRoot Mean Squared Error (RMSE): 0.522627649942162\nMean Absolute Error (MAE): 0.01861515633681594\nR-squared (R2): 0.9996713036422928\n",
  "history_begin_time" : 1690183598760,
  "history_end_time" : 1690188035224,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "6xjxMElJsQcT",
  "history_input" : "import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport joblib\n\n# Load your dataset into a pandas DataFrame (Assuming the target variable is named 'target')\ndata = pd.read_csv('/home/chetana/gridmet_test_run/final_training_data.csv')\ndata['date'] = pd.to_datetime(data['date'])\n\ndata['year'] = data['date'].dt.year\ndata['month'] = data['date'].dt.month\ndata['day'] = data['date'].dt.day\ndata['day_of_week'] = data['date'].dt.dayofweek\ndata.drop('date', axis=1, inplace=True)\ndata.drop('station_elevation', inplace=True, errors='ignore')\n\n# Split the data into features (X) and target variable (y)\nX = data.drop('swe_value', axis=1)\ny = data['swe_value']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create an instance of the ExtraTreesRegressor\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Save the trained model\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et.pkl')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)",
  "history_output" : "sh: line 1:  3498 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python model_creation_et.py\n",
  "history_begin_time" : 1690181880290,
  "history_end_time" : 1690182769094,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "CmqLU87uaBqg",
  "history_input" : "import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport joblib\n\n# Load your dataset into a pandas DataFrame (Assuming the target variable is named 'target')\ndata = pd.read_csv('/home/chetana/gridmet_test_run/final_training_data.csv')\ndata['date'] = pd.to_datetime(data['date'])\n\ndata['year'] = data['date'].dt.year\ndata['month'] = data['date'].dt.month\ndata['day'] = data['date'].dt.day\ndata['day_of_week'] = data['date'].dt.dayofweek\ndata.drop('date', axis=1, inplace=True)\ndata.drop('station_elevation', inplace=True, errors='ignore')\n\n# Split the data into features (X) and target variable (y)\nX = data.drop('swe_value', axis=1)\ny = data['swe_value']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create an instance of the ExtraTreesRegressor\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Save the trained model\njoblib.dump(model, '/home/chetana/gridmet_test_run/extra_trees_model_new_cleaned.pkl')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)",
  "history_output" : "",
  "history_begin_time" : 1690181828675,
  "history_end_time" : 1690181880397,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "9q73kepee06",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1689632033812,
  "history_end_time" : 1689632033812,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "qwbpszj1zsd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1689631636466,
  "history_end_time" : 1689631636466,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "eysx0rzout5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1689135058049,
  "history_end_time" : 1689135058049,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "98ix1ruct8b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1688416860238,
  "history_end_time" : 1688416907376,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "jf7wuu",
  "indicator" : "Stopped"
},{
  "history_id" : "l3kafxzcge2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1688416833718,
  "history_end_time" : 1688416848468,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "jf7wuu",
  "indicator" : "Stopped"
},{
  "history_id" : "7yjxs28gi5x",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1688416668379,
  "history_end_time" : 1688416822957,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "jf7wuu",
  "indicator" : "Stopped"
},{
  "history_id" : "t5rl94var9p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1688416628777,
  "history_end_time" : 1688416660675,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "jf7wuu",
  "indicator" : "Stopped"
},{
  "history_id" : "cngsteuadza",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1688416567324,
  "history_end_time" : 1688416575019,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "jf7wuu",
  "indicator" : "Stopped"
},{
  "history_id" : "aujzoisawvb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1687546866701,
  "history_end_time" : 1687546866701,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "5s07joi429b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1687546866700,
  "history_end_time" : 1687546866700,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8b93o7ohce1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1687463685023,
  "history_end_time" : 1687463685023,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xofb5qnwgdu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1687463635418,
  "history_end_time" : 1687463635418,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "lfzvnwggrq7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1686236147017,
  "history_end_time" : 1686237909495,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "jf7wuu",
  "indicator" : "Stopped"
},{
  "history_id" : "tird6r03tln",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1686235960974,
  "history_end_time" : 1686235985414,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "jf7wuu",
  "indicator" : "Stopped"
},{
  "history_id" : "z81kun8snlx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1686235529630,
  "history_end_time" : 1686235529630,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "jf7wuu",
  "indicator" : "Skipped"
},{
  "history_id" : "fhts1fcqjkp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1686235448229,
  "history_end_time" : 1686235482630,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "jf7wuu",
  "indicator" : "Stopped"
},{
  "history_id" : "qls83mskdpg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1686235402214,
  "history_end_time" : 1686235424784,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "jf7wuu",
  "indicator" : "Stopped"
},{
  "history_id" : "gfajsrwlo41",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1686153654214,
  "history_end_time" : 1686153654214,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "jf7wuu",
  "indicator" : "Skipped"
},{
  "history_id" : "5maeasyhj6k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1682984798068,
  "history_end_time" : 1682984800295,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "jf7wuu",
  "indicator" : "Stopped"
},{
  "history_id" : "8yiga4qnyu6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1681516927642,
  "history_end_time" : 1681516927642,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "7uzioe97mtj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1681039707714,
  "history_end_time" : 1681039707714,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "67e50unk58m",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1681039689408,
  "history_end_time" : 1681039697767,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "x6z5e26f3ce",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1681007820130,
  "history_end_time" : 1681007820130,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "w3i8n0so6ut",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1679442743789,
  "history_end_time" : 1679442743789,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "jf7wuu",
  "indicator" : "Skipped"
},{
  "history_id" : "x27pic4nwpm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1679442743786,
  "history_end_time" : 1679442743786,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "jf7wuu",
  "indicator" : "Skipped"
},{
  "history_id" : "bvs8uz5xbbf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1679332584794,
  "history_end_time" : 1679332584794,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "jf7wuu",
  "indicator" : "Skipped"
},{
  "history_id" : "59y1s4pkvrd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1679332584792,
  "history_end_time" : 1679332584792,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "jf7wuu",
  "indicator" : "Skipped"
},{
  "history_id" : "xirsz1q5mkk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1679191258499,
  "history_end_time" : 1679191258499,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "2jifky",
  "indicator" : "Skipped"
},{
  "history_id" : "n8l0wozhwym",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1679191258498,
  "history_end_time" : 1679191258498,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "2jifky",
  "indicator" : "Skipped"
},{
  "history_id" : "0dlh0b65ied",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1679091534992,
  "history_end_time" : 1679091744970,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "m7frw6mvc7l",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1679091534989,
  "history_end_time" : 1679091744970,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "momkreu5mym",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1679091527588,
  "history_end_time" : 1679091533673,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "t5b2qr1jxcc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1679091527474,
  "history_end_time" : 1679091533670,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "vom43i7j3hr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678887964100,
  "history_end_time" : 1678888215698,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "m4d3q8trcw4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678887964099,
  "history_end_time" : 1678888215698,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "3n2jdto12sr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678887944952,
  "history_end_time" : 1678887946446,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "ahtayuj3p23",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678887944952,
  "history_end_time" : 1678887946446,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "lrico30fenj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678887287259,
  "history_end_time" : 1678887836233,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "4w39lx6pztw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678887287256,
  "history_end_time" : 1678887836233,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "by8meu20u7q",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678886689975,
  "history_end_time" : 1678887010629,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "ez93uyks65b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678886689971,
  "history_end_time" : 1678887010629,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "jobhqqmxooe",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678884557547,
  "history_end_time" : 1678884986358,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "p6pikyg4kzo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678884557544,
  "history_end_time" : 1678884986358,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "lc7dh0bk1nt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678884501901,
  "history_end_time" : 1678884535354,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "iwksrzooy8u",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678884501896,
  "history_end_time" : 1678884535354,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "do4voej56i9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678884207346,
  "history_end_time" : 1678884438334,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "hr93w8dmntp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678884207344,
  "history_end_time" : 1678884438334,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "9xd2br1kj51",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678884125223,
  "history_end_time" : 1678884140269,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "1p8qdrfmnci",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678884125219,
  "history_end_time" : 1678884140269,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "rj8jtb3f9ju",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678884039393,
  "history_end_time" : 1678884042290,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "bqusxuoq0ap",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678884039391,
  "history_end_time" : 1678884042290,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "jodwh5h8gbl",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, '''max_features='auto',''' max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/jodwh5h8gbl/model_creation_et.py\", line 41\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, '''max_features='auto',''' max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                                                                                                                              ^\nSyntaxError: positional argument follows keyword argument\n",
  "history_begin_time" : 1678883327449,
  "history_end_time" : 1678883775458,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "cwuekjmdf02",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678883288648,
  "history_end_time" : 1678883775458,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "cpr2c0kwo6r",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, '''max_features='auto',''' max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/cpr2c0kwo6r/model_creation_et.py\", line 41\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, '''max_features='auto',''' max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                                                                                                                              ^\nSyntaxError: positional argument follows keyword argument\n",
  "history_begin_time" : 1678756761625,
  "history_end_time" : 1678756762958,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "jsw29vfpu7a",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678756752348,
  "history_end_time" : 1678756752348,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "vjsry0r527p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678756683316,
  "history_end_time" : 1678756684837,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "sg16yn842et",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678749631735,
  "history_end_time" : 1678749936029,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "8lin9pzz4mo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678749631729,
  "history_end_time" : 1678749936028,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "i3zmoilod8t",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678748546946,
  "history_end_time" : 1678748546946,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "7kpcfb9mu2i",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678747245317,
  "history_end_time" : 1678747245317,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "ht7h5ztk5n5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678747245311,
  "history_end_time" : 1678747245311,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "tjra16hmblf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678746481227,
  "history_end_time" : 1678746793060,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "igrpr0damup",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678746481216,
  "history_end_time" : 1678746793059,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "1hxpubqx1o2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678743868500,
  "history_end_time" : 1678744167107,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "40i1rmriogy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678743868499,
  "history_end_time" : 1678744167106,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "ftbuf61nd2c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678743629021,
  "history_end_time" : 1678743629021,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "r01ls4uw6ln",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678743629019,
  "history_end_time" : 1678743629019,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "3nfv5zqd4yk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678743144823,
  "history_end_time" : 1678743615535,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "x1vt5lc2e3d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678743144822,
  "history_end_time" : 1678743615535,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "qujqvj5dm0z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678742583290,
  "history_end_time" : 1678742583290,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "5q2qgz4w80d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678742583289,
  "history_end_time" : 1678742583289,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "nbe1lshvwta",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678742209486,
  "history_end_time" : 1678742571511,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "bbzgz2dpidl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678742209483,
  "history_end_time" : 1678742571511,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "27f0v0pud9x",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678738707276,
  "history_end_time" : 1678738707276,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "19er2t3yx1g",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678738707274,
  "history_end_time" : 1678738707274,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "jigh9nl4t7u",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678725425014,
  "history_end_time" : 1678725425014,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "u40lgnasntg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678725424892,
  "history_end_time" : 1678725424892,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "f4e1hlra6gf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678723111834,
  "history_end_time" : 1678725408358,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "53h8avrpx9p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678723111823,
  "history_end_time" : 1678725408356,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "198bqxz7f5j",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678649002073,
  "history_end_time" : 1678649002073,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "l9vzd1m2hgn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678649002070,
  "history_end_time" : 1678649002070,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "fdyq176fimx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678648366923,
  "history_end_time" : 1678648366923,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "h2eqa3wch4x",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678648366920,
  "history_end_time" : 1678648366920,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "ab1s92l6dds",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678648337357,
  "history_end_time" : 1678648341661,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "k66k748cjga",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678648337344,
  "history_end_time" : 1678648341660,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "ptufi5ed55k",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, '''max_features='auto',''' max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/ptufi5ed55k/model_creation_et.py\", line 41\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, '''max_features='auto',''' max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                                                                                                                              ^\nSyntaxError: positional argument follows keyword argument\n",
  "history_begin_time" : 1678564607188,
  "history_end_time" : 1678564608540,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "ztn0nuiyjls",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678564567758,
  "history_end_time" : 1678564567758,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "gver70bqw8e",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, '''max_features='auto',''' max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/gver70bqw8e/model_creation_et.py\", line 41\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, '''max_features='auto',''' max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                                                                                                                              ^\n",
  "history_begin_time" : 1678564554066,
  "history_end_time" : 1678564556718,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Failed"
},{
  "history_id" : "ulcflprd983",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678564538715,
  "history_end_time" : 1678564538715,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "nrebx1i5ifc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678557921962,
  "history_end_time" : 1678557923651,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "682ra80i8pr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678557921959,
  "history_end_time" : 1678557923651,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "lrrc1qqwq5i",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678557705831,
  "history_end_time" : 1678557898800,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "80zg2e6vaos",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678557705828,
  "history_end_time" : 1678557898797,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "jl5u26463i2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678497028189,
  "history_end_time" : 1678497028189,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "sca2o5ok8gi",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678497028182,
  "history_end_time" : 1678497028182,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "xqituu4q5i6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678330214225,
  "history_end_time" : 1678330214225,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "spbodsukvkp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678330214214,
  "history_end_time" : 1678330214214,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "obudlra6og6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678326965120,
  "history_end_time" : 1694185584966,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "a3x0h3mbkuu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678326965116,
  "history_end_time" : 1694185584964,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "kap4u053z7c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678312067635,
  "history_end_time" : 1678312067635,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "jn8bqywrdja",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678312067634,
  "history_end_time" : 1678312067634,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "e3z2qgnjafo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678312030785,
  "history_end_time" : 1678312065074,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "cvwairb0jum",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678312030783,
  "history_end_time" : 1678312065074,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "x02amd37k5c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678312001517,
  "history_end_time" : 1694185586322,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "yqabpj4jltq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678312001514,
  "history_end_time" : 1694185586320,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "qhylqo10hfp",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "Running",
  "history_begin_time" : 1678283527468,
  "history_end_time" : 1678283530931,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "hl54nd58w0q",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678283483050,
  "history_end_time" : 1678283483050,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "fvn9t5s5si4",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "",
  "history_begin_time" : 1678242441647,
  "history_end_time" : 1694185588571,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "o54278n0yq2",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "",
  "history_begin_time" : 1678241840649,
  "history_end_time" : 1694185588093,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "jvrh1qrc4ke",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678241816411,
  "history_end_time" : 1694185588095,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "zoii7a8c6id",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678241789982,
  "history_end_time" : 1694185588573,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "q17o676sm48",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "Running",
  "history_begin_time" : 1678206373516,
  "history_end_time" : 1678206379284,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "hzwl6x6ou71",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678206364593,
  "history_end_time" : 1678206379284,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "v7srx06hl8p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678206134299,
  "history_end_time" : 1678206143169,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "kyxwzdjfljs",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "",
  "history_begin_time" : 1678201922865,
  "history_end_time" : 1678201928753,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "qqfniw5afu5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678201909683,
  "history_end_time" : 1678201909683,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "uts8tc46swe",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678201729145,
  "history_end_time" : 1694185589152,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "51qdf4vba94",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678201707338,
  "history_end_time" : 1694185589780,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "xgbvb3ptufa",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "Running",
  "history_begin_time" : 1678201689552,
  "history_end_time" : 1694185590817,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "f5cphfuvuza",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678201688178,
  "history_end_time" : 1678201703983,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "h0ax84nv5ps",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678201677299,
  "history_end_time" : 1678201687067,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "8m640b1gnyh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678201567679,
  "history_end_time" : 1694185590819,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "ujh45u33r3a",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "Running",
  "history_begin_time" : 1678201349654,
  "history_end_time" : 1678201516420,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "qn7lt6lo79u",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678201335975,
  "history_end_time" : 1678201516420,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "nlee6yb3cjv",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "Running",
  "history_begin_time" : 1678155242795,
  "history_end_time" : 1678155246205,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "thqn4wuiunq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678155182777,
  "history_end_time" : 1678155182777,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "12iqou6zysl",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "Running",
  "history_begin_time" : 1678144764677,
  "history_end_time" : 1678154846288,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "nzpbe86zcn9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678144724394,
  "history_end_time" : 1678154846289,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "ys75jo865i9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677959621661,
  "history_end_time" : 1677959722653,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "l7a23a0tsyu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677959587591,
  "history_end_time" : 1694185592257,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "p2ank3i8ner",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677959568303,
  "history_end_time" : 1677959583158,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "tblt9j1d4yz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677958880515,
  "history_end_time" : 1677958952870,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "pxxhxai3lli",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677958856300,
  "history_end_time" : 1694185593062,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "78ovvdm45oc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677958842103,
  "history_end_time" : 1677958849893,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "j54wcvp6q0h",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677958735471,
  "history_end_time" : 1677958754132,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "5qpliw6brrq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677958274938,
  "history_end_time" : 1677958291220,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "kelo9bm9vdq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677867848039,
  "history_end_time" : 1694185597332,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "7cbcru82qll",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "",
  "history_begin_time" : 1677858942787,
  "history_end_time" : 1677867648730,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "i173z3i008b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677858933368,
  "history_end_time" : 1677867648731,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "3565kdel9w6",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "Running",
  "history_begin_time" : 1677858785007,
  "history_end_time" : 1694185596833,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "pltvqv6m0tc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677858755854,
  "history_end_time" : 1694185596842,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "wljc2q02aq3",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "Running",
  "history_begin_time" : 1677809798041,
  "history_end_time" : 1694185596457,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "oxnl06gn0y3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677809796048,
  "history_end_time" : 1677809840777,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "al3i0kx4hrs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677809675931,
  "history_end_time" : 1694185596458,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "90cytqomkyx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677809602159,
  "history_end_time" : 1694185623001,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "fxth98ciqa5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677809590504,
  "history_end_time" : 1694185622580,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "wek1291c7hm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677809556021,
  "history_end_time" : 1677809573439,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "wbhwq52ke16",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677809539135,
  "history_end_time" : 1677809554697,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "x2b5zquvuqe",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677809468337,
  "history_end_time" : 1694185621792,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "lc0x0t7umv4",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "Running",
  "history_begin_time" : 1677809443300,
  "history_end_time" : 1694185620826,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "dcai1drgazp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677809401972,
  "history_end_time" : 1694185620826,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "nyp0xgtsk31",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677809343299,
  "history_end_time" : 1694185620473,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "pnse9odtzx6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677809175089,
  "history_end_time" : 1677809306589,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "4303b7if6sj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677809134775,
  "history_end_time" : 1677809171462,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "8igf2uwvyiq",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "Running",
  "history_begin_time" : 1677808129491,
  "history_end_time" : 1694185619659,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "m2xwctn2jot",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677808073975,
  "history_end_time" : 1694185619659,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "7bobc7qn3hs",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "Running",
  "history_begin_time" : 1677797442565,
  "history_end_time" : 1694185619266,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "ujn46b5m4w3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677797429029,
  "history_end_time" : 1694185619267,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "g1j21xr0oiz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677797242671,
  "history_end_time" : 1694185618942,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "1lftp48gag8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677797147691,
  "history_end_time" : 1694185616024,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "us2hdq1usiv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677796558982,
  "history_end_time" : 1677797113233,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "ai4zvwtziuw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677796509193,
  "history_end_time" : 1694185615035,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "hqtlnijwnr3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677796344437,
  "history_end_time" : 1677796528249,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "kogqt0qg0n9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677796268588,
  "history_end_time" : 1694185613621,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "0ihd6l4byta",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677795409798,
  "history_end_time" : 1694185613238,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "SgXJuQWz8nZ6",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "",
  "history_begin_time" : 1677795380459,
  "history_end_time" : 1677795384024,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "cwXuiqmY986e",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, '''ccp_alpha=0.0, '''criterion='mse',\n                    max_depth=None, max_features='auto', max_leaf_nodes=None,\n                    max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/cwXuiqmY986e/model_creation_et.py\", line 41\n    etmodel = ExtraTreesRegressor(bootstrap=False, '''ccp_alpha=0.0, '''criterion='mse',\n                                                                        ^\nSyntaxError: positional argument follows keyword argument\n",
  "history_begin_time" : 1677795302230,
  "history_end_time" : 1677795303603,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "XvyLQqdVavqM",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, '''ccp_alpha=0.0, '''criterion='mse',\n                    max_depth=None, max_features='auto', max_leaf_nodes=None,\n                    max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/XvyLQqdVavqM/model_creation_et.py\", line 41\n    etmodel = ExtraTreesRegressor(bootstrap=False, '''ccp_alpha=0.0, '''criterion='mse',\n                                                                        ^\nSyntaxError: positional argument follows keyword argument\n",
  "history_begin_time" : 1677793964308,
  "history_end_time" : 1677793965998,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "vimr1lwsdjs",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, '''ccp_alpha=0.0, '''criterion='mse',\n                    max_depth=None, max_features='auto', max_leaf_nodes=None,\n                    max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/vimr1lwsdjs/model_creation_et.py\", line 41\n    etmodel = ExtraTreesRegressor(bootstrap=False, '''ccp_alpha=0.0, '''criterion='mse',\n                                                                        ^\nSyntaxError: positional argument follows keyword argument\n",
  "history_begin_time" : 1677793001269,
  "history_end_time" : 1694185612781,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "2ntz59ki52j",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677792975432,
  "history_end_time" : 1694185612782,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "r89ppgysk1n",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677792528369,
  "history_end_time" : 1694185612306,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "zdz52q2yyqg",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, '''ccp_alpha=0.0, '''criterion='Squared_error',\n                    max_depth=None, max_features='auto', max_leaf_nodes=None,\n                    max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/zdz52q2yyqg/model_creation_et.py\", line 41\n    etmodel = ExtraTreesRegressor(bootstrap=False, '''ccp_alpha=0.0, '''criterion='Squared_error',\n                                                                        ^\nSyntaxError: positional argument follows keyword argument\n",
  "history_begin_time" : 1677788298641,
  "history_end_time" : 1694185610914,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "mv9fbtply9r",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, '''ccp_alpha=0.0, '''criterion='Squared_error',\n                    max_depth=None, max_features='auto', max_leaf_nodes=None,\n                    max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/mv9fbtply9r/model_creation_et.py\", line 41\n    etmodel = ExtraTreesRegressor(bootstrap=False, '''ccp_alpha=0.0, '''criterion='Squared_error',\n                                                                        ^\nSyntaxError: positional argument follows keyword argument\n",
  "history_begin_time" : 1677787299491,
  "history_end_time" : 1677787302314,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Failed"
},{
  "history_id" : "p0jrc0bawq2",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, '''ccp_alpha=0.0, '''criterion='Squared_error',\n                    max_depth=None, max_features='auto', max_leaf_nodes=None,\n                    max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/p0jrc0bawq2/model_creation_et.py\", line 41\n    etmodel = ExtraTreesRegressor(bootstrap=False, '''ccp_alpha=0.0, '''criterion='Squared_error',\n                                                                        ^\nSyntaxError: positional argument follows keyword argument\n",
  "history_begin_time" : 1677786007165,
  "history_end_time" : 1677786042656,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "44osepq95md",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, '''ccp_alpha=0.0, '''criterion='mse',\n                    max_depth=None, max_features='auto', max_leaf_nodes=None,\n                    max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/44osepq95md/model_creation_et.py\", line 41\n    etmodel = ExtraTreesRegressor(bootstrap=False, '''ccp_alpha=0.0, '''criterion='mse',\n                                                                        ^\nSyntaxError: positional argument follows keyword argument\n",
  "history_begin_time" : 1677784297826,
  "history_end_time" : 1677784516789,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "fqdvnc3ye4k",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, '''ccp_alpha=0.0, '''criterion='mse',\n                    max_depth=None, max_features='auto', max_leaf_nodes=None,\n                    max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/fqdvnc3ye4k/model_creation_et.py\", line 41\n    etmodel = ExtraTreesRegressor(bootstrap=False, '''ccp_alpha=0.0, '''criterion='mse',\n                                                                        ^\nSyntaxError: positional argument follows keyword argument\n",
  "history_begin_time" : 1677784217490,
  "history_end_time" : 1677784272026,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "r1vrkdjzteo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677782901843,
  "history_end_time" : 1677782901843,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "qpd2obfw4gw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677781745188,
  "history_end_time" : 1677781745188,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "gkqk23tbnot",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677719046617,
  "history_end_time" : 1677719046617,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "larrhbov708",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677718179135,
  "history_end_time" : 1677718179135,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "30btjnidi77",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677679556280,
  "history_end_time" : 1677679556280,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "azor3e1w5e8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677679507392,
  "history_end_time" : 1677679549087,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "3oupjwx05cr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677636286907,
  "history_end_time" : 1677636286907,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "z91ec2c4nw3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677636150406,
  "history_end_time" : 1677636150406,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "3t3qp8wo53r",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677636137259,
  "history_end_time" : 1677636142814,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "ytvx91dzhed",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677636063809,
  "history_end_time" : 1677636063809,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "khe8kun5odk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677635881870,
  "history_end_time" : 1677635881870,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "69lrf1ok03p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677617762838,
  "history_end_time" : 1677617762838,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "grzfbkbq2kx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677606170929,
  "history_end_time" : 1677606170929,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "y7ji3ht3gil",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677606114276,
  "history_end_time" : 1677606114276,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "m9lcvcuo3c6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677582848599,
  "history_end_time" : 1677582848599,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "nv1t10fecju",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677525426794,
  "history_end_time" : 1677525426794,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "x52mvh1badz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677462325798,
  "history_end_time" : 1694185608921,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "3ovzc0prpk6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677462312533,
  "history_end_time" : 1694185608713,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "6m42zlfsw02",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677462312139,
  "history_end_time" : 1694185608658,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "4vbwbnhus5l",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677428742782,
  "history_end_time" : 1677428742782,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "phao2vccjib",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677428687392,
  "history_end_time" : 1677428687392,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "2swgbu68bjy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677426263067,
  "history_end_time" : 1677426263067,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "1jt9eyy62lk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677379889852,
  "history_end_time" : 1677379889852,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "w9hjq62ze1m",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677379837843,
  "history_end_time" : 1677379837843,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "j3953xnzxq9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677352478071,
  "history_end_time" : 1677352478071,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "66ng73o3gvf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677352389952,
  "history_end_time" : 1677352389952,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "wquyqn9rv4b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677352335906,
  "history_end_time" : 1677352335906,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "otzk0605ter",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677344120032,
  "history_end_time" : 1677344120032,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "0ygsa2ws0vx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677282603056,
  "history_end_time" : 1677282603056,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "uizqfnkv0wv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677273712247,
  "history_end_time" : 1677273712247,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "d6xu5pbrytp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677273698763,
  "history_end_time" : 1677273703950,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "1845po94k4z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677273673930,
  "history_end_time" : 1677273679536,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "u61o9hml2hk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677273657819,
  "history_end_time" : 1677273665454,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "j6zdipsj8je",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677273536028,
  "history_end_time" : 1677273536028,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "rb7iamod6ew",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677273519125,
  "history_end_time" : 1677273525488,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "gjmgr9hr3yf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677273371313,
  "history_end_time" : 1677273371313,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "o1pd2uoc6q4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677273340455,
  "history_end_time" : 1677273345444,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "of20so3kxku",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677273323509,
  "history_end_time" : 1677273332235,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "f7xar6rs2u3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677273146676,
  "history_end_time" : 1677273146676,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "j83ij1ykj68",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677273100255,
  "history_end_time" : 1677273134494,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "skry6ozkbot",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677201275944,
  "history_end_time" : 1677201275944,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "o681jwo8pg0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677192311086,
  "history_end_time" : 1677192311086,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "5bqw99zh8zu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677192268383,
  "history_end_time" : 1677192268383,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "8ky10a6tm4u",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677191916754,
  "history_end_time" : 1677191916754,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "zasor8tpcmc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677184296714,
  "history_end_time" : 1677184296714,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "6pa1byly1qq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677184173571,
  "history_end_time" : 1677184173571,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "yoortox4njv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677113476484,
  "history_end_time" : 1677113476484,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "cyicrcuhhjf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677108238657,
  "history_end_time" : 1677108238657,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "a1h6nbp65ru",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677108212593,
  "history_end_time" : 1677108229742,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "jbmaaqkc9e0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677107869818,
  "history_end_time" : 1677107869818,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "4v6e2ibngqs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677107757971,
  "history_end_time" : 1677107757971,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "s94tfipll1r",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677107718312,
  "history_end_time" : 1677107718312,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "m2l3iismjzj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677107637174,
  "history_end_time" : 1677107705677,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "mmhh9pfpvlg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677107604043,
  "history_end_time" : 1677107608764,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "pquz6rrbmgl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677107556540,
  "history_end_time" : 1677107562722,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "3htm8c52oe9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677107525024,
  "history_end_time" : 1677107538165,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "1f7ux1xp2mi",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677107501109,
  "history_end_time" : 1677107501109,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "pxucsq8r4am",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677107474177,
  "history_end_time" : 1677107474177,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "f2dkrk2pmu1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677106516128,
  "history_end_time" : 1677106516128,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "bx7trqxgrro",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677106477079,
  "history_end_time" : 1677106477079,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "wi6e1e44p19",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677106431070,
  "history_end_time" : 1677106431070,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "u2icr9zvao8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677106134930,
  "history_end_time" : 1677106147568,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "wzff5s8zv76",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677106011081,
  "history_end_time" : 1677106011081,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "8olpnegried",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677030843747,
  "history_end_time" : 1677030843747,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "3o445va6bqq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677030771175,
  "history_end_time" : 1677030771175,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "swdqdiafcqk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677030672669,
  "history_end_time" : 1677030672669,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "306kn13sjle",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677030563929,
  "history_end_time" : 1677030563929,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "ne1j9r3nm0w",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677025528693,
  "history_end_time" : 1677025528693,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "m00zyr7sljq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677025468127,
  "history_end_time" : 1677025468127,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "drg1qh658bg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677017826432,
  "history_end_time" : 1677017826432,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "310nga444u7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677017218442,
  "history_end_time" : 1677017218442,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "zjyheh6u5mt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677016681334,
  "history_end_time" : 1677016681334,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "13gbz0wcb5i",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677016143020,
  "history_end_time" : 1677016143020,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "xkm8bd3wag0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677016063973,
  "history_end_time" : 1677016063973,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "0073hv1382a",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677015787660,
  "history_end_time" : 1677015787660,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "i77b5df6xrc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677015739971,
  "history_end_time" : 1677015739971,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "ctfu2dm9o17",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677014885004,
  "history_end_time" : 1677014885004,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "uhk2mkj5ju8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677014795678,
  "history_end_time" : 1677014795678,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "h8pgmg9hvho",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677014228165,
  "history_end_time" : 1677014228165,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "gq4qu6p72lj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677013908469,
  "history_end_time" : 1677013908469,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "5gljdptb46g",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677013833089,
  "history_end_time" : 1677013833089,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "qivi4tky3kb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677011873127,
  "history_end_time" : 1677011873127,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "9zhd7q5q5vb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677008198619,
  "history_end_time" : 1677008198619,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "kbbakevo9gs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677008164614,
  "history_end_time" : 1677008164614,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "do4c9tqwtud",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677001999618,
  "history_end_time" : 1677001999618,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "buyji6soaq9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677001732278,
  "history_end_time" : 1677001732278,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8k8g83zu09v",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677001593936,
  "history_end_time" : 1677001593936,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "fd1arrmx9vp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677000536834,
  "history_end_time" : 1677000536834,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "qfz8j1sw1c3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1676999722181,
  "history_end_time" : 1676999722181,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "yb51al",
  "indicator" : "Skipped"
},{
  "history_id" : "bes3kx1wi4p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1676999599222,
  "history_end_time" : 1676999599222,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "j17rfdoh01c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1676862212462,
  "history_end_time" : 1676862212462,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "nwng5zrdym8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1676329536290,
  "history_end_time" : 1676329536290,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "52fnz9ltr21",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1676329491819,
  "history_end_time" : 1676329491819,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0osyvj4ndiq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1676063613437,
  "history_end_time" : 1676063613437,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "gwe3690is8p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1675783782336,
  "history_end_time" : 1675783782336,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "04zyiua3ijy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1672014982968,
  "history_end_time" : 1672014982968,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "a4s9ojdwemc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1671944382481,
  "history_end_time" : 1671944382481,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "j2ah2w2eqk3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1670910617306,
  "history_end_time" : 1670910617306,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "le8g78ua173",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1670910501221,
  "history_end_time" : 1670910501221,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "reku5fe9dlo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1670910268486,
  "history_end_time" : 1670910268486,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "o55rlxsn2vs",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677809171461,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "dk6xytv1cdt",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678201703981,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "tdt9pbwnsgl",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1694185611225,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "9rsjvvfcyut",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677785529433,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "bj5tl30ajfh",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677797113232,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "nxovax1j4w5",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677809840774,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "ouj55ecjegv",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677959722651,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "sz3cnzsa0ua",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677959583156,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "xzezv4jhhkx",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678206143157,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "d9dmvrxiemj",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678201687065,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "j0jk3ys184q",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677958849892,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "jj4dj5wmx7m",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677809573438,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "fvwci1tohs2",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677785383334,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "gg4dzw0n47p",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677796528249,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "s3cz1e64o6y",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677809306588,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "x24xk449thi",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677809554695,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "s0xi6r85xh4",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677958291218,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "sn34u2x6zi2",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677958754131,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "lxkdko99o9n",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677958952869,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "ilag9pn5tin",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678756684831,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},]
