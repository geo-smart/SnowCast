[{
  "history_id" : "dflrj55i04a",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696863953215,
  "history_end_time" : 1696863953215,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "y81h47ikh5a",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696862402939,
  "history_end_time" : 1696862402939,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "zyxjbtdxg8w",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696832263662,
  "history_end_time" : 1696832263662,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "d6obxzxzsq4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696831867366,
  "history_end_time" : 1696831867366,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "m1v68kv1z6n",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696830174321,
  "history_end_time" : 1696830174321,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "IHKu4kBgvo0U",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        data['date'] = data['date'].dt.strftime('%j').astype(int)\n        \n        \n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        data = data.drop('date', axis=1)\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-09\ntest start date:  2023-02-10\ntest end date:  2023-10-09\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 5.062777971786551\nMSE is 36.16279019039616\nR2 score is -0.31183152440839823\nRMSE is 6.01355054775431\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1696826919077,
  "history_end_time" : 1696827053499,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "rg261zqcmpy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696787541901,
  "history_end_time" : 1696787541901,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "yigrpm4yl31",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696786838184,
  "history_end_time" : 1696786838184,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "kK1jJ5DLAk7F",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        data['date'] = data['date'].dt.strftime('%j').astype(int)\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.8386623327054622\nMSE is 2.18864148919649\nR2 score is 0.9206054376324482\nRMSE is 1.479405789226367\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230810173220.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1696786235515,
  "history_end_time" : 1696786345801,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "manOEK7LHDg5",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        df['date'] = df['date'].dt.strftime('%j').astype(int)\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/manOEK7LHDg5/model_creation_et.py\", line 149, in <module>\n    hole.preprocessing()\n  File \"/home/chetana/gw-workspace/manOEK7LHDg5/model_creation_et.py\", line 95, in preprocessing\n    df['date'] = df['date'].dt.strftime('%j').astype(int)\nNameError: name 'df' is not defined\n",
  "history_begin_time" : 1696786216000,
  "history_end_time" : 1696786221218,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "vxFeDxtb9BaI",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.2418702240507661\nMSE is 0.1308238310622578\nR2 score is 0.9952542703472885\nRMSE is 0.3616957714188235\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230810172628.joblib\nFeature image is saved /home/chetana/gridmet_test_run/testing_output/et-model-feature-importance-latest.png\n",
  "history_begin_time" : 1696785920176,
  "history_end_time" : 1696785989973,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "EVLYlRYUnlK1",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/testing_output/et-model-feature-importance-latest.png')\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.2418702240507661\nMSE is 0.1308238310622578\nR2 score is 0.9952542703472885\nRMSE is 0.3616957714188235\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230810172411.joblib\n",
  "history_begin_time" : 1696785786641,
  "history_end_time" : 1696785852836,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "axHOn5CgQbpD",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features\n        feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n        default_weight = 1.0\n\n        # Create an array of sample weights based on feature_weights\n        sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/testing_output/et-model-feature-importance-latest.png')\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n#hole.post_processing()\n",
  "history_output" : "today date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/axHOn5CgQbpD/model_creation_et.py\", line 145, in <module>\n    hole.train()\n  File \"/home/chetana/gw-workspace/axHOn5CgQbpD/model_creation_et.py\", line 121, in train\n    self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 352, in fit\n    sample_weight = _check_sample_weight(sample_weight, X)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1851, in _check_sample_weight\n    raise ValueError(\nValueError: sample_weight.shape == (19,), expected (806960,)!\n",
  "history_begin_time" : 1696784706722,
  "history_end_time" : 1696784711688,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "rqBGxRZ1IwOR",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features\n        feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n\n        # Create an array of sample weights based on feature_weights\n        sample_weights = np.array([feature_weights[feature] for feature in columns])\n        return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/testing_output/et-model-feature-importance-latest.png')\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n#hole.post_processing()\n",
  "history_output" : "today date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/rqBGxRZ1IwOR/model_creation_et.py\", line 143, in <module>\n    hole.preprocessing()\n  File \"/home/chetana/gw-workspace/rqBGxRZ1IwOR/model_creation_et.py\", line 111, in preprocessing\n    self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n  File \"/home/chetana/gw-workspace/rqBGxRZ1IwOR/model_creation_et.py\", line 78, in create_sample_weights\n    sample_weights = np.array([feature_weights[feature] for feature in columns])\n  File \"/home/chetana/gw-workspace/rqBGxRZ1IwOR/model_creation_et.py\", line 78, in <listcomp>\n    sample_weights = np.array([feature_weights[feature] for feature in columns])\nKeyError: 'lat'\n",
  "history_begin_time" : 1696784580005,
  "history_end_time" : 1696784584877,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "51WHRddf4VBx",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns=[]):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features\n        feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n\n        # Create an array of sample weights based on feature_weights\n        sample_weights = np.array([feature_weights[feature] for feature in columns])\n        return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/testing_output/et-model-feature-importance-latest.png')\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/51WHRddf4VBx/model_creation_et.py\", line 111\n    self.weights = self.create_sample_weights(y_train, scale_factor=30.0, X.columns)\n                                                                                   ^\nSyntaxError: positional argument follows keyword argument\n",
  "history_begin_time" : 1696784499922,
  "history_end_time" : 1696784499972,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "z4hy4LPbzGM6",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/testing_output/et-model-feature-importance-latest.png')\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "history_output" : "today date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.2418702240507661\nMSE is 0.1308238310622578\nR2 score is 0.9952542703472885\nRMSE is 0.3616957714188235\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230810165814.joblib\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/z4hy4LPbzGM6/model_creation_et.py\", line 141, in <module>\n    hole.post_processing()\n  File \"/home/chetana/gw-workspace/z4hy4LPbzGM6/model_creation_et.py\", line 120, in post_processing\n    feature_names = self.test_x.columns\nAttributeError: 'numpy.ndarray' object has no attribute 'columns'\n",
  "history_begin_time" : 1696784228244,
  "history_end_time" : 1696784295611,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "s6fivoz18uu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696771780875,
  "history_end_time" : 1696771780875,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "fm7x3xxjwzp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696602943931,
  "history_end_time" : 1696602943931,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "c0lqdaQM1lFT",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-10-06\ntest start date:  2023-01-18\ntest end date:  2023-10-06\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.2418702240507661\nMSE is 0.1308238310622578\nR2 score is 0.9952542703472885\nRMSE is 0.3616957714188235\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230610142049.joblib\n",
  "history_begin_time" : 1696601981398,
  "history_end_time" : 1696602050424,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "vqa8bl7n5MsU",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-10-06\ntest start date:  2023-01-18\ntest end date:  2023-10-06\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.2418702240507661\nMSE is 0.1308238310622578\nR2 score is 0.9952542703472885\nRMSE is 0.3616957714188235\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230610043724.joblib\n",
  "history_begin_time" : 1696566980416,
  "history_end_time" : 1696567045454,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Cw6XSX9G0roP",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    # Define a custom loss function that penalizes errors for values >10 more\n    def custom_loss(y_true, y_pred):\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split = 2,\n                                   min_samples_leaf = 1,\n                                   n_jobs=5\n                                  )\n\n    # Create sample weights based on target values\n    def create_sample_weights(self, y, scale_factor):\n        # Here, we'll use a linear scaling of weights based on target values\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n      \n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        #data.dropna(inplace=True)\n        \n#         for column in data.columns:\n#           data[column] = pd.to_numeric(data[column], errors='coerce')\n#           print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        \n    def train(self):\n    \tself.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-10-06\ntest start date:  2023-01-18\ntest end date:  2023-10-06\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.2418702240507661\nMSE is 0.1308238310622578\nR2 score is 0.9952542703472885\nRMSE is 0.3616957714188235\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230610043115.joblib\n",
  "history_begin_time" : 1696566602468,
  "history_end_time" : 1696566676744,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Sb3AmZigUsjd",
  "history_input" : "\"\"\"\nThis script defines the ETHole class, which is used to train and evaluate an Extra Trees Regression model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regression model for hole analysis.\n\nFunctions:\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    custom_loss(y_true, y_pred): Defines a custom loss function that penalizes errors for values greater than 10.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values.\n    preprocessing(): Performs data preprocessing, including reading and cleaning the training data.\n    train(): Trains the Extra Trees Regression model.\n    post_processing(): Generates a feature importance plot for the trained model.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n    \n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n        \n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        Defines a custom loss function that penalizes errors for values greater than 10.\n        \n        Args:\n            y_true (array-like): True target values.\n            y_pred (array-like): Predicted values.\n\n        Returns:\n            array-like: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def create_sample_weights(self, y, scale_factor):\n        \"\"\"\n        Creates sample weights based on target values.\n\n        Args:\n            y (array-like): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            array-like: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n\n    def preprocessing(self):\n        \"\"\"\n        Performs data preprocessing, including reading and cleaning the training data.\n        \"\"\"\n        # ... (The rest of the preprocessing code)\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regression model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Generates a feature importance plot for the trained model.\n        \"\"\"\n        # ... (Feature importance plotting code)\n\nif __name__ == \"__main__\":\n    hole = ETHole()\n    hole.preprocessing()\n    hole.train()\n    hole.test()\n    hole.evaluate()\n    hole.save()\n",
  "history_output" : "today date = 2023-10-06\ntest start date:  2023-01-18\ntest end date:  2023-10-06\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/Sb3AmZigUsjd/model_creation_et.py\", line 96, in <module>\n    hole.train()\n  File \"/home/chetana/gw-workspace/Sb3AmZigUsjd/model_creation_et.py\", line 85, in train\n    self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\nAttributeError: 'ETHole' object has no attribute 'weights'\n",
  "history_begin_time" : 1696566191001,
  "history_end_time" : 1696566198513,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "uedj6totfkn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696432484314,
  "history_end_time" : 1696432484314,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "efl8praw053",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696432299750,
  "history_end_time" : 1696432482232,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "fnkhes8u7m1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695827991081,
  "history_end_time" : 1695827991081,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "99jqmk8pwas",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695827889170,
  "history_end_time" : 1695827964214,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "0wb2mv0ju2q",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695827855637,
  "history_end_time" : 1695827867006,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ksynwlbrskg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695696616109,
  "history_end_time" : 1695696616109,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "p8kgbjxxtlt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695694257321,
  "history_end_time" : 1695694257321,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "e4zli9f8mgt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695693585740,
  "history_end_time" : 1695693585740,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "99nd0nw7x8j",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695693149359,
  "history_end_time" : 1695693149359,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "K881SY9UeMNq",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    # Define a custom loss function that penalizes errors for values >10 more\n    def custom_loss(y_true, y_pred):\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split = 2,\n                                   min_samples_leaf = 1,\n                                   n_jobs=5\n                                  )\n\n    # Create sample weights based on target values\n    def create_sample_weights(self, y, scale_factor):\n        # Here, we'll use a linear scaling of weights based on target values\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n      \n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        #data.dropna(inplace=True)\n        \n#         for column in data.columns:\n#           data[column] = pd.to_numeric(data[column], errors='coerce')\n#           print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        \n    def train(self):\n    \tself.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-24\ntest start date:  2022-01-16\ntest end date:  2023-09-24\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.2418702240507661\nMSE is 0.1308238310622578\nR2 score is 0.9952542703472885\nRMSE is 0.3616957714188235\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib\n",
  "history_begin_time" : 1695584738226,
  "history_end_time" : 1695584804213,
  "history_notes" : "continuous weights",
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "GmBc52mKQWw1",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    # Define a custom loss function that penalizes errors for values >10 more\n    def custom_loss(y_true, y_pred):\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split = 2,\n                                   min_samples_leaf = 1,\n                                   n_jobs=5\n                                  )\n\n    # Create sample weights based on target values\n    def create_sample_weights(y, scale_factor):\n        # Here, we'll use a linear scaling of weights based on target values\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n      \n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        #data.dropna(inplace=True)\n        \n#         for column in data.columns:\n#           data[column] = pd.to_numeric(data[column], errors='coerce')\n#           print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        \n    def train(self):\n    \tself.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-24\ntest start date:  2022-01-16\ntest end date:  2023-09-24\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/GmBc52mKQWw1/model_creation_et.py\", line 97, in <module>\n    hole.preprocessing()\n  File \"/home/chetana/gw-workspace/GmBc52mKQWw1/model_creation_et.py\", line 72, in preprocessing\n    self.weights = self.create_sample_weights(y_train, scale_factor=30.0)\nTypeError: create_sample_weights() got multiple values for argument 'scale_factor'\n",
  "history_begin_time" : 1695584709170,
  "history_end_time" : 1695584714057,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "pPhvEQHNr2uY",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    # Define a custom loss function that penalizes errors for values >10 more\n    def custom_loss(y_true, y_pred):\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split = 2,\n                                   min_samples_leaf = 1,\n                                   n_jobs=5\n                                  )\n\n    # Create sample weights based on target values\n    def create_sample_weights(y, scale_factor):\n        # Here, we'll use a linear scaling of weights based on target values\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n      \n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        #data.dropna(inplace=True)\n        \n#         for column in data.columns:\n#           data[column] = pd.to_numeric(data[column], errors='coerce')\n#           print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = create_sample_weights(y_train, scale_factor=30.0)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        \n    def train(self):\n    \tself.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-24\ntest start date:  2022-01-16\ntest end date:  2023-09-24\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/pPhvEQHNr2uY/model_creation_et.py\", line 97, in <module>\n    hole.preprocessing()\n  File \"/home/chetana/gw-workspace/pPhvEQHNr2uY/model_creation_et.py\", line 72, in preprocessing\n    self.weights = create_sample_weights(y_train, scale_factor=30.0)\nNameError: name 'create_sample_weights' is not defined\n",
  "history_begin_time" : 1695584671507,
  "history_end_time" : 1695584676278,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "9uK9jG4Kv5p9",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    # Define a custom loss function that penalizes errors for values >10 more\n    def custom_loss(y_true, y_pred):\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split = 2,\n                                   min_samples_leaf = 1,\n                                   n_jobs=5\n                                  )\n\n    # Create sample weights based on target values\n\tdef create_sample_weights(y, scale_factor):\n        # Here, we'll use a linear scaling of weights based on target values\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n      \n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        #data.dropna(inplace=True)\n        \n#         for column in data.columns:\n#           data[column] = pd.to_numeric(data[column], errors='coerce')\n#           print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = create_sample_weights(y_train, scale_factor=30.0)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        \n    def train(self):\n    \tself.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/9uK9jG4Kv5p9/model_creation_et.py\", line 33\n    def create_sample_weights(y, scale_factor):\nTabError: inconsistent use of tabs and spaces in indentation\n",
  "history_begin_time" : 1695584647798,
  "history_end_time" : 1695584647849,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "e88i8iluz95",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695580915840,
  "history_end_time" : 1695580915840,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0fx1MwweWvJY",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    # Define a custom loss function that penalizes errors for values >10 more\n    def custom_loss(y_true, y_pred):\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split = 2,\n                                   min_samples_leaf = 1,\n                                   n_jobs=5\n                                  )\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        #data.dropna(inplace=True)\n        \n#         for column in data.columns:\n#           data[column] = pd.to_numeric(data[column], errors='coerce')\n#           print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = np.where(y_train > 10, 2, 1) # Assign a weight of 2 for values >10, 1 otherwise\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        \n    def train(self):\n    \tself.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-24\ntest start date:  2022-10-16\ntest end date:  2023-09-24\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.026495932883911957\nMSE is 0.00951323829310003\nR2 score is 0.9996549003595576\nRMSE is 0.09753583081668003\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409183401.joblib\n",
  "history_begin_time" : 1695580296667,
  "history_end_time" : 1695580443099,
  "history_notes" : "first weighted model exciting",
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "IXyR50Xve2wz",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    # Define a custom loss function that penalizes errors for values >10 more\n    def custom_loss(y_true, y_pred):\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split = 2,\n                                   min_samples_leaf = 1,\n                                   n_jobs=5\n                                  )\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        #data.dropna(inplace=True)\n        \n#         for column in data.columns:\n#           data[column] = pd.to_numeric(data[column], errors='coerce')\n#           print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        weights = np.where(y_train > 10, 2, 1) # Assign a weight of 2 for values >10, 1 otherwise\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, sample_weight=weights)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-24\ntest start date:  2022-10-16\ntest end date:  2023-09-24\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/IXyR50Xve2wz/model_creation_et.py\", line 89, in <module>\n    hole.preprocessing()\n  File \"/home/chetana/gw-workspace/IXyR50Xve2wz/model_creation_et.py\", line 65, in preprocessing\n    weights = np.where(y_train > 10, 2, 1) # Assign a weight of 2 for values >10, 1 otherwise\nUnboundLocalError: local variable 'y_train' referenced before assignment\n",
  "history_begin_time" : 1695578209913,
  "history_end_time" : 1695578214419,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "4WnrVFRd7aqx",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    # Define a custom loss function that penalizes errors for values >10 more\n    def custom_loss(y_true, y_pred):\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split = 2,\n                                   min_samples_leaf = 1,\n                                   n_jobs=5\n                                  )\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        #data.dropna(inplace=True)\n        \n#         for column in data.columns:\n#           data[column] = pd.to_numeric(data[column], errors='coerce')\n#           print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,\n                                   criterion=self.custom_loss  # Use the custom loss function\n                                                           )\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-24\ntest start date:  2022-10-16\ntest end date:  2023-09-24\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/4WnrVFRd7aqx/model_creation_et.py\", line 89, in <module>\n    hole.preprocessing()\n  File \"/home/chetana/gw-workspace/4WnrVFRd7aqx/model_creation_et.py\", line 65, in preprocessing\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\", line 189, in wrapper\n    params = func_sig.bind(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/inspect.py\", line 3045, in bind\n    return self._bind(args, kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/inspect.py\", line 3034, in _bind\n    raise TypeError(\nTypeError: got an unexpected keyword argument 'criterion'\n",
  "history_begin_time" : 1695578130634,
  "history_end_time" : 1695578135221,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "I7rL698ObX2L",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    # Define a custom loss function that penalizes errors for values >10 more\n    def custom_loss(y_true, y_pred):\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split = 2,\n                                   min_samples_leaf = 1,\n                                   n_jobs=5,\n                                   criterion=self.custom_loss  # Use the custom loss function\n                                  )\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        #data.dropna(inplace=True)\n        \n#         for column in data.columns:\n#           data[column] = pd.to_numeric(data[column], errors='coerce')\n#           print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-24\ntest start date:  2022-10-16\ntest end date:  2023-09-24\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/I7rL698ObX2L/model_creation_et.py\", line 89, in <module>\n    hole.train()\n  File \"/home/chetana/gw-workspace/I7rL698ObX2L/base_hole.py\", line 47, in train\n    self.classifier.fit(self.train_x, self.train_y)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 1144, in wrapper\n    estimator._validate_params()\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 637, in _validate_params\n    validate_parameter_constraints(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'criterion' parameter of ExtraTreesRegressor must be a str among {'squared_error', 'poisson', 'absolute_error', 'friedman_mse'}. Got <bound method ETHole.custom_loss of <__main__.ETHole object at 0x7fa51bd1f970>> instead.\n",
  "history_begin_time" : 1695578054284,
  "history_end_time" : 1695578059133,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "kKYW1aG3rxLD",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    # Define a custom loss function that penalizes errors for values >10 more\n    def custom_loss(y_true, y_pred):\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split = 2,\n                                   min_samples_leaf = 1,\n                                   n_jobs=5,\n                                   criterion=self.custom_loss  # Use the custom loss function)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        #data.dropna(inplace=True)\n        \n#         for column in data.columns:\n#           data[column] = pd.to_numeric(data[column], errors='coerce')\n#           print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/kKYW1aG3rxLD/model_creation_et.py\", line 32\n    def preprocessing(self):\n    ^\nSyntaxError: invalid syntax\n",
  "history_begin_time" : 1695578043313,
  "history_end_time" : 1695578043362,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "YMVpuBus5IJn",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split = 2,\n                                   min_samples_leaf = 1,\n                                   n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        #data.dropna(inplace=True)\n        \n#         for column in data.columns:\n#           data[column] = pd.to_numeric(data[column], errors='coerce')\n#           print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-24\ntest start date:  2022-10-16\ntest end date:  2023-09-24\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.03996316546049505\nMSE is 0.01770245434222269\nR2 score is 0.9993578305893085\nRMSE is 0.1330505706196809\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409175058.joblib\n",
  "history_begin_time" : 1695577722326,
  "history_end_time" : 1695577860442,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "xNy9FfuRqxcH",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split = 2,\n                                   min_samples_leaf = 1,\n                                   n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        #data.dropna(inplace=True)\n        \n#         for column in data.columns:\n#           data[column] = pd.to_numeric(data[column], errors='coerce')\n#           print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        # data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "",
  "history_begin_time" : 1695577667666,
  "history_end_time" : 1695577681011,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "7fcSe5JOxed2",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split = 2,\n                                   min_samples_leaf = 1,\n                                   n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        #data.dropna(inplace=True)\n        \n#         for column in data.columns:\n#           data[column] = pd.to_numeric(data[column], errors='coerce')\n#           print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-24\ntest start date:  2022-10-16\ntest end date:  2023-09-24\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.03996316546049504\nMSE is 0.01770245434222269\nR2 score is 0.9993578305893085\nRMSE is 0.1330505706196809\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409173829.joblib\n",
  "history_begin_time" : 1695576970319,
  "history_end_time" : 1695577111965,
  "history_notes" : "train model with filled value -999, et hyper parameters are changed to 200 estimates",
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "lNzQ4e7EtxLO",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   max_features='auto',\n                                   random_state=42, \n                                   min_samples_split = 2,\n                                   min_samples_leaf = 1,\n                                   n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        #data.dropna(inplace=True)\n        \n#         for column in data.columns:\n#           data[column] = pd.to_numeric(data[column], errors='coerce')\n#           print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-24\ntest start date:  2022-10-16\ntest end date:  2023-09-24\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/lNzQ4e7EtxLO/model_creation_et.py\", line 82, in <module>\n    hole.train()\n  File \"/home/chetana/gw-workspace/lNzQ4e7EtxLO/base_hole.py\", line 47, in train\n    self.classifier.fit(self.train_x, self.train_y)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 1144, in wrapper\n    estimator._validate_params()\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 637, in _validate_params\n    validate_parameter_constraints(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of ExtraTreesRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
  "history_begin_time" : 1695576953790,
  "history_end_time" : 1695576958964,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "SZlLQu8dajwb",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   max_features='auto',\n                                   random_state=42, \n                                   min_samples_split = 2,\n                                   min_samples_leaf = 1,\n                                   n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        #data.dropna(inplace=True)\n        \n        for column in data.columns:\n          data[column] = pd.to_numeric(data[column], errors='coerce')\n          print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-24\ntest start date:  2022-10-16\ntest end date:  2023-09-24\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\ncount    1.022700e+06\nmean     4.419400e+04\nstd      4.217545e+02\nmin      4.346400e+04\n25%      4.382900e+04\n50%      4.419400e+04\n75%      4.455900e+04\nmax      4.492400e+04\nName: date, dtype: float64\ncount    1.022700e+06\nmean     4.165838e+01\nstd      3.632464e+00\nmin      3.335825e+01\n25%      3.891814e+01\n50%      4.107190e+01\n75%      4.461398e+01\nmax      4.897107e+01\nName: lat, dtype: float64\ncount    1.022700e+06\nmean    -1.141534e+02\nstd      5.423376e+00\nmin     -1.234486e+02\n25%     -1.197812e+02\n50%     -1.137737e+02\n75%     -1.100367e+02\nmax     -1.051948e+02\nName: lon, dtype: float64\ncount    1.022700e+06\nmean     5.046323e+01\nstd      9.547185e+01\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      3.500000e+01\nmax      2.550000e+02\nName: SWE, dtype: float64\ncount    1.022700e+06\nmean     2.434436e+02\nstd      5.313797e+00\nmin      2.410000e+02\n25%      2.410000e+02\n50%      2.410000e+02\n75%      2.410000e+02\nmax      2.550000e+02\nName: Flag, dtype: float64\ncount    1.022700e+06\nmean    -1.038741e+01\nstd      1.165852e+02\nmin     -9.990000e+02\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\ncount    1.022700e+06\nmean     5.113495e+05\nstd      2.952282e+05\nmin      0.000000e+00\n25%      2.556748e+05\n50%      5.113495e+05\n75%      7.670242e+05\nmax      1.022699e+06\nName: Unnamed: 0, dtype: float64\ncount    1.022700e+06\nmean     2.720187e+02\nstd      8.384982e+00\nmin      2.328000e+02\n25%      2.663000e+02\n50%      2.722000e+02\n75%      2.784000e+02\nmax      2.983000e+02\nName: air_temperature_tmmn, dtype: float64\ncount    1.022700e+06\nmean     3.943372e+00\nstd      2.478664e+00\nmin      0.000000e+00\n25%      1.900000e+00\n50%      3.500000e+00\n75%      5.800000e+00\nmax      1.510000e+01\nName: potential_evapotranspiration, dtype: float64\ncount    1.022700e+06\nmean     6.258550e-01\nstd      5.233133e-01\nmin      0.000000e+00\n25%      2.100000e-01\n50%      4.700000e-01\n75%      9.300000e-01\nmax      3.870000e+00\nName: mean_vapor_pressure_deficit, dtype: float64\ncount    1.022700e+06\nmean     7.123571e+01\nstd      2.030150e+01\nmin      8.200000e+00\n25%      5.560000e+01\n50%      7.240000e+01\n75%      8.820001e+01\nmax      1.000000e+02\nName: relative_humidity_rmax, dtype: float64\ncount    1.022700e+06\nmean     3.578010e+01\nstd      1.888091e+01\nmin      1.000000e+00\n25%      2.130000e+01\n50%      3.160000e+01\n75%      4.740000e+01\nmax      1.000000e+02\nName: relative_humidity_rmin, dtype: float64\ncount    1.022700e+06\nmean     2.689152e+00\nstd      7.456402e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      2.200000e+00\nmax      5.042000e+02\nName: precipitation_amount, dtype: float64\ncount    1.022700e+06\nmean     2.837481e+02\nstd      9.912702e+00\nmin      2.444000e+02\n25%      2.759000e+02\n50%      2.831000e+02\n75%      2.920000e+02\nmax      3.144000e+02\nName: air_temperature_tmmx, dtype: float64\ncount    1.022700e+06\nmean     4.214964e+00\nstd      2.043559e+00\nmin      5.000000e-01\n25%      2.800000e+00\n50%      3.700000e+00\n75%      5.200000e+00\nmax      1.860000e+01\nName: wind_speed, dtype: float64\ncount    1.022700e+06\nmean     2.397170e+03\nstd      6.647262e+02\nmin      7.584437e+02\n25%      1.948688e+03\n50%      2.481006e+03\n75%      2.895791e+03\nmax      3.823385e+03\nName: elevation, dtype: float64\ncount    1.022700e+06\nmean     8.998188e+01\nstd      2.818171e-02\nmin      8.939863e+01\n25%      8.998083e+01\n50%      8.998801e+01\n75%      8.999234e+01\nmax      8.999674e+01\nName: slope, dtype: float64\ncount    1.022700e+06\nmean    -3.909783e+03\nstd      4.385982e+03\nmin     -1.917768e+04\n25%     -6.479865e+03\n50%     -3.710969e+03\n75%     -5.754078e+02\nmax      9.069726e+03\nName: curvature, dtype: float64\ncount    1.022700e+06\nmean     1.722213e+02\nstd      1.031039e+02\nmin      4.159291e-01\n25%      9.217068e+01\n50%      1.691301e+02\n75%      2.498582e+02\nmax      3.586563e+02\nName: aspect, dtype: float64\ncount    1.022700e+06\nmean     6.965357e-02\nstd      5.171924e-01\nmin     -7.853438e-01\n25%     -3.907275e-01\n50%      1.334633e-01\n75%      5.767317e-01\nmax      7.853980e-01\nName: eastness, dtype: float64\ncount    1.022700e+06\nmean    -4.509867e-02\nstd      6.412035e-01\nmin     -7.853694e-01\n25%     -7.356204e-01\n50%     -1.913248e-01\n75%      6.495535e-01\nmax      7.853850e-01\nName: northness, dtype: float64\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/SZlLQu8dajwb/model_creation_et.py\", line 82, in <module>\n    hole.train()\n  File \"/home/chetana/gw-workspace/SZlLQu8dajwb/base_hole.py\", line 47, in train\n    self.classifier.fit(self.train_x, self.train_y)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 1144, in wrapper\n    estimator._validate_params()\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 637, in _validate_params\n    validate_parameter_constraints(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of ExtraTreesRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
  "history_begin_time" : 1695576932758,
  "history_end_time" : 1695576939620,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "saFXiljrZtWk",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   max_features='auto'\n                                   random_state=42, \n                                   min_samples_split = 2,\n                                   min_samples_leaf = 1,\n                                   n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        #data.dropna(inplace=True)\n        \n        for column in data.columns:\n          data[column] = pd.to_numeric(data[column], errors='coerce')\n          print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/saFXiljrZtWk/model_creation_et.py\", line 21\n    random_state=42, \n    ^\nSyntaxError: invalid syntax\n",
  "history_begin_time" : 1695576924525,
  "history_end_time" : 1695576924581,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "7y63qodifmo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695576291648,
  "history_end_time" : 1695576291648,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "334xzj03og9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695575931006,
  "history_end_time" : 1695575931006,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "entvyyon2fm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695535769205,
  "history_end_time" : 1695535769205,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vom430oimh9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695535478679,
  "history_end_time" : 1695535478679,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rmyzq7a5njs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695535214017,
  "history_end_time" : 1695535214017,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wcr6vv8vlmj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695534943581,
  "history_end_time" : 1695534943581,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "hdbp7vizsnt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695534671821,
  "history_end_time" : 1695534671821,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "im9qgpmuk2t",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695533024089,
  "history_end_time" : 1695533024089,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "088synr3ttf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695529187858,
  "history_end_time" : 1695529187858,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "92dtcihps8j",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695528505176,
  "history_end_time" : 1695528505176,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "iOWJ0XFoLPt4",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        \n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        #data.fillna(-999, inplace=True)\n        data.dropna(inplace=True)\n        \n        for column in data.columns:\n          data[column] = pd.to_numeric(data[column], errors='coerce')\n          print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-24\ntest start date:  2022-08-22\ntest end date:  2023-09-24\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\ncount    1.008700e+06\nmean     4.418437e+04\nstd      4.163835e+02\nmin      4.346400e+04\n25%      4.382400e+04\n50%      4.418400e+04\n75%      4.454500e+04\nmax      4.490500e+04\nName: date, dtype: float64\ncount    1.008700e+06\nmean     4.165838e+01\nstd      3.632464e+00\nmin      3.335825e+01\n25%      3.891814e+01\n50%      4.107190e+01\n75%      4.461398e+01\nmax      4.897107e+01\nName: lat, dtype: float64\ncount    1.008700e+06\nmean    -1.141534e+02\nstd      5.423377e+00\nmin     -1.234486e+02\n25%     -1.197812e+02\n50%     -1.137737e+02\n75%     -1.100367e+02\nmax     -1.051948e+02\nName: lon, dtype: float64\ncount    1.008700e+06\nmean     5.049100e+01\nstd      9.551420e+01\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      3.600000e+01\nmax      2.550000e+02\nName: SWE, dtype: float64\ncount    1.008700e+06\nmean     2.434455e+02\nstd      5.315359e+00\nmin      2.410000e+02\n25%      2.410000e+02\n50%      2.410000e+02\n75%      2.410000e+02\nmax      2.550000e+02\nName: Flag, dtype: float64\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\ncount    1.008700e+06\nmean     5.164940e+05\nstd      2.937986e+05\nmin      0.000000e+00\n25%      2.655018e+05\n50%      5.183495e+05\n75%      7.705242e+05\nmax      1.022699e+06\nName: Unnamed: 0, dtype: float64\ncount    1.008700e+06\nmean     2.721254e+02\nstd      8.333993e+00\nmin      2.380000e+02\n25%      2.664000e+02\n50%      2.723000e+02\n75%      2.785000e+02\nmax      2.983000e+02\nName: air_temperature_tmmn, dtype: float64\ncount    1.008700e+06\nmean     3.937009e+00\nstd      2.476735e+00\nmin      0.000000e+00\n25%      1.800000e+00\n50%      3.500000e+00\n75%      5.800000e+00\nmax      1.510000e+01\nName: potential_evapotranspiration, dtype: float64\ncount    1.008700e+06\nmean     6.318230e-01\nstd      5.238681e-01\nmin      0.000000e+00\n25%      2.100000e-01\n50%      4.800000e-01\n75%      9.400000e-01\nmax      3.870000e+00\nName: mean_vapor_pressure_deficit, dtype: float64\ncount    1.008700e+06\nmean     7.107944e+01\nstd      2.027642e+01\nmin      8.200000e+00\n25%      5.540000e+01\n50%      7.220001e+01\n75%      8.800000e+01\nmax      1.000000e+02\nName: relative_humidity_rmax, dtype: float64\ncount    1.008700e+06\nmean     3.555765e+01\nstd      1.877470e+01\nmin      1.000000e+00\n25%      2.120000e+01\n50%      3.150000e+01\n75%      4.690000e+01\nmax      1.000000e+02\nName: relative_humidity_rmin, dtype: float64\ncount    1.008700e+06\nmean     2.606782e+00\nstd      7.298533e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      2.100000e+00\nmax      5.042000e+02\nName: precipitation_amount, dtype: float64\ncount    1.008700e+06\nmean     2.838897e+02\nstd      9.873665e+00\nmin      2.444000e+02\n25%      2.760000e+02\n50%      2.833000e+02\n75%      2.921000e+02\nmax      3.144000e+02\nName: air_temperature_tmmx, dtype: float64\ncount    1.008700e+06\nmean     4.192890e+00\nstd      2.023231e+00\nmin      5.000000e-01\n25%      2.700000e+00\n50%      3.700000e+00\n75%      5.200000e+00\nmax      1.860000e+01\nName: wind_speed, dtype: float64\ncount    1.008700e+06\nmean     2.397170e+03\nstd      6.647262e+02\nmin      7.584437e+02\n25%      1.948688e+03\n50%      2.481006e+03\n75%      2.895791e+03\nmax      3.823385e+03\nName: elevation, dtype: float64\ncount    1.008700e+06\nmean     8.998188e+01\nstd      2.818171e-02\nmin      8.939863e+01\n25%      8.998083e+01\n50%      8.998801e+01\n75%      8.999234e+01\nmax      8.999674e+01\nName: slope, dtype: float64\ncount    1.008700e+06\nmean    -3.909783e+03\nstd      4.385982e+03\nmin     -1.917768e+04\n25%     -6.479865e+03\n50%     -3.710969e+03\n75%     -5.754078e+02\nmax      9.069726e+03\nName: curvature, dtype: float64\ncount    1.008700e+06\nmean     1.722213e+02\nstd      1.031039e+02\nmin      4.159291e-01\n25%      9.217068e+01\n50%      1.691301e+02\n75%      2.498582e+02\nmax      3.586563e+02\nName: aspect, dtype: float64\ncount    1.008700e+06\nmean     6.965357e-02\nstd      5.171924e-01\nmin     -7.853438e-01\n25%     -3.907275e-01\n50%      1.334633e-01\n75%      5.767317e-01\nmax      7.853980e-01\nName: eastness, dtype: float64\ncount    1.008700e+06\nmean    -4.509867e-02\nstd      6.412035e-01\nmin     -7.853694e-01\n25%     -7.356204e-01\n50%     -1.913248e-01\n75%      6.495535e-01\nmax      7.853850e-01\nName: northness, dtype: float64\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.04118521859819647\nMSE is 0.01923175954198464\nR2 score is 0.9993023539305406\nRMSE is 0.138678619628206\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409005655.joblib\n",
  "history_begin_time" : 1695516939134,
  "history_end_time" : 1695517016699,
  "history_notes" : "this is a good run with single time input",
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "o3zxlj2ipb2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695515862387,
  "history_end_time" : 1695515862387,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "OcbdyL9bmuhQ",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        \n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        \n        \n        for column in data.columns:\n          data[column] = pd.to_numeric(data[column], errors='coerce')\n          print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-23\ntest start date:  2022-08-22\ntest end date:  2023-09-23\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\ncount    1.022700e+06\nmean     4.419400e+04\nstd      4.217545e+02\nmin      4.346400e+04\n25%      4.382900e+04\n50%      4.419400e+04\n75%      4.455900e+04\nmax      4.492400e+04\nName: date, dtype: float64\ncount    1.022700e+06\nmean     4.165838e+01\nstd      3.632464e+00\nmin      3.335825e+01\n25%      3.891814e+01\n50%      4.107190e+01\n75%      4.461398e+01\nmax      4.897107e+01\nName: lat, dtype: float64\ncount    1.022700e+06\nmean    -1.141534e+02\nstd      5.423376e+00\nmin     -1.234486e+02\n25%     -1.197812e+02\n50%     -1.137737e+02\n75%     -1.100367e+02\nmax     -1.051948e+02\nName: lon, dtype: float64\ncount    1.022700e+06\nmean     5.046323e+01\nstd      9.547185e+01\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      3.500000e+01\nmax      2.550000e+02\nName: SWE, dtype: float64\ncount    1.022700e+06\nmean     2.434436e+02\nstd      5.313797e+00\nmin      2.410000e+02\n25%      2.410000e+02\n50%      2.410000e+02\n75%      2.410000e+02\nmax      2.550000e+02\nName: Flag, dtype: float64\ncount    1.022700e+06\nmean    -1.038741e+01\nstd      1.165852e+02\nmin     -9.990000e+02\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\ncount    1.022700e+06\nmean     5.113495e+05\nstd      2.952282e+05\nmin      0.000000e+00\n25%      2.556748e+05\n50%      5.113495e+05\n75%      7.670242e+05\nmax      1.022699e+06\nName: Unnamed: 0, dtype: float64\ncount    1.022700e+06\nmean     2.720187e+02\nstd      8.384982e+00\nmin      2.328000e+02\n25%      2.663000e+02\n50%      2.722000e+02\n75%      2.784000e+02\nmax      2.983000e+02\nName: air_temperature_tmmn, dtype: float64\ncount    1.022700e+06\nmean     3.943372e+00\nstd      2.478664e+00\nmin      0.000000e+00\n25%      1.900000e+00\n50%      3.500000e+00\n75%      5.800000e+00\nmax      1.510000e+01\nName: potential_evapotranspiration, dtype: float64\ncount    1.022700e+06\nmean     6.258550e-01\nstd      5.233133e-01\nmin      0.000000e+00\n25%      2.100000e-01\n50%      4.700000e-01\n75%      9.300000e-01\nmax      3.870000e+00\nName: mean_vapor_pressure_deficit, dtype: float64\ncount    1.022700e+06\nmean     7.123571e+01\nstd      2.030150e+01\nmin      8.200000e+00\n25%      5.560000e+01\n50%      7.240000e+01\n75%      8.820001e+01\nmax      1.000000e+02\nName: relative_humidity_rmax, dtype: float64\ncount    1.022700e+06\nmean     3.578010e+01\nstd      1.888091e+01\nmin      1.000000e+00\n25%      2.130000e+01\n50%      3.160000e+01\n75%      4.740000e+01\nmax      1.000000e+02\nName: relative_humidity_rmin, dtype: float64\ncount    1.022700e+06\nmean     2.689152e+00\nstd      7.456402e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      2.200000e+00\nmax      5.042000e+02\nName: precipitation_amount, dtype: float64\ncount    1.022700e+06\nmean     2.837481e+02\nstd      9.912702e+00\nmin      2.444000e+02\n25%      2.759000e+02\n50%      2.831000e+02\n75%      2.920000e+02\nmax      3.144000e+02\nName: air_temperature_tmmx, dtype: float64\ncount    1.022700e+06\nmean     4.214964e+00\nstd      2.043559e+00\nmin      5.000000e-01\n25%      2.800000e+00\n50%      3.700000e+00\n75%      5.200000e+00\nmax      1.860000e+01\nName: wind_speed, dtype: float64\ncount    1.022700e+06\nmean     2.397170e+03\nstd      6.647262e+02\nmin      7.584437e+02\n25%      1.948688e+03\n50%      2.481006e+03\n75%      2.895791e+03\nmax      3.823385e+03\nName: elevation, dtype: float64\ncount    1.022700e+06\nmean     8.998188e+01\nstd      2.818171e-02\nmin      8.939863e+01\n25%      8.998083e+01\n50%      8.998801e+01\n75%      8.999234e+01\nmax      8.999674e+01\nName: slope, dtype: float64\ncount    1.022700e+06\nmean    -3.909783e+03\nstd      4.385982e+03\nmin     -1.917768e+04\n25%     -6.479865e+03\n50%     -3.710969e+03\n75%     -5.754078e+02\nmax      9.069726e+03\nName: curvature, dtype: float64\ncount    1.022700e+06\nmean     1.722213e+02\nstd      1.031039e+02\nmin      4.159291e-01\n25%      9.217068e+01\n50%      1.691301e+02\n75%      2.498582e+02\nmax      3.586563e+02\nName: aspect, dtype: float64\ncount    1.022700e+06\nmean     6.965357e-02\nstd      5.171924e-01\nmin     -7.853438e-01\n25%     -3.907275e-01\n50%      1.334633e-01\n75%      5.767317e-01\nmax      7.853980e-01\nName: eastness, dtype: float64\ncount    1.022700e+06\nmean    -4.509867e-02\nstd      6.412035e-01\nmin     -7.853694e-01\n25%     -7.356204e-01\n50%     -1.913248e-01\n75%      6.495535e-01\nmax      7.853850e-01\nName: northness, dtype: float64\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.04118521859819647\nMSE is 0.019231759541984646\nR2 score is 0.9993023539305406\nRMSE is 0.13867861962820602\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232309230152.joblib\n",
  "history_begin_time" : 1695510037968,
  "history_end_time" : 1695510114132,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "OJB1gOLpNHvG",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        \n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        \n        \n        for column in df.columns:\n          data[column] = pd.to_numeric(data[column], errors='coerce')\n          print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-23\ntest start date:  2022-08-22\ntest end date:  2023-09-23\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/OJB1gOLpNHvG/model_creation_et.py\", line 78, in <module>\n    hole.preprocessing()\n  File \"/home/chetana/gw-workspace/OJB1gOLpNHvG/model_creation_et.py\", line 36, in preprocessing\n    for column in df.columns:\nNameError: name 'df' is not defined\n",
  "history_begin_time" : 1695509883508,
  "history_end_time" : 1695509887821,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "NZW00kwn82tk",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n        \n        \n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        \n        \n        for column in df.columns:\n          data[column] = pd.to_numeric(data[column], errors='coerce')\n\t\t  print(data[column].describe())\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/NZW00kwn82tk/model_creation_et.py\", line 38\n    print(data[column].describe())\nTabError: inconsistent use of tabs and spaces in indentation\n",
  "history_begin_time" : 1695509871186,
  "history_end_time" : 1695509871238,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "nRq1unqaokFS",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-23\ntest start date:  2022-08-22\ntest end date:  2023-09-23\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.04118521859819647\nMSE is 0.019231759541984646\nR2 score is 0.9993023539305406\nRMSE is 0.13867861962820602\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232309222136.joblib\n",
  "history_begin_time" : 1695507621632,
  "history_end_time" : 1695507697311,
  "history_notes" : "filter out all the rows with snotel values are -999",
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "g31ZYj3L6VHU",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        # remove all the rows that has swe_value is -999\n        data = data[(data['swe_value'] != -999)\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/g31ZYj3L6VHU/model_creation_et.py\", line 35\n    print(\"get swe statistics\")\n    ^\nSyntaxError: invalid syntax\n",
  "history_begin_time" : 1695507608820,
  "history_end_time" : 1695507608873,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "q4NMFmJmxJV2",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-23\ntest start date:  2022-08-22\ntest end date:  2023-09-23\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\ncount    1.022700e+06\nmean    -1.038741e+01\nstd      1.165852e+02\nmin     -9.990000e+02\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.16053394446074246\nMSE is 12.979812945726998\nR2 score is 0.9990612195729837\nRMSE is 3.6027507470996376\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232309221830.joblib\n",
  "history_begin_time" : 1695507442850,
  "history_end_time" : 1695507511043,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "rQMkWCTFD3ef",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        print(\"get swe statistics\")\n        data[\"swe_value\"].describe()\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-23\ntest start date:  2022-08-22\ntest end date:  2023-09-23\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nget swe statistics\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.16053394446074246\nMSE is 12.979812945726998\nR2 score is 0.9990612195729837\nRMSE is 3.6027507470996376\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232309221708.joblib\n",
  "history_begin_time" : 1695507361018,
  "history_end_time" : 1695507429955,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "zT13CRVkykEQ",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data[\"swe_value\"].describe()\n        \n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-23\ntest start date:  2022-08-22\ntest end date:  2023-09-23\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.16053394446074246\nMSE is 12.979812945726998\nR2 score is 0.9990612195729837\nRMSE is 3.6027507470996376\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232309221403.joblib\n",
  "history_begin_time" : 1695507177204,
  "history_end_time" : 1695507244480,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "cehkpk8gmm2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695506423837,
  "history_end_time" : 1695506423837,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "nDlh0wcDHzOB",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-23\ntest start date:  2022-08-21\ntest end date:  2023-09-23\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.16053394446074246\nMSE is 12.979812945726998\nR2 score is 0.9990612195729837\nRMSE is 3.6027507470996376\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232309213724.joblib\n",
  "history_begin_time" : 1695504978037,
  "history_end_time" : 1695505045071,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ZVRHEHpWD8gw",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-1, inplace=True)\n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-23\ntest start date:  2022-08-21\ntest end date:  2023-09-23\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.03204651901828608\nMSE is 0.012338813175906847\nR2 score is 0.9995505484963834\nRMSE is 0.11108021055033541\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232309161426.joblib\n",
  "history_begin_time" : 1695485595285,
  "history_end_time" : 1695485667151,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "WdjAbq1VfvzF",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-1, inplace=True)\n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        print(\"output feature: \", y.columns)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-23\ntest start date:  2022-08-21\ntest end date:  2023-09-23\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ninput features and order:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/WdjAbq1VfvzF/model_creation_et.py\", line 64, in <module>\n    hole.preprocessing()\n  File \"/home/chetana/gw-workspace/WdjAbq1VfvzF/model_creation_et.py\", line 40, in preprocessing\n    print(\"output feature: \", y.columns)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\", line 5902, in __getattr__\n    return object.__getattribute__(self, name)\nAttributeError: 'Series' object has no attribute 'columns'\n",
  "history_begin_time" : 1695485570009,
  "history_end_time" : 1695485575241,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "1p61q591oys",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695418741332,
  "history_end_time" : 1695418741332,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "76J4jahYIYCp",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-1, inplace=True)\n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-22\ntest start date:  2022-03-21\ntest end date:  2023-09-22\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.03204651901828608\nMSE is 0.012338813175906847\nR2 score is 0.9995505484963834\nRMSE is 0.11108021055033541\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232209213743.joblib\n",
  "history_begin_time" : 1695418592123,
  "history_end_time" : 1695418664191,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "6vltonehnnu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695417619670,
  "history_end_time" : 1695417619670,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "mq68zgagfl1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695417171272,
  "history_end_time" : 1695417171272,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ejhkftcb2kw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695417052726,
  "history_end_time" : 1695417052726,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8tv9prsrjqa",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695416916014,
  "history_end_time" : 1695416916014,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "RqcaX3dt8P2y",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-1, inplace=True)\n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-20\ntest start date:  2022-03-19\ntest end date:  2023-09-20\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.032046519018286077\nMSE is 0.012338813175906847\nR2 score is 0.9995505484963834\nRMSE is 0.11108021055033541\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232009042228.joblib\n",
  "history_begin_time" : 1695183676880,
  "history_end_time" : 1695183749089,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "qHjYM16c9SeC",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-1, inplace=True)\n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-20\ntest start date:  2022-03-19\ntest end date:  2023-09-20\n/home/chetana\ndownloading http://www.northwestknowledge.net/metdata/data/tmmn_2019.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/tmmn_2019.nc\ndownloading http://www.northwestknowledge.net/metdata/data/tmmn_2020.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/tmmn_2020.nc\ndownloading http://www.northwestknowledge.net/metdata/data/tmmn_2021.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/tmmn_2021.nc\ndownloading http://www.northwestknowledge.net/metdata/data/tmmn_2022.nc\ndownloading http://www.northwestknowledge.net/metdata/data/tmmx_2019.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/tmmx_2019.nc\ndownloading http://www.northwestknowledge.net/metdata/data/tmmx_2020.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/tmmx_2020.nc\ndownloading http://www.northwestknowledge.net/metdata/data/tmmx_2021.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/tmmx_2021.nc\ndownloading http://www.northwestknowledge.net/metdata/data/tmmx_2022.nc\ndownloading http://www.northwestknowledge.net/metdata/data/pr_2019.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/pr_2019.nc\ndownloading http://www.northwestknowledge.net/metdata/data/pr_2020.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/pr_2020.nc\ndownloading http://www.northwestknowledge.net/metdata/data/pr_2021.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/pr_2021.nc\ndownloading http://www.northwestknowledge.net/metdata/data/pr_2022.nc\ndownloading http://www.northwestknowledge.net/metdata/data/vpd_2019.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/vpd_2019.nc\ndownloading http://www.northwestknowledge.net/metdata/data/vpd_2020.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/vpd_2020.nc\ndownloading http://www.northwestknowledge.net/metdata/data/vpd_2021.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/vpd_2021.nc\ndownloading http://www.northwestknowledge.net/metdata/data/vpd_2022.nc\ndownloading http://www.northwestknowledge.net/metdata/data/etr_2019.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/etr_2019.nc\ndownloading http://www.northwestknowledge.net/metdata/data/etr_2020.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/etr_2020.nc\ndownloading http://www.northwestknowledge.net/metdata/data/etr_2021.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/etr_2021.nc\ndownloading http://www.northwestknowledge.net/metdata/data/etr_2022.nc\ndownloading http://www.northwestknowledge.net/metdata/data/rmax_2019.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/rmax_2019.nc\ndownloading http://www.northwestknowledge.net/metdata/data/rmax_2020.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/rmax_2020.nc\ndownloading http://www.northwestknowledge.net/metdata/data/rmax_2021.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/rmax_2021.nc\ndownloading http://www.northwestknowledge.net/metdata/data/rmax_2022.nc\ndownloading http://www.northwestknowledge.net/metdata/data/rmin_2019.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/rmin_2019.nc\ndownloading http://www.northwestknowledge.net/metdata/data/rmin_2020.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/rmin_2020.nc\ndownloading http://www.northwestknowledge.net/metdata/data/rmin_2021.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/rmin_2021.nc\ndownloading http://www.northwestknowledge.net/metdata/data/rmin_2022.nc\ndownloading http://www.northwestknowledge.net/metdata/data/vs_2019.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/vs_2019.nc\ndownloading http://www.northwestknowledge.net/metdata/data/vs_2020.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/vs_2020.nc\ndownloading http://www.northwestknowledge.net/metdata/data/vs_2021.nc\ndownload_file\nFile downloaded successfully and saved as: /home/chetana/gridmet_test_run/gridmet_climatology/vs_2021.nc\ndownloading http://www.northwestknowledge.net/metdata/data/vs_2022.nc\nreading values from /home/chetana/gridmet_test_run/gridmet_climatology/rmin_2020.nc\n['relative_humidity']\ngot result_df :           day        lat         lon  relative_humidity\n0 2020-01-01  41.993149 -120.178715          68.400002\n1 2020-01-02  41.993149 -120.178715          55.700001\n2 2020-01-03  41.993149 -120.178715          30.300001\n3 2020-01-04  41.993149 -120.178715          59.600002\n4 2020-01-05  41.993149 -120.178715          74.300003\ncompleted extracting data for /home/chetana/gridmet_test_run/gridmet_climatology/rmin_2020.nc\nreading values from /home/chetana/gridmet_test_run/gridmet_climatology/rmin_2021.nc\n['relative_humidity']\ngot result_df :           day        lat         lon  relative_humidity\n0 2021-01-01  41.993149 -120.178715          39.000000\n1 2021-01-02  41.993149 -120.178715          74.800003\n2 2021-01-03  41.993149 -120.178715          70.400002\n3 2021-01-04  41.993149 -120.178715          65.599998\n4 2021-01-05  41.993149 -120.178715          51.299999\ncompleted extracting data for /home/chetana/gridmet_test_run/gridmet_climatology/rmin_2021.nc\nreading values from /home/chetana/gridmet_test_run/gridmet_climatology/tmmx_2019.nc\n['air_temperature']\ngot result_df :           day        lat         lon  air_temperature\n0 2019-01-01  41.993149 -120.178715       271.299988\n1 2019-01-02  41.993149 -120.178715       276.100006\n2 2019-01-03  41.993149 -120.178715       274.399994\n3 2019-01-04  41.993149 -120.178715       277.600006\n4 2019-01-05  41.993149 -120.178715       273.000000\ncompleted extracting data for /home/chetana/gridmet_test_run/gridmet_climatology/tmmx_2019.nc\nreading values from /home/chetana/gridmet_test_run/gridmet_climatology/vpd_2021.nc\n['mean_vapor_pressure_deficit']\ngot result_df :           day        lat         lon  mean_vapor_pressure_deficit\n0 2021-01-01  41.993149 -120.178715                         0.25\n1 2021-01-02  41.993149 -120.178715                         0.10\n2 2021-01-03  41.993149 -120.178715                         0.14\n3 2021-01-04  41.993149 -120.178715                         0.12\n4 2021-01-05  41.993149 -120.178715                         0.17\ncompleted extracting data for /home/chetana/gridmet_test_run/gridmet_climatology/vpd_2021.nc\nreading values from /home/chetana/gridmet_test_run/gridmet_climatology/tmmx_2021.nc\n['air_temperature']\ngot result_df :           day        lat         lon  air_temperature\n0 2021-01-01  41.993149 -120.178715       275.299988\n1 2021-01-02  41.993149 -120.178715       270.299988\n2 2021-01-03  41.993149 -120.178715       272.000000\n3 2021-01-04  41.993149 -120.178715       273.100006\n4 2021-01-05  41.993149 -120.178715       271.299988\ncompleted extracting data for /home/chetana/gridmet_test_run/gridmet_climatology/tmmx_2021.nc\nreading values from /home/chetana/gridmet_test_run/gridmet_climatology/tmmn_2021.nc\n['air_temperature']\ngot result_df :           day        lat         lon  air_temperature\n\nStream closed",
  "history_begin_time" : 1695181505166,
  "history_end_time" : 1695181862957,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "3lii5cmbrsp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695106488972,
  "history_end_time" : 1695106488972,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wua7sjfad6u",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695106316193,
  "history_end_time" : 1695106316193,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "P0Df13PW3ECJ",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.fillna(-1, inplace=True)\n        #data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-19\ntest start date:  2022-11-08\ntest end date:  2023-09-19\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.03204651901828608\nMSE is 0.012338813175906847\nR2 score is 0.9995505484963834\nRMSE is 0.11108021055033541\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231909064713.joblib\n",
  "history_begin_time" : 1695105962305,
  "history_end_time" : 1695106034703,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "AWi7EyZNrRPC",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path, index=False)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data.fillna(-1, inplace=True)\n        #data = data.dropna(subset=['swe_value'])\n        #data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-19\ntest start date:  2022-11-08\ntest end date:  2023-09-19\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/AWi7EyZNrRPC/model_creation_et.py\", line 60, in <module>\n    hole.preprocessing()\n  File \"/home/chetana/gw-workspace/AWi7EyZNrRPC/model_creation_et.py\", line 23, in preprocessing\n    data = pd.read_csv(training_data_path, index=False)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 211, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\nTypeError: read_csv() got an unexpected keyword argument 'index'\n",
  "history_begin_time" : 1695105938017,
  "history_end_time" : 1695105941435,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "t7x6j2oe4gg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695054045020,
  "history_end_time" : 1695054045020,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "oa76rrum9qe",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695054019753,
  "history_end_time" : 1695054032321,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "kwvy6u7x98t",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695053979884,
  "history_end_time" : 1695054019272,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "jubu7gk4fiy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695053793399,
  "history_end_time" : 1695053793399,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "bhbmmsvk33k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695053733401,
  "history_end_time" : 1695053733401,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "j6dq4311j4v",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694971144816,
  "history_end_time" : 1694972839686,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "kw1wdamj6c7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970707918,
  "history_end_time" : 1694970707918,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rgafsfd0xef",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594755,
  "history_end_time" : 1694970594755,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "p7aevsltpml",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970131540,
  "history_end_time" : 1694970131540,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "lbb12p0cjoo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694969350047,
  "history_end_time" : 1694969350047,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "g2iY6d7LVeBd",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-17\ntest start date:  2023-06-05\ntest end date:  2023-09-17\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned.csv\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.04118521859819645\nMSE is 0.01923175954198464\nR2 score is 0.9993023539305406\nRMSE is 0.138678619628206\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231709164339.joblib\n",
  "history_begin_time" : 1694968944152,
  "history_end_time" : 1694969020522,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "5r2MpZFJV2mo",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/5r2MpZFJV2mo/model_creation_et.py\", line 10, in <module>\n    from model_creation_rf import RandomForestHole\n  File \"/home/chetana/gw-workspace/5r2MpZFJV2mo/model_creation_rf.py\", line 4, in <module>\n    import seaborn as sns\nModuleNotFoundError: No module named 'seaborn'\n",
  "history_begin_time" : 1694968924075,
  "history_end_time" : 1694968925498,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "t4sjhnV98O2k",
  "history_input" : "import pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/t4sjhnV98O2k/model_creation_et.py\", line 3, in <module>\n    import sklearn\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/__init__.py\", line 83, in <module>\n    from .base import clone\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 19, in <module>\n    from .utils import _IS_32BIT\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/__init__.py\", line 19, in <module>\n    from . import _joblib, metadata_routing\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/_joblib.py\", line 8, in <module>\n    from joblib import (\nImportError: cannot import name 'Memory' from 'joblib' (unknown location)\n",
  "history_begin_time" : 1694968314546,
  "history_end_time" : 1694968315106,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "HSwESbXlH0YB",
  "history_input" : "import pandas as pd\nimport joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/HSwESbXlH0YB/model_creation_et.py\", line 3, in <module>\n    from sklearn.model_selection import train_test_split\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/__init__.py\", line 82, in <module>\n    from .base import clone\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 17, in <module>\n    from .utils import _IS_32BIT\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/__init__.py\", line 24, in <module>\n    from . import _joblib\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/_joblib.py\", line 8, in <module>\n    from joblib import logger\nImportError: cannot import name 'logger' from 'joblib' (unknown location)\n",
  "history_begin_time" : 1694968197183,
  "history_end_time" : 1694968198209,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "qcjvMJHXphni",
  "history_input" : "import pandas as pd\nimport joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/qcjvMJHXphni/model_creation_et.py\", line 3, in <module>\n    from sklearn.model_selection import train_test_split\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/__init__.py\", line 82, in <module>\n    from .base import clone\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 17, in <module>\n    from .utils import _IS_32BIT\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/__init__.py\", line 21, in <module>\n    from . import _joblib\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/_joblib.py\", line 8, in <module>\n    from joblib import logger\nImportError: cannot import name 'logger' from 'joblib' (unknown location)\n",
  "history_begin_time" : 1694968168274,
  "history_end_time" : 1694968168862,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "PXvxxAukg96a",
  "history_input" : "import pandas as pd\nimport joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/PXvxxAukg96a/model_creation_et.py\", line 3, in <module>\n    from sklearn.model_selection import train_test_split\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/__init__.py\", line 83, in <module>\n    from .base import clone\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 19, in <module>\n    from .utils import _IS_32BIT\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/__init__.py\", line 19, in <module>\n    from . import _joblib, metadata_routing\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/_joblib.py\", line 8, in <module>\n    from joblib import (\nImportError: cannot import name 'Memory' from 'joblib' (unknown location)\n",
  "history_begin_time" : 1694967862951,
  "history_end_time" : 1694967863495,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "ywvUzqzJyDbs",
  "history_input" : "import pandas as pd\nimport joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/ywvUzqzJyDbs/model_creation_et.py\", line 3, in <module>\n    from sklearn.model_selection import train_test_split\nModuleNotFoundError: No module named 'sklearn'\n",
  "history_begin_time" : 1694967838508,
  "history_end_time" : 1694967838953,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "Xqd1x3zMb3S1",
  "history_input" : "import pandas as pd\nimport joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/Xqd1x3zMb3S1/model_creation_et.py\", line 3, in <module>\n    from sklearn.model_selection import train_test_split\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/__init__.py\", line 83, in <module>\n    from .base import clone\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 19, in <module>\n    from .utils import _IS_32BIT\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/__init__.py\", line 19, in <module>\n    from . import _joblib, metadata_routing\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/_joblib.py\", line 8, in <module>\n    from joblib import (\nImportError: cannot import name 'Memory' from 'joblib' (unknown location)\n",
  "history_begin_time" : 1694967807802,
  "history_end_time" : 1694967808407,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "ST2ZkEFB7NuM",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/ST2ZkEFB7NuM/model_creation_et.py\", line 2, in <module>\n    from sklearn.model_selection import train_test_split\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/__init__.py\", line 83, in <module>\n    from .base import clone\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 19, in <module>\n    from .utils import _IS_32BIT\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/__init__.py\", line 19, in <module>\n    from . import _joblib, metadata_routing\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/_joblib.py\", line 8, in <module>\n    from joblib import (\nImportError: cannot import name 'Memory' from 'joblib' (unknown location)\n",
  "history_begin_time" : 1694967314258,
  "history_end_time" : 1694967314842,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "JwAYVOM0ojNv",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        data = pd.read_csv(f'{working_dir}/final_merged_data_3yrs_cleaned.csv')\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-17\ntest start date:  2023-06-02\ntest end date:  2023-09-17\n/home/chetana\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.04054373450976585\nMSE is 0.019460380355903556\nR2 score is 0.9992940605441825\nRMSE is 0.13950046722467835\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231709073529.joblib\n",
  "history_begin_time" : 1694936050368,
  "history_end_time" : 1694936130379,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "2IL60E8cCSty",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        data = pd.read_csv(f'{working_dir}/final_merged_data_3yrs_cleaned.csv')\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-17\n\nStream closed",
  "history_begin_time" : 1694933333485,
  "history_end_time" : 1694934368281,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "PnMVuDEuXl5O",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        data = pd.read_csv(f'{working_dir}/final_merged_data_3yrs_cleaned.csv')\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/PnMVuDEuXl5O/model_creation_et.py\", line 9, in <module>\n    from model_creation_rf import RandomForestHole\n  File \"/home/chetana/gw-workspace/PnMVuDEuXl5O/model_creation_rf.py\", line 14, in <module>\n    import geopandas as gpd\nModuleNotFoundError: No module named 'geopandas'\n",
  "history_begin_time" : 1694932508760,
  "history_end_time" : 1694932510906,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "5uxpYGi1Zce8",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        data = pd.read_csv(f'{working_dir}/final_merged_data_3yrs_cleaned.csv')\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/5uxpYGi1Zce8/model_creation_et.py\", line 9, in <module>\n    from model_creation_rf import RandomForestHole\n  File \"/home/chetana/gw-workspace/5uxpYGi1Zce8/model_creation_rf.py\", line 14, in <module>\n    import geopandas as gpd\nModuleNotFoundError: No module named 'geopandas'\n",
  "history_begin_time" : 1694932415610,
  "history_end_time" : 1694932420903,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "ko43z5o93uu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694905307654,
  "history_end_time" : 1694905307654,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9jrqt33jlvc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887122,
  "history_end_time" : 1694897887122,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "uf9QlGjBqJdg",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom base_hole import BaseHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(BaseHole):\n    def __init__(self):\n        super().__init__()\n        self.classifier = self.get_model()\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        data = pd.read_csv(f'{working_dir}/final_merged_data_3yrs_cleaned.csv')\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data = data.dropna(subset=['swe_value'])\n        data = data.drop('Unnamed: 0', axis=1)\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def train(self):\n        self.classifier.fit(self.train_x, self.train_y)\n\n    def test(self):\n        self.test_y_results = self.classifier.predict(self.test_x)\n        print(self.test_y_results)\n        return self.test_y_results\n\n    def evaluate(self):\n        y_pred = self.test_y_results\n        mse = mean_squared_error(self.test_y, y_pred)\n        rmse = np.sqrt(mse)\n        mae = mean_absolute_error(self.test_y, y_pred)\n        r2 = r2_score(self.test_y, y_pred)\n\n        print(\"Mean Squared Error (MSE):\", mse)\n        print(\"Root Mean Squared Error (RMSE):\", rmse)\n        print(\"Mean Absolute Error (MAE):\", mae)\n        print(\"R-squared (R2):\", r2)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-11\n/home/chetana\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n[ 0.     0.068 18.1   ...  9.998  0.     0.   ]\nMean Squared Error (MSE): 0.01923175954198464\nRoot Mean Squared Error (RMSE): 0.138678619628206\nMean Absolute Error (MAE): 0.04118521859819645\nR-squared (R2): 0.9993023539305406\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231109162144.joblib\n",
  "history_begin_time" : 1694449231456,
  "history_end_time" : 1694449305690,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "dvxriBVTdSQM",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom base_hole import BaseHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(BaseHole):\n    def __init__(self):\n        super().__init__()\n        self.classifier = self.get_model()\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        data = pd.read_csv(f'{working_dir}/final_merged_data_3yrs_cleaned.csv')\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n        data = data.dropna(subset=['swe_value'])\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def train(self):\n        self.classifier.fit(self.train_x, self.train_y)\n\n    def test(self):\n        self.test_y_results = self.classifier.predict(self.test_x)\n        print(self.test_y_results)\n        return self.test_y_results\n\n    def evaluate(self):\n        y_pred = self.test_y_results\n        mse = mean_squared_error(self.test_y, y_pred)\n        rmse = np.sqrt(mse)\n        mae = mean_absolute_error(self.test_y, y_pred)\n        r2 = r2_score(self.test_y, y_pred)\n\n        print(\"Mean Squared Error (MSE):\", mse)\n        print(\"Root Mean Squared Error (RMSE):\", rmse)\n        print(\"Mean Absolute Error (MAE):\", mae)\n        print(\"R-squared (R2):\", r2)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-11\n/home/chetana\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'Unnamed: 0',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness'],\n      dtype='object')\n[ 0.     0.    17.959 ... 10.     0.194  0.   ]\nMean Squared Error (MSE): 0.009067688708238262\nRoot Mean Squared Error (RMSE): 0.09522441235438664\nMean Absolute Error (MAE): 0.02579866164370087\nR-squared (R2): 0.9996710629949083\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231109161531.joblib\n",
  "history_begin_time" : 1694448843878,
  "history_end_time" : 1694448932500,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "VIkVpdpAgKE5",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom base_hole import BaseHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(BaseHole):\n    def __init__(self):\n        super().__init__()\n        self.classifier = self.get_model()\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        data = pd.read_csv(f'{working_dir}/final_merged_data_3yrs_cleaned.csv')\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n\n        data = data.dropna(subset=['swe_value'])\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def train(self):\n        self.classifier.fit(self.train_x, self.train_y)\n\n    def test(self):\n        self.test_y_results = self.classifier.predict(self.test_x)\n        print(self.test_y_results)\n        return self.test_y_results\n\n    def evaluate(self):\n        y_pred = self.test_y_results\n        mse = mean_squared_error(self.test_y, y_pred)\n        rmse = np.sqrt(mse)\n        mae = mean_absolute_error(self.test_y, y_pred)\n        r2 = r2_score(self.test_y, y_pred)\n\n        print(\"Mean Squared Error (MSE):\", mse)\n        print(\"Root Mean Squared Error (RMSE):\", rmse)\n        print(\"Mean Absolute Error (MAE):\", mae)\n        print(\"R-squared (R2):\", r2)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-11\n/home/chetana\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'Unnamed: 0.2', 'Unnamed: 0.1',\n       'Unnamed: 0', 'air_temperature_tmmn', 'air_temperature_tmmn.1',\n       'air_temperature_tmmn.2', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmax.2', 'relative_humidity_rmax.1',\n       'relative_humidity_rmin', 'relative_humidity_rmin.2',\n       'relative_humidity_rmin.1', 'precipitation_amount',\n       'air_temperature_tmmx', 'air_temperature_tmmx.2',\n       'air_temperature_tmmx.1', 'wind_speed', 'elevation', 'slope',\n       'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/VIkVpdpAgKE5/model_creation_et.py\", line 80, in <module>\n    hole.train()\n  File \"/home/chetana/gw-workspace/VIkVpdpAgKE5/model_creation_et.py\", line 43, in train\n    self.classifier.fit(self.train_x, self.train_y)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 345, in fit\n    X, y = self._validate_data(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/base.py\", line 584, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1106, in check_X_y\n    X = check_array(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 921, in check_array\n    _assert_all_finite(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 161, in _assert_all_finite\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nExtraTreesRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
  "history_begin_time" : 1694410397580,
  "history_end_time" : 1694410420225,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "o3b7XcBttSLF",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom base_hole import BaseHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(BaseHole):\n    def __init__(self):\n        super().__init__()\n        self.classifier = self.get_model()\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        data = pd.read_csv(f'{working_dir}/final_merged_data_3_yrs.csv')\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n\n        data = data.dropna(subset=['swe_value'])\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def train(self):\n        self.classifier.fit(self.train_x, self.train_y)\n\n    def test(self):\n        self.test_y_results = self.classifier.predict(self.test_x)\n        print(self.test_y_results)\n        return self.test_y_results\n\n    def evaluate(self):\n        y_pred = self.test_y_results\n        mse = mean_squared_error(self.test_y, y_pred)\n        rmse = np.sqrt(mse)\n        mae = mean_absolute_error(self.test_y, y_pred)\n        r2 = r2_score(self.test_y, y_pred)\n\n        print(\"Mean Squared Error (MSE):\", mse)\n        print(\"Root Mean Squared Error (RMSE):\", rmse)\n        print(\"Mean Absolute Error (MAE):\", mae)\n        print(\"R-squared (R2):\", r2)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-11\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/o3b7XcBttSLF/model_creation_et.py\", line 79, in <module>\n    hole.preprocessing()\n  File \"/home/chetana/gw-workspace/o3b7XcBttSLF/model_creation_et.py\", line 23, in preprocessing\n    data = pd.read_csv(f'{working_dir}/final_merged_data_3_yrs.csv')\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 211, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 950, in read_csv\n    return _read(filepath_or_buffer, kwds)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 611, in _read\n    return parser.read(nrows)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1778, in read\n    ) = self._engine.read(  # type: ignore[attr-defined]\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 232, in read\n    data = _concatenate_chunks(chunks)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 402, in _concatenate_chunks\n    result[name] = np.concatenate(arrs)  # type: ignore[arg-type]\n  File \"<__array_function__ internals>\", line 180, in concatenate\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 1.94 GiB for an array with shape (260876568,) and data type float64\n",
  "history_begin_time" : 1694402136494,
  "history_end_time" : 1694402937425,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "axdRlsiN5IU4",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom base_hole import BaseHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(BaseHole):\n    def __init__(self):\n        super().__init__()\n        self.classifier = self.get_model()\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        data = pd.read_csv(f'{working_dir}/final_merged_data_3_yrs.csv')\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n\n        data = data.dropna(subset=['swe_value'])\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def train(self):\n        self.classifier.fit(self.train_x, self.train_y)\n\n    def test(self):\n        self.test_y_results = self.classifier.predict(self.test_x)\n        print(self.test_y_results)\n        return self.test_y_results\n\n    def evaluate(self):\n        y_pred = self.test_y_results\n        mse = mean_squared_error(self.test_y, y_pred)\n        rmse = np.sqrt(mse)\n        mae = mean_absolute_error(self.test_y, y_pred)\n        r2 = r2_score(self.test_y, y_pred)\n\n        print(\"Mean Squared Error (MSE):\", mse)\n        print(\"Root Mean Squared Error (RMSE):\", rmse)\n        print(\"Mean Absolute Error (MAE):\", mae)\n        print(\"R-squared (R2):\", r2)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-09-11\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/axdRlsiN5IU4/model_creation_et.py\", line 79, in <module>\n    hole.preprocessing()\n  File \"/home/chetana/gw-workspace/axdRlsiN5IU4/model_creation_et.py\", line 23, in preprocessing\n    data = pd.read_csv(f'{working_dir}/final_merged_data_3_yrs.csv')\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 211, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 950, in read_csv\n    return _read(filepath_or_buffer, kwds)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 611, in _read\n    return parser.read(nrows)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1778, in read\n    ) = self._engine.read(  # type: ignore[attr-defined]\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 232, in read\n    data = _concatenate_chunks(chunks)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 402, in _concatenate_chunks\n    result[name] = np.concatenate(arrs)  # type: ignore[arg-type]\n  File \"<__array_function__ internals>\", line 180, in concatenate\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 1.94 GiB for an array with shape (260876568,) and data type float64\n",
  "history_begin_time" : 1694391223370,
  "history_end_time" : 1694391960461,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "S8O3DzqO9YLd",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom base_hole import BaseHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(BaseHole):\n    def __init__(self):\n        super().__init__()\n        self.classifier = self.get_model()\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        data = pd.read_csv(f'{working_dir}/final_merged_data_3_yrs.csv')\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n\n        data = data.dropna(subset=['swe_value'])\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def train(self):\n        self.classifier.fit(self.train_x, self.train_y)\n\n    def test(self):\n        self.test_y_results = self.classifier.predict(self.test_x)\n        print(self.test_y_results)\n        return self.test_y_results\n\n    def evaluate(self):\n        y_pred = self.test_y_results\n        mse = mean_squared_error(self.test_y, y_pred)\n        rmse = np.sqrt(mse)\n        mae = mean_absolute_error(self.test_y, y_pred)\n        r2 = r2_score(self.test_y, y_pred)\n\n        print(\"Mean Squared Error (MSE):\", mse)\n        print(\"Root Mean Squared Error (RMSE):\", rmse)\n        print(\"Mean Absolute Error (MAE):\", mae)\n        print(\"R-squared (R2):\", r2)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/S8O3DzqO9YLd/model_creation_et.py\", line 79, in <module>\n    hole.preprocessing()\n  File \"/home/chetana/gw-workspace/S8O3DzqO9YLd/model_creation_et.py\", line 23, in preprocessing\n    data = pd.read_csv(f'{working_dir}/final_merged_data_3_yrs.csv')\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 211, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 950, in read_csv\n    return _read(filepath_or_buffer, kwds)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 611, in _read\n    return parser.read(nrows)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1795, in read\n    df = DataFrame(col_dict, columns=columns, index=index)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/frame.py\", line 664, in __init__\n    mgr = dict_to_mgr(data, index, columns, dtype=dtype, copy=copy, typ=manager)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/construction.py\", line 493, in dict_to_mgr\n    return arrays_to_mgr(arrays, columns, index, dtype=dtype, typ=typ, consolidate=copy)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/construction.py\", line 154, in arrays_to_mgr\n    return create_block_manager_from_column_arrays(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/managers.py\", line 2199, in create_block_manager_from_column_arrays\n    blocks = _form_blocks(arrays, consolidate)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/managers.py\", line 2273, in _form_blocks\n    values, placement = _stack_arrays(list(tup_block), dtype)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/managers.py\", line 2312, in _stack_arrays\n    stacked = np.empty(shape, dtype=dtype)\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 25.0 GiB for an array with shape (13, 257808468) and data type float64\n",
  "history_begin_time" : 1692212520391,
  "history_end_time" : 1692213100724,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "R9u67QWwzhLb",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom base_hole import BaseHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(BaseHole):\n    def __init__(self):\n        super().__init__()\n        self.classifier = self.get_model()\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        data = pd.read_csv(f'{working_dir}/final_merged_data.csv')\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n\n        data = data.dropna(subset=['swe_value'])\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def train(self):\n        self.classifier.fit(self.train_x, self.train_y)\n\n    def test(self):\n        self.test_y_results = self.classifier.predict(self.test_x)\n        print(self.test_y_results)\n        return self.test_y_results\n\n    def evaluate(self):\n        y_pred = self.test_y_results\n        mse = mean_squared_error(self.test_y, y_pred)\n        rmse = np.sqrt(mse)\n        mae = mean_absolute_error(self.test_y, y_pred)\n        r2 = r2_score(self.test_y, y_pred)\n\n        print(\"Mean Squared Error (MSE):\", mse)\n        print(\"Root Mean Squared Error (RMSE):\", rmse)\n        print(\"Mean Absolute Error (MAE):\", mae)\n        print(\"R-squared (R2):\", r2)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\nrequired features: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity', 'precipitation_amount', 'wind_speed', 'elevation',\n       'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n",
  "history_begin_time" : 1692207360402,
  "history_end_time" : 1692207407545,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "KFjNkIUbl4HW",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom base_hole import BaseHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(BaseHole):\n    def __init__(self):\n        super().__init__()\n        self.classifier = self.get_model()\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        data = pd.read_csv(f'{working_dir}/final_merged_data.csv')\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n\n        data = data.dropna(subset=['swe_value'])\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def train(self):\n        self.classifier.fit(self.train_x, self.train_y)\n\n    def test(self):\n        self.test_y_results = self.classifier.predict(self.test_x)\n        print(self.test_y_results)\n        return self.test_y_results\n\n    def evaluate(self):\n        y_pred = self.test_y_results\n        mse = mean_squared_error(self.test_y, y_pred)\n        rmse = np.sqrt(mse)\n        mae = mean_absolute_error(self.test_y, y_pred)\n        r2 = r2_score(self.test_y, y_pred)\n\n        print(\"Mean Squared Error (MSE):\", mse)\n        print(\"Root Mean Squared Error (RMSE):\", rmse)\n        print(\"Mean Absolute Error (MAE):\", mae)\n        print(\"R-squared (R2):\", r2)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\n#hole = ETHole()\n#hole.preprocessing()\n#hole.train()\n#hole.test()\n#hole.evaluate()\n#hole.save()\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\n",
  "history_begin_time" : 1692207343906,
  "history_end_time" : 1692207346178,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "J25fAku3u3mU",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom base_hole import BaseHole\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\nclass ETHole(BaseHole):\n    def __init__(self):\n        super().__init__()\n        self.classifier = self.get_model()\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        data = pd.read_csv(f'{working_dir}/final_merged_data.csv')\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n\n        data = data.dropna(subset=['swe_value'])\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def train(self):\n        self.classifier.fit(self.train_x, self.train_y)\n\n    def test(self):\n        self.test_y_results = self.classifier.predict(self.test_x)\n        print(self.test_y_results)\n        return self.test_y_results\n\n    def evaluate(self):\n        y_pred = self.test_y_results\n        mse = mean_squared_error(self.test_y, y_pred)\n        rmse = np.sqrt(mse)\n        mae = mean_absolute_error(self.test_y, y_pred)\n        r2 = r2_score(self.test_y, y_pred)\n\n        print(\"Mean Squared Error (MSE):\", mse)\n        print(\"Root Mean Squared Error (RMSE):\", rmse)\n        print(\"Mean Absolute Error (MAE):\", mae)\n        print(\"R-squared (R2):\", r2)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\n[0.    0.    0.    ... 0.    5.727 7.5  ]\nMean Squared Error (MSE): 0.012246736115475262\nRoot Mean Squared Error (RMSE): 0.11066497239630642\nMean Absolute Error (MAE): 0.028775089404619033\nR-squared (R2): 0.9995573145490274\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231608170807.joblib\n",
  "history_begin_time" : 1692205623853,
  "history_end_time" : 1692205688215,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "nYAyGBpoCdjS",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom base_hole import BaseHole\nfrom snowcast_utils import work_dir\n\nclass ETHole(BaseHole):\n    def __init__(self):\n        super().__init__()\n        self.classifier = self.get_model()\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        data = pd.read_csv(f'{working_dir}/final_merged_data.csv')\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n\n        data = data.dropna(subset=['swe_value'])\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def train(self):\n        self.classifier.fit(self.train_x, self.train_y)\n\n    def test(self):\n        self.test_y_results = self.classifier.predict(self.test_x)\n        print(self.test_y_results)\n        return self.test_y_results\n\n    def evaluate(self):\n        y_pred = self.test_y_results\n        mse = mean_squared_error(self.test_y, y_pred)\n        rmse = np.sqrt(mse)\n        mae = mean_absolute_error(self.test_y, y_pred)\n        r2 = r2_score(self.test_y, y_pred)\n\n        print(\"Mean Squared Error (MSE):\", mse)\n        print(\"Root Mean Squared Error (RMSE):\", rmse)\n        print(\"Mean Absolute Error (MAE):\", mae)\n        print(\"R-squared (R2):\", r2)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/nYAyGBpoCdjS/model_creation_et.py\", line 76, in <module>\n    hole.preprocessing()\n  File \"/home/chetana/gw-workspace/nYAyGBpoCdjS/model_creation_et.py\", line 21, in preprocessing\n    data = pd.read_csv(f'{working_dir}/final_merged_data.csv')\nNameError: name 'working_dir' is not defined\n",
  "history_begin_time" : 1692205604873,
  "history_end_time" : 1692205607185,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "MqqZMNs1OrsZ",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom base_hole import BaseHole\nfrom snowcast_utils import work_dir\n\nclass ETHole(BaseHole):\n    def __init__(self):\n        super().__init__()\n        self.classifier = self.get_model()\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        data = pd.read_csv(f'{working_dir}/final_merged_data.csv')\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n\n        data = data.dropna(subset=['swe_value'])\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def train(self):\n        self.classifier.fit(self.train_x, self.train_y)\n\n    def test(self):\n        self.test_y_results = self.classifier.predict(self.test_x)\n        print(self.test_y_results)\n        return self.test_y_results\n\n    def evaluate(self):\n        y_pred = self.test_y_results\n        mse = mean_squared_error(self.test_y, y_pred)\n        rmse = np.sqrt(mse)\n        mae = mean_absolute_error(self.test_y, y_pred)\n        r2 = r2_score(self.test_y, y_pred)\n\n        print(\"Mean Squared Error (MSE):\", mse)\n        print(\"Root Mean Squared Error (RMSE):\", rmse)\n        print(\"Mean Absolute Error (MAE):\", mae)\n        print(\"R-squared (R2):\", r2)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/MqqZMNs1OrsZ/model_creation_et.py\", line 9, in <module>\n    from base_hole import BaseHole\n  File \"/home/chetana/gw-workspace/MqqZMNs1OrsZ/base_hole.py\", line 44\n    train[['swe_value']].to_numpy().astype('float')\nIndentationError: unexpected indent\n",
  "history_begin_time" : 1692205546154,
  "history_end_time" : 1692205548509,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "sP5GUrvlsQpj",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom base_hole import BaseHole\nfrom snowcast_utils import work_dir\n\nclass ETHole(BaseHole):\n    def __init__(self):\n        super().__init__()\n        self.classifier = self.get_model()\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        data = pd.read_csv(f'{working_dir}/final_merged_data.csv')\n        #data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n\n        data = data.dropna(subset=['swe_value'])\n        #data = data[data['swe_value'] != 0]\n        \n        X = data.drop('swe_value', axis=1)\n        y = data['swe_value']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def train(self):\n        self.classifier.fit(self.train_x, self.train_y)\n\n    def test(self):\n        self.test_y_results = self.classifier.predict(self.test_x)\n        print(self.test_y_results)\n        return self.test_y_results\n\n    def evaluate(self):\n        y_pred = self.test_y_results\n        mse = mean_squared_error(self.test_y, y_pred)\n        rmse = np.sqrt(mse)\n        mae = mean_absolute_error(self.test_y, y_pred)\n        r2 = r2_score(self.test_y, y_pred)\n\n        print(\"Mean Squared Error (MSE):\", mse)\n        print(\"Root Mean Squared Error (RMSE):\", rmse)\n        print(\"Mean Absolute Error (MAE):\", mae)\n        print(\"R-squared (R2):\", r2)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/sP5GUrvlsQpj/model_creation_et.py\", line 9, in <module>\n    from base_hole import BaseHole\n  File \"/home/chetana/gw-workspace/sP5GUrvlsQpj/base_hole.py\", line 54\n    train[['swe_snotel']].to_numpy().astype('float')\nIndentationError: unexpected indent\n",
  "history_begin_time" : 1692205321568,
  "history_end_time" : 1692205338424,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "QrmY0DboAfF6",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom base_hole import BaseHole\nfrom snowcast_utils import work_dir\n\nclass ETHole(BaseHole):\n    def __init__(self):\n        super().__init__()\n        self.classifier = self.get_model()\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        data = pd.read_csv('/home/chetana/gridmet_test_run/training_ready_3_yr_data.csv')\n        data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n\n        data = data.dropna(subset=['SWE_x'])\n        data = data[data['SWE_x'] != 0]\n        \n        X = data.drop('SWE_x', axis=1)\n        y = data['SWE_x']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def train(self):\n        self.classifier.fit(self.train_x, self.train_y)\n\n    def test(self):\n        self.test_y_results = self.classifier.predict(self.test_x)\n        print(self.test_y_results)\n        return self.test_y_results\n\n    def evaluate(self):\n        y_pred = self.test_y_results\n        mse = mean_squared_error(self.test_y, y_pred)\n        rmse = np.sqrt(mse)\n        mae = mean_absolute_error(self.test_y, y_pred)\n        r2 = r2_score(self.test_y, y_pred)\n\n        print(\"Mean Squared Error (MSE):\", mse)\n        print(\"Root Mean Squared Error (RMSE):\", rmse)\n        print(\"Mean Absolute Error (MAE):\", mae)\n        print(\"R-squared (R2):\", r2)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\n",
  "history_output" : "today date = 2023-08-14\n/home/chetana\n[ 0.1   20.7    2.91  ... 42.1   15.801 23.299]\nMean Squared Error (MSE): 0.003595932019352251\nRoot Mean Squared Error (RMSE): 0.05996609057919527\nMean Absolute Error (MAE): 0.023765050305137387\nR-squared (R2): 0.9999695218580148\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231408051359.joblib\n",
  "history_begin_time" : 1691990027241,
  "history_end_time" : 1691990039476,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ZR85qwP7FFJZ",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom base_hole import BaseHole\nfrom snowcast_utils import work_dir\n\nclass ETHole(BaseHole):\n    def __init__(self):\n        super().__init__()\n        self.classifier = self.get_model()\n\n    def get_model(self):\n        return ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\n\n    def preprocessing(self):\n        data = pd.read_csv('/home/chetana/gridmet_test_run/training_ready_3_yr_data.csv')\n        data = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\n        data['date'] = pd.to_datetime(data['date'])\n        reference_date = pd.to_datetime('1900-01-01')\n        data['date'] = (data['date'] - reference_date).dt.days\n\n        data = data.dropna(subset=['SWE_x'])\n        data = data[data['SWE_x'] != 0]\n        \n        X = data.drop('SWE_x', axis=1)\n        y = data['SWE_x']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n\n    def train(self):\n        self.classifier.fit(self.train_x, self.train_y)\n\n    def test(self):\n        self.test_y_results = self.classifier.predict(self.test_x)\n        return self.test_y_results\n\n    def evaluate(self):\n        y_pred = self.test_y_results\n        mse = mean_squared_error(self.test_y, y_pred)\n        rmse = np.sqrt(mse)\n        mae = mean_absolute_error(self.test_y, y_pred)\n        r2 = r2_score(self.test_y, y_pred)\n\n        print(\"Mean Squared Error (MSE):\", mse)\n        print(\"Root Mean Squared Error (RMSE):\", rmse)\n        print(\"Mean Absolute Error (MAE):\", mae)\n        print(\"R-squared (R2):\", r2)\n\n    def post_processing(self):\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.test_x.columns\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        plt.savefig(f'{work_dir}/et-model-feature-importance.png')\n\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/ZR85qwP7FFJZ/model_creation_et.py\", line 9, in <module>\n    from base_hole import BaseHole\n  File \"/home/chetana/gw-workspace/ZR85qwP7FFJZ/base_hole.py\", line 10, in <module>\n    import seaborn as sns\nModuleNotFoundError: No module named 'seaborn'\n",
  "history_begin_time" : 1691989899567,
  "history_end_time" : 1691989901046,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "FK87yyg8OlrQ",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = pd.read_csv('/home/chetana/gridmet_test_run/training_ready_3_yr_data.csv')\n\ndata = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\nprint(\"data head: \", data.head())\n\ndata['date'] = pd.to_datetime(data['date'])\n# Calculate the number of days since the year 1900\nreference_date = pd.to_datetime('1900-01-01')\ndata['date'] = (data['date'] - reference_date).dt.days\n\nprint(\"data head after date change: \", data.head())\n\ndata = data.dropna(subset=['SWE_x'])\ncolumn_means = data.mean()\n#data = data.fillna(column_means) \n# if there are NA, remove the rows\n# add the drop rows code here\n\ndata = data[data['SWE_x'] != 0]\nX = data.drop('SWE_x', axis=1)\ny = data['SWE_x']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\nmodel.fit(X_train, y_train)\n\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et_3_yrs.pkl')\n\nfeature_importances = model.feature_importances_\nfeature_names = X.columns\nsorted_indices = np.argsort(feature_importances)[::-1]\nsorted_importances = feature_importances[sorted_indices]\nsorted_feature_names = feature_names[sorted_indices]\n\nplt.figure(figsize=(10, 6))\nplt.bar(range(X.shape[1]), sorted_importances, tick_label=sorted_feature_names)\nplt.xticks(rotation=90)\nplt.xlabel('Feature')\nplt.ylabel('Feature Importance')\nplt.title('Feature Importance Plot (ET model)')\nplt.tight_layout()\nplt.savefig('/home/chetana/gridmet_test_run/3_year_feat_importance.png')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "data head:           date  SWE_x        lat  ...  northness  AMSR_SWE  AMSR_Flag\n0  2019-01-01   10.9  41.993149  ...   0.649194        17        241\n1  2019-01-02   11.0  41.993149  ...   0.649194        14        241\n2  2019-01-03   11.0  41.993149  ...   0.649194        13        241\n3  2019-01-04   11.1  41.993149  ...   0.649194         0        241\n4  2019-01-05   11.1  41.993149  ...   0.649194       255        255\n[5 rows x 20 columns]\ndata head after date change:      date  SWE_x        lat  ...  northness  AMSR_SWE  AMSR_Flag\n0  43464   10.9  41.993149  ...   0.649194        17        241\n1  43465   11.0  41.993149  ...   0.649194        14        241\n2  43466   11.0  41.993149  ...   0.649194        13        241\n3  43467   11.1  41.993149  ...   0.649194         0        241\n4  43468   11.1  41.993149  ...   0.649194       255        255\n[5 rows x 20 columns]\nMean Squared Error (MSE): 0.003595932019352251\nRoot Mean Squared Error (RMSE): 0.05996609057919527\nMean Absolute Error (MAE): 0.023765050305137366\nR-squared (R2): 0.9999695218580148\n",
  "history_begin_time" : 1691874241459,
  "history_end_time" : 1691874252182,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "5qsrTBMRasQ6",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = pd.read_csv('/home/chetana/gridmet_test_run/training_ready_3_yr_data.csv')\n\ndata = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\nprint(\"data head: \", data.head())\n\ndata['date'] = pd.to_datetime(data['date'])\n# Calculate the number of days since the year 1900\nreference_date = pd.to_datetime('1900-01-01')\ndata['date'] = (data['date'] - reference_date).dt.days\n\nprint(\"data head after date change: \", data.head())\n\ndata = data.dropna(subset=['SWE_x'])\ncolumn_means = data.mean()\ndata = data.fillna(column_means)\ndata = data[data['SWE_x'] != 0]\nX = data.drop('SWE_x', axis=1)\ny = data['SWE_x']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\nmodel.fit(X_train, y_train)\n\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et_3_yrs.pkl')\n\nfeature_importances = model.feature_importances_\nfeature_names = X.columns\nsorted_indices = np.argsort(feature_importances)[::-1]\nsorted_importances = feature_importances[sorted_indices]\nsorted_feature_names = feature_names[sorted_indices]\n\nplt.figure(figsize=(10, 6))\nplt.bar(range(X.shape[1]), sorted_importances, tick_label=sorted_feature_names)\nplt.xticks(rotation=90)\nplt.xlabel('Feature')\nplt.ylabel('Feature Importance')\nplt.title('Feature Importance Plot (ET model)')\nplt.tight_layout()\nplt.savefig('/home/chetana/gridmet_test_run/3_year_feat_importance.png')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "data head:           date  SWE_x        lat  ...  northness  AMSR_SWE  AMSR_Flag\n0  2019-01-01   10.9  41.993149  ...   0.649194        17        241\n1  2019-01-02   11.0  41.993149  ...   0.649194        14        241\n2  2019-01-03   11.0  41.993149  ...   0.649194        13        241\n3  2019-01-04   11.1  41.993149  ...   0.649194         0        241\n4  2019-01-05   11.1  41.993149  ...   0.649194       255        255\n[5 rows x 20 columns]\ndata head after date change:      date  SWE_x        lat  ...  northness  AMSR_SWE  AMSR_Flag\n0  43464   10.9  41.993149  ...   0.649194        17        241\n1  43465   11.0  41.993149  ...   0.649194        14        241\n2  43466   11.0  41.993149  ...   0.649194        13        241\n3  43467   11.1  41.993149  ...   0.649194         0        241\n4  43468   11.1  41.993149  ...   0.649194       255        255\n[5 rows x 20 columns]\nMean Squared Error (MSE): 0.003595932019352253\nRoot Mean Squared Error (RMSE): 0.05996609057919528\nMean Absolute Error (MAE): 0.023765050305137404\nR-squared (R2): 0.9999695218580148\n",
  "history_begin_time" : 1691874187098,
  "history_end_time" : 1691874198086,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ExiCUGADgOn6",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = pd.read_csv('/home/chetana/gridmet_test_run/training_ready_3_yr_data.csv')\n\ndata = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\ndata['date'] = pd.to_datetime(data['date'])\n# Calculate the number of days since the year 1900\nreference_date = pd.to_datetime('1900-01-01')\ndata['date'] = (data['date'] - reference_date).dt.days\n\ndata = data.dropna(subset=['SWE_x'])\ncolumn_means = data.mean()\ndata = data.fillna(column_means)\ndata = data[data['SWE_x'] != 0]\nX = data.drop('SWE_x', axis=1)\ny = data['SWE_x']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\nmodel.fit(X_train, y_train)\n\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et_3_yrs.pkl')\n\nfeature_importances = model.feature_importances_\nfeature_names = X.columns\nsorted_indices = np.argsort(feature_importances)[::-1]\nsorted_importances = feature_importances[sorted_indices]\nsorted_feature_names = feature_names[sorted_indices]\n\nplt.figure(figsize=(10, 6))\nplt.bar(range(X.shape[1]), sorted_importances, tick_label=sorted_feature_names)\nplt.xticks(rotation=90)\nplt.xlabel('Feature')\nplt.ylabel('Feature Importance')\nplt.title('Feature Importance Plot (ET model)')\nplt.tight_layout()\nplt.savefig('/home/chetana/gridmet_test_run/3_year_feat_importance.png')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "Mean Squared Error (MSE): 0.003595932019352251\nRoot Mean Squared Error (RMSE): 0.05996609057919527\nMean Absolute Error (MAE): 0.023765050305137383\nR-squared (R2): 0.9999695218580148\n",
  "history_begin_time" : 1691712485585,
  "history_end_time" : 1691712497782,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "jC1MjUg1PHZi",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = pd.read_csv('/home/chetana/gridmet_test_run/training_ready_3_yr_data.csv')\n\ndata = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\ndata['date'] = pd.to_datetime(data['date'])\n# Calculate the number of days since the year 1900\nreference_date = pd.to_datetime('1900-01-01')\ndata['date'] = (data['date'] - reference_date).dt.days\n\ndata = data.dropna(subset=['SWE_x'])\ndata.drop([\"SWE_x\"], axis=1, inplace=True)\ncolumn_means = data.mean()\ndata = data.fillna(column_means)\ndata = data[data['SWE_x'] != 0]\nX = data.drop('SWE_x', axis=1)\ny = data['SWE_x']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\nmodel.fit(X_train, y_train)\n\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et_3_yrs.pkl')\n\nfeature_importances = model.feature_importances_\nfeature_names = X.columns\nsorted_indices = np.argsort(feature_importances)[::-1]\nsorted_importances = feature_importances[sorted_indices]\nsorted_feature_names = feature_names[sorted_indices]\n\nplt.figure(figsize=(10, 6))\nplt.bar(range(X.shape[1]), sorted_importances, tick_label=sorted_feature_names)\nplt.xticks(rotation=90)\nplt.xlabel('Feature')\nplt.ylabel('Feature Importance')\nplt.title('Feature Importance Plot (ET model)')\nplt.tight_layout()\nplt.savefig('/home/chetana/gridmet_test_run/3_year_feat_importance.png')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 3802, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 138, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 165, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5745, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5753, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'SWE_x'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/jC1MjUg1PHZi/model_creation_et.py\", line 24, in <module>\n    data = data[data['SWE_x'] != 0]\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/frame.py\", line 3807, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 3804, in get_loc\n    raise KeyError(key) from err\nKeyError: 'SWE_x'\n",
  "history_begin_time" : 1691705771543,
  "history_end_time" : 1691705774761,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "nni8OGXv1ont",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = pd.read_csv('/home/chetana/gridmet_test_run/training_ready_3_yr_data.csv')\n\ndata = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\ndata['date'] = pd.to_datetime(data['date'])\n# Calculate the number of days since the year 1900\nreference_date = pd.to_datetime('1900-01-01')\ndata['date'] = (data['date'] - reference_date).dt.days\n\ndata = data.dropna(subset=['SWE_x'])\ncolumn_means = data.mean()\ndata = data.fillna(column_means)\ndata = data[data['SWE_x'] != 0]\nX = data.drop('SWE_x', axis=1)\ny = data['SWE_x']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\nmodel.fit(X_train, y_train)\n\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et_3_yrs.pkl')\n\nfeature_importances = model.feature_importances_\nfeature_names = X.columns\nsorted_indices = np.argsort(feature_importances)[::-1]\nsorted_importances = feature_importances[sorted_indices]\nsorted_feature_names = feature_names[sorted_indices]\n\nplt.figure(figsize=(10, 6))\nplt.bar(range(X.shape[1]), sorted_importances, tick_label=sorted_feature_names)\nplt.xticks(rotation=90)\nplt.xlabel('Feature')\nplt.ylabel('Feature Importance')\nplt.title('Feature Importance Plot (ET model)')\nplt.tight_layout()\nplt.savefig('/home/chetana/gridmet_test_run/3_year_feat_importance.png')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "Mean Squared Error (MSE): 0.003595932019352253\nRoot Mean Squared Error (RMSE): 0.05996609057919528\nMean Absolute Error (MAE): 0.023765050305137394\nR-squared (R2): 0.9999695218580148\n",
  "history_begin_time" : 1691699066884,
  "history_end_time" : 1691699078819,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Rchm5ktH6KZC",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = pd.read_csv('/home/chetana/gridmet_test_run/training_ready_3_yr_data.csv')\n\ndata = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\ndata['date'] = pd.to_datetime(data['date'])\n# Calculate the number of days since the year 1900\nreference_date = pd.to_datetime('1900-01-01')\ndata['days_since_reference'] = (data['date'] - reference_date).dt.days\n\ndata = data.dropna(subset=['SWE_x'])\ncolumn_means = data.mean()\ndata = data.fillna(column_means)\n\n# Create a new feature 'scaled_date' by applying Min-Max scaling to 'days_since_reference'\nscaler = MinMaxScaler()\ndata['scaled_date'] = scaler.fit_transform(data['days_since_reference'].values.reshape(-1, 1))\n\n# Drop the original 'date' and 'days_since_reference' columns\ndata = data.drop(['date', 'days_since_reference'], axis=1)\ndata = data[data['SWE_x'] != 0]\nX = data.drop('SWE_x', axis=1)\ny = data['SWE_x']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\nmodel.fit(X_train, y_train)\n\nfeature_importances = model.feature_importances_\nfeature_names = X.columns\nsorted_indices = np.argsort(feature_importances)[::-1]\nsorted_importances = feature_importances[sorted_indices]\nsorted_feature_names = feature_names[sorted_indices]\n\nplt.figure(figsize=(10, 6))\nplt.bar(range(X.shape[1]), sorted_importances, tick_label=sorted_feature_names)\nplt.xticks(rotation=90)\nplt.xlabel('Feature')\nplt.ylabel('Feature Importance')\nplt.title('Feature Importance Plot (ET model)')\nplt.tight_layout()\nplt.savefig('/home/chetana/gridmet_test_run/3_year_feat_importance.png')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et_3_yrs.pkl')\n\n",
  "history_output" : "/home/chetana/gw-workspace/Rchm5ktH6KZC/model_creation_et.py:21: FutureWarning: DataFrame.mean and DataFrame.median with numeric_only=None will include datetime64 and datetime64tz columns in a future version.\n  column_means = data.mean()\nMean Squared Error (MSE): 0.004759203996921045\nRoot Mean Squared Error (RMSE): 0.06898698425732962\nMean Absolute Error (MAE): 0.030414838638743724\nR-squared (R2): 0.9999596622810515\n",
  "history_begin_time" : 1691698875001,
  "history_end_time" : 1691698887629,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "uXjUGKo5YuvJ",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = pd.read_csv('/home/chetana/gridmet_test_run/training_ready_3_yr_data.csv')\n\ndata = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\ndata['date'] = pd.to_datetime(data['date'])\n# Calculate the number of days since the year 1900\nreference_date = pd.to_datetime('1900-01-01')\ndata['days_since_reference'] = (data['date'] - reference_date).dt.days\n\ndata = data.dropna(subset=['SWE_x'])\ncolumn_means = data.mean()\ndata = data.fillna(column_means)\n\n# Create a new feature 'scaled_date' by applying Min-Max scaling to 'days_since_reference'\nscaler = MinMaxScaler()\ndata['scaled_date'] = scaler.fit_transform(data['days_since_reference'].values.reshape(-1, 1))\n\n# Drop the original 'date' and 'days_since_reference' columns\ndata = data.drop(['date', 'days_since_reference'], axis=1)\ndata = data[data['SWE_x'] != 0]\nX = data.drop('SWE_x', axis=1)\ny = data['SWE_x']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\nmodel.fit(X_train, y_train)\n\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et_3_yrs.pkl')\n\nfeature_importances = model.feature_importances_\nfeature_names = X.columns\nsorted_indices = np.argsort(feature_importances)[::-1]\nsorted_importances = feature_importances[sorted_indices]\nsorted_feature_names = feature_names[sorted_indices]\n\nplt.figure(figsize=(10, 6))\nplt.bar(range(X.shape[1]), sorted_importances, tick_label=sorted_feature_names)\nplt.xticks(rotation=90)\nplt.xlabel('Feature')\nplt.ylabel('Feature Importance')\nplt.title('Feature Importance Plot (ET model)')\nplt.tight_layout()\nplt.savefig('/home/chetana/gridmet_test_run/3_year_feat_importance.png')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "/home/chetana/gw-workspace/uXjUGKo5YuvJ/model_creation_et.py:21: FutureWarning: DataFrame.mean and DataFrame.median with numeric_only=None will include datetime64 and datetime64tz columns in a future version.\n  column_means = data.mean()\nMean Squared Error (MSE): 0.004759203996921045\nRoot Mean Squared Error (RMSE): 0.06898698425732962\nMean Absolute Error (MAE): 0.030414838638743755\nR-squared (R2): 0.9999596622810515\n",
  "history_begin_time" : 1691698571995,
  "history_end_time" : 1691698590209,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "92hc0mnajmz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531335784,
  "history_end_time" : 1691531335784,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "74y00fa0xz2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531292708,
  "history_end_time" : 1691531292708,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "8uezvdj2bcq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531254583,
  "history_end_time" : 1691531284898,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "81wb8wgbook",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531163795,
  "history_end_time" : 1691531163795,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "36mylr46r1x",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531120847,
  "history_end_time" : 1691531120847,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "otwietge3p9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531060879,
  "history_end_time" : 1691531060879,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "q5dot4n29tp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530848273,
  "history_end_time" : 1691530848273,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "02b17spfycw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717685,
  "history_end_time" : 1691530721103,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "snn2vy9r7fv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530690111,
  "history_end_time" : 1691530716747,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "x4smkvnd2m6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530621011,
  "history_end_time" : 1691530622437,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "yaxbmjcown7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530617158,
  "history_end_time" : 1691530617158,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "gfzu6kpsjik",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530599783,
  "history_end_time" : 1691530614282,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "5jOMOGL2oGsX",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = pd.read_csv('/home/chetana/gridmet_test_run/training_ready_3_yr_data.csv')\n\ndata = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\ndata['date'] = pd.to_datetime(data['date'])\n# Calculate the number of days since the year 1900\nreference_date = pd.to_datetime('1900-01-01')\ndata['days_since_reference'] = (data['date'] - reference_date).dt.days\n\ndata = data.dropna(subset=['SWE_x'])\ncolumn_means = data.mean()\ndata = data.fillna(column_means)\n\n# Create a new feature 'scaled_date' by applying Min-Max scaling to 'days_since_reference'\nscaler = MinMaxScaler()\ndata['scaled_date'] = scaler.fit_transform(data['days_since_reference'].values.reshape(-1, 1))\n\n# Drop the original 'date' and 'days_since_reference' columns\ndata = data.drop(['date', 'days_since_reference'], axis=1)\ndata = data[data['SWE_x'] != 0]\nX = data.drop('SWE_x', axis=1)\ny = data['SWE_x']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\nmodel.fit(X_train, y_train)\n\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et_3_yrs.pkl')\n\nfeature_importances = model.feature_importances_\nfeature_names = X.columns\nsorted_indices = np.argsort(feature_importances)[::-1]\nsorted_importances = feature_importances[sorted_indices]\nsorted_feature_names = feature_names[sorted_indices]\n\nplt.figure(figsize=(10, 6))\nplt.bar(range(X.shape[1]), sorted_importances, tick_label=sorted_feature_names)\nplt.xticks(rotation=90)\nplt.xlabel('Feature')\nplt.ylabel('Feature Importance')\nplt.title('Feature Importance Plot (ET model)')\nplt.tight_layout()\nplt.savefig('/home/chetana/gridmet_test_run/3_year_feat_importance.png')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "/home/chetana/gw-workspace/5jOMOGL2oGsX/model_creation_et.py:21: FutureWarning: DataFrame.mean and DataFrame.median with numeric_only=None will include datetime64 and datetime64tz columns in a future version.\n  column_means = data.mean()\nMean Squared Error (MSE): 0.004759203996921045\nRoot Mean Squared Error (RMSE): 0.06898698425732962\nMean Absolute Error (MAE): 0.030414838638743745\nR-squared (R2): 0.9999596622810515\n",
  "history_begin_time" : 1691419342566,
  "history_end_time" : 1691419354455,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "e9XLw96qFYGN",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = pd.read_csv('/home/chetana/gridmet_test_run/training_ready_3_yr_data.csv')\n\ndata = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\ndata['date'] = pd.to_datetime(data['date'])\n# Calculate the number of days since the year 1900\nreference_date = pd.to_datetime('1900-01-01')\ndata['days_since_reference'] = (data['date'] - reference_date).dt.days\n\ndata = data.dropna(subset=['SWE_x'])\ncolumn_means = data.mean()\ndata = data.fillna(column_means)\n\n# Create a new feature 'scaled_date' by applying Min-Max scaling to 'days_since_reference'\nscaler = MinMaxScaler()\ndata['scaled_date'] = scaler.fit_transform(data['days_since_reference'].values.reshape(-1, 1))\n\n# Drop the original 'date' and 'days_since_reference' columns\ndata = data.drop(['date', 'days_since_reference'], axis=1)\n\nX = data.drop('SWE_x', axis=1)\ny = data['SWE_x']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\nmodel.fit(X_train, y_train)\n\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et_3_yrs.pkl')\n\nfeature_importances = model.feature_importances_\nfeature_names = X.columns\nsorted_indices = np.argsort(feature_importances)[::-1]\nsorted_importances = feature_importances[sorted_indices]\nsorted_feature_names = feature_names[sorted_indices]\n\nplt.figure(figsize=(10, 6))\nplt.bar(range(X.shape[1]), sorted_importances, tick_label=sorted_feature_names)\nplt.xticks(rotation=90)\nplt.xlabel('Feature')\nplt.ylabel('Feature Importance')\nplt.title('Feature Importance Plot (ET model)')\nplt.tight_layout()\nplt.savefig('/home/chetana/gridmet_test_run/3_year_feat_importance.png')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "/home/chetana/gw-workspace/e9XLw96qFYGN/model_creation_et.py:21: FutureWarning: DataFrame.mean and DataFrame.median with numeric_only=None will include datetime64 and datetime64tz columns in a future version.\n  column_means = data.mean()\nMean Squared Error (MSE): 0.21048511653392757\nRoot Mean Squared Error (RMSE): 0.45878656969655024\nMean Absolute Error (MAE): 0.1828980567059585\nR-squared (R2): 0.998475849147189\n",
  "history_begin_time" : 1691394069688,
  "history_end_time" : 1691394091471,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "tgkCOi8hxVkp",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.feature_selection import SelectKBest, f_regression\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = pd.read_csv('/home/chetana/gridmet_test_run/training_ready_3_yr_data.csv')\n\n# Rename columns using the 'rename' method and reassign to 'data'\ndata = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\ndata['date'] = pd.to_datetime(data['date'])\n\n# Extract year, month, day, and day_of_week\ndata['year'] = data['date'].dt.year\ndata['month'] = data['date'].dt.month\ndata['day'] = data['date'].dt.day\ndata['day_of_week'] = data['date'].dt.dayofweek\n\n# Drop date-related columns and unnecessary ones\ndata = data.drop(['date', 'AMSR_SWE', 'AMSR_Flag'], axis=1)\n\n# Fill missing values with column means\ndata = data.dropna(subset=['SWE_x'])\ncolumn_means = data.mean()\ndata = data.fillna(column_means)\n\n# Separate numerical and categorical features\nX_numeric = data.drop(['SWE_x'], axis=1).select_dtypes(include=[np.number])\nX_categorical = data.drop(['SWE_x'], axis=1).select_dtypes(include=[object])\n\n# Normalize numerical features\nscaler = StandardScaler()\nX_numeric_scaled = scaler.fit_transform(X_numeric)\n\n# One-Hot encode categorical features\nencoder = OneHotEncoder()\nX_categorical_encoded = encoder.fit_transform(X_categorical).toarray()\n\n# Combine numerical and categorical features\nX = np.concatenate((X_numeric_scaled, X_categorical_encoded), axis=1)\n\ny = data['SWE_x']\n\n# Get feature names after one-hot encoding\nencoded_feature_names = encoder.get_feature_names(input_features=X_categorical.columns)\nfeature_names = np.concatenate((X_numeric.columns, encoded_feature_names))\n\n# Use SelectKBest for feature selection\nk = 10  # Choose the number of top features to select\nselector = SelectKBest(score_func=f_regression, k=k)\nX_selected = selector.fit_transform(X, y)\n\nX_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\nmodel.fit(X_train, y_train)\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et_3_yrs.pkl')\n\nfeature_importances = model.feature_importances_\nselected_feature_names = feature_names[selector.get_support()]\nsorted_indices = np.argsort(feature_importances)[::-1]\nsorted_importances = feature_importances[sorted_indices]\nsorted_feature_names = selected_feature_names[sorted_indices]\n\nplt.figure(figsize=(10, 6))\nplt.bar(range(X_selected.shape[1]), sorted_importances, tick_label=sorted_feature_names)\nplt.xticks(rotation=90)\nplt.xlabel('Feature')\nplt.ylabel('Feature Importance')\nplt.title('Feature Importance Plot (ET model)')\nplt.tight_layout()\nplt.savefig('/home/chetana/gridmet_test_run/3_year_feat_importance.png')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/tgkCOi8hxVkp/model_creation_et.py\", line 51, in <module>\n    encoded_feature_names = encoder.get_feature_names(input_features=X_categorical.columns)\nAttributeError: 'OneHotEncoder' object has no attribute 'get_feature_names'\n",
  "history_begin_time" : 1691391782065,
  "history_end_time" : 1691391786339,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "mrn5j0Is9AX0",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.feature_selection import SelectKBest, f_regression\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = pd.read_csv('/home/chetana/gridmet_test_run/training_ready_3_yr_data.csv')\n\n# Rename columns using the 'rename' method and reassign to 'data'\ndata = data.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\ndata['date'] = pd.to_datetime(data['date'])\n\n# Extract year, month, day, and day_of_week\ndata['year'] = data['date'].dt.year\ndata['month'] = data['date'].dt.month\ndata['day'] = data['date'].dt.day\ndata['day_of_week'] = data['date'].dt.dayofweek\n\n# Drop date-related columns and unnecessary ones\ndata = data.drop(['date', 'AMSR_SWE', 'AMSR_Flag'], axis=1)\n\n# Fill missing values with column means\ndata = data.dropna(subset=['SWE_x'])\ncolumn_means = data.mean()\ndata = data.fillna(column_means)\n\n# Separate numerical and categorical features\nX_numeric = data.drop(['SWE_x'], axis=1).select_dtypes(include=[np.number])\nX_categorical = data.drop(['SWE_x'], axis=1).select_dtypes(include=[object])\n\n# Normalize numerical features\nscaler = StandardScaler()\nX_numeric_scaled = scaler.fit_transform(X_numeric)\n\n# One-Hot encode categorical features\nencoder = OneHotEncoder()\nX_categorical_encoded = encoder.fit_transform(X_categorical).toarray()\n\n# Combine numerical and categorical features\nX = np.concatenate((X_numeric_scaled, X_categorical_encoded), axis=1)\n\ny = data['SWE_x']\n\n# Use SelectKBest for feature selection\nk = 10  # Choose the number of top features to select\nselector = SelectKBest(score_func=f_regression, k=k)\nX_selected = selector.fit_transform(X, y)\n\nX_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\nmodel.fit(X_train, y_train)\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et_3_yrs.pkl')\n\nfeature_importances = model.feature_importances_\nselected_feature_names = [feature_names[i] for i in selector.get_support()]\nsorted_indices = np.argsort(feature_importances)[::-1]\nsorted_importances = feature_importances[sorted_indices]\nsorted_feature_names = [selected_feature_names[i] for i in sorted_indices]\n\nplt.figure(figsize=(10, 6))\nplt.bar(range(X_selected.shape[1]), sorted_importances, tick_label=sorted_feature_names)\nplt.xticks(rotation=90)\nplt.xlabel('Feature')\nplt.ylabel('Feature Importance')\nplt.title('Feature Importance Plot (ET model)')\nplt.tight_layout()\nplt.savefig('/home/chetana/gridmet_test_run/3_year_feat_importance.png')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/mrn5j0Is9AX0/model_creation_et.py\", line 61, in <module>\n    selected_feature_names = [feature_names[i] for i in selector.get_support()]\n  File \"/home/chetana/gw-workspace/mrn5j0Is9AX0/model_creation_et.py\", line 61, in <listcomp>\n    selected_feature_names = [feature_names[i] for i in selector.get_support()]\nNameError: name 'feature_names' is not defined\n",
  "history_begin_time" : 1691391688470,
  "history_end_time" : 1691391710161,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "61kBYiJRNjio",
  "history_input" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = pd.read_csv('/home/chetana/gridmet_test_run/training_ready_3_yr_data.csv')\n\ndata.rename(columns={'SWE_y': 'AMSR_SWE', 'Flag': 'AMSR_Flag'})\n\ndata['date'] = pd.to_datetime(data['date'])\n\ndata['date'] = pd.to_datetime(data['date'])\n\ndata['year'] = data['date'].dt.year\ndata['month'] = data['date'].dt.month\ndata['day'] = data['date'].dt.day\ndata['day_of_week'] = data['date'].dt.dayofweek\ndata = data.drop('date', axis=1)\ndata = data.dropna(subset=['SWE_x'])\ncolumn_means = data.mean()\ndata = data.fillna(column_means)\n\nX = data.drop('SWE_x', axis=1)\ny = data['SWE_x']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=5)\nmodel.fit(X_train, y_train)\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et_3_yrs.pkl')\n\nfeature_importances = model.feature_importances_\nfeature_names = X.columns\nsorted_indices = np.argsort(feature_importances)[::-1]\nsorted_importances = feature_importances[sorted_indices]\nsorted_feature_names = feature_names[sorted_indices]\n\nplt.figure(figsize=(10, 6))\nplt.bar(range(X.shape[1]), sorted_importances, tick_label=sorted_feature_names)\nplt.xticks(rotation=90)\nplt.xlabel('Feature')\nplt.ylabel('Feature Importance')\nplt.title('Feature Importance Plot (ET model)')\nplt.tight_layout()\nplt.savefig('/home/chetana/gridmet_test_run/3_year_feat_importance.png')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "Mean Squared Error (MSE): 0.0003353819369225782\nRoot Mean Squared Error (RMSE): 0.018313435967141124\nMean Absolute Error (MAE): 0.003012902198164983\nR-squared (R2): 0.9999975714545827\n",
  "history_begin_time" : 1691389986668,
  "history_end_time" : 1691390020138,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "0GPTEq69olB5",
  "history_input" : "import dask.dataframe as dd\nfrom dask_ml.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport joblib\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = dd.read_csv('/home/chetana/gridmet_test_run/model_training_cleaned.csv')\ndata['date'] = dd.to_datetime(data['date'])\n\ndata['date'] = dd.to_datetime(data['date'])\n\ndata['year'] = data['date'].dt.year\ndata['month'] = data['date'].dt.month\ndata['day'] = data['date'].dt.day\ndata['day_of_week'] = data['date'].dt.dayofweek\ndata = data.drop('date', axis=1)\ndata = data.dropna(subset=['swe_value'])\ncolumn_means = data.mean()\ndata = data.fillna(column_means)\n\nX = data.drop('swe_value', axis=1)\ny = data['swe_value']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1)\nmodel.fit(X_train, y_train)\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et.pkl')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask_ml/model_selection/_split.py:462: FutureWarning: The default value for 'shuffle' must be specified when splitting DataFrames. In the future DataFrames will automatically be shuffled within blocks prior to splitting. Specify 'shuffle=True' to adopt the future behavior now, or 'shuffle=False' to retain the previous behavior.\n  warnings.warn(\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/0GPTEq69olB5/model_creation_et.py\", line 27, in <module>\n    model.fit(X_train, y_train)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 345, in fit\n    X, y = self._validate_data(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/base.py\", line 584, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1106, in check_X_y\n    X = check_array(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/utils/_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/core.py\", line 599, in __array__\n    x = np.array(self._computed)\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 17.8 GiB for an array with shape (125800448, 19) and data type float64\n",
  "history_begin_time" : 1690608867593,
  "history_end_time" : 1690609370894,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "2oxKExkdXgAE",
  "history_input" : "import dask.dataframe as dd\nfrom dask_ml.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport joblib\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = dd.read_csv('/home/chetana/gridmet_test_run/model_training_cleaned.csv')\ndata['date'] = dd.to_datetime(data['date'])\n\ndata['year'] = data['date'].dt.year\ndata['month'] = data['date'].dt.month\ndata['day'] = data['date'].dt.day\ndata['day_of_week'] = data['date'].dt.dayofweek\ndata = data.drop('date', axis=1)\n\n# Split the data into features (X) and target variable (y)\nX = data.drop('swe_value', axis=1)\ny = data['swe_value']\n\n# Split the data into small chunks (adjust chunk_size based on available memory)\nchunk_size = 100000  # You may need to adjust this value based on your system's memory capacity\nX_chunks = [X.compute().iloc[i:i + chunk_size] for i in range(0, len(X), chunk_size)]\ny_chunks = [y.compute().iloc[i:i + chunk_size] for i in range(0, len(y), chunk_size)]\n\n# Create an instance of the ExtraTreesRegressor\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n\n# Train the model with the initial chunk\nmodel.fit(X_chunks[0], y_chunks[0])\n\n# Save the trained model after initial training\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et.pkl')\n\n# Perform incremental learning on the remaining chunks\nfor i in range(1, len(X_chunks)):\n    model.n_estimators += 10  # Increment n_estimators for each chunk\n    model.fit(X_chunks[i], y_chunks[i])\n\n# Save the trained model after incremental learning\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et.pkl')\n\n# Split the data into a separate test set\nX_test, y_test = train_test_split(X_chunks[-1], y_chunks[-1], test_size=0.2, random_state=42)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/2oxKExkdXgAE/model_creation_et.py\", line 23, in <module>\n    X_chunks = [X.compute().iloc[i:i + chunk_size] for i in range(0, len(X), chunk_size)]\n  File \"/home/chetana/gw-workspace/2oxKExkdXgAE/model_creation_et.py\", line 23, in <listcomp>\n    X_chunks = [X.compute().iloc[i:i + chunk_size] for i in range(0, len(X), chunk_size)]\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/dispatch.py\", line 68, in concat\n    return func(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/backends.py\", line 667, in concat_pandas\n    out = pd.concat(dfs3, join=join, sort=False)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"<__array_function__ internals>\", line 180, in concatenate\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 17.6 GiB for an array with shape (15, 157282561) and data type float64\n",
  "history_begin_time" : 1690602019835,
  "history_end_time" : 1690602894751,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "eF7GGgv9wMf3",
  "history_input" : "import dask.dataframe as dd\nfrom dask_ml.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport joblib\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = dd.read_csv('/home/chetana/gridmet_test_run/model_training_cleaned.csv')\ndata['date'] = dd.to_datetime(data['date'])\n\ndata['year'] = data['date'].dt.year\ndata['month'] = data['date'].dt.month\ndata['day'] = data['date'].dt.day\ndata['day_of_week'] = data['date'].dt.dayofweek\ndata = data.drop('date', axis=1)\n\n# Split the data into features (X) and target variable (y)\nX = data.drop('swe_value', axis=1)\ny = data['swe_value']\n\n# Split the data into training and test sets using Dask\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create an instance of the ExtraTreesRegressor\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n\n# Train the model with the initial training set\nmodel.fit(X_train, y_train)\n\n# Save the trained model after initial training\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et.pkl')\n\n# Load the model to perform incremental learning in batches\nmodel = joblib.load('/home/chetana/gridmet_test_run/model_creation_et.pkl')\n\n# Perform incremental learning on the remaining data\nbatch_size = 1000\nfor i in range(0, len(X_train), batch_size):\n    X_batch = X_train.iloc[i:i+batch_size]\n    y_batch = y_train.iloc[i:i+batch_size]\n    model.fit(X_batch, y_batch)\n\n# Save the trained model after incremental learning\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et.pkl')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask_ml/model_selection/_split.py:462: FutureWarning: The default value for 'shuffle' must be specified when splitting DataFrames. In the future DataFrames will automatically be shuffled within blocks prior to splitting. Specify 'shuffle=True' to adopt the future behavior now, or 'shuffle=False' to retain the previous behavior.\n  warnings.warn(\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/eF7GGgv9wMf3/model_creation_et.py\", line 28, in <module>\n    model.fit(X_train, y_train)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 345, in fit\n    X, y = self._validate_data(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/base.py\", line 584, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1106, in check_X_y\n    X = check_array(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/utils/_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/core.py\", line 599, in __array__\n    x = np.array(self._computed)\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 17.8 GiB for an array with shape (125834411, 19) and data type float64\n",
  "history_begin_time" : 1690601270988,
  "history_end_time" : 1690601582198,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "fVChoQSreHyt",
  "history_input" : "import dask.dataframe as dd\nfrom dask_ml.model_selection import train_test_split\nfrom dask_ml.wrappers import Incremental\nimport joblib\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = dd.read_csv('/home/chetana/gridmet_test_run/model_training_cleaned.csv')\ndata['date'] = dd.to_datetime(data['date'])\n\ndata['year'] = data['date'].dt.year\ndata['month'] = data['date'].dt.month\ndata['day'] = data['date'].dt.day\ndata['day_of_week'] = data['date'].dt.dayofweek\ndata = data.drop('date', axis=1)\n\n# Split the data into features (X) and target variable (y)\nX = data.drop('swe_value', axis=1)\ny = data['swe_value']\n\n# Split the data into a small initial training set and a larger pool\nX_train_init, X_pool, y_train_init, y_pool = train_test_split(X, y, test_size=0.9, random_state=42)\n\n# Create an instance of the ExtraTreesRegressor wrapped in Incremental\nmodel = Incremental(ExtraTreesRegressor(n_estimators=100, random_state=42))\n\n# Train the model with the initial training set\nmodel.fit(X_train_init, y_train_init)\n\n# The remaining data (X_pool, y_pool) can be used for incremental learning\n# For example, you can loop over the remaining data in smaller batches and update the model\nbatch_size = 1000\nfor i in range(0, len(X_pool), batch_size):\n    X_batch = X_pool.iloc[i:i+batch_size]\n    y_batch = y_pool.iloc[i:i+batch_size]\n    # Perform incremental learning\n    model.partial_fit(X_batch, y_batch)\n\n# Save the trained model after incremental learning\nmodel.steps[-1][1].n_jobs = 1  # Ensure n_jobs = 1 to avoid potential issues with joblib.dump\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et.pkl')\n\n# Split the data into a separate test set\nX_test, y_test = train_test_split(X_pool, y_pool, test_size=0.2, random_state=42)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask_ml/model_selection/_split.py:462: FutureWarning: The default value for 'shuffle' must be specified when splitting DataFrames. In the future DataFrames will automatically be shuffled within blocks prior to splitting. Specify 'shuffle=True' to adopt the future behavior now, or 'shuffle=False' to retain the previous behavior.\n  warnings.warn(\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/fVChoQSreHyt/model_creation_et.py\", line 29, in <module>\n    model.fit(X_train_init, y_train_init)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask_ml/wrappers.py\", line 579, in fit\n    self._fit_for_estimator(estimator, X, y, **fit_kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask_ml/wrappers.py\", line 563, in _fit_for_estimator\n    result = fit(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask_ml/_partial.py\", line 98, in fit\n    raise ValueError(msg.format(type(model)))\nValueError: The class '<class 'sklearn.ensemble._forest.ExtraTreesRegressor'>' does not implement 'partial_fit'.\n",
  "history_begin_time" : 1690601225728,
  "history_end_time" : 1690601233258,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ixqC9z45MoTa",
  "history_input" : "import dask.dataframe as dd\nfrom dask_ml.model_selection import train_test_split\nfrom dask_ml.wrappers import Incremental\nimport joblib\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = dd.read_csv('/home/chetana/gridmet_test_run/model_training_cleaned.csv')\ndata['date'] = dd.to_datetime(data['date'])\n\ndata['year'] = data['date'].dt.year\ndata['month'] = data['date'].dt.month\ndata['day'] = data['date'].dt.day\ndata['day_of_week'] = data['date'].dt.dayofweek\ndata = data.drop('date', axis=1)\n\n# Split the data into features (X) and target variable (y)\nX = data.drop('swe_value', axis=1)\ny = data['swe_value']\n\n# Split the data into a small initial training set and a larger pool\nX_train_init, X_pool, y_train_init, y_pool = train_test_split(X, y, test_size=0.9, random_state=42)\n\n# Create an instance of the ExtraTreesRegressor wrapped in Incremental\nmodel = Incremental(ExtraTreesRegressor(n_estimators=100, random_state=42))\n\n# Train the model with the initial training set\nmodel.fit(X_train_init, y_train_init)\n\n# The remaining data (X_pool, y_pool) can be used for incremental learning\n# For example, you can loop over the remaining data in smaller batches and update the model\nbatch_size = 1000\nfor i in range(0, len(X_pool), batch_size):\n    X_batch = X_pool.iloc[i:i+batch_size]\n    y_batch = y_pool.iloc[i:i+batch_size]\n    # Perform incremental learning\n    model.partial_fit(X_batch, y_batch)\n\n# Save the trained model after incremental learning\nmodel.steps[-1][1].n_jobs = 1  # Ensure n_jobs = 1 to avoid potential issues with joblib.dump\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et.pkl')\n\n# Split the data into a separate test set\nX_test, y_test = train_test_split(X_pool, y_pool, test_size=0.2, random_state=42)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/ixqC9z45MoTa/model_creation_et.py\", line 2, in <module>\n    from dask_ml.model_selection import train_test_split\nModuleNotFoundError: No module named 'dask_ml'\n",
  "history_begin_time" : 1690601139207,
  "history_end_time" : 1690601153621,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "jjIg5Pl53i1w",
  "history_input" : "import numpy as np\nimport pandas as pd\nimport dask.dataframe as dd\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport joblib\nfrom scipy import sparse\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = dd.read_csv('/home/chetana/gridmet_test_run/model_training_cleaned.csv')\ndata['date'] = dd.to_datetime(data['date'])\n\ndata['year'] = data['date'].dt.year\ndata['month'] = data['date'].dt.month\ndata['day'] = data['date'].dt.day\ndata['day_of_week'] = data['date'].dt.dayofweek\ndata = data.drop('date', axis=1)\n\n# Convert the Dask DataFrame to a Pandas DataFrame\ndata = data.compute()\n\nprint('feature order:', list(data.keys()))\n\n# Split the data into features (X) and target variable (y)\nX = data.drop('swe_value', axis=1)\ny = data['swe_value']\n\n# Convert X to a sparse matrix if it contains many zero values\nX_sparse = sparse.csr_matrix(X)  # Use sparse.csr_matrix for compressed sparse row format\n\n# Split the data into a small initial training set and a larger pool\nX_train_init, X_pool, y_train_init, y_pool = train_test_split(X_sparse, y, test_size=0.9, random_state=42)\n\n# Create an instance of the ExtraTreesRegressor\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n\n# Train the model with the initial training set\nmodel.fit(X_train_init, y_train_init)\n\n# The remaining data (X_pool, y_pool) can be used for incremental learning\n# For example, you can loop over the remaining data in smaller batches and update the model\nbatch_size = 1000\nfor i in range(0, len(X_pool), batch_size):\n    X_batch = X_pool[i:i+batch_size]\n    y_batch = y_pool.iloc[i:i+batch_size]\n    # Perform incremental learning with numpy instead of model.partial_fit\n    model.n_estimators += 10  # Increment n_estimators for each batch\n    model.fit(X_batch, y_batch)\n\n# Save the trained model after incremental learning\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et.pkl')\n\n# Split the data into a separate test set\nX_test, y_test = train_test_split(X_pool, y_pool, test_size=0.2, random_state=42)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "feature order: ['lat', 'lon', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'swe_value', 'slope', 'curvature', 'aspect', 'eastness', 'northness', 'year', 'month', 'day', 'day_of_week']\nTraceback (most recent call last):\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/scipy/sparse/_compressed.py\", line 79, in __init__\n    arg1 = np.asarray(arg1)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 2070, in __array__\n    return np.asarray(self._values, dtype=dtype)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/frame.py\", line 958, in _values\n    self._consolidate_inplace()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 5980, in _consolidate_inplace\n    self._protect_consolidate(f)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 5968, in _protect_consolidate\n    result = f()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 5978, in f\n    self._mgr = self._mgr.consolidate()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/managers.py\", line 686, in consolidate\n    bm._consolidate_inplace()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/managers.py\", line 1873, in _consolidate_inplace\n    self.blocks, self.refs = _consolidate_with_refs(self.blocks, self.refs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/managers.py\", line 2350, in _consolidate_with_refs\n    merged_blocks, consolidated = _merge_blocks(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/managers.py\", line 2388, in _merge_blocks\n    new_values = new_values[argsort]\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 4.69 GiB for an array with shape (4, 157282561) and data type int64\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/jjIg5Pl53i1w/model_creation_et.py\", line 30, in <module>\n    X_sparse = sparse.csr_matrix(X)  # Use sparse.csr_matrix for compressed sparse row format\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/scipy/sparse/_compressed.py\", line 81, in __init__\n    raise ValueError(\"unrecognized {}_matrix constructor usage\"\nValueError: unrecognized csr_matrix constructor usage\n",
  "history_begin_time" : 1690600623346,
  "history_end_time" : 1690601093473,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "bUprvU4W0AnJ",
  "history_input" : "import numpy as np\nimport pandas as pd\nimport dask.dataframe as dd\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport joblib\nfrom scipy import sparse\n\n# Load your dataset into a Dask DataFrame (Assuming the target variable is named 'target')\ndata = dd.read_csv('/home/chetana/gridmet_test_run/model_training_cleaned.csv')\ndata['date'] = dd.to_datetime(data['date'])\n\ndata['year'] = data['date'].dt.year\ndata['month'] = data['date'].dt.month\ndata['day'] = data['date'].dt.day\ndata['day_of_week'] = data['date'].dt.dayofweek\ndata = data.drop('date', axis=1)\n\n# Convert the Dask DataFrame to a Pandas DataFrame\ndata = data.compute()\n\nprint('feature order:', list(data.keys()))\n\n# Split the data into features (X) and target variable (y)\nX = data.drop('swe_value', axis=1)\ny = data['swe_value']\n\n# Split the data into a small initial training set and a larger pool\nX_train_init, X_pool, y_train_init, y_pool = train_test_split(X, y, test_size=0.9, random_state=42)\n\n# Create an instance of the ExtraTreesRegressor\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n\n# Train the model with the initial training set\nmodel.fit(X_train_init, y_train_init)\n\n# The remaining data (X_pool, y_pool) can be used for incremental learning\n# For example, you can loop over the remaining data in smaller batches and update the model\nbatch_size = 1000\nfor i in range(0, len(X_pool), batch_size):\n    X_batch = X_pool.iloc[i:i+batch_size]\n    y_batch = y_pool.iloc[i:i+batch_size]\n    model.partial_fit(X_batch, y_batch)\n\n# Save the trained model after incremental learning\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et.pkl')\n\n# Split the data into a separate test set\nX_test, y_test = train_test_split(X_pool, y_pool, test_size=0.2, random_state=42)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "feature order: ['lat', 'lon', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'swe_value', 'slope', 'curvature', 'aspect', 'eastness', 'northness', 'year', 'month', 'day', 'day_of_week']\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/bUprvU4W0AnJ/model_creation_et.py\", line 30, in <module>\n    X_train_init, X_pool, y_train_init, y_pool = train_test_split(X, y, test_size=0.9, random_state=42)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/model_selection/_split.py\", line 2585, in train_test_split\n    return list(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/model_selection/_split.py\", line 2587, in <genexpr>\n    (_safe_indexing(a, train), _safe_indexing(a, test)) for a in arrays\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/utils/__init__.py\", line 354, in _safe_indexing\n    return _pandas_indexing(X, indices, indices_dtype, axis=axis)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/utils/__init__.py\", line 196, in _pandas_indexing\n    return X.take(key, axis=axis)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 3871, in take\n    return self._take(indices, axis)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 3884, in _take\n    self._consolidate_inplace()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 5980, in _consolidate_inplace\n    self._protect_consolidate(f)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 5968, in _protect_consolidate\n    result = f()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 5978, in f\n    self._mgr = self._mgr.consolidate()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/managers.py\", line 686, in consolidate\n    bm._consolidate_inplace()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/managers.py\", line 1873, in _consolidate_inplace\n    self.blocks, self.refs = _consolidate_with_refs(self.blocks, self.refs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/managers.py\", line 2350, in _consolidate_with_refs\n    merged_blocks, consolidated = _merge_blocks(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/managers.py\", line 2388, in _merge_blocks\n    new_values = new_values[argsort]\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 4.69 GiB for an array with shape (4, 157282561) and data type int64\n",
  "history_begin_time" : 1690599663437,
  "history_end_time" : 1690600090410,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "xeDnisnyxmwF",
  "history_input" : "import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport joblib\n\n# Load your dataset into a pandas DataFrame (Assuming the target variable is named 'target')\ndata = pd.read_csv('/home/chetana/gridmet_test_run/model_training_cleaned.csv')\ndata['date'] = pd.to_datetime(data['date'])\n\ndata['year'] = data['date'].dt.year\ndata['month'] = data['date'].dt.month\ndata['day'] = data['date'].dt.day\ndata['day_of_week'] = data['date'].dt.dayofweek\ndata.drop('date', axis=1, inplace=True)\n\nprint('feature order:', list(data.keys()))\n\n# Split the data into features (X) and target variable (y)\nX = data.drop('swe_value', axis=1)\ny = data['swe_value']\n\n# Split the data into a small initial training set and a larger pool\nX_train_init, X_pool, y_train_init, y_pool = train_test_split(X, y, test_size=0.9, random_state=42)\n\n# Create an instance of the ExtraTreesRegressor\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n\n# Train the model with the initial training set\nmodel.fit(X_train_init, y_train_init)\n\n# The remaining data (X_pool, y_pool) can be used for incremental learning\n# For example, you can loop over the remaining data in smaller batches and update the model\nbatch_size = 10000\nfor i in range(0, len(X_pool), batch_size):\n    X_batch = X_pool[i:i+batch_size]\n    y_batch = y_pool[i:i+batch_size]\n    model.partial_fit(X_batch, y_batch)\n\n# Save the trained model after incremental learning\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et.pkl')\n\n# Split the data into a separate test set\nX_test, y_test = train_test_split(X_pool, y_pool, test_size=0.2, random_state=42)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "sh: line 1:  4596 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python model_creation_et.py\n",
  "history_begin_time" : 1690598943604,
  "history_end_time" : 1690599490481,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "T4yTfk1DDCOL",
  "history_input" : "import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport joblib\n\n# Load your dataset into a pandas DataFrame (Assuming the target variable is named 'target')\ndata = pd.read_csv('/home/chetana/gridmet_test_run/model_training_cleaned.csv')\ndata['date'] = pd.to_datetime(data['date'])\n\ndata['year'] = data['date'].dt.year\ndata['month'] = data['date'].dt.month\ndata['day'] = data['date'].dt.day\ndata['day_of_week'] = data['date'].dt.dayofweek\ndata.drop('date', axis=1, inplace=True)\n\nprint('feature order:', list(data.keys()))\n\n# Split the data into features (X) and target variable (y)\nX = data.drop('swe_value', axis=1)\ny = data['swe_value']\n\n# Split the data into a small initial training set and a larger pool\nX_train_init, X_pool, y_train_init, y_pool = train_test_split(X, y, test_size=0.9, random_state=42)\n\n# Create an instance of the ExtraTreesRegressor\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n\n# Train the model with the initial training set\nmodel.fit(X_train_init, y_train_init)\n\n# The remaining data (X_pool, y_pool) can be used for incremental learning\n# For example, you can loop over the remaining data in smaller batches and update the model\nbatch_size = 1000\nfor i in range(0, len(X_pool), batch_size):\n    X_batch = X_pool[i:i+batch_size]\n    y_batch = y_pool[i:i+batch_size]\n    model.partial_fit(X_batch, y_batch)\n\n# Save the trained model after incremental learning\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et.pkl')\n\n# Split the data into a separate test set\nX_test, y_test = train_test_split(X_pool, y_pool, test_size=0.2, random_state=42)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "",
  "history_begin_time" : 1690598882203,
  "history_end_time" : 1690598943609,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "aJb4B7Ivh3tl",
  "history_input" : "from sklearn.model_selection import train_test_split\n\n# Split the data into a small initial training set and a larger pool\nX_train_init, X_pool, y_train_init, y_pool = train_test_split(X, y, test_size=0.9, random_state=42)\n\n# Create an instance of the ExtraTreesRegressor\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n\n# Train the model with the initial training set\nmodel.fit(X_train_init, y_train_init)\n\n# The remaining data (X_pool, y_pool) can be used for incremental learning\n# For example, you can loop over the remaining data in smaller batches and update the model\nbatch_size = 100000\nfor i in range(0, len(X_pool), batch_size):\n    X_batch = X_pool[i:i+batch_size]\n    y_batch = y_pool[i:i+batch_size]\n    model.partial_fit(X_batch, y_batch)\n\n# Save the trained model after incremental learning\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et.pkl')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/aJb4B7Ivh3tl/model_creation_et.py\", line 4, in <module>\n    X_train_init, X_pool, y_train_init, y_pool = train_test_split(X, y, test_size=0.9, random_state=42)\nNameError: name 'X' is not defined\n",
  "history_begin_time" : 1690598724999,
  "history_end_time" : 1690598731191,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "exFtmqlKnhGn",
  "history_input" : "import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport joblib\n\n# Load your dataset into a pandas DataFrame (Assuming the target variable is named 'target')\ndata = pd.read_csv('/home/chetana/gridmet_test_run/model_training_cleaned.csv')\ndata['date'] = pd.to_datetime(data['date'])\n\ndata['year'] = data['date'].dt.year\ndata['month'] = data['date'].dt.month\ndata['day'] = data['date'].dt.day\ndata['day_of_week'] = data['date'].dt.dayofweek\ndata.drop('date', axis=1, inplace=True)\n# data.drop('station_elevation', inplace=True, errors='ignore')\n\nprint('feature order:', list(data.keys()))\n\n# Split the data into features (X) and target variable (y)\nX = data.drop('swe_value', axis=1)\ny = data['swe_value']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create an instance of the ExtraTreesRegressor\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Save the trained model\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et.pkl')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)",
  "history_output" : "feature order: ['lat', 'lon', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'swe_value', 'slope', 'curvature', 'aspect', 'eastness', 'northness', 'year', 'month', 'day', 'day_of_week']\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/exFtmqlKnhGn/model_creation_et.py\", line 26, in <module>\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/model_selection/_split.py\", line 2585, in train_test_split\n    return list(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/model_selection/_split.py\", line 2587, in <genexpr>\n    (_safe_indexing(a, train), _safe_indexing(a, test)) for a in arrays\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/utils/__init__.py\", line 354, in _safe_indexing\n    return _pandas_indexing(X, indices, indices_dtype, axis=axis)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/sklearn/utils/__init__.py\", line 196, in _pandas_indexing\n    return X.take(key, axis=axis)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 3871, in take\n    return self._take(indices, axis)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 3884, in _take\n    self._consolidate_inplace()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 5980, in _consolidate_inplace\n    self._protect_consolidate(f)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 5968, in _protect_consolidate\n    result = f()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 5978, in f\n    self._mgr = self._mgr.consolidate()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/managers.py\", line 686, in consolidate\n    bm._consolidate_inplace()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/managers.py\", line 1873, in _consolidate_inplace\n    self.blocks, self.refs = _consolidate_with_refs(self.blocks, self.refs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/managers.py\", line 2350, in _consolidate_with_refs\n    merged_blocks, consolidated = _merge_blocks(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/managers.py\", line 2381, in _merge_blocks\n    new_values = np.vstack([b.values for b in blocks])  # type: ignore[misc]\n  File \"<__array_function__ internals>\", line 180, in vstack\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/numpy/core/shape_base.py\", line 282, in vstack\n    return _nx.concatenate(arrs, 0)\n  File \"<__array_function__ internals>\", line 180, in concatenate\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 4.69 GiB for an array with shape (4, 157282561) and data type int64\n",
  "history_begin_time" : 1690597078460,
  "history_end_time" : 1690597751471,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "4qHeW6At3bIM",
  "history_input" : "import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport joblib\n\n# Load your dataset into a pandas DataFrame (Assuming the target variable is named 'target')\ndata = pd.read_csv('/home/chetana/gridmet_test_run/final_training_data.csv')\ndata['date'] = pd.to_datetime(data['date'])\n\ndata['year'] = data['date'].dt.year\ndata['month'] = data['date'].dt.month\ndata['day'] = data['date'].dt.day\ndata['day_of_week'] = data['date'].dt.dayofweek\ndata.drop('date', axis=1, inplace=True)\ndata.drop('station_elevation', inplace=True, errors='ignore')\n\n# Split the data into features (X) and target variable (y)\nX = data.drop('swe_value', axis=1)\ny = data['swe_value']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create an instance of the ExtraTreesRegressor\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Save the trained model\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et.pkl')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)",
  "history_output" : "Mean Squared Error (MSE): 0.273139660484067\nRoot Mean Squared Error (RMSE): 0.522627649942162\nMean Absolute Error (MAE): 0.01861515633681594\nR-squared (R2): 0.9996713036422928\n",
  "history_begin_time" : 1690183598760,
  "history_end_time" : 1690188035224,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "6xjxMElJsQcT",
  "history_input" : "import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport joblib\n\n# Load your dataset into a pandas DataFrame (Assuming the target variable is named 'target')\ndata = pd.read_csv('/home/chetana/gridmet_test_run/final_training_data.csv')\ndata['date'] = pd.to_datetime(data['date'])\n\ndata['year'] = data['date'].dt.year\ndata['month'] = data['date'].dt.month\ndata['day'] = data['date'].dt.day\ndata['day_of_week'] = data['date'].dt.dayofweek\ndata.drop('date', axis=1, inplace=True)\ndata.drop('station_elevation', inplace=True, errors='ignore')\n\n# Split the data into features (X) and target variable (y)\nX = data.drop('swe_value', axis=1)\ny = data['swe_value']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create an instance of the ExtraTreesRegressor\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Save the trained model\njoblib.dump(model, '/home/chetana/gridmet_test_run/model_creation_et.pkl')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)",
  "history_output" : "sh: line 1:  3498 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python model_creation_et.py\n",
  "history_begin_time" : 1690181880290,
  "history_end_time" : 1690182769094,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "CmqLU87uaBqg",
  "history_input" : "import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport joblib\n\n# Load your dataset into a pandas DataFrame (Assuming the target variable is named 'target')\ndata = pd.read_csv('/home/chetana/gridmet_test_run/final_training_data.csv')\ndata['date'] = pd.to_datetime(data['date'])\n\ndata['year'] = data['date'].dt.year\ndata['month'] = data['date'].dt.month\ndata['day'] = data['date'].dt.day\ndata['day_of_week'] = data['date'].dt.dayofweek\ndata.drop('date', axis=1, inplace=True)\ndata.drop('station_elevation', inplace=True, errors='ignore')\n\n# Split the data into features (X) and target variable (y)\nX = data.drop('swe_value', axis=1)\ny = data['swe_value']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create an instance of the ExtraTreesRegressor\nmodel = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Save the trained model\njoblib.dump(model, '/home/chetana/gridmet_test_run/extra_trees_model_new_cleaned.pkl')\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R2):\", r2)",
  "history_output" : "",
  "history_begin_time" : 1690181828675,
  "history_end_time" : 1690181880397,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "9q73kepee06",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1689632033812,
  "history_end_time" : 1689632033812,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "qwbpszj1zsd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1689631636466,
  "history_end_time" : 1689631636466,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "eysx0rzout5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1689135058049,
  "history_end_time" : 1689135058049,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "98ix1ruct8b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1688416860238,
  "history_end_time" : 1688416907376,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "jf7wuu",
  "indicator" : "Stopped"
},{
  "history_id" : "l3kafxzcge2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1688416833718,
  "history_end_time" : 1688416848468,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "jf7wuu",
  "indicator" : "Stopped"
},{
  "history_id" : "7yjxs28gi5x",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1688416668379,
  "history_end_time" : 1688416822957,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "jf7wuu",
  "indicator" : "Stopped"
},{
  "history_id" : "t5rl94var9p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1688416628777,
  "history_end_time" : 1688416660675,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "jf7wuu",
  "indicator" : "Stopped"
},{
  "history_id" : "cngsteuadza",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1688416567324,
  "history_end_time" : 1688416575019,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "jf7wuu",
  "indicator" : "Stopped"
},{
  "history_id" : "aujzoisawvb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1687546866701,
  "history_end_time" : 1687546866701,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "5s07joi429b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1687546866700,
  "history_end_time" : 1687546866700,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8b93o7ohce1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1687463685023,
  "history_end_time" : 1687463685023,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xofb5qnwgdu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1687463635418,
  "history_end_time" : 1687463635418,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "lfzvnwggrq7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1686236147017,
  "history_end_time" : 1686237909495,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "jf7wuu",
  "indicator" : "Stopped"
},{
  "history_id" : "tird6r03tln",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1686235960974,
  "history_end_time" : 1686235985414,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "jf7wuu",
  "indicator" : "Stopped"
},{
  "history_id" : "z81kun8snlx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1686235529630,
  "history_end_time" : 1686235529630,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "jf7wuu",
  "indicator" : "Skipped"
},{
  "history_id" : "fhts1fcqjkp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1686235448229,
  "history_end_time" : 1686235482630,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "jf7wuu",
  "indicator" : "Stopped"
},{
  "history_id" : "qls83mskdpg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1686235402214,
  "history_end_time" : 1686235424784,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "jf7wuu",
  "indicator" : "Stopped"
},{
  "history_id" : "gfajsrwlo41",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1686153654214,
  "history_end_time" : 1686153654214,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "jf7wuu",
  "indicator" : "Skipped"
},{
  "history_id" : "5maeasyhj6k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1682984798068,
  "history_end_time" : 1682984800295,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "jf7wuu",
  "indicator" : "Stopped"
},{
  "history_id" : "8yiga4qnyu6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1681516927642,
  "history_end_time" : 1681516927642,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "7uzioe97mtj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1681039707714,
  "history_end_time" : 1681039707714,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "67e50unk58m",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1681039689408,
  "history_end_time" : 1681039697767,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "x6z5e26f3ce",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1681007820130,
  "history_end_time" : 1681007820130,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "w3i8n0so6ut",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1679442743789,
  "history_end_time" : 1679442743789,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "jf7wuu",
  "indicator" : "Skipped"
},{
  "history_id" : "x27pic4nwpm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1679442743786,
  "history_end_time" : 1679442743786,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "jf7wuu",
  "indicator" : "Skipped"
},{
  "history_id" : "bvs8uz5xbbf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1679332584794,
  "history_end_time" : 1679332584794,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "jf7wuu",
  "indicator" : "Skipped"
},{
  "history_id" : "59y1s4pkvrd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1679332584792,
  "history_end_time" : 1679332584792,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "jf7wuu",
  "indicator" : "Skipped"
},{
  "history_id" : "xirsz1q5mkk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1679191258499,
  "history_end_time" : 1679191258499,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "2jifky",
  "indicator" : "Skipped"
},{
  "history_id" : "n8l0wozhwym",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1679191258498,
  "history_end_time" : 1679191258498,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "2jifky",
  "indicator" : "Skipped"
},{
  "history_id" : "0dlh0b65ied",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1679091534992,
  "history_end_time" : 1679091744970,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "m7frw6mvc7l",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1679091534989,
  "history_end_time" : 1679091744970,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "momkreu5mym",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1679091527588,
  "history_end_time" : 1679091533673,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "t5b2qr1jxcc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1679091527474,
  "history_end_time" : 1679091533670,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "vom43i7j3hr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678887964100,
  "history_end_time" : 1678888215698,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "m4d3q8trcw4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678887964099,
  "history_end_time" : 1678888215698,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "3n2jdto12sr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678887944952,
  "history_end_time" : 1678887946446,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "ahtayuj3p23",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678887944952,
  "history_end_time" : 1678887946446,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "lrico30fenj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678887287259,
  "history_end_time" : 1678887836233,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "4w39lx6pztw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678887287256,
  "history_end_time" : 1678887836233,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "by8meu20u7q",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678886689975,
  "history_end_time" : 1678887010629,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "ez93uyks65b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678886689971,
  "history_end_time" : 1678887010629,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "jobhqqmxooe",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678884557547,
  "history_end_time" : 1678884986358,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "p6pikyg4kzo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678884557544,
  "history_end_time" : 1678884986358,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "lc7dh0bk1nt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678884501901,
  "history_end_time" : 1678884535354,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "iwksrzooy8u",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678884501896,
  "history_end_time" : 1678884535354,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "do4voej56i9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678884207346,
  "history_end_time" : 1678884438334,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "hr93w8dmntp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678884207344,
  "history_end_time" : 1678884438334,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "9xd2br1kj51",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678884125223,
  "history_end_time" : 1678884140269,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "1p8qdrfmnci",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678884125219,
  "history_end_time" : 1678884140269,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "rj8jtb3f9ju",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678884039393,
  "history_end_time" : 1678884042290,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "bqusxuoq0ap",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678884039391,
  "history_end_time" : 1678884042290,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "jodwh5h8gbl",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, '''max_features='auto',''' max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/jodwh5h8gbl/model_creation_et.py\", line 41\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, '''max_features='auto',''' max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                                                                                                                              ^\nSyntaxError: positional argument follows keyword argument\n",
  "history_begin_time" : 1678883327449,
  "history_end_time" : 1678883775458,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "cwuekjmdf02",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678883288648,
  "history_end_time" : 1678883775458,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "cpr2c0kwo6r",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, '''max_features='auto',''' max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/cpr2c0kwo6r/model_creation_et.py\", line 41\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, '''max_features='auto',''' max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                                                                                                                              ^\nSyntaxError: positional argument follows keyword argument\n",
  "history_begin_time" : 1678756761625,
  "history_end_time" : 1678756762958,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "jsw29vfpu7a",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678756752348,
  "history_end_time" : 1678756752348,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "vjsry0r527p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678756683316,
  "history_end_time" : 1678756684837,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "sg16yn842et",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678749631735,
  "history_end_time" : 1678749936029,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "8lin9pzz4mo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678749631729,
  "history_end_time" : 1678749936028,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "i3zmoilod8t",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678748546946,
  "history_end_time" : 1678748546946,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "7kpcfb9mu2i",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678747245317,
  "history_end_time" : 1678747245317,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "ht7h5ztk5n5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678747245311,
  "history_end_time" : 1678747245311,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "tjra16hmblf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678746481227,
  "history_end_time" : 1678746793060,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "igrpr0damup",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678746481216,
  "history_end_time" : 1678746793059,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "1hxpubqx1o2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678743868500,
  "history_end_time" : 1678744167107,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "40i1rmriogy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678743868499,
  "history_end_time" : 1678744167106,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "ftbuf61nd2c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678743629021,
  "history_end_time" : 1678743629021,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "r01ls4uw6ln",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678743629019,
  "history_end_time" : 1678743629019,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "3nfv5zqd4yk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678743144823,
  "history_end_time" : 1678743615535,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "x1vt5lc2e3d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678743144822,
  "history_end_time" : 1678743615535,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "qujqvj5dm0z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678742583290,
  "history_end_time" : 1678742583290,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "5q2qgz4w80d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678742583289,
  "history_end_time" : 1678742583289,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "nbe1lshvwta",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678742209486,
  "history_end_time" : 1678742571511,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "bbzgz2dpidl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678742209483,
  "history_end_time" : 1678742571511,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "27f0v0pud9x",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678738707276,
  "history_end_time" : 1678738707276,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "19er2t3yx1g",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678738707274,
  "history_end_time" : 1678738707274,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "jigh9nl4t7u",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678725425014,
  "history_end_time" : 1678725425014,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "u40lgnasntg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678725424892,
  "history_end_time" : 1678725424892,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "f4e1hlra6gf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678723111834,
  "history_end_time" : 1678725408358,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "53h8avrpx9p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678723111823,
  "history_end_time" : 1678725408356,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "198bqxz7f5j",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678649002073,
  "history_end_time" : 1678649002073,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "l9vzd1m2hgn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678649002070,
  "history_end_time" : 1678649002070,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "fdyq176fimx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678648366923,
  "history_end_time" : 1678648366923,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "h2eqa3wch4x",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678648366920,
  "history_end_time" : 1678648366920,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "ab1s92l6dds",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678648337357,
  "history_end_time" : 1678648341661,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "k66k748cjga",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678648337344,
  "history_end_time" : 1678648341660,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "ptufi5ed55k",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, '''max_features='auto',''' max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/ptufi5ed55k/model_creation_et.py\", line 41\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, '''max_features='auto',''' max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                                                                                                                              ^\nSyntaxError: positional argument follows keyword argument\n",
  "history_begin_time" : 1678564607188,
  "history_end_time" : 1678564608540,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "ztn0nuiyjls",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678564567758,
  "history_end_time" : 1678564567758,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "gver70bqw8e",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, '''max_features='auto',''' max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/gver70bqw8e/model_creation_et.py\", line 41\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, '''max_features='auto',''' max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                                                                                                                              ^\n",
  "history_begin_time" : 1678564554066,
  "history_end_time" : 1678564556718,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Failed"
},{
  "history_id" : "ulcflprd983",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678564538715,
  "history_end_time" : 1678564538715,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "nrebx1i5ifc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678557921962,
  "history_end_time" : 1678557923651,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "682ra80i8pr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678557921959,
  "history_end_time" : 1678557923651,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "lrrc1qqwq5i",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678557705831,
  "history_end_time" : 1678557898800,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "80zg2e6vaos",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678557705828,
  "history_end_time" : 1678557898797,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "jl5u26463i2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678497028189,
  "history_end_time" : 1678497028189,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "sca2o5ok8gi",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678497028182,
  "history_end_time" : 1678497028182,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "xqituu4q5i6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678330214225,
  "history_end_time" : 1678330214225,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "spbodsukvkp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678330214214,
  "history_end_time" : 1678330214214,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "obudlra6og6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678326965120,
  "history_end_time" : 1694185584966,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "a3x0h3mbkuu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678326965116,
  "history_end_time" : 1694185584964,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "kap4u053z7c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678312067635,
  "history_end_time" : 1678312067635,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "jn8bqywrdja",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678312067634,
  "history_end_time" : 1678312067634,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "e3z2qgnjafo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678312030785,
  "history_end_time" : 1678312065074,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "cvwairb0jum",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678312030783,
  "history_end_time" : 1678312065074,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "x02amd37k5c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678312001517,
  "history_end_time" : 1694185586322,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "yqabpj4jltq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678312001514,
  "history_end_time" : 1694185586320,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "qhylqo10hfp",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "Running",
  "history_begin_time" : 1678283527468,
  "history_end_time" : 1678283530931,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "hl54nd58w0q",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678283483050,
  "history_end_time" : 1678283483050,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "fvn9t5s5si4",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "",
  "history_begin_time" : 1678242441647,
  "history_end_time" : 1694185588571,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "o54278n0yq2",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "",
  "history_begin_time" : 1678241840649,
  "history_end_time" : 1694185588093,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "jvrh1qrc4ke",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678241816411,
  "history_end_time" : 1694185588095,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "zoii7a8c6id",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678241789982,
  "history_end_time" : 1694185588573,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "q17o676sm48",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "Running",
  "history_begin_time" : 1678206373516,
  "history_end_time" : 1678206379284,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "hzwl6x6ou71",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678206364593,
  "history_end_time" : 1678206379284,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "v7srx06hl8p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678206134299,
  "history_end_time" : 1678206143169,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "kyxwzdjfljs",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "",
  "history_begin_time" : 1678201922865,
  "history_end_time" : 1678201928753,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "qqfniw5afu5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678201909683,
  "history_end_time" : 1678201909683,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "uts8tc46swe",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678201729145,
  "history_end_time" : 1694185589152,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "51qdf4vba94",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678201707338,
  "history_end_time" : 1694185589780,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "xgbvb3ptufa",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "Running",
  "history_begin_time" : 1678201689552,
  "history_end_time" : 1694185590817,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "f5cphfuvuza",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678201688178,
  "history_end_time" : 1678201703983,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "h0ax84nv5ps",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678201677299,
  "history_end_time" : 1678201687067,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "8m640b1gnyh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678201567679,
  "history_end_time" : 1694185590819,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "ujh45u33r3a",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "Running",
  "history_begin_time" : 1678201349654,
  "history_end_time" : 1678201516420,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "qn7lt6lo79u",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678201335975,
  "history_end_time" : 1678201516420,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "nlee6yb3cjv",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "Running",
  "history_begin_time" : 1678155242795,
  "history_end_time" : 1678155246205,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "thqn4wuiunq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678155182777,
  "history_end_time" : 1678155182777,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "12iqou6zysl",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "Running",
  "history_begin_time" : 1678144764677,
  "history_end_time" : 1678154846288,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "nzpbe86zcn9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678144724394,
  "history_end_time" : 1678154846289,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "ys75jo865i9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677959621661,
  "history_end_time" : 1677959722653,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "l7a23a0tsyu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677959587591,
  "history_end_time" : 1694185592257,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "p2ank3i8ner",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677959568303,
  "history_end_time" : 1677959583158,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "tblt9j1d4yz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677958880515,
  "history_end_time" : 1677958952870,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "pxxhxai3lli",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677958856300,
  "history_end_time" : 1694185593062,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "78ovvdm45oc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677958842103,
  "history_end_time" : 1677958849893,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "j54wcvp6q0h",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677958735471,
  "history_end_time" : 1677958754132,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "5qpliw6brrq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677958274938,
  "history_end_time" : 1677958291220,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "kelo9bm9vdq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677867848039,
  "history_end_time" : 1694185597332,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "7cbcru82qll",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "",
  "history_begin_time" : 1677858942787,
  "history_end_time" : 1677867648730,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "i173z3i008b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677858933368,
  "history_end_time" : 1677867648731,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "3565kdel9w6",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "Running",
  "history_begin_time" : 1677858785007,
  "history_end_time" : 1694185596833,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "pltvqv6m0tc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677858755854,
  "history_end_time" : 1694185596842,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "wljc2q02aq3",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "Running",
  "history_begin_time" : 1677809798041,
  "history_end_time" : 1694185596457,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "oxnl06gn0y3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677809796048,
  "history_end_time" : 1677809840777,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "al3i0kx4hrs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677809675931,
  "history_end_time" : 1694185596458,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "90cytqomkyx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677809602159,
  "history_end_time" : 1694185623001,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "fxth98ciqa5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677809590504,
  "history_end_time" : 1694185622580,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "wek1291c7hm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677809556021,
  "history_end_time" : 1677809573439,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "wbhwq52ke16",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677809539135,
  "history_end_time" : 1677809554697,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "x2b5zquvuqe",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677809468337,
  "history_end_time" : 1694185621792,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "lc0x0t7umv4",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "Running",
  "history_begin_time" : 1677809443300,
  "history_end_time" : 1694185620826,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "dcai1drgazp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677809401972,
  "history_end_time" : 1694185620826,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "nyp0xgtsk31",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677809343299,
  "history_end_time" : 1694185620473,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "pnse9odtzx6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677809175089,
  "history_end_time" : 1677809306589,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "4303b7if6sj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677809134775,
  "history_end_time" : 1677809171462,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "8igf2uwvyiq",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "Running",
  "history_begin_time" : 1677808129491,
  "history_end_time" : 1694185619659,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "m2xwctn2jot",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677808073975,
  "history_end_time" : 1694185619659,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "7bobc7qn3hs",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "Running",
  "history_begin_time" : 1677797442565,
  "history_end_time" : 1694185619266,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "ujn46b5m4w3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677797429029,
  "history_end_time" : 1694185619267,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "g1j21xr0oiz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677797242671,
  "history_end_time" : 1694185618942,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "1lftp48gag8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677797147691,
  "history_end_time" : 1694185616024,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "us2hdq1usiv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677796558982,
  "history_end_time" : 1677797113233,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "ai4zvwtziuw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677796509193,
  "history_end_time" : 1694185615035,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "hqtlnijwnr3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677796344437,
  "history_end_time" : 1677796528249,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "kogqt0qg0n9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677796268588,
  "history_end_time" : 1694185613621,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "0ihd6l4byta",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677795409798,
  "history_end_time" : 1694185613238,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "SgXJuQWz8nZ6",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "",
  "history_begin_time" : 1677795380459,
  "history_end_time" : 1677795384024,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "cwXuiqmY986e",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, '''ccp_alpha=0.0, '''criterion='mse',\n                    max_depth=None, max_features='auto', max_leaf_nodes=None,\n                    max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/cwXuiqmY986e/model_creation_et.py\", line 41\n    etmodel = ExtraTreesRegressor(bootstrap=False, '''ccp_alpha=0.0, '''criterion='mse',\n                                                                        ^\nSyntaxError: positional argument follows keyword argument\n",
  "history_begin_time" : 1677795302230,
  "history_end_time" : 1677795303603,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "XvyLQqdVavqM",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, '''ccp_alpha=0.0, '''criterion='mse',\n                    max_depth=None, max_features='auto', max_leaf_nodes=None,\n                    max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/XvyLQqdVavqM/model_creation_et.py\", line 41\n    etmodel = ExtraTreesRegressor(bootstrap=False, '''ccp_alpha=0.0, '''criterion='mse',\n                                                                        ^\nSyntaxError: positional argument follows keyword argument\n",
  "history_begin_time" : 1677793964308,
  "history_end_time" : 1677793965998,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "vimr1lwsdjs",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, '''ccp_alpha=0.0, '''criterion='mse',\n                    max_depth=None, max_features='auto', max_leaf_nodes=None,\n                    max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/vimr1lwsdjs/model_creation_et.py\", line 41\n    etmodel = ExtraTreesRegressor(bootstrap=False, '''ccp_alpha=0.0, '''criterion='mse',\n                                                                        ^\nSyntaxError: positional argument follows keyword argument\n",
  "history_begin_time" : 1677793001269,
  "history_end_time" : 1694185612781,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "2ntz59ki52j",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677792975432,
  "history_end_time" : 1694185612782,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "r89ppgysk1n",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677792528369,
  "history_end_time" : 1694185612306,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "zdz52q2yyqg",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, '''ccp_alpha=0.0, '''criterion='Squared_error',\n                    max_depth=None, max_features='auto', max_leaf_nodes=None,\n                    max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/zdz52q2yyqg/model_creation_et.py\", line 41\n    etmodel = ExtraTreesRegressor(bootstrap=False, '''ccp_alpha=0.0, '''criterion='Squared_error',\n                                                                        ^\nSyntaxError: positional argument follows keyword argument\n",
  "history_begin_time" : 1677788298641,
  "history_end_time" : 1694185610914,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "mv9fbtply9r",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, '''ccp_alpha=0.0, '''criterion='Squared_error',\n                    max_depth=None, max_features='auto', max_leaf_nodes=None,\n                    max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/mv9fbtply9r/model_creation_et.py\", line 41\n    etmodel = ExtraTreesRegressor(bootstrap=False, '''ccp_alpha=0.0, '''criterion='Squared_error',\n                                                                        ^\nSyntaxError: positional argument follows keyword argument\n",
  "history_begin_time" : 1677787299491,
  "history_end_time" : 1677787302314,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Failed"
},{
  "history_id" : "p0jrc0bawq2",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, '''ccp_alpha=0.0, '''criterion='Squared_error',\n                    max_depth=None, max_features='auto', max_leaf_nodes=None,\n                    max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/p0jrc0bawq2/model_creation_et.py\", line 41\n    etmodel = ExtraTreesRegressor(bootstrap=False, '''ccp_alpha=0.0, '''criterion='Squared_error',\n                                                                        ^\nSyntaxError: positional argument follows keyword argument\n",
  "history_begin_time" : 1677786007165,
  "history_end_time" : 1677786042656,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "44osepq95md",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, '''ccp_alpha=0.0, '''criterion='mse',\n                    max_depth=None, max_features='auto', max_leaf_nodes=None,\n                    max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/44osepq95md/model_creation_et.py\", line 41\n    etmodel = ExtraTreesRegressor(bootstrap=False, '''ccp_alpha=0.0, '''criterion='mse',\n                                                                        ^\nSyntaxError: positional argument follows keyword argument\n",
  "history_begin_time" : 1677784297826,
  "history_end_time" : 1677784516789,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "fqdvnc3ye4k",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, '''ccp_alpha=0.0, '''criterion='mse',\n                    max_depth=None, max_features='auto', max_leaf_nodes=None,\n                    max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/fqdvnc3ye4k/model_creation_et.py\", line 41\n    etmodel = ExtraTreesRegressor(bootstrap=False, '''ccp_alpha=0.0, '''criterion='mse',\n                                                                        ^\nSyntaxError: positional argument follows keyword argument\n",
  "history_begin_time" : 1677784217490,
  "history_end_time" : 1677784272026,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "r1vrkdjzteo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677782901843,
  "history_end_time" : 1677782901843,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "qpd2obfw4gw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677781745188,
  "history_end_time" : 1677781745188,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "gkqk23tbnot",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677719046617,
  "history_end_time" : 1677719046617,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "larrhbov708",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677718179135,
  "history_end_time" : 1677718179135,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "30btjnidi77",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677679556280,
  "history_end_time" : 1677679556280,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "azor3e1w5e8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677679507392,
  "history_end_time" : 1677679549087,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "3oupjwx05cr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677636286907,
  "history_end_time" : 1677636286907,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "z91ec2c4nw3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677636150406,
  "history_end_time" : 1677636150406,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "3t3qp8wo53r",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677636137259,
  "history_end_time" : 1677636142814,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "ytvx91dzhed",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677636063809,
  "history_end_time" : 1677636063809,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "khe8kun5odk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677635881870,
  "history_end_time" : 1677635881870,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "69lrf1ok03p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677617762838,
  "history_end_time" : 1677617762838,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "grzfbkbq2kx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677606170929,
  "history_end_time" : 1677606170929,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "y7ji3ht3gil",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677606114276,
  "history_end_time" : 1677606114276,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "m9lcvcuo3c6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677582848599,
  "history_end_time" : 1677582848599,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "nv1t10fecju",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677525426794,
  "history_end_time" : 1677525426794,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "x52mvh1badz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677462325798,
  "history_end_time" : 1694185608921,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "3ovzc0prpk6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677462312533,
  "history_end_time" : 1694185608713,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "6m42zlfsw02",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677462312139,
  "history_end_time" : 1694185608658,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "4vbwbnhus5l",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677428742782,
  "history_end_time" : 1677428742782,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "phao2vccjib",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677428687392,
  "history_end_time" : 1677428687392,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "2swgbu68bjy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677426263067,
  "history_end_time" : 1677426263067,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "1jt9eyy62lk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677379889852,
  "history_end_time" : 1677379889852,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "w9hjq62ze1m",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677379837843,
  "history_end_time" : 1677379837843,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "j3953xnzxq9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677352478071,
  "history_end_time" : 1677352478071,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "66ng73o3gvf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677352389952,
  "history_end_time" : 1677352389952,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "wquyqn9rv4b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677352335906,
  "history_end_time" : 1677352335906,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "otzk0605ter",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677344120032,
  "history_end_time" : 1677344120032,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "0ygsa2ws0vx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677282603056,
  "history_end_time" : 1677282603056,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "uizqfnkv0wv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677273712247,
  "history_end_time" : 1677273712247,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "d6xu5pbrytp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677273698763,
  "history_end_time" : 1677273703950,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "1845po94k4z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677273673930,
  "history_end_time" : 1677273679536,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "u61o9hml2hk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677273657819,
  "history_end_time" : 1677273665454,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "j6zdipsj8je",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677273536028,
  "history_end_time" : 1677273536028,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "rb7iamod6ew",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677273519125,
  "history_end_time" : 1677273525488,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "gjmgr9hr3yf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677273371313,
  "history_end_time" : 1677273371313,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "o1pd2uoc6q4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677273340455,
  "history_end_time" : 1677273345444,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "of20so3kxku",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677273323509,
  "history_end_time" : 1677273332235,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "f7xar6rs2u3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677273146676,
  "history_end_time" : 1677273146676,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "j83ij1ykj68",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677273100255,
  "history_end_time" : 1677273134494,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "skry6ozkbot",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677201275944,
  "history_end_time" : 1677201275944,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "o681jwo8pg0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677192311086,
  "history_end_time" : 1677192311086,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "5bqw99zh8zu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677192268383,
  "history_end_time" : 1677192268383,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "8ky10a6tm4u",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677191916754,
  "history_end_time" : 1677191916754,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "zasor8tpcmc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677184296714,
  "history_end_time" : 1677184296714,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "6pa1byly1qq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677184173571,
  "history_end_time" : 1677184173571,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "yoortox4njv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677113476484,
  "history_end_time" : 1677113476484,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "cyicrcuhhjf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677108238657,
  "history_end_time" : 1677108238657,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "a1h6nbp65ru",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677108212593,
  "history_end_time" : 1677108229742,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "jbmaaqkc9e0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677107869818,
  "history_end_time" : 1677107869818,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "4v6e2ibngqs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677107757971,
  "history_end_time" : 1677107757971,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "s94tfipll1r",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677107718312,
  "history_end_time" : 1677107718312,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "m2l3iismjzj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677107637174,
  "history_end_time" : 1677107705677,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "mmhh9pfpvlg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677107604043,
  "history_end_time" : 1677107608764,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "pquz6rrbmgl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677107556540,
  "history_end_time" : 1677107562722,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "3htm8c52oe9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677107525024,
  "history_end_time" : 1677107538165,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "1f7ux1xp2mi",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677107501109,
  "history_end_time" : 1677107501109,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "pxucsq8r4am",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677107474177,
  "history_end_time" : 1677107474177,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "f2dkrk2pmu1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677106516128,
  "history_end_time" : 1677106516128,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "bx7trqxgrro",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677106477079,
  "history_end_time" : 1677106477079,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "wi6e1e44p19",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677106431070,
  "history_end_time" : 1677106431070,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "u2icr9zvao8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677106134930,
  "history_end_time" : 1677106147568,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "wzff5s8zv76",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677106011081,
  "history_end_time" : 1677106011081,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "8olpnegried",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677030843747,
  "history_end_time" : 1677030843747,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "3o445va6bqq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677030771175,
  "history_end_time" : 1677030771175,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "swdqdiafcqk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677030672669,
  "history_end_time" : 1677030672669,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "306kn13sjle",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677030563929,
  "history_end_time" : 1677030563929,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "ne1j9r3nm0w",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677025528693,
  "history_end_time" : 1677025528693,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "m00zyr7sljq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677025468127,
  "history_end_time" : 1677025468127,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "drg1qh658bg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677017826432,
  "history_end_time" : 1677017826432,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "310nga444u7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677017218442,
  "history_end_time" : 1677017218442,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "zjyheh6u5mt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677016681334,
  "history_end_time" : 1677016681334,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "13gbz0wcb5i",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677016143020,
  "history_end_time" : 1677016143020,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "xkm8bd3wag0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677016063973,
  "history_end_time" : 1677016063973,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "0073hv1382a",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677015787660,
  "history_end_time" : 1677015787660,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "i77b5df6xrc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677015739971,
  "history_end_time" : 1677015739971,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "ctfu2dm9o17",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677014885004,
  "history_end_time" : 1677014885004,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "uhk2mkj5ju8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677014795678,
  "history_end_time" : 1677014795678,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "h8pgmg9hvho",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677014228165,
  "history_end_time" : 1677014228165,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "gq4qu6p72lj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677013908469,
  "history_end_time" : 1677013908469,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "5gljdptb46g",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677013833089,
  "history_end_time" : 1677013833089,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "qivi4tky3kb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677011873127,
  "history_end_time" : 1677011873127,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "9zhd7q5q5vb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677008198619,
  "history_end_time" : 1677008198619,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "kbbakevo9gs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677008164614,
  "history_end_time" : 1677008164614,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "do4c9tqwtud",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677001999618,
  "history_end_time" : 1677001999618,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "buyji6soaq9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677001732278,
  "history_end_time" : 1677001732278,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8k8g83zu09v",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677001593936,
  "history_end_time" : 1677001593936,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "fd1arrmx9vp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677000536834,
  "history_end_time" : 1677000536834,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "qfz8j1sw1c3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1676999722181,
  "history_end_time" : 1676999722181,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "yb51al",
  "indicator" : "Skipped"
},{
  "history_id" : "bes3kx1wi4p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1676999599222,
  "history_end_time" : 1676999599222,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "j17rfdoh01c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1676862212462,
  "history_end_time" : 1676862212462,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "nwng5zrdym8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1676329536290,
  "history_end_time" : 1676329536290,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "52fnz9ltr21",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1676329491819,
  "history_end_time" : 1676329491819,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0osyvj4ndiq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1676063613437,
  "history_end_time" : 1676063613437,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "gwe3690is8p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1675783782336,
  "history_end_time" : 1675783782336,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "04zyiua3ijy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1672014982968,
  "history_end_time" : 1672014982968,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "a4s9ojdwemc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1671944382481,
  "history_end_time" : 1671944382481,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "j2ah2w2eqk3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1670910617306,
  "history_end_time" : 1670910617306,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "le8g78ua173",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1670910501221,
  "history_end_time" : 1670910501221,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "reku5fe9dlo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1670910268486,
  "history_end_time" : 1670910268486,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "o55rlxsn2vs",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677809171461,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "dk6xytv1cdt",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678201703981,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "tdt9pbwnsgl",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1694185611225,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "9rsjvvfcyut",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677785529433,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "bj5tl30ajfh",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677797113232,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "nxovax1j4w5",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677809840774,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "ouj55ecjegv",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677959722651,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "sz3cnzsa0ua",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677959583156,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "xzezv4jhhkx",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678206143157,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "d9dmvrxiemj",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678201687065,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "j0jk3ys184q",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677958849892,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "jj4dj5wmx7m",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677809573438,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "fvwci1tohs2",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677785383334,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "gg4dzw0n47p",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677796528249,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "s3cz1e64o6y",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677809306588,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "x24xk449thi",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677809554695,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "s0xi6r85xh4",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677958291218,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "sn34u2x6zi2",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677958754131,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "lxkdko99o9n",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1677958952869,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "ilag9pn5tin",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678756684831,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},]
