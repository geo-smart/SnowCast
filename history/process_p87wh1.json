[{
  "history_id" : "ZDPiMEr28hff",
  "history_input" : "\n\n# Write first python in Geoweaver\nimport pandas as pd\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nimport geojson\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom snowcast_utils import *\nimport traceback\n\nfrom datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\nprint(sys.path)\n\ntry:\n  from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n  from bs4 import BeautifulSoup\n\nprint(\"testing...\")\ndef get_nearest_stations_for_all_grids():\n  \n  print(\"Start to get nearest stations..\")\n  \n  cache_file = f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\"\n  \n  if os.path.exists(cache_file):\n    with open(cache_file, \"rb\") as fp:\n      return pickle.load(fp)\n  \n  filepath = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n  # Read in grid cells\n  with open(filepath) as f:\n    grid_cells = geojson.load(f)\n  \n  # Retrieve and store cell IDs and their corresponding coordinates\n  cell_ID = []\n  ID_to_coord = {}\n\n  for i in range(len(grid_cells['features'])):\n    id = grid_cells['features'][i]['properties']['cell_id']\n    coords = grid_cells['features'][i]['geometry']['coordinates']\n    if id is not None and coords is not None: \n      cell_ID.append(id)\n      ID_to_coord[id] = coords\n\n  nohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n  \n  query_urls = []\n  \n  stations = []\n  \n  current_date = datetime.now()\n\n  # Retrieve nearest stations for each grid cell\n  for i in range(int(len(cell_ID))):\n    for j in range(1):\n      latitude = round(ID_to_coord.get(cell_ID[i])[0][j][1], 2)\n      longitude = round(ID_to_coord.get(cell_ID[i])[0][j][0], 2)\n\n      response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = current_date.year, month = current_date.month, day = current_date.day))\n      webContent = response.read().decode('UTF-8')\n      parsed_html = BeautifulSoup(webContent)\n\n      snow_water_eq_table = parsed_html.find_all('table')[7]\n      nearest_station = snow_water_eq_table.find_all('td')[0].text\n\n      stations.append(nearest_station)\n  \n  # save stations somewhere for cache\n  \n  print(\"Nearest station list is completed. Saving to cache..\")\n  \n  with open(f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\", \"wb\") as fp:\n    pickle.dump(stations, fp)\n          \n  return stations\n\ndef get_snotel_time_series(start_date = '2016-1-1', end_date = '2022-7-12'):\n  \n  print(f\"Retrieving data from {start_date} to {end_date}\")\n  # read the grid geometry file\n  homedir = os.path.expanduser('~')\n  print(homedir)\n  # read grid cell\n  github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n  # read grid cell\n  #submission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n  #all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n  #all_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n  stations = get_nearest_stations_for_all_grids()\n\n  start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n  end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n  # NOTE: this url only allows user to query one year's worth of data\n  nohrsc_time_series_url = 'https://www.nohrsc.noaa.gov/interactive/html/graph.html?station={station}&w=600&h=400&o=a&uc=0&by={start_year}&bm={start_month}&bd={start_day}&bh=0&ey={end_year}&em={end_month}&ed={end_day}&eh=23&data=1&units=0&region=us'\n\n  dates = []\n  snow_water_eq = []\n  # Keep track of cell IDs for each time series\n  new_cell_IDs = []\n\n  # Retrieve time series snow water equivalent data for each grid cell's nearest station\n  for i in range(len(stations)):\n\n    if stations[i] != '\\nNo observations within 62 miles\\n':\n\n      for j in range(end_date.year - start_date.year + 1):\n\n        response = urllib.request.urlopen(nohrsc_time_series_url.format(station = stations[i], start_year = start_date.year + j, start_month = start_date.month, start_day = start_date.day, end_year = end_date.year, end_month = end_date.month, end_day = end_date.day))\n        webContent = response.read().decode('UTF-8')\n        parsed_html = BeautifulSoup(webContent)\n\n        data_table = parsed_html.find_all('tbody')[3].find_all('tr')\n\n        for k in range(len(data_table)):\n          new_cell_IDs.append(cell_ID[i])\n          dates.append(data_table[k].find_all('td')[0].text)\n          snow_water_eq.append(data_table[k].find_all('td')[2].text)\n    else:\n      new_cell_IDs.append(cell_ID[i])\n      dates.append('N/A')\n      snow_water_eq.append('N/A')\n\n  df = pd.DataFrame({'cell_IDs': new_cell_IDs, 'date_times': dates, 'swe': snow_water_eq})\n  df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv\")\n  print(\"All SNOTEL time series are saved.\")\n\nget_nearest_stations_for_all_grids()\n#get_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n",
  "history_output" : "Running",
  "history_begin_time" : 1657723764825,
  "history_end_time" : null,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Running"
},{
  "history_id" : "SfJGJqI8KnVZ",
  "history_input" : "\n\n# Write first python in Geoweaver\nimport pandas as pd\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nimport geojson\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom snowcast_utils import *\nimport traceback\n\nfrom datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\nprint(sys.path)\n\ntry:\n  from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n  from bs4 import BeautifulSoup\n\nprint(\"testing...\")\ndef get_nearest_stations_for_all_grids():\n  \n  print(\"Start to get nearest stations..\")\n  \n  cache_file = f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\"\n  \n  if os.path.exists(cache_file):\n    with open(cache_file, \"rb\") as fp:\n      return pickle.load(fp)\n  \n  filepath = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n  # Read in grid cells\n  with open(filepath) as f:\n    grid_cells = geojson.load(f)\n  \n  # Retrieve and store cell IDs and their corresponding coordinates\n  cell_ID = []\n  ID_to_coord = {}\n\n  for i in range(len(grid_cells['features'])):\n    id = grid_cells['features'][i]['properties']['cell_id']\n    coords = grid_cells['features'][i]['geometry']['coordinates']\n    if id is not None and coords is not None: \n      cell_ID.append(id)\n      ID_to_coord[id] = coords\n\n  nohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n  \n  query_urls = []\n  \n  stations = []\n  \n  current_date = datetime.now()\n\n  # Retrieve nearest stations for each grid cell\n  for i in range(int(len(cell_ID))):\n    for j in range(1):\n      latitude = round(ID_to_coord.get(cell_ID[i])[0][j][1], 2)\n      longitude = round(ID_to_coord.get(cell_ID[i])[0][j][0], 2)\n\n      response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = current_date.year, month = current_date.month, day = current_date.day))\n      webContent = response.read().decode('UTF-8')\n      parsed_html = BeautifulSoup(webContent)\n\n      snow_water_eq_table = parsed_html.find_all('table')[7]\n      nearest_station = snow_water_eq_table.find_all('td')[0].text\n\n      stations.append(nearest_station)\n  \n  # save stations somewhere for cache\n  \n  print(\"Nearest station list is completed. Saving to cache..\")\n  \n  with open(f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\", \"wb\") as fp:\n    pickle.dump(stations, fp)\n          \n  return stations\n\n# def get_snotel_time_series(start_date = '2016-1-1', end_date = '2022-7-12'):\n  \n#   print(f\"Retrieving data from {start_date} to {end_date}\")\n#   # read the grid geometry file\n#   homedir = os.path.expanduser('~')\n#   print(homedir)\n#   # read grid cell\n#   github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n#   # read grid cell\n#   #submission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n#   #all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n#   #all_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n#   stations = get_nearest_stations_for_all_grids()\n\n#   start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n#   end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n#   # NOTE: this url only allows user to query one year's worth of data\n#   nohrsc_time_series_url = 'https://www.nohrsc.noaa.gov/interactive/html/graph.html?station={station}&w=600&h=400&o=a&uc=0&by={start_year}&bm={start_month}&bd={start_day}&bh=0&ey={end_year}&em={end_month}&ed={end_day}&eh=23&data=1&units=0&region=us'\n\n#   dates = []\n#   snow_water_eq = []\n#   # Keep track of cell IDs for each time series\n#   new_cell_IDs = []\n\n#   # Retrieve time series snow water equivalent data for each grid cell's nearest station\n#   for i in range(len(stations)):\n\n#     if stations[i] != '\\nNo observations within 62 miles\\n':\n\n#       for j in range(end_date.year - start_date.year + 1):\n\n#         response = urllib.request.urlopen(nohrsc_time_series_url.format(station = stations[i], start_year = start_date.year + j, start_month = start_date.month, start_day = start_date.day, end_year = end_date.year, end_month = end_date.month, end_day = end_date.day))\n#         webContent = response.read().decode('UTF-8')\n#         parsed_html = BeautifulSoup(webContent)\n\n#         data_table = parsed_html.find_all('tbody')[3].find_all('tr')\n\n#         for k in range(len(data_table)):\n#           new_cell_IDs.append(cell_ID[i])\n#           dates.append(data_table[k].find_all('td')[0].text)\n#           snow_water_eq.append(data_table[k].find_all('td')[2].text)\n#     else:\n#       new_cell_IDs.append(cell_ID[i])\n#       dates.append('N/A')\n#       snow_water_eq.append('N/A')\n\n#   df = pd.DataFrame({'cell_IDs': new_cell_IDs, 'date_times': dates, 'swe': snow_water_eq})\n#   df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv\")\n#   print(\"All SNOTEL time series are saved.\")\n\nget_nearest_stations_for_all_grids()\n#get_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n",
  "history_output" : "Running",
  "history_begin_time" : 1657723643446,
  "history_end_time" : 1657723761625,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "Ll9HqIE6XbfA",
  "history_input" : "\n\n# Write first python in Geoweaver\nimport pandas as pd\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nimport geojson\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom snowcast_utils import *\nimport traceback\n\nfrom datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\nprint(sys.path)\n\ntry:\n  from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n  from bs4 import BeautifulSoup\n\nprint(\"testing...\")\ndef get_nearest_stations_for_all_grids():\n  \n  print(\"Start to get nearest stations..\")\n  \n  cache_file = f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\"\n  \n  if os.path.exists(cache_file):\n    with open(cache_file, \"rb\") as fp:\n      return pickle.load(fp)\n  \n  filepath = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n  # Read in grid cells\n  with open(filepath) as f:\n    grid_cells = geojson.load(f)\n  \n  # Retrieve and store cell IDs and their corresponding coordinates\n  cell_ID = []\n  ID_to_coord = {}\n\n  for i in range(len(grid_cells['features'])):\n    id = grid_cells['features'][i]['properties']['cell_id']\n    coords = grid_cells['features'][i]['geometry']['coordinates']\n    if id is not None and coords is not None: \n      cell_ID.append(id)\n      ID_to_coord[id] = coords\n\n  nohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n  \n  query_urls = []\n  \n  stations = []\n  \n  current_date = datetime.now()\n\n#   # Retrieve nearest stations for each grid cell\n#   for i in range(int(len(cell_ID))):\n#     for j in range(1):\n#       latitude = round(ID_to_coord.get(cell_ID[i])[0][j][1], 2)\n#       longitude = round(ID_to_coord.get(cell_ID[i])[0][j][0], 2)\n\n#       response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = current_date.year, month = current_date.month, day = current_date.day))\n#       webContent = response.read().decode('UTF-8')\n#       parsed_html = BeautifulSoup(webContent)\n\n#       snow_water_eq_table = parsed_html.find_all('table')[7]\n#       nearest_station = snow_water_eq_table.find_all('td')[0].text\n\n#       stations.append(nearest_station)\n  \n#   # save stations somewhere for cache\n  \n#   print(\"Nearest station list is completed. Saving to cache..\")\n  \n#   with open(f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\", \"wb\") as fp:\n#     pickle.dump(stations, fp)\n          \n#   return stations\n\n# def get_snotel_time_series(start_date = '2016-1-1', end_date = '2022-7-12'):\n  \n#   print(f\"Retrieving data from {start_date} to {end_date}\")\n#   # read the grid geometry file\n#   homedir = os.path.expanduser('~')\n#   print(homedir)\n#   # read grid cell\n#   github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n#   # read grid cell\n#   #submission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n#   #all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n#   #all_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n#   stations = get_nearest_stations_for_all_grids()\n\n#   start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n#   end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n#   # NOTE: this url only allows user to query one year's worth of data\n#   nohrsc_time_series_url = 'https://www.nohrsc.noaa.gov/interactive/html/graph.html?station={station}&w=600&h=400&o=a&uc=0&by={start_year}&bm={start_month}&bd={start_day}&bh=0&ey={end_year}&em={end_month}&ed={end_day}&eh=23&data=1&units=0&region=us'\n\n#   dates = []\n#   snow_water_eq = []\n#   # Keep track of cell IDs for each time series\n#   new_cell_IDs = []\n\n#   # Retrieve time series snow water equivalent data for each grid cell's nearest station\n#   for i in range(len(stations)):\n\n#     if stations[i] != '\\nNo observations within 62 miles\\n':\n\n#       for j in range(end_date.year - start_date.year + 1):\n\n#         response = urllib.request.urlopen(nohrsc_time_series_url.format(station = stations[i], start_year = start_date.year + j, start_month = start_date.month, start_day = start_date.day, end_year = end_date.year, end_month = end_date.month, end_day = end_date.day))\n#         webContent = response.read().decode('UTF-8')\n#         parsed_html = BeautifulSoup(webContent)\n\n#         data_table = parsed_html.find_all('tbody')[3].find_all('tr')\n\n#         for k in range(len(data_table)):\n#           new_cell_IDs.append(cell_ID[i])\n#           dates.append(data_table[k].find_all('td')[0].text)\n#           snow_water_eq.append(data_table[k].find_all('td')[2].text)\n#     else:\n#       new_cell_IDs.append(cell_ID[i])\n#       dates.append('N/A')\n#       snow_water_eq.append('N/A')\n\n#   df = pd.DataFrame({'cell_IDs': new_cell_IDs, 'date_times': dates, 'swe': snow_water_eq})\n#   df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv\")\n#   print(\"All SNOTEL time series are saved.\")\n\nget_nearest_stations_for_all_grids()\n#get_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n",
  "history_output" : "today date = 2022-07-13\n/Users/joe\n2022-06-25 06:00:00\n['/Users/joe/gw-workspace/Ll9HqIE6XbfA', '/Users/l21-n02609-comm/opt/anaconda3/lib/python39.zip', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/lib-dynload', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/aeosa', '/Users/joe/Documents/GitHub/twitterscraper', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/IPython/extensions']\ntesting...\nStart to get nearest stations..\n",
  "history_begin_time" : 1657723632608,
  "history_end_time" : 1657723640864,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "2K5NVRTCsW5c",
  "history_input" : "\n\n# Write first python in Geoweaver\nimport pandas as pd\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nimport geojson\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom snowcast_utils import *\nimport traceback\n\nfrom datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\nprint(sys.path)\n\ntry:\n  from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n  from bs4 import BeautifulSoup\n\nprint(\"testing...\")\ndef get_nearest_stations_for_all_grids():\n  \n  print(\"Start to get nearest stations..\")\n  \n  cache_file = f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\"\n  \n  if os.path.exists(cache_file):\n    with open(cache_file, \"rb\") as fp:\n      return pickle.load(fp)\n  \n  filepath = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n  # Read in grid cells\n  with open(filepath) as f:\n    grid_cells = geojson.load(f)\n  \n  # Retrieve and store cell IDs and their corresponding coordinates\n  cell_ID = []\n  ID_to_coord = {}\n\n  for i in range(len(grid_cells['features'])):\n    id = grid_cells['features'][i]['properties']['cell_id']\n    coords = grid_cells['features'][i]['geometry']['coordinates']\n    if id is not None and coords is not None: \n      cell_ID.append(id)\n      ID_to_coord[id] = coords\n\n#   nohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n  \n#   query_urls = []\n  \n#   stations = []\n  \n#   current_date = datetime.now()\n\n#   # Retrieve nearest stations for each grid cell\n#   for i in range(int(len(cell_ID))):\n#     for j in range(1):\n#       latitude = round(ID_to_coord.get(cell_ID[i])[0][j][1], 2)\n#       longitude = round(ID_to_coord.get(cell_ID[i])[0][j][0], 2)\n\n#       response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = current_date.year, month = current_date.month, day = current_date.day))\n#       webContent = response.read().decode('UTF-8')\n#       parsed_html = BeautifulSoup(webContent)\n\n#       snow_water_eq_table = parsed_html.find_all('table')[7]\n#       nearest_station = snow_water_eq_table.find_all('td')[0].text\n\n#       stations.append(nearest_station)\n  \n#   # save stations somewhere for cache\n  \n#   print(\"Nearest station list is completed. Saving to cache..\")\n  \n#   with open(f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\", \"wb\") as fp:\n#     pickle.dump(stations, fp)\n          \n#   return stations\n\n# def get_snotel_time_series(start_date = '2016-1-1', end_date = '2022-7-12'):\n  \n#   print(f\"Retrieving data from {start_date} to {end_date}\")\n#   # read the grid geometry file\n#   homedir = os.path.expanduser('~')\n#   print(homedir)\n#   # read grid cell\n#   github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n#   # read grid cell\n#   #submission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n#   #all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n#   #all_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n#   stations = get_nearest_stations_for_all_grids()\n\n#   start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n#   end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n#   # NOTE: this url only allows user to query one year's worth of data\n#   nohrsc_time_series_url = 'https://www.nohrsc.noaa.gov/interactive/html/graph.html?station={station}&w=600&h=400&o=a&uc=0&by={start_year}&bm={start_month}&bd={start_day}&bh=0&ey={end_year}&em={end_month}&ed={end_day}&eh=23&data=1&units=0&region=us'\n\n#   dates = []\n#   snow_water_eq = []\n#   # Keep track of cell IDs for each time series\n#   new_cell_IDs = []\n\n#   # Retrieve time series snow water equivalent data for each grid cell's nearest station\n#   for i in range(len(stations)):\n\n#     if stations[i] != '\\nNo observations within 62 miles\\n':\n\n#       for j in range(end_date.year - start_date.year + 1):\n\n#         response = urllib.request.urlopen(nohrsc_time_series_url.format(station = stations[i], start_year = start_date.year + j, start_month = start_date.month, start_day = start_date.day, end_year = end_date.year, end_month = end_date.month, end_day = end_date.day))\n#         webContent = response.read().decode('UTF-8')\n#         parsed_html = BeautifulSoup(webContent)\n\n#         data_table = parsed_html.find_all('tbody')[3].find_all('tr')\n\n#         for k in range(len(data_table)):\n#           new_cell_IDs.append(cell_ID[i])\n#           dates.append(data_table[k].find_all('td')[0].text)\n#           snow_water_eq.append(data_table[k].find_all('td')[2].text)\n#     else:\n#       new_cell_IDs.append(cell_ID[i])\n#       dates.append('N/A')\n#       snow_water_eq.append('N/A')\n\n#   df = pd.DataFrame({'cell_IDs': new_cell_IDs, 'date_times': dates, 'swe': snow_water_eq})\n#   df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv\")\n#   print(\"All SNOTEL time series are saved.\")\n\nget_nearest_stations_for_all_grids()\n#get_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n",
  "history_output" : "today date = 2022-07-13\n/Users/joe\n2022-06-25 06:00:00\n['/Users/joe/gw-workspace/2K5NVRTCsW5c', '/Users/l21-n02609-comm/opt/anaconda3/lib/python39.zip', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/lib-dynload', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/aeosa', '/Users/joe/Documents/GitHub/twitterscraper', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/IPython/extensions']\ntesting...\nStart to get nearest stations..\n",
  "history_begin_time" : 1657723618325,
  "history_end_time" : 1657723626628,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "dbKrjd9FlSS7",
  "history_input" : "\n\n# Write first python in Geoweaver\nimport pandas as pd\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nimport geojson\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom snowcast_utils import *\nimport traceback\n\nfrom datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\nprint(sys.path)\n\ntry:\n  from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n  from bs4 import BeautifulSoup\n\nprint(\"testing...\")\ndef get_nearest_stations_for_all_grids():\n  \n  print(\"Start to get nearest stations..\")\n  \n  cache_file = f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\"\n  \n  if os.path.exists(cache_file):\n    with open(cache_file, \"rb\") as fp:\n      return pickle.load(fp)\n  \n  filepath = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n  # Read in grid cells\n  with open(filepath) as f:\n    grid_cells = geojson.load(f)\n  \n#   # Retrieve and store cell IDs and their corresponding coordinates\n#   cell_ID = []\n#   ID_to_coord = {}\n\n#   for i in range(len(grid_cells['features'])):\n#     id = grid_cells['features'][i]['properties']['cell_id']\n#     coords = grid_cells['features'][i]['geometry']['coordinates']\n#     if id is not None and coords is not None: \n#       cell_ID.append(id)\n#       ID_to_coord[id] = coords\n\n#   nohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n  \n#   query_urls = []\n  \n#   stations = []\n  \n#   current_date = datetime.now()\n\n#   # Retrieve nearest stations for each grid cell\n#   for i in range(int(len(cell_ID))):\n#     for j in range(1):\n#       latitude = round(ID_to_coord.get(cell_ID[i])[0][j][1], 2)\n#       longitude = round(ID_to_coord.get(cell_ID[i])[0][j][0], 2)\n\n#       response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = current_date.year, month = current_date.month, day = current_date.day))\n#       webContent = response.read().decode('UTF-8')\n#       parsed_html = BeautifulSoup(webContent)\n\n#       snow_water_eq_table = parsed_html.find_all('table')[7]\n#       nearest_station = snow_water_eq_table.find_all('td')[0].text\n\n#       stations.append(nearest_station)\n  \n#   # save stations somewhere for cache\n  \n#   print(\"Nearest station list is completed. Saving to cache..\")\n  \n#   with open(f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\", \"wb\") as fp:\n#     pickle.dump(stations, fp)\n          \n#   return stations\n\n# def get_snotel_time_series(start_date = '2016-1-1', end_date = '2022-7-12'):\n  \n#   print(f\"Retrieving data from {start_date} to {end_date}\")\n#   # read the grid geometry file\n#   homedir = os.path.expanduser('~')\n#   print(homedir)\n#   # read grid cell\n#   github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n#   # read grid cell\n#   #submission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n#   #all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n#   #all_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n#   stations = get_nearest_stations_for_all_grids()\n\n#   start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n#   end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n#   # NOTE: this url only allows user to query one year's worth of data\n#   nohrsc_time_series_url = 'https://www.nohrsc.noaa.gov/interactive/html/graph.html?station={station}&w=600&h=400&o=a&uc=0&by={start_year}&bm={start_month}&bd={start_day}&bh=0&ey={end_year}&em={end_month}&ed={end_day}&eh=23&data=1&units=0&region=us'\n\n#   dates = []\n#   snow_water_eq = []\n#   # Keep track of cell IDs for each time series\n#   new_cell_IDs = []\n\n#   # Retrieve time series snow water equivalent data for each grid cell's nearest station\n#   for i in range(len(stations)):\n\n#     if stations[i] != '\\nNo observations within 62 miles\\n':\n\n#       for j in range(end_date.year - start_date.year + 1):\n\n#         response = urllib.request.urlopen(nohrsc_time_series_url.format(station = stations[i], start_year = start_date.year + j, start_month = start_date.month, start_day = start_date.day, end_year = end_date.year, end_month = end_date.month, end_day = end_date.day))\n#         webContent = response.read().decode('UTF-8')\n#         parsed_html = BeautifulSoup(webContent)\n\n#         data_table = parsed_html.find_all('tbody')[3].find_all('tr')\n\n#         for k in range(len(data_table)):\n#           new_cell_IDs.append(cell_ID[i])\n#           dates.append(data_table[k].find_all('td')[0].text)\n#           snow_water_eq.append(data_table[k].find_all('td')[2].text)\n#     else:\n#       new_cell_IDs.append(cell_ID[i])\n#       dates.append('N/A')\n#       snow_water_eq.append('N/A')\n\n#   df = pd.DataFrame({'cell_IDs': new_cell_IDs, 'date_times': dates, 'swe': snow_water_eq})\n#   df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv\")\n#   print(\"All SNOTEL time series are saved.\")\n\nget_nearest_stations_for_all_grids()\n#get_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n",
  "history_output" : "today date = 2022-07-13\n/Users/joe\n2022-06-25 06:00:00\n['/Users/joe/gw-workspace/dbKrjd9FlSS7', '/Users/l21-n02609-comm/opt/anaconda3/lib/python39.zip', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/lib-dynload', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/aeosa', '/Users/joe/Documents/GitHub/twitterscraper', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/IPython/extensions']\ntesting...\nStart to get nearest stations..\n",
  "history_begin_time" : 1657723604044,
  "history_end_time" : 1657723612456,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "C0d9Lsg5NR1Y",
  "history_input" : "\n\n# Write first python in Geoweaver\nimport pandas as pd\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nimport geojson\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom snowcast_utils import *\nimport traceback\n\nfrom datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\nprint(sys.path)\n\ntry:\n  from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n  from bs4 import BeautifulSoup\n\nprint(\"testing...\")\ndef get_nearest_stations_for_all_grids():\n  \n  print(\"Start to get nearest stations..\")\n  \n  cache_file = f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\"\n  \n  if os.path.exists(cache_file):\n    with open(cache_file, \"rb\") as fp:\n      return pickle.load(fp)\n  \n#   filepath = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n#   # Read in grid cells\n#   with open(filepath) as f:\n#     grid_cells = geojson.load(f)\n  \n#   # Retrieve and store cell IDs and their corresponding coordinates\n#   cell_ID = []\n#   ID_to_coord = {}\n\n#   for i in range(len(grid_cells['features'])):\n#     id = grid_cells['features'][i]['properties']['cell_id']\n#     coords = grid_cells['features'][i]['geometry']['coordinates']\n#     if id is not None and coords is not None: \n#       cell_ID.append(id)\n#       ID_to_coord[id] = coords\n\n#   nohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n  \n#   query_urls = []\n  \n#   stations = []\n  \n#   current_date = datetime.now()\n\n#   # Retrieve nearest stations for each grid cell\n#   for i in range(int(len(cell_ID))):\n#     for j in range(1):\n#       latitude = round(ID_to_coord.get(cell_ID[i])[0][j][1], 2)\n#       longitude = round(ID_to_coord.get(cell_ID[i])[0][j][0], 2)\n\n#       response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = current_date.year, month = current_date.month, day = current_date.day))\n#       webContent = response.read().decode('UTF-8')\n#       parsed_html = BeautifulSoup(webContent)\n\n#       snow_water_eq_table = parsed_html.find_all('table')[7]\n#       nearest_station = snow_water_eq_table.find_all('td')[0].text\n\n#       stations.append(nearest_station)\n  \n#   # save stations somewhere for cache\n  \n#   print(\"Nearest station list is completed. Saving to cache..\")\n  \n#   with open(f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\", \"wb\") as fp:\n#     pickle.dump(stations, fp)\n          \n#   return stations\n\n# def get_snotel_time_series(start_date = '2016-1-1', end_date = '2022-7-12'):\n  \n#   print(f\"Retrieving data from {start_date} to {end_date}\")\n#   # read the grid geometry file\n#   homedir = os.path.expanduser('~')\n#   print(homedir)\n#   # read grid cell\n#   github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n#   # read grid cell\n#   #submission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n#   #all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n#   #all_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n#   stations = get_nearest_stations_for_all_grids()\n\n#   start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n#   end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n#   # NOTE: this url only allows user to query one year's worth of data\n#   nohrsc_time_series_url = 'https://www.nohrsc.noaa.gov/interactive/html/graph.html?station={station}&w=600&h=400&o=a&uc=0&by={start_year}&bm={start_month}&bd={start_day}&bh=0&ey={end_year}&em={end_month}&ed={end_day}&eh=23&data=1&units=0&region=us'\n\n#   dates = []\n#   snow_water_eq = []\n#   # Keep track of cell IDs for each time series\n#   new_cell_IDs = []\n\n#   # Retrieve time series snow water equivalent data for each grid cell's nearest station\n#   for i in range(len(stations)):\n\n#     if stations[i] != '\\nNo observations within 62 miles\\n':\n\n#       for j in range(end_date.year - start_date.year + 1):\n\n#         response = urllib.request.urlopen(nohrsc_time_series_url.format(station = stations[i], start_year = start_date.year + j, start_month = start_date.month, start_day = start_date.day, end_year = end_date.year, end_month = end_date.month, end_day = end_date.day))\n#         webContent = response.read().decode('UTF-8')\n#         parsed_html = BeautifulSoup(webContent)\n\n#         data_table = parsed_html.find_all('tbody')[3].find_all('tr')\n\n#         for k in range(len(data_table)):\n#           new_cell_IDs.append(cell_ID[i])\n#           dates.append(data_table[k].find_all('td')[0].text)\n#           snow_water_eq.append(data_table[k].find_all('td')[2].text)\n#     else:\n#       new_cell_IDs.append(cell_ID[i])\n#       dates.append('N/A')\n#       snow_water_eq.append('N/A')\n\n#   df = pd.DataFrame({'cell_IDs': new_cell_IDs, 'date_times': dates, 'swe': snow_water_eq})\n#   df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv\")\n#   print(\"All SNOTEL time series are saved.\")\n\nget_nearest_stations_for_all_grids()\n#get_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n",
  "history_output" : "today date = 2022-07-13\n/Users/joe\n2022-06-25 06:00:00\n['/Users/joe/gw-workspace/C0d9Lsg5NR1Y', '/Users/l21-n02609-comm/opt/anaconda3/lib/python39.zip', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/lib-dynload', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/aeosa', '/Users/joe/Documents/GitHub/twitterscraper', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/IPython/extensions']\ntesting...\nStart to get nearest stations..\n",
  "history_begin_time" : 1657723582455,
  "history_end_time" : 1657723590128,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "pausxvA6pPK7",
  "history_input" : "\n\n# Write first python in Geoweaver\nimport pandas as pd\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nimport geojson\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom snowcast_utils import *\nimport traceback\n\nfrom datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\nprint(sys.path)\n\ntry:\n  from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n  from bs4 import BeautifulSoup\n\nprint(\"testing...\")\ndef get_nearest_stations_for_all_grids():\n  \n  print(\"Start to get nearest stations..\")\n  \n  cache_file = f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\"\n  \n  if os.path.exists(cache_file):\n    with open(cache_file, \"rb\") as fp:\n      return pickle.load(fp)\n  \n  filepath = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n  # Read in grid cells\n  with open(filepath) as f:\n    grid_cells = geojson.load(f)\n  \n  # Retrieve and store cell IDs and their corresponding coordinates\n  cell_ID = []\n  ID_to_coord = {}\n\n  for i in range(len(grid_cells['features'])):\n    id = grid_cells['features'][i]['properties']['cell_id']\n    coords = grid_cells['features'][i]['geometry']['coordinates']\n    if id is not None and coords is not None: \n      cell_ID.append(id)\n      ID_to_coord[id] = coords\n\n  nohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n  \n  query_urls = []\n  \n  stations = []\n  \n  current_date = datetime.now()\n\n  # Retrieve nearest stations for each grid cell\n  for i in range(int(len(cell_ID))):\n    for j in range(1):\n      latitude = round(ID_to_coord.get(cell_ID[i])[0][j][1], 2)\n      longitude = round(ID_to_coord.get(cell_ID[i])[0][j][0], 2)\n\n      response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = current_date.year, month = current_date.month, day = current_date.day))\n      webContent = response.read().decode('UTF-8')\n      parsed_html = BeautifulSoup(webContent)\n\n      snow_water_eq_table = parsed_html.find_all('table')[7]\n      nearest_station = snow_water_eq_table.find_all('td')[0].text\n\n      stations.append(nearest_station)\n  \n  # save stations somewhere for cache\n  \n  print(\"Nearest station list is completed. Saving to cache..\")\n  \n  with open(f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\", \"wb\") as fp:\n    pickle.dump(stations, fp)\n          \n  return stations\n\ndef get_snotel_time_series(start_date = '2016-1-1', end_date = '2022-7-12'):\n  \n  print(f\"Retrieving data from {start_date} to {end_date}\")\n  # read the grid geometry file\n  homedir = os.path.expanduser('~')\n  print(homedir)\n  # read grid cell\n  github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n  # read grid cell\n  #submission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n  #all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n  #all_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n  stations = get_nearest_stations_for_all_grids()\n\n  start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n  end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n  # NOTE: this url only allows user to query one year's worth of data\n  nohrsc_time_series_url = 'https://www.nohrsc.noaa.gov/interactive/html/graph.html?station={station}&w=600&h=400&o=a&uc=0&by={start_year}&bm={start_month}&bd={start_day}&bh=0&ey={end_year}&em={end_month}&ed={end_day}&eh=23&data=1&units=0&region=us'\n\n  dates = []\n  snow_water_eq = []\n  # Keep track of cell IDs for each time series\n  new_cell_IDs = []\n\n  # Retrieve time series snow water equivalent data for each grid cell's nearest station\n  for i in range(len(stations)):\n\n    if stations[i] != '\\nNo observations within 62 miles\\n':\n\n      for j in range(end_date.year - start_date.year + 1):\n\n        response = urllib.request.urlopen(nohrsc_time_series_url.format(station = stations[i], start_year = start_date.year + j, start_month = start_date.month, start_day = start_date.day, end_year = end_date.year, end_month = end_date.month, end_day = end_date.day))\n        webContent = response.read().decode('UTF-8')\n        parsed_html = BeautifulSoup(webContent)\n\n        data_table = parsed_html.find_all('tbody')[3].find_all('tr')\n\n        for k in range(len(data_table)):\n          new_cell_IDs.append(cell_ID[i])\n          dates.append(data_table[k].find_all('td')[0].text)\n          snow_water_eq.append(data_table[k].find_all('td')[2].text)\n    else:\n      new_cell_IDs.append(cell_ID[i])\n      dates.append('N/A')\n      snow_water_eq.append('N/A')\n\n  df = pd.DataFrame({'cell_IDs': new_cell_IDs, 'date_times': dates, 'swe': snow_water_eq})\n  df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv\")\n  print(\"All SNOTEL time series are saved.\")\n\nget_nearest_stations_for_all_grids()\n#get_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n",
  "history_output" : "/Users/joe/gw-workspace/pausxvA6pPK7/data_snotel_real_time.py:75: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\nThe code that caused this warning is on line 75 of the file /Users/joe/gw-workspace/pausxvA6pPK7/data_snotel_real_time.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n  parsed_html = BeautifulSoup(webContent)\n",
  "history_begin_time" : 1657723522823,
  "history_end_time" : 1657723559609,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ndEC5Lk60Xnd",
  "history_input" : "\n\n# Write first python in Geoweaver\nimport pandas as pd\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nimport geojson\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom snowcast_utils import *\nimport traceback\n\nfrom datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\nprint(sys.path)\n\ntry:\n  from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n  from bs4 import BeautifulSoup\n\nprint(\"testing...\")\ndef get_nearest_stations_for_all_grids():\n  \n  print(\"Start to get nearest stations..\")\n  \n  cache_file = f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\"\n  \n  if os.path.exists(cache_file):\n    with open(cache_file, \"rb\") as fp:\n      return pickle.load(fp)\n  \n  filepath = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n  # Read in grid cells\n  with open(filepath) as f:\n      grid_cells = geojson.load(f)\n  \n  # Retrieve and store cell IDs and their corresponding coordinates\n  cell_ID = []\n  ID_to_coord = {}\n\n  for i in range(len(grid_cells['features'])):\n    id = grid_cells['features'][i]['properties']['cell_id']\n    coords = grid_cells['features'][i]['geometry']['coordinates']\n    if id is not None and coords is not None: \n      cell_ID.append(id)\n      ID_to_coord[id] = coords\n\n  nohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n  \n  query_urls = []\n  \n  stations = []\n  \n  current_date = datetime.now()\n\n  # Retrieve nearest stations for each grid cell\n  for i in range(int(len(cell_ID))):\n    for j in range(1):\n      latitude = round(ID_to_coord.get(cell_ID[i])[0][j][1], 2)\n      longitude = round(ID_to_coord.get(cell_ID[i])[0][j][0], 2)\n\n      response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = current_date.year, month = current_date.month, day = current_date.day))\n      webContent = response.read().decode('UTF-8')\n      parsed_html = BeautifulSoup(webContent)\n\n      snow_water_eq_table = parsed_html.find_all('table')[7]\n      nearest_station = snow_water_eq_table.find_all('td')[0].text\n\n      stations.append(nearest_station)\n  \n  # save stations somewhere for cache\n  \n  print(\"Nearest station list is completed. Saving to cache..\")\n  \n  with open(f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\", \"wb\") as fp:\n    pickle.dump(stations, fp)\n          \n  return stations\n\ndef get_snotel_time_series(start_date = '2016-1-1', end_date = '2022-7-12'):\n  \n  print(f\"Retrieving data from {start_date} to {end_date}\")\n  # read the grid geometry file\n  homedir = os.path.expanduser('~')\n  print(homedir)\n  # read grid cell\n  github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n  # read grid cell\n  #submission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n  #all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n  #all_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n  stations = get_nearest_stations_for_all_grids()\n\n  start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n  end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n  # NOTE: this url only allows user to query one year's worth of data\n  nohrsc_time_series_url = 'https://www.nohrsc.noaa.gov/interactive/html/graph.html?station={station}&w=600&h=400&o=a&uc=0&by={start_year}&bm={start_month}&bd={start_day}&bh=0&ey={end_year}&em={end_month}&ed={end_day}&eh=23&data=1&units=0&region=us'\n\n  dates = []\n  snow_water_eq = []\n  # Keep track of cell IDs for each time series\n  new_cell_IDs = []\n\n  # Retrieve time series snow water equivalent data for each grid cell's nearest station\n  for i in range(len(stations)):\n\n    if stations[i] != '\\nNo observations within 62 miles\\n':\n\n      for j in range(end_date.year - start_date.year + 1):\n\n        response = urllib.request.urlopen(nohrsc_time_series_url.format(station = stations[i], start_year = start_date.year + j, start_month = start_date.month, start_day = start_date.day, end_year = end_date.year, end_month = end_date.month, end_day = end_date.day))\n        webContent = response.read().decode('UTF-8')\n        parsed_html = BeautifulSoup(webContent)\n\n        data_table = parsed_html.find_all('tbody')[3].find_all('tr')\n\n        for k in range(len(data_table)):\n          new_cell_IDs.append(cell_ID[i])\n          dates.append(data_table[k].find_all('td')[0].text)\n          snow_water_eq.append(data_table[k].find_all('td')[2].text)\n    else:\n      new_cell_IDs.append(cell_ID[i])\n      dates.append('N/A')\n      snow_water_eq.append('N/A')\n\n  df = pd.DataFrame({'cell_IDs': new_cell_IDs, 'date_times': dates, 'swe': snow_water_eq})\n  df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv\")\n  print(\"All SNOTEL time series are saved.\")\n\nget_nearest_stations_for_all_grids()\n#get_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n",
  "history_output" : "Running",
  "history_begin_time" : 1657723483547,
  "history_end_time" : 1657723520113,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "tBAal9o8alnZ",
  "history_input" : "\n\n# Write first python in Geoweaver\nimport pandas as pd\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nimport geojson\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom snowcast_utils import *\nimport traceback\n\nfrom datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\nprint(sys.path)\n\ntry:\n  from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n  from bs4 import BeautifulSoup\n\nprint(\"testing...\")\ndef get_nearest_stations_for_all_grids():\n  \n  print(\"Start to get nearest stations..\")\n  \n  cache_file = f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\"\n  \n  if os.path.exists(cache_file):\n      with open(cache_file, \"rb\") as fp:\n          return pickle.load(fp)\n  \n  filepath = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n  # Read in grid cells\n  with open(filepath) as f:\n      grid_cells = geojson.load(f)\n  \n  # Retrieve and store cell IDs and their corresponding coordinates\n  cell_ID = []\n  ID_to_coord = {}\n\n  for i in range(len(grid_cells['features'])):\n      id = grid_cells['features'][i]['properties']['cell_id']\n      coords = grid_cells['features'][i]['geometry']['coordinates']\n      if id is not None and coords is not None: \n          cell_ID.append(id)\n          ID_to_coord[id] = coords\n\n  nohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n  \n  query_urls = []\n  \n  stations = []\n  \n  current_date = datetime.now()\n\n  # Retrieve nearest stations for each grid cell\n  for i in range(int(len(cell_ID))):\n      for j in range(1):\n          latitude = round(ID_to_coord.get(cell_ID[i])[0][j][1], 2)\n          longitude = round(ID_to_coord.get(cell_ID[i])[0][j][0], 2)\n\n          #response = urllib.request.urlopen(test_noaa_query_url)\n          response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = current_date.year, month = current_date.month, day = current_date.day))\n          webContent = response.read().decode('UTF-8')\n          parsed_html = BeautifulSoup(webContent)\n\n          snow_water_eq_table = parsed_html.find_all('table')[7]\n          nearest_station = snow_water_eq_table.find_all('td')[0].text\n\n          stations.append(nearest_station)\n  \n  # save stations somewhere for cache\n  \n  print(\"Nearest station list is completed. Saving to cache..\")\n  \n  with open(f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\", \"wb\") as fp:\n    pickle.dump(stations, fp)\n          \n  return stations\n\ndef get_snotel_time_series(start_date = '2016-1-1', end_date = '2022-7-12'):\n  \n  print(f\"Retrieving data from {start_date} to {end_date}\")\n  # read the grid geometry file\n  homedir = os.path.expanduser('~')\n  print(homedir)\n  # read grid cell\n  github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n  # read grid cell\n  #submission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n  #all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n  #all_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n  stations = get_nearest_stations_for_all_grids()\n\n  start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n  end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n  # NOTE: this url only allows user to query one year's worth of data\n  nohrsc_time_series_url = 'https://www.nohrsc.noaa.gov/interactive/html/graph.html?station={station}&w=600&h=400&o=a&uc=0&by={start_year}&bm={start_month}&bd={start_day}&bh=0&ey={end_year}&em={end_month}&ed={end_day}&eh=23&data=1&units=0&region=us'\n\n  dates = []\n  snow_water_eq = []\n  # Keep track of cell IDs for each time series\n  new_cell_IDs = []\n\n  # Retrieve time series snow water equivalent data for each grid cell's nearest station\n  for i in range(len(stations)):\n\n      if stations[i] != '\\nNo observations within 62 miles\\n':\n\n          for j in range(end_date.year - start_date.year + 1):\n\n              response = urllib.request.urlopen(nohrsc_time_series_url.format(station = stations[i], start_year = start_date.year + j, start_month = start_date.month, start_day = start_date.day, end_year = end_date.year, end_month = end_date.month, end_day = end_date.day))\n              webContent = response.read().decode('UTF-8')\n              parsed_html = BeautifulSoup(webContent)\n\n              data_table = parsed_html.find_all('tbody')[3].find_all('tr')\n\n              for k in range(len(data_table)):\n                  new_cell_IDs.append(cell_ID[i])\n                  dates.append(data_table[k].find_all('td')[0].text)\n                  snow_water_eq.append(data_table[k].find_all('td')[2].text)\n\n      else:\n          new_cell_IDs.append(cell_ID[i])\n          dates.append('N/A')\n          snow_water_eq.append('N/A')\n\n\n  #time_series = [new_cell_IDs, dates, snow_water_eq]\n  #print(time_series)\n  df = pd.DataFrame({'cell_IDs': new_cell_IDs, 'date_times': dates, 'swe': snow_water_eq})\n  df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv\")\n  print(\"All SNOTEL time series are saved.\")\n\nget_nearest_stations_for_all_grids()\n#get_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n",
  "history_output" : "Running",
  "history_begin_time" : 1657723247572,
  "history_end_time" : 1657723479709,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "89n4CSFpv48D",
  "history_input" : "\n\n# Write first python in Geoweaver\nimport pandas as pd\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nimport geojson\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom snowcast_utils import *\nimport traceback\n\nfrom datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\nprint(sys.path)\n\ntry:\n  from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n  from bs4 import BeautifulSoup\n\ndef get_nearest_stations_for_all_grids():\n  \n  print(\"Start to get nearest stations..\")\n  \n  cache_file = f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\"\n  \n  if os.path.exists(cache_file):\n      with open(cache_file, \"rb\") as fp:\n          return pickle.load(fp)\n  \n  filepath = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n  # Read in grid cells\n  with open(filepath) as f:\n      grid_cells = geojson.load(f)\n  \n  # Retrieve and store cell IDs and their corresponding coordinates\n  cell_ID = []\n  ID_to_coord = {}\n\n  for i in range(len(grid_cells['features'])):\n      id = grid_cells['features'][i]['properties']['cell_id']\n      coords = grid_cells['features'][i]['geometry']['coordinates']\n      if id is not None and coords is not None: \n          cell_ID.append(id)\n          ID_to_coord[id] = coords\n\n  nohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n  \n  query_urls = []\n  \n  stations = []\n  \n  current_date = datetime.now()\n\n  # Retrieve nearest stations for each grid cell\n  for i in range(int(len(cell_ID))):\n      for j in range(1):\n          latitude = round(ID_to_coord.get(cell_ID[i])[0][j][1], 2)\n          longitude = round(ID_to_coord.get(cell_ID[i])[0][j][0], 2)\n\n          #response = urllib.request.urlopen(test_noaa_query_url)\n          response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = current_date.year, month = current_date.month, day = current_date.day))\n          webContent = response.read().decode('UTF-8')\n          parsed_html = BeautifulSoup(webContent)\n\n          snow_water_eq_table = parsed_html.find_all('table')[7]\n          nearest_station = snow_water_eq_table.find_all('td')[0].text\n\n          stations.append(nearest_station)\n  \n  # save stations somewhere for cache\n  \n  print(\"Nearest station list is completed. Saving to cache..\")\n  \n  with open(f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\", \"wb\") as fp:\n    pickle.dump(stations, fp)\n          \n  return stations\n\ndef get_snotel_time_series(start_date = '2016-1-1', end_date = '2022-7-12'):\n  \n  print(f\"Retrieving data from {start_date} to {end_date}\")\n  # read the grid geometry file\n  homedir = os.path.expanduser('~')\n  print(homedir)\n  # read grid cell\n  github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n  # read grid cell\n  #submission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n  #all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n  #all_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n  stations = get_nearest_stations_for_all_grids()\n\n  start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n  end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n  # NOTE: this url only allows user to query one year's worth of data\n  nohrsc_time_series_url = 'https://www.nohrsc.noaa.gov/interactive/html/graph.html?station={station}&w=600&h=400&o=a&uc=0&by={start_year}&bm={start_month}&bd={start_day}&bh=0&ey={end_year}&em={end_month}&ed={end_day}&eh=23&data=1&units=0&region=us'\n\n  dates = []\n  snow_water_eq = []\n  # Keep track of cell IDs for each time series\n  new_cell_IDs = []\n\n  # Retrieve time series snow water equivalent data for each grid cell's nearest station\n  for i in range(len(stations)):\n\n      if stations[i] != '\\nNo observations within 62 miles\\n':\n\n          for j in range(end_date.year - start_date.year + 1):\n\n              response = urllib.request.urlopen(nohrsc_time_series_url.format(station = stations[i], start_year = start_date.year + j, start_month = start_date.month, start_day = start_date.day, end_year = end_date.year, end_month = end_date.month, end_day = end_date.day))\n              webContent = response.read().decode('UTF-8')\n              parsed_html = BeautifulSoup(webContent)\n\n              data_table = parsed_html.find_all('tbody')[3].find_all('tr')\n\n              for k in range(len(data_table)):\n                  new_cell_IDs.append(cell_ID[i])\n                  dates.append(data_table[k].find_all('td')[0].text)\n                  snow_water_eq.append(data_table[k].find_all('td')[2].text)\n\n      else:\n          new_cell_IDs.append(cell_ID[i])\n          dates.append('N/A')\n          snow_water_eq.append('N/A')\n\n\n  #time_series = [new_cell_IDs, dates, snow_water_eq]\n  #print(time_series)\n  df = pd.DataFrame({'cell_IDs': new_cell_IDs, 'date_times': dates, 'swe': snow_water_eq})\n  df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv\")\n  print(\"All SNOTEL time series are saved.\")\n\nget_nearest_stations_for_all_grids()\n#get_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n",
  "history_output" : "Running",
  "history_begin_time" : 1657723181795,
  "history_end_time" : 1657723222839,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "TXUSvrxuTsss",
  "history_input" : "\n\n# Write first python in Geoweaver\nimport pandas as pd\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nimport geojson\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom snowcast_utils import *\nimport traceback\n\nfrom datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\nprint(sys.path)\n\ntry:\n  from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n  from bs4 import BeautifulSoup\n\ndef get_nearest_stations_for_all_grids():\n  \n  print(\"Start to get nearest stations..\")\n  \n  cache_file = f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\"\n  \n  if os.path.exists(cache_file):\n      with open(, \"rb\") as fp:\n          return pickle.load(fp)\n  \n  filepath = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n  # Read in grid cells\n  with open(filepath) as f:\n      grid_cells = geojson.load(f)\n  \n  # Retrieve and store cell IDs and their corresponding coordinates\n  cell_ID = []\n  ID_to_coord = {}\n\n  for i in range(len(grid_cells['features'])):\n      id = grid_cells['features'][i]['properties']['cell_id']\n      coords = grid_cells['features'][i]['geometry']['coordinates']\n      if id is not None and coords is not None: \n          cell_ID.append(id)\n          ID_to_coord[id] = coords\n\n  nohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n  \n  query_urls = []\n  \n  stations = []\n  \n  current_date = datetime.now()\n\n  # Retrieve nearest stations for each grid cell\n  for i in range(int(len(cell_ID))):\n      for j in range(1):\n          latitude = round(ID_to_coord.get(cell_ID[i])[0][j][1], 2)\n          longitude = round(ID_to_coord.get(cell_ID[i])[0][j][0], 2)\n\n          #response = urllib.request.urlopen(test_noaa_query_url)\n          response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = current_date.year, month = current_date.month, day = current_date.day))\n          webContent = response.read().decode('UTF-8')\n          parsed_html = BeautifulSoup(webContent)\n\n          snow_water_eq_table = parsed_html.find_all('table')[7]\n          nearest_station = snow_water_eq_table.find_all('td')[0].text\n\n          stations.append(nearest_station)\n  \n  # save stations somewhere for cache\n  \n  print(\"Nearest station list is completed. Saving to cache..\")\n  \n  with open(f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\", \"wb\") as fp:\n    pickle.dump(stations, fp)\n          \n  return stations\n\ndef get_snotel_time_series(start_date = '2016-1-1', end_date = '2022-7-12'):\n  \n  print(f\"Retrieving data from {start_date} to {end_date}\")\n  # read the grid geometry file\n  homedir = os.path.expanduser('~')\n  print(homedir)\n  # read grid cell\n  github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n  # read grid cell\n  #submission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n  #all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n  #all_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n  stations = get_nearest_stations_for_all_grids()\n\n  start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n  end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n  # NOTE: this url only allows user to query one year's worth of data\n  nohrsc_time_series_url = 'https://www.nohrsc.noaa.gov/interactive/html/graph.html?station={station}&w=600&h=400&o=a&uc=0&by={start_year}&bm={start_month}&bd={start_day}&bh=0&ey={end_year}&em={end_month}&ed={end_day}&eh=23&data=1&units=0&region=us'\n\n  dates = []\n  snow_water_eq = []\n  # Keep track of cell IDs for each time series\n  new_cell_IDs = []\n\n  # Retrieve time series snow water equivalent data for each grid cell's nearest station\n  for i in range(len(stations)):\n\n      if stations[i] != '\\nNo observations within 62 miles\\n':\n\n          for j in range(end_date.year - start_date.year + 1):\n\n              response = urllib.request.urlopen(nohrsc_time_series_url.format(station = stations[i], start_year = start_date.year + j, start_month = start_date.month, start_day = start_date.day, end_year = end_date.year, end_month = end_date.month, end_day = end_date.day))\n              webContent = response.read().decode('UTF-8')\n              parsed_html = BeautifulSoup(webContent)\n\n              data_table = parsed_html.find_all('tbody')[3].find_all('tr')\n\n              for k in range(len(data_table)):\n                  new_cell_IDs.append(cell_ID[i])\n                  dates.append(data_table[k].find_all('td')[0].text)\n                  snow_water_eq.append(data_table[k].find_all('td')[2].text)\n\n      else:\n          new_cell_IDs.append(cell_ID[i])\n          dates.append('N/A')\n          snow_water_eq.append('N/A')\n\n\n  #time_series = [new_cell_IDs, dates, snow_water_eq]\n  #print(time_series)\n  df = pd.DataFrame({'cell_IDs': new_cell_IDs, 'date_times': dates, 'swe': snow_water_eq})\n  df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv\")\n  print(\"All SNOTEL time series are saved.\")\n\nget_nearest_stations_for_all_grids()\n#get_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n",
  "history_output" : "  File \"/Users/joe/gw-workspace/TXUSvrxuTsss/data_snotel_real_time.py\", line 39\n    with open(, \"rb\") as fp:\n              ^\nSyntaxError: invalid syntax\n",
  "history_begin_time" : 1657723142399,
  "history_end_time" : 1657723142485,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "CSt2h6DtFqI6",
  "history_input" : "\n\n# Write first python in Geoweaver\nimport pandas as pd\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nimport geojson\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom snowcast_utils import *\nimport traceback\n\nfrom datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\nprint(sys.path)\n\n\ntry:\n  from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n  from bs4 import BeautifulSoup\n\ndef get_nearest_stations_for_all_grids():\n  \n  filepath = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n  # Read in grid cells\n  with open(filepath) as f:\n      grid_cells = geojson.load(f)\n  \n  # Retrieve and store cell IDs and their corresponding coordinates\n  cell_ID = []\n  ID_to_coord = {}\n\n  for i in range(len(grid_cells['features'])):\n      id = grid_cells['features'][i]['properties']['cell_id']\n      coords = grid_cells['features'][i]['geometry']['coordinates']\n      if id is not None and coords is not None: \n          cell_ID.append(id)\n          ID_to_coord[id] = coords\n\n  nohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n  \n  query_urls = []\n  \n  stations = []\n  \n  current_date = datetime.now()\n\n  # Retrieve nearest stations for each grid cell\n  for i in range(int(len(cell_ID))):\n      for j in range(1):\n          latitude = round(ID_to_coord.get(cell_ID[i])[0][j][1], 2)\n          longitude = round(ID_to_coord.get(cell_ID[i])[0][j][0], 2)\n\n          #response = urllib.request.urlopen(test_noaa_query_url)\n          response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = current_date.year, month = current_date.month, day = current_date.day))\n          webContent = response.read().decode('UTF-8')\n          parsed_html = BeautifulSoup(webContent)\n\n          snow_water_eq_table = parsed_html.find_all('table')[7]\n          nearest_station = snow_water_eq_table.find_all('td')[0].text\n\n          stations.append(nearest_station)\n          \n  # save stations somewhere for cache\n          \n  return stations\n\ndef get_snotel_time_series(start_date = '2016-1-1', end_date = '2022-7-12'):\n  \n\n  # read the grid geometry file\n  homedir = os.path.expanduser('~')\n  print(homedir)\n  # read grid cell\n  github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n  # read grid cell\n  #submission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n  #all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n  #all_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n  stations = get_nearest_stations_for_all_grids()\n\n  start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n  end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n  # NOTE: this url only allows user to query one year's worth of data\n  nohrsc_time_series_url = 'https://www.nohrsc.noaa.gov/interactive/html/graph.html?station={station}&w=600&h=400&o=a&uc=0&by={start_year}&bm={start_month}&bd={start_day}&bh=0&ey={end_year}&em={end_month}&ed={end_day}&eh=23&data=1&units=0&region=us'\n\n  dates = []\n  snow_water_eq = []\n  # Keep track of cell IDs for each time series\n  new_cell_IDs = []\n\n  # Retrieve time series snow water equivalent data for each grid cell's nearest station\n  for i in range(len(stations)):\n\n      if stations[i] != '\\nNo observations within 62 miles\\n':\n\n          for j in range(end_date.year - start_date.year + 1):\n\n              response = urllib.request.urlopen(nohrsc_time_series_url.format(station = stations[i], start_year = start_date.year + j, start_month = start_date.month, start_day = start_date.day, end_year = end_date.year, end_month = end_date.month, end_day = end_date.day))\n              webContent = response.read().decode('UTF-8')\n              parsed_html = BeautifulSoup(webContent)\n\n              data_table = parsed_html.find_all('tbody')[3].find_all('tr')\n\n              for k in range(len(data_table)):\n                  new_cell_IDs.append(cell_ID[i])\n                  dates.append(data_table[k].find_all('td')[0].text)\n                  snow_water_eq.append(data_table[k].find_all('td')[2].text)\n\n      else:\n          new_cell_IDs.append(cell_ID[i])\n          dates.append('N/A')\n          snow_water_eq.append('N/A')\n\n\n  #time_series = [new_cell_IDs, dates, snow_water_eq]\n  #print(time_series)\n  df = pd.DataFrame({'cell_IDs': new_cell_IDs, 'date_times': dates, 'swe': snow_water_eq})\n  df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv\")\n  print(\"Saved\")\n\nget_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n",
  "history_output" : "/Users/joe/gw-workspace/CSt2h6DtFqI6/data_snotel_real_time.py:68: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\nThe code that caused this warning is on line 68 of the file /Users/joe/gw-workspace/CSt2h6DtFqI6/data_snotel_real_time.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n  parsed_html = BeautifulSoup(webContent)\n",
  "history_begin_time" : 1657722616402,
  "history_end_time" : 1657722907783,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "OSWtH3AM9hv8",
  "history_input" : "\n\n# Write first python in Geoweaver\nimport pandas as pd\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nimport geojson\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom snowcast_utils import *\nimport traceback\n\nfrom datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\nprint(sys.path)\n\ndef get_nearest_stations_for_all_grids():\n  \n  filepath = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n  # Read in grid cells\n  with open(filepath) as f:\n      grid_cells = geojson.load(f)\n  \n  # Retrieve and store cell IDs and their corresponding coordinates\n  cell_ID = []\n  ID_to_coord = {}\n\n  for i in range(len(grid_cells['features'])):\n      id = grid_cells['features'][i]['properties']['cell_id']\n      coords = grid_cells['features'][i]['geometry']['coordinates']\n      if id is not None and coords is not None: \n          cell_ID.append(id)\n          ID_to_coord[id] = coords\n\n  nohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n  \n  query_urls = []\n  \n  stations = []\n  \n  current_date = datetime.now()\n\n  # Retrieve nearest stations for each grid cell\n  for i in range(int(len(cell_ID))):\n      for j in range(1):\n          latitude = round(ID_to_coord.get(cell_ID[i])[0][j][1], 2)\n          longitude = round(ID_to_coord.get(cell_ID[i])[0][j][0], 2)\n\n          #response = urllib.request.urlopen(test_noaa_query_url)\n          response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = current_date.year, month = current_date.month, day = current_date.day))\n          webContent = response.read().decode('UTF-8')\n          parsed_html = BeautifulSoup(webContent)\n\n          snow_water_eq_table = parsed_html.find_all('table')[7]\n          nearest_station = snow_water_eq_table.find_all('td')[0].text\n\n          stations.append(nearest_station)\n          \n  # save stations somewhere for cache\n          \n  return stations\n\ndef get_snotel_time_series(start_date = '2016-1-1', end_date = '2022-7-12'):\n  \n  try:\n      from BeautifulSoup import BeautifulSoup\n  except ImportError:\n      from bs4 import BeautifulSoup\n\n  # read the grid geometry file\n  homedir = os.path.expanduser('~')\n  print(homedir)\n  # read grid cell\n  github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n  # read grid cell\n  #submission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n  #all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n  #all_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n  stations = get_nearest_stations_for_all_grids()\n\n  start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n  end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n  # NOTE: this url only allows user to query one year's worth of data\n  nohrsc_time_series_url = 'https://www.nohrsc.noaa.gov/interactive/html/graph.html?station={station}&w=600&h=400&o=a&uc=0&by={start_year}&bm={start_month}&bd={start_day}&bh=0&ey={end_year}&em={end_month}&ed={end_day}&eh=23&data=1&units=0&region=us'\n\n  dates = []\n  snow_water_eq = []\n  # Keep track of cell IDs for each time series\n  new_cell_IDs = []\n\n  # Retrieve time series snow water equivalent data for each grid cell's nearest station\n  for i in range(len(stations)):\n\n      if stations[i] != '\\nNo observations within 62 miles\\n':\n\n          for j in range(end_date.year - start_date.year + 1):\n\n              response = urllib.request.urlopen(nohrsc_time_series_url.format(station = stations[i], start_year = start_date.year + j, start_month = start_date.month, start_day = start_date.day, end_year = end_date.year, end_month = end_date.month, end_day = end_date.day))\n              webContent = response.read().decode('UTF-8')\n              parsed_html = BeautifulSoup(webContent)\n\n              data_table = parsed_html.find_all('tbody')[3].find_all('tr')\n\n              for k in range(len(data_table)):\n                  new_cell_IDs.append(cell_ID[i])\n                  dates.append(data_table[k].find_all('td')[0].text)\n                  snow_water_eq.append(data_table[k].find_all('td')[2].text)\n\n      else:\n          new_cell_IDs.append(cell_ID[i])\n          dates.append('N/A')\n          snow_water_eq.append('N/A')\n\n\n  #time_series = [new_cell_IDs, dates, snow_water_eq]\n  #print(time_series)\n  df = pd.DataFrame({'cell_IDs': new_cell_IDs, 'date_times': dates, 'swe': snow_water_eq})\n  df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv\")\n  print(\"Saved\")\n\nget_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n",
  "history_output" : "today date = 2022-07-13\n/Users/joe\n2022-06-25 06:00:00\n['/Users/joe/gw-workspace/OSWtH3AM9hv8', '/Users/l21-n02609-comm/opt/anaconda3/lib/python39.zip', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/lib-dynload', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/aeosa', '/Users/joe/Documents/GitHub/twitterscraper', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/IPython/extensions']\n/Users/joe\nTraceback (most recent call last):\n  File \"/Users/joe/gw-workspace/OSWtH3AM9hv8/data_snotel_real_time.py\", line 132, in <module>\n    get_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n  File \"/Users/joe/gw-workspace/OSWtH3AM9hv8/data_snotel_real_time.py\", line 90, in get_snotel_time_series\n    stations = get_nearest_stations_for_all_grids()\n  File \"/Users/joe/gw-workspace/OSWtH3AM9hv8/data_snotel_real_time.py\", line 62, in get_nearest_stations_for_all_grids\n    parsed_html = BeautifulSoup(webContent)\nNameError: name 'BeautifulSoup' is not defined\n",
  "history_begin_time" : 1657722575997,
  "history_end_time" : 1657722585169,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "1lfi8hciLztH",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n# Write first python in Geoweaver\nimport pandas as pd\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nimport geojson\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom snowcast_utils import *\nimport traceback\n\nprint(sys.path)\n\ndef get_nearest_stations_for_all_grids():\n  \n  filepath = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n  # Read in grid cells\n  with open(filepath) as f:\n      grid_cells = geojson.load(f)\n  \n  # Retrieve and store cell IDs and their corresponding coordinates\n  cell_ID = []\n  ID_to_coord = {}\n\n  for i in range(len(grid_cells['features'])):\n      id = grid_cells['features'][i]['properties']['cell_id']\n      coords = grid_cells['features'][i]['geometry']['coordinates']\n      if id is not None and coords is not None: \n          cell_ID.append(id)\n          ID_to_coord[id] = coords\n\n  nohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n  \n  query_urls = []\n  \n  stations = []\n  \n  current_date = datetime.now()\n\n  # Retrieve nearest stations for each grid cell\n  for i in range(int(len(cell_ID))):\n      for j in range(1):\n          latitude = round(ID_to_coord.get(cell_ID[i])[0][j][1], 2)\n          longitude = round(ID_to_coord.get(cell_ID[i])[0][j][0], 2)\n\n          #response = urllib.request.urlopen(test_noaa_query_url)\n          response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = current_date.year, month = current_date.month, day = current_date.day))\n          webContent = response.read().decode('UTF-8')\n          parsed_html = BeautifulSoup(webContent)\n\n          snow_water_eq_table = parsed_html.find_all('table')[7]\n          nearest_station = snow_water_eq_table.find_all('td')[0].text\n\n          stations.append(nearest_station)\n          \n  # save stations somewhere for cache\n          \n  return stations\n\ndef get_snotel_time_series(start_date = '2016-1-1', end_date = '2022-7-12'):\n  \n  try:\n      from BeautifulSoup import BeautifulSoup\n  except ImportError:\n      from bs4 import BeautifulSoup\n\n  # read the grid geometry file\n  homedir = os.path.expanduser('~')\n  print(homedir)\n  # read grid cell\n  github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n  # read grid cell\n  #submission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n  #all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n  #all_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n  stations = get_nearest_stations_for_all_grids()\n\n  start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n  end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n  # NOTE: this url only allows user to query one year's worth of data\n  nohrsc_time_series_url = 'https://www.nohrsc.noaa.gov/interactive/html/graph.html?station={station}&w=600&h=400&o=a&uc=0&by={start_year}&bm={start_month}&bd={start_day}&bh=0&ey={end_year}&em={end_month}&ed={end_day}&eh=23&data=1&units=0&region=us'\n\n  dates = []\n  snow_water_eq = []\n  # Keep track of cell IDs for each time series\n  new_cell_IDs = []\n\n  # Retrieve time series snow water equivalent data for each grid cell's nearest station\n  for i in range(len(stations)):\n\n      if stations[i] != '\\nNo observations within 62 miles\\n':\n\n          for j in range(end_date.year - start_date.year + 1):\n\n              response = urllib.request.urlopen(nohrsc_time_series_url.format(station = stations[i], start_year = start_date.year + j, start_month = start_date.month, start_day = start_date.day, end_year = end_date.year, end_month = end_date.month, end_day = end_date.day))\n              webContent = response.read().decode('UTF-8')\n              parsed_html = BeautifulSoup(webContent)\n\n              data_table = parsed_html.find_all('tbody')[3].find_all('tr')\n\n              for k in range(len(data_table)):\n                  new_cell_IDs.append(cell_ID[i])\n                  dates.append(data_table[k].find_all('td')[0].text)\n                  snow_water_eq.append(data_table[k].find_all('td')[2].text)\n\n      else:\n          new_cell_IDs.append(cell_ID[i])\n          dates.append('N/A')\n          snow_water_eq.append('N/A')\n\n\n  #time_series = [new_cell_IDs, dates, snow_water_eq]\n  #print(time_series)\n  df = pd.DataFrame({'cell_IDs': new_cell_IDs, 'date_times': dates, 'swe': snow_water_eq})\n  df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv\")\n  print(\"Saved\")\n\nget_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n",
  "history_output" : "today date = 2022-07-13\n/Users/joe\n2022-06-25 06:00:00\n['/Users/joe/gw-workspace/1lfi8hciLztH', '/Users/l21-n02609-comm/opt/anaconda3/lib/python39.zip', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/lib-dynload', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/aeosa', '/Users/joe/Documents/GitHub/twitterscraper', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/IPython/extensions']\n/Users/joe\nTraceback (most recent call last):\n  File \"/Users/joe/gw-workspace/1lfi8hciLztH/data_snotel_real_time.py\", line 130, in <module>\n    get_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n  File \"/Users/joe/gw-workspace/1lfi8hciLztH/data_snotel_real_time.py\", line 88, in get_snotel_time_series\n    stations = get_nearest_stations_for_all_grids()\n  File \"/Users/joe/gw-workspace/1lfi8hciLztH/data_snotel_real_time.py\", line 49, in get_nearest_stations_for_all_grids\n    current_date = datetime.now()\nAttributeError: module 'datetime' has no attribute 'now'\n",
  "history_begin_time" : 1657722524243,
  "history_end_time" : 1657722533065,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "ZtXLVBWGDmwb",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n# Write first python in Geoweaver\nimport pandas as pd\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nimport geojson\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom snowcast_utils import *\nimport traceback\n\nprint(sys.path)\n\ndef get_nearest_stations_for_all_grids():\n  \n  filepath = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n  # Read in grid cells\n  with open(filepath) as f:\n      grid_cells = geojson.load(f)\n  \n  # Retrieve and store cell IDs and their corresponding coordinates\n  cell_ID = []\n  ID_to_coord = {}\n\n  for i in range(len(grid_cells['features'])):\n      id = grid_cells['features'][i]['properties']['cell_id']\n      coords = grid_cells['features'][i]['geometry']['coordinates']\n      if id is not None and coords is not None: \n          cell_ID.append(id)\n          ID_to_coord[id] = coords\n\n  nohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n  \n  query_urls = []\n  \n  stations = []\n\n  # Retrieve nearest stations for each grid cell\n  for i in range(int(len(cell_ID))):\n      for j in range(1):\n          latitude = round(ID_to_coord.get(cell_ID[i])[0][j][1], 2)\n          longitude = round(ID_to_coord.get(cell_ID[i])[0][j][0], 2)\n\n          #response = urllib.request.urlopen(test_noaa_query_url)\n          response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = start_date.year, month = start_date.month, day = start_date.day))\n          webContent = response.read().decode('UTF-8')\n          parsed_html = BeautifulSoup(webContent)\n\n          snow_water_eq_table = parsed_html.find_all('table')[7]\n          nearest_station = snow_water_eq_table.find_all('td')[0].text\n\n          stations.append(nearest_station)\n          \n  # save stations somewhere for cache\n          \n  return stations\n\ndef get_snotel_time_series(start_date = '2016-1-1', end_date = '2022-7-12'):\n  \n  try:\n      from BeautifulSoup import BeautifulSoup\n  except ImportError:\n      from bs4 import BeautifulSoup\n\n  # read the grid geometry file\n  homedir = os.path.expanduser('~')\n  print(homedir)\n  # read grid cell\n  github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n  # read grid cell\n  #submission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n  #all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n  #all_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n  stations = get_nearest_stations_for_all_grids()\n\n  start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n  end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n  # NOTE: this url only allows user to query one year's worth of data\n  nohrsc_time_series_url = 'https://www.nohrsc.noaa.gov/interactive/html/graph.html?station={station}&w=600&h=400&o=a&uc=0&by={start_year}&bm={start_month}&bd={start_day}&bh=0&ey={end_year}&em={end_month}&ed={end_day}&eh=23&data=1&units=0&region=us'\n\n  dates = []\n  snow_water_eq = []\n  # Keep track of cell IDs for each time series\n  new_cell_IDs = []\n\n  # Retrieve time series snow water equivalent data for each grid cell's nearest station\n  for i in range(len(stations)):\n\n      if stations[i] != '\\nNo observations within 62 miles\\n':\n\n          for j in range(end_date.year - start_date.year + 1):\n\n              response = urllib.request.urlopen(nohrsc_time_series_url.format(station = stations[i], start_year = start_date.year + j, start_month = start_date.month, start_day = start_date.day, end_year = end_date.year, end_month = end_date.month, end_day = end_date.day))\n              webContent = response.read().decode('UTF-8')\n              parsed_html = BeautifulSoup(webContent)\n\n              data_table = parsed_html.find_all('tbody')[3].find_all('tr')\n\n              for k in range(len(data_table)):\n                  new_cell_IDs.append(cell_ID[i])\n                  dates.append(data_table[k].find_all('td')[0].text)\n                  snow_water_eq.append(data_table[k].find_all('td')[2].text)\n\n      else:\n          new_cell_IDs.append(cell_ID[i])\n          dates.append('N/A')\n          snow_water_eq.append('N/A')\n\n\n  #time_series = [new_cell_IDs, dates, snow_water_eq]\n  #print(time_series)\n  df = pd.DataFrame({'cell_IDs': new_cell_IDs, 'date_times': dates, 'swe': snow_water_eq})\n  df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv\")\n  print(\"Saved\")\n\nget_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n",
  "history_output" : "today date = 2022-07-13\n/Users/joe\n2022-06-25 06:00:00\n['/Users/joe/gw-workspace/ZtXLVBWGDmwb', '/Users/l21-n02609-comm/opt/anaconda3/lib/python39.zip', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/lib-dynload', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/aeosa', '/Users/joe/Documents/GitHub/twitterscraper', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/IPython/extensions']\n/Users/joe\nTraceback (most recent call last):\n  File \"/Users/joe/gw-workspace/ZtXLVBWGDmwb/data_snotel_real_time.py\", line 128, in <module>\n    get_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n  File \"/Users/joe/gw-workspace/ZtXLVBWGDmwb/data_snotel_real_time.py\", line 86, in get_snotel_time_series\n    stations = get_nearest_stations_for_all_grids()\n  File \"/Users/joe/gw-workspace/ZtXLVBWGDmwb/data_snotel_real_time.py\", line 56, in get_nearest_stations_for_all_grids\n    response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = start_date.year, month = start_date.month, day = start_date.day))\nNameError: name 'start_date' is not defined\n",
  "history_begin_time" : 1657722402661,
  "history_end_time" : 1657722413189,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "ehMWOkjwzGkE",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n# Write first python in Geoweaver\nimport pandas as pd\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nimport geojson\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom datetime import date\nfrom snowcast_utils import *\nimport traceback\n\nprint(sys.path)\n\ndef get_snotel_time_series(start_date = '2016-1-1', end_date = '2022-7-12'):\n  \n  \n  \n  try:\n      from BeautifulSoup import BeautifulSoup\n  except ImportError:\n      from bs4 import BeautifulSoup\n\n  # read the grid geometry file\n  homedir = os.path.expanduser('~')\n  print(homedir)\n  # read grid cell\n  github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n  # read grid cell\n  #submission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n  #all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n  #all_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n  filepath = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n  # Read in grid cells\n  with open(filepath) as f:\n      grid_cells = geojson.load(f)\n\n\n  # Retrieve and store cell IDs and their corresponding coordinates\n  cell_ID = []\n  ID_to_coord = {}\n\n  for i in range(len(grid_cells['features'])):\n      id = grid_cells['features'][i]['properties']['cell_id']\n      coords = grid_cells['features'][i]['geometry']['coordinates']\n      if id is not None and coords is not None: \n          cell_ID.append(id)\n          ID_to_coord[id] = coords\n\n  nohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n  test_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=1, day=1)\n  #print(test_noaa_query_url)\n\n  query_urls = []\n\n  start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n  end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n\n  stations = []\n\n  # Retrieve nearest stations for each grid cell\n  for i in range(int(len(cell_ID))):\n      for j in range(1):\n          latitude = round(ID_to_coord.get(cell_ID[i])[0][j][1], 2)\n          longitude = round(ID_to_coord.get(cell_ID[i])[0][j][0], 2)\n\n          #response = urllib.request.urlopen(test_noaa_query_url)\n          response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = start_date.year, month = start_date.month, day = start_date.day))\n          webContent = response.read().decode('UTF-8')\n          parsed_html = BeautifulSoup(webContent)\n\n          snow_water_eq_table = parsed_html.find_all('table')[7]\n          nearest_station = snow_water_eq_table.find_all('td')[0].text\n\n          stations.append(nearest_station)\n\n\n  # NOTE: this url only allows user to query one year's worth of data\n  nohrsc_time_series_url = 'https://www.nohrsc.noaa.gov/interactive/html/graph.html?station={station}&w=600&h=400&o=a&uc=0&by={start_year}&bm={start_month}&bd={start_day}&bh=0&ey={end_year}&em={end_month}&ed={end_day}&eh=23&data=1&units=0&region=us'\n\n  dates = []\n  snow_water_eq = []\n  # Keep track of cell IDs for each time series\n  new_cell_IDs = []\n\n  # Retrieve time series snow water equivalent data for each grid cell's nearest station\n  for i in range(len(stations)):\n\n      if stations[i] != '\\nNo observations within 62 miles\\n':\n\n          for j in range(end_date.year - start_date.year + 1):\n\n              response = urllib.request.urlopen(nohrsc_time_series_url.format(station = stations[i], start_year = start_date.year + j, start_month = start_date.month, start_day = start_date.day, end_year = end_date.year, end_month = end_date.month, end_day = end_date.day))\n              webContent = response.read().decode('UTF-8')\n              parsed_html = BeautifulSoup(webContent)\n\n              data_table = parsed_html.find_all('tbody')[3].find_all('tr')\n\n              for k in range(len(data_table)):\n                  new_cell_IDs.append(cell_ID[i])\n                  dates.append(data_table[k].find_all('td')[0].text)\n                  snow_water_eq.append(data_table[k].find_all('td')[2].text)\n\n      else:\n          new_cell_IDs.append(cell_ID[i])\n          dates.append('N/A')\n          snow_water_eq.append('N/A')\n\n\n  #time_series = [new_cell_IDs, dates, snow_water_eq]\n  #print(time_series)\n  df = pd.DataFrame({'cell_IDs': new_cell_IDs, 'date_times': dates, 'swe': snow_water_eq})\n  df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv\")\n  print(\"Saved\")\n\nget_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n",
  "history_output" : "today date = 2022-07-13\n/Users/joe\n2022-06-25 06:00:00\n['/Users/joe/gw-workspace/ehMWOkjwzGkE', '/Users/l21-n02609-comm/opt/anaconda3/lib/python39.zip', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/lib-dynload', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/aeosa', '/Users/joe/Documents/GitHub/twitterscraper', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/IPython/extensions']\n/Users/joe\nTraceback (most recent call last):\n  File \"/Users/joe/gw-workspace/ehMWOkjwzGkE/data_snotel_real_time.py\", line 128, in <module>\n    get_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n  File \"/Users/joe/gw-workspace/ehMWOkjwzGkE/data_snotel_real_time.py\", line 68, in get_snotel_time_series\n    start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\nAttributeError: module 'datetime' has no attribute 'strptime'\n",
  "history_begin_time" : 1657722177275,
  "history_end_time" : 1657722188251,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "mZgDbtZX493O",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n# Write first python in Geoweaver\nimport pandas as pd\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nimport geojson\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom datetime import date\nfrom snowcast_utils import *\nimport traceback\n\nprint(sys.path)\n\ndef get_snotel_time_series():\n  \n  try:\n      from BeautifulSoup import BeautifulSoup\n  except ImportError:\n      from bs4 import BeautifulSoup\n\n  # read the grid geometry file\n  homedir = os.path.expanduser('~')\n  print(homedir)\n  # read grid cell\n  github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n  # read grid cell\n  #submission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n  #all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n  #all_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n  filepath = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n  # Read in grid cells\n  with open(filepath) as f:\n      grid_cells = geojson.load(f)\n\n\n  # Retrieve and store cell IDs and their corresponding coordinates\n  cell_ID = []\n  ID_to_coord = {}\n\n  for i in range(len(grid_cells['features'])):\n      id = grid_cells['features'][i]['properties']['cell_id']\n      coords = grid_cells['features'][i]['geometry']['coordinates']\n      if id is not None and coords is not None: \n          cell_ID.append(id)\n          ID_to_coord[id] = coords\n\n  nohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n  test_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=1, day=1)\n  #print(test_noaa_query_url)\n\n  start_date = '2016-1-1'\n  end_date = '2022-7-12'\n  query_urls = []\n\n  start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n  end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n\n  stations = []\n\n  # Retrieve nearest stations for each grid cell\n  for i in range(int(len(cell_ID))):\n      for j in range(1):\n          latitude = round(ID_to_coord.get(cell_ID[i])[0][j][1], 2)\n          longitude = round(ID_to_coord.get(cell_ID[i])[0][j][0], 2)\n\n          #response = urllib.request.urlopen(test_noaa_query_url)\n          response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = start_date.year, month = start_date.month, day = start_date.day))\n          webContent = response.read().decode('UTF-8')\n          parsed_html = BeautifulSoup(webContent)\n\n          snow_water_eq_table = parsed_html.find_all('table')[7]\n          nearest_station = snow_water_eq_table.find_all('td')[0].text\n\n          stations.append(nearest_station)\n\n\n  # NOTE: this url only allows user to query one year's worth of data\n  nohrsc_time_series_url = 'https://www.nohrsc.noaa.gov/interactive/html/graph.html?station={station}&w=600&h=400&o=a&uc=0&by={start_year}&bm={start_month}&bd={start_day}&bh=0&ey={end_year}&em={end_month}&ed={end_day}&eh=23&data=1&units=0&region=us'\n\n  dates = []\n  snow_water_eq = []\n  # Keep track of cell IDs for each time series\n  new_cell_IDs = []\n\n  # Retrieve time series snow water equivalent data for each grid cell's nearest station\n  for i in range(len(stations)):\n\n      if stations[i] != '\\nNo observations within 62 miles\\n':\n\n          for j in range(end_date.year - start_date.year + 1):\n\n              response = urllib.request.urlopen(nohrsc_time_series_url.format(station = stations[i], start_year = start_date.year + j, start_month = start_date.month, start_day = start_date.day, end_year = end_date.year, end_month = end_date.month, end_day = end_date.day))\n              webContent = response.read().decode('UTF-8')\n              parsed_html = BeautifulSoup(webContent)\n\n              data_table = parsed_html.find_all('tbody')[3].find_all('tr')\n\n              for k in range(len(data_table)):\n                  new_cell_IDs.append(cell_ID[i])\n                  dates.append(data_table[k].find_all('td')[0].text)\n                  snow_water_eq.append(data_table[k].find_all('td')[2].text)\n\n      else:\n          new_cell_IDs.append(cell_ID[i])\n          dates.append('N/A')\n          snow_water_eq.append('N/A')\n\n\n  #time_series = [new_cell_IDs, dates, snow_water_eq]\n  #print(time_series)\n  df = pd.DataFrame({'cell_IDs': new_cell_IDs, 'date_times': dates, 'swe': snow_water_eq})\n  df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv\")\n  print(\"Saved\")\n\nget_snotel_time_series()\n",
  "history_output" : "",
  "history_begin_time" : 1657722125439,
  "history_end_time" : 1657722133477,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "i6L0QnAP3ReA",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n# Write first python in Geoweaver\nimport pandas as pd\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nimport geojson\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom datetime import date\nfrom snowcast_utils import *\nimport traceback\n\nprint(sys.path)\n\ndef get_snotel_time_series():\n  \n  try:\n      from BeautifulSoup import BeautifulSoup\n  except ImportError:\n      from bs4 import BeautifulSoup\n\n  # read the grid geometry file\n  homedir = os.path.expanduser('~')\n  print(homedir)\n  # read grid cell\n  github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n  # read grid cell\n  #submission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n  #all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n  #all_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n  filepath = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n  # Read in grid cells\n  with open(filepath) as f:\n      grid_cells = geojson.load(f)\n\n\n  # Retrieve and store cell IDs and their corresponding coordinates\n  cell_ID = []\n  ID_to_coord = {}\n\n  for i in range(len(grid_cells['features'])):\n      id = grid_cells['features'][i]['properties']['cell_id']\n      coords = grid_cells['features'][i]['geometry']['coordinates']\n      if id is not None and coords is not None: \n          cell_ID.append(id)\n          ID_to_coord[id] = coords\n\n  nohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n  test_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=1, day=1)\n  #print(test_noaa_query_url)\n\n  start_date = '2016-1-1'\n  end_date = '2022-7-12'\n  query_urls = []\n\n  start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n  end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n\n  stations = []\n\n  # Retrieve nearest stations for each grid cell\n  for i in range(int(len(cell_ID))):\n      for j in range(1):\n          latitude = round(ID_to_coord.get(cell_ID[i])[0][j][1], 2)\n          longitude = round(ID_to_coord.get(cell_ID[i])[0][j][0], 2)\n\n          #response = urllib.request.urlopen(test_noaa_query_url)\n          response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = start_date.year, month = start_date.month, day = start_date.day))\n          webContent = response.read().decode('UTF-8')\n          parsed_html = BeautifulSoup(webContent)\n\n          snow_water_eq_table = parsed_html.find_all('table')[7]\n          nearest_station = snow_water_eq_table.find_all('td')[0].text\n\n          stations.append(nearest_station)\n\n\n  # NOTE: this url only allows user to query one year's worth of data\n  nohrsc_time_series_url = 'https://www.nohrsc.noaa.gov/interactive/html/graph.html?station={station}&w=600&h=400&o=a&uc=0&by={start_year}&bm={start_month}&bd={start_day}&bh=0&ey={end_year}&em={end_month}&ed={end_day}&eh=23&data=1&units=0&region=us'\n\n  dates = []\n  snow_water_eq = []\n  # Keep track of cell IDs for each time series\n  new_cell_IDs = []\n\n  # Retrieve time series snow water equivalent data for each grid cell's nearest station\n  for i in range(len(stations)):\n\n      if stations[i] != '\\nNo observations within 62 miles\\n':\n\n          for j in range(end_date.year - start_date.year + 1):\n\n              response = urllib.request.urlopen(nohrsc_time_series_url.format(station = stations[i], start_year = start_date.year + j, start_month = start_date.month, start_day = start_date.day, end_year = end_date.year, end_month = end_date.month, end_day = end_date.day))\n              webContent = response.read().decode('UTF-8')\n              parsed_html = BeautifulSoup(webContent)\n\n              data_table = parsed_html.find_all('tbody')[3].find_all('tr')\n\n              for k in range(len(data_table)):\n                  new_cell_IDs.append(cell_ID[i])\n                  dates.append(data_table[k].find_all('td')[0].text)\n                  snow_water_eq.append(data_table[k].find_all('td')[2].text)\n\n      else:\n          new_cell_IDs.append(cell_ID[i])\n          dates.append('N/A')\n          snow_water_eq.append('N/A')\n\n\n  #time_series = [new_cell_IDs, dates, snow_water_eq]\n  #print(time_series)\n  df = pd.DataFrame({'cell_IDs': new_cell_IDs, 'date_times': dates, 'swe': snow_water_eq})\n  df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv')\n  print(\"Saved\")\n\nget_snotel_time_series()\n",
  "history_output" : "  File \"/Users/joe/gw-workspace/i6L0QnAP3ReA/data_snotel_real_time.py\", line 125\n    df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv')\n                                                                               ^\nSyntaxError: EOL while scanning string literal\n",
  "history_begin_time" : 1657722089959,
  "history_end_time" : 1657722090211,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "lr1ja58rjmw",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\nsnotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\ndf = snotel_point.get_daily_data(\n    datetime(2020, 1, 2), datetime(2020, 1, 20),\n    [snotel_point.ALLOWED_VARIABLES.SWE]\n)\nprint(df)",
  "history_output" : "                                                                        geometry  ...  datasource\ndatetime                  site                                                    ...            \n2020-01-02 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-03 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-04 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-05 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-06 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-07 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-08 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-09 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-10 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-11 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-12 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-13 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-14 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-15 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-16 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-17 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-18 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-19 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-20 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n[19 rows x 4 columns]\n",
  "history_begin_time" : 1654519487343,
  "history_end_time" : 1654519489546,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "TyafqfQQDnVh",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\nsnotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\ndf = snotel_point.get_daily_data(\n    datetime(2020, 1, 2), datetime(2020, 1, 20),\n    [snotel_point.ALLOWED_VARIABLES.SWE]\n)\nprint(df)",
  "history_output" : "                                                                        geometry  ...  datasource\ndatetime                  site                                                    ...            \n2020-01-02 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-03 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-04 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-05 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-06 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-07 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-08 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-09 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-10 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-11 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-12 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-13 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-14 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-15 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-16 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-17 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-18 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-19 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-20 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n[19 rows x 4 columns]\n",
  "history_begin_time" : 1654519412744,
  "history_end_time" : 1654519415600,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "7YvDLFJgCKL5",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\nsnotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\ndf = snotel_point.get_daily_data(\n    datetime(2020, 1, 2), datetime(2020, 1, 20),\n    [snotel_point.ALLOWED_VARIABLES.SWE]\n)\nprint(df)",
  "history_output" : "                                                                        geometry  ...  datasource\ndatetime                  site                                                    ...            \n2020-01-02 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-03 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-04 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-05 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-06 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-07 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-08 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-09 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-10 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-11 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-12 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-13 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-14 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-15 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-16 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-17 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-18 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-19 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-20 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n[19 rows x 4 columns]\n",
  "history_begin_time" : 1654352628137,
  "history_end_time" : 1654352630624,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "rk6VnLnSmQsu",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\nsnotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\ndf = snotel_point.get_daily_data(\n    datetime(2020, 1, 2), datetime(2020, 1, 20),\n    [snotel_point.ALLOWED_VARIABLES.SWE]\n)\nprint(df)",
  "history_output" : "                                                                        geometry  ...  datasource\ndatetime                  site                                                    ...            \n2020-01-02 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-03 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-04 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-05 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-06 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-07 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-08 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-09 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-10 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-11 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-12 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-13 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-14 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-15 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-16 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-17 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-18 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-19 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-20 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n[19 rows x 4 columns]\n",
  "history_begin_time" : 1654290337491,
  "history_end_time" : 1654290340959,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "lIdSPYNQQx0T",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\nsnotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\ndf = snotel_point.get_daily_data(\n    datetime(2020, 1, 2), datetime(2020, 1, 20),\n    [snotel_point.ALLOWED_VARIABLES.SWE]\n)\nprint(df)",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/joe/gw-workspace/lIdSPYNQQx0T/data_snotel_real_time.py\", line 2, in <module>\n    from metloom.pointdata import SnotelPointData\nModuleNotFoundError: No module named 'metloom'\n",
  "history_begin_time" : 1654290321059,
  "history_end_time" : 1654290321135,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "PGQqshu1imog",
  "history_input" : "import os\nfrom datetime import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport contextily as ctx\n\nimport ulmo # this library has big trouble in Python 3.9\n\nsites = ulmo.cuahsi.wof.get_sites(wsdlurl)\n\n#Preview first item in dictionary\nprint(next(iter(sites.items())))\n\nsites_df = pd.DataFrame.from_dict(sites, orient='index').dropna()\nprint(sites_df.head())\n\nsites_df['geometry'] = [Point(float(loc['longitude']), float(loc['latitude'])) for loc in sites_df['location']]\n\nsites_df = sites_df.drop(columns='location')\nsites_df = sites_df.astype({\"elevation_m\":float})\n\nprint(sites_df.head())\n\nprint(sites_df.loc['SNOTEL:301_CA_SNTL'])\n\nprint(sites_df.loc['SNOTEL:301_CA_SNTL']['site_property'])\n\nsites_gdf_all = gpd.GeoDataFrame(sites_df, crs='EPSG:4326')\nprint(sites_gdf_all.head())\n\nprint(sites_gdf_all.shape)\n\n#geojson of state polygons\nstates_url = 'http://eric.clst.org/assets/wiki/uploads/Stuff/gz_2010_us_040_00_5m.json'\nstates_gdf = gpd.read_file(states_url)\n\nf, ax = plt.subplots(figsize=(10,6))\nsites_gdf_all.plot(ax=ax, column='elevation_m', markersize=3, cmap='inferno', legend=True, legend_kwds={'label': \"Elevation (m)\"})\n#This prevents matplotlib from updating the axes extent (states polygons cover larger area than SNOTEL points)\nax.autoscale(False)\nstates_gdf.plot(ax=ax, facecolor='none', edgecolor='k', alpha=0.3);\n\nsites_gdf_conus = sites_gdf_all[~(sites_gdf_all.index.str.contains('AK'))]\n\nprint(sites_gdf_conus.shape)\n\nf, ax = plt.subplots(figsize=(10,6))\nsites_gdf_conus.plot(ax=ax, column='elevation_m', markersize=3, cmap='inferno', legend=True, legend_kwds={'label': \"Elevation (m)\"})\nax.autoscale(False)\nstates_gdf.plot(ax=ax, facecolor='none', edgecolor='k', alpha=0.3);\n\nsites_fn = 'snotel_conus_sites.json'\nif not os.path.exists(sites_fn):\n    sites_gdf_conus.to_file(sites_fn, driver='GeoJSON')\n\n    ",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/joe/gw-workspace/PGQqshu1imog/data_snotel_real_time.py\", line 10, in <module>\n    import ulmo # this library has big trouble in Python 3.9\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/ulmo/__init__.py\", line 9, in <module>\n    from . import cpc\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/ulmo/cpc/__init__.py\", line 1, in <module>\n    from . import drought\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/ulmo/cpc/drought/__init__.py\", line 8, in <module>\n    from .core import get_data\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/ulmo/cpc/drought/core.py\", line 19, in <module>\n    from ulmo import util\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/ulmo/util/__init__.py\", line 1, in <module>\n    from .misc import (\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/ulmo/util/misc.py\", line 8, in <module>\n    import urlparse\nModuleNotFoundError: No module named 'urlparse'\n",
  "history_begin_time" : 1654289821663,
  "history_end_time" : 1654289824479,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "zZDxYhYoYJa0",
  "history_input" : "import os\nfrom datetime import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport contextily as ctx\nimport ulmo\n\nsites = ulmo.cuahsi.wof.get_sites(wsdlurl)\n\n#Preview first item in dictionary\nprint(next(iter(sites.items())))\n\nsites_df = pd.DataFrame.from_dict(sites, orient='index').dropna()\nprint(sites_df.head())\n\nsites_df['geometry'] = [Point(float(loc['longitude']), float(loc['latitude'])) for loc in sites_df['location']]\n\nsites_df = sites_df.drop(columns='location')\nsites_df = sites_df.astype({\"elevation_m\":float})\n\nprint(sites_df.head())\n\nprint(sites_df.loc['SNOTEL:301_CA_SNTL'])\n\nprint(sites_df.loc['SNOTEL:301_CA_SNTL']['site_property'])\n\nsites_gdf_all = gpd.GeoDataFrame(sites_df, crs='EPSG:4326')\nprint(sites_gdf_all.head())\n\nprint(sites_gdf_all.shape)\n\n#geojson of state polygons\nstates_url = 'http://eric.clst.org/assets/wiki/uploads/Stuff/gz_2010_us_040_00_5m.json'\nstates_gdf = gpd.read_file(states_url)\n\nf, ax = plt.subplots(figsize=(10,6))\nsites_gdf_all.plot(ax=ax, column='elevation_m', markersize=3, cmap='inferno', legend=True, legend_kwds={'label': \"Elevation (m)\"})\n#This prevents matplotlib from updating the axes extent (states polygons cover larger area than SNOTEL points)\nax.autoscale(False)\nstates_gdf.plot(ax=ax, facecolor='none', edgecolor='k', alpha=0.3);\n\nsites_gdf_conus = sites_gdf_all[~(sites_gdf_all.index.str.contains('AK'))]\n\nprint(sites_gdf_conus.shape)\n\nf, ax = plt.subplots(figsize=(10,6))\nsites_gdf_conus.plot(ax=ax, column='elevation_m', markersize=3, cmap='inferno', legend=True, legend_kwds={'label': \"Elevation (m)\"})\nax.autoscale(False)\nstates_gdf.plot(ax=ax, facecolor='none', edgecolor='k', alpha=0.3);\n\nsites_fn = 'snotel_conus_sites.json'\nif not os.path.exists(sites_fn):\n    sites_gdf_conus.to_file(sites_fn, driver='GeoJSON')",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/joe/gw-workspace/zZDxYhYoYJa0/data_snotel_real_time.py\", line 9, in <module>\n    import ulmo\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/ulmo/__init__.py\", line 9, in <module>\n    from . import cpc\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/ulmo/cpc/__init__.py\", line 1, in <module>\n    from . import drought\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/ulmo/cpc/drought/__init__.py\", line 8, in <module>\n    from .core import get_data\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/ulmo/cpc/drought/core.py\", line 19, in <module>\n    from ulmo import util\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/ulmo/util/__init__.py\", line 1, in <module>\n    from .misc import (\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/ulmo/util/misc.py\", line 8, in <module>\n    import urlparse\nModuleNotFoundError: No module named 'urlparse'\n",
  "history_begin_time" : 1653916164155,
  "history_end_time" : 1653916166101,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "z8rfe8YRjyRe",
  "history_input" : "import os\nfrom datetime import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport contextily as ctx\nimport ulmo\n\nsites = ulmo.cuahsi.wof.get_sites(wsdlurl)\n\n#Preview first item in dictionary\nprint(next(iter(sites.items())))\n\nsites_df = pd.DataFrame.from_dict(sites, orient='index').dropna()\nprint(sites_df.head())\n\nsites_df['geometry'] = [Point(float(loc['longitude']), float(loc['latitude'])) for loc in sites_df['location']]\n\nsites_df = sites_df.drop(columns='location')\nsites_df = sites_df.astype({\"elevation_m\":float})\n\nprint(sites_df.head())\n\nprint(sites_df.loc['SNOTEL:301_CA_SNTL'])\n\nprint(sites_df.loc['SNOTEL:301_CA_SNTL']['site_property'])\n\nsites_gdf_all = gpd.GeoDataFrame(sites_df, crs='EPSG:4326')\nprint(sites_gdf_all.head())\n\nprint(sites_gdf_all.shape)\n\n#geojson of state polygons\nstates_url = 'http://eric.clst.org/assets/wiki/uploads/Stuff/gz_2010_us_040_00_5m.json'\nstates_gdf = gpd.read_file(states_url)\n\nf, ax = plt.subplots(figsize=(10,6))\nsites_gdf_all.plot(ax=ax, column='elevation_m', markersize=3, cmap='inferno', legend=True, legend_kwds={'label': \"Elevation (m)\"})\n#This prevents matplotlib from updating the axes extent (states polygons cover larger area than SNOTEL points)\nax.autoscale(False)\nstates_gdf.plot(ax=ax, facecolor='none', edgecolor='k', alpha=0.3);\n\nsites_gdf_conus = sites_gdf_all[~(sites_gdf_all.index.str.contains('AK'))]\n\nprint(sites_gdf_conus.shape)\n\nf, ax = plt.subplots(figsize=(10,6))\nsites_gdf_conus.plot(ax=ax, column='elevation_m', markersize=3, cmap='inferno', legend=True, legend_kwds={'label': \"Elevation (m)\"})\nax.autoscale(False)\nstates_gdf.plot(ax=ax, facecolor='none', edgecolor='k', alpha=0.3);\n\nsites_fn = 'snotel_conus_sites.json'\nif not os.path.exists(sites_fn):\n    sites_gdf_conus.to_file(sites_fn, driver='GeoJSON')",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/joe/gw-workspace/z8rfe8YRjyRe/data_snotel_real_time.py\", line 9, in <module>\n    import ulmo\nModuleNotFoundError: No module named 'ulmo'\n",
  "history_begin_time" : 1653916136107,
  "history_end_time" : 1653916139031,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "NzcbQhlCCWbM",
  "history_input" : "import os\nfrom datetime import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport contextily as ctx\nimport ulmo\n\nsites = ulmo.cuahsi.wof.get_sites(wsdlurl)\n\n#Preview first item in dictionary\nprint(next(iter(sites.items())))\n\nsites_df = pd.DataFrame.from_dict(sites, orient='index').dropna()\nprint(sites_df.head())\n\nsites_df['geometry'] = [Point(float(loc['longitude']), float(loc['latitude'])) for loc in sites_df['location']]\n\nsites_df = sites_df.drop(columns='location')\nsites_df = sites_df.astype({\"elevation_m\":float})\n\nprint(sites_df.head())\n\nprint(sites_df.loc['SNOTEL:301_CA_SNTL'])\n\nprint(sites_df.loc['SNOTEL:301_CA_SNTL']['site_property'])\n\nsites_gdf_all = gpd.GeoDataFrame(sites_df, crs='EPSG:4326')\nprint(sites_gdf_all.head())\n\nprint(sites_gdf_all.shape)\n\n#geojson of state polygons\nstates_url = 'http://eric.clst.org/assets/wiki/uploads/Stuff/gz_2010_us_040_00_5m.json'\nstates_gdf = gpd.read_file(states_url)\n\nf, ax = plt.subplots(figsize=(10,6))\nsites_gdf_all.plot(ax=ax, column='elevation_m', markersize=3, cmap='inferno', legend=True, legend_kwds={'label': \"Elevation (m)\"})\n#This prevents matplotlib from updating the axes extent (states polygons cover larger area than SNOTEL points)\nax.autoscale(False)\nstates_gdf.plot(ax=ax, facecolor='none', edgecolor='k', alpha=0.3);\n\nsites_gdf_conus = sites_gdf_all[~(sites_gdf_all.index.str.contains('AK'))]\n\nprint(sites_gdf_conus.shape)\n\nf, ax = plt.subplots(figsize=(10,6))\nsites_gdf_conus.plot(ax=ax, column='elevation_m', markersize=3, cmap='inferno', legend=True, legend_kwds={'label': \"Elevation (m)\"})\nax.autoscale(False)\nstates_gdf.plot(ax=ax, facecolor='none', edgecolor='k', alpha=0.3);\n\nsites_fn = 'snotel_conus_sites.json'\nif not os.path.exists(sites_fn):\n    sites_gdf_conus.to_file(sites_fn, driver='GeoJSON')",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/joe/gw-workspace/NzcbQhlCCWbM/data_snotel_real_time.py\", line 8, in <module>\n    import contextily as ctx\nModuleNotFoundError: No module named 'contextily'\n",
  "history_begin_time" : 1653916109409,
  "history_end_time" : 1653916111149,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Done"
},]
