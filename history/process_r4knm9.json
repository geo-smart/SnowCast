[{
  "history_id" : "jnn34vo22j7",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    data = data.drop(\"date\", axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\ntoday date = 2023-10-09\ntest start date:  2023-09-16\ntest end date:  2023-10-09\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib\nFile '/home/chetana/gridmet_test_run/test_data_predicted.csv' has been removed.\nnew_data shape:  (462204, 21)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib\n    lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0  49.0 -125.000    0   241  ...     -999.0  -999.0    -999.0     -999.0\n1  49.0 -124.964    0   241  ...     -999.0  -999.0    -999.0     -999.0\n2  49.0 -124.928    0   241  ...     -999.0  -999.0    -999.0     -999.0\n3  49.0 -124.892    0   241  ...     -999.0  -999.0    -999.0     -999.0\n4  49.0 -124.856    0   241  ...     -999.0  -999.0    -999.0     -999.0\n[5 rows x 18 columns]\nhow many rows are there? 462204\npredicted swe:  count    462204.000000\nmean          6.633628\nstd           1.415936\nmin           3.486000\n25%           5.415000\n50%           6.884500\n75%           7.514500\nmax          13.949500\nName: predicted_swe, dtype: float64\nIndex(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n[0.04633082 0.049006   0.05922091 0.00830327 0.07838727 0.11662276\n 0.10793431 0.07455769 0.07209162 0.03498797 0.10763383 0.05324669\n 0.03938958 0.03150809 0.03559248 0.02661422 0.03019245 0.02838003]\n",
  "history_begin_time" : 1696864201207,
  "history_end_time" : 1696864243272,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "slhd03iqugl",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    data = data.drop(\"date\", axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\ntoday date = 2023-10-09\ntest start date:  2023-09-15\ntest end date:  2023-10-09\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib\nFile '/home/chetana/gridmet_test_run/test_data_predicted.csv' has been removed.\nnew_data shape:  (462204, 21)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib\n    lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0  49.0 -125.000    0   241  ...     -999.0  -999.0    -999.0     -999.0\n1  49.0 -124.964    0   241  ...     -999.0  -999.0    -999.0     -999.0\n2  49.0 -124.928    0   241  ...     -999.0  -999.0    -999.0     -999.0\n3  49.0 -124.892    0   241  ...     -999.0  -999.0    -999.0     -999.0\n4  49.0 -124.856    0   241  ...     -999.0  -999.0    -999.0     -999.0\n[5 rows x 18 columns]\nhow many rows are there? 462204\npredicted swe:  count    462204.000000\nmean          7.079997\nstd           1.527712\nmin           3.739000\n25%           5.930500\n50%           6.967000\n75%           7.775000\nmax          13.356500\nName: predicted_swe, dtype: float64\nIndex(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n[0.04633082 0.049006   0.05922091 0.00830327 0.07838727 0.11662276\n 0.10793431 0.07455769 0.07209162 0.03498797 0.10763383 0.05324669\n 0.03938958 0.03150809 0.03559248 0.02661422 0.03019245 0.02838003]\n",
  "history_begin_time" : 1696862681200,
  "history_end_time" : 1696862735745,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "rnzdlwmbrtq",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    data = data.drop(\"date\", axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\ntoday date = 2023-10-09\ntest start date:  2023-06-15\ntest end date:  2023-10-09\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib\nFile '/home/chetana/gridmet_test_run/test_data_predicted.csv' has been removed.\nnew_data shape:  (462204, 21)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib\n    lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0  49.0 -125.000    0   241  ...     -999.0  -999.0    -999.0     -999.0\n1  49.0 -124.964    0   241  ...     -999.0  -999.0    -999.0     -999.0\n2  49.0 -124.928    0   241  ...     -999.0  -999.0    -999.0     -999.0\n3  49.0 -124.892    0   241  ...     -999.0  -999.0    -999.0     -999.0\n4  49.0 -124.856    0   241  ...     -999.0  -999.0    -999.0     -999.0\n[5 rows x 18 columns]\nhow many rows are there? 462204\npredicted swe:  count    462204.000000\nmean          7.097843\nstd           1.343252\nmin           3.136000\n25%           6.070000\n50%           7.022500\n75%           7.771500\nmax          12.641500\nName: predicted_swe, dtype: float64\nIndex(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n[0.04633082 0.049006   0.05922091 0.00830327 0.07838727 0.11662276\n 0.10793431 0.07455769 0.07209162 0.03498797 0.10763383 0.05324669\n 0.03938958 0.03150809 0.03559248 0.02661422 0.03019245 0.02838003]\n",
  "history_begin_time" : 1696832501356,
  "history_end_time" : 1696832529964,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "0Sc91JbgAPF6",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    data = data.drop(\"date\", axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\ntoday date = 2023-10-09\ntest start date:  2023-02-12\ntest end date:  2023-10-09\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib\nnew_data shape:  (462204, 21)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib\n    lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0  49.0 -125.000    0   241  ...     -999.0  -999.0    -999.0     -999.0\n1  49.0 -124.964    0   241  ...     -999.0  -999.0    -999.0     -999.0\n2  49.0 -124.928    0   241  ...     -999.0  -999.0    -999.0     -999.0\n3  49.0 -124.892    0   241  ...     -999.0  -999.0    -999.0     -999.0\n4  49.0 -124.856    0   241  ...     -999.0  -999.0    -999.0     -999.0\n[5 rows x 18 columns]\nhow many rows are there? 462204\npredicted swe:  count    462204.000000\nmean          8.052420\nstd           1.940919\nmin           3.401000\n25%           6.861500\n50%           7.556500\n75%           8.707500\nmax          16.004000\nName: predicted_swe, dtype: float64\nIndex(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n[0.04633082 0.049006   0.05922091 0.00830327 0.07838727 0.11662276\n 0.10793431 0.07455769 0.07209162 0.03498797 0.10763383 0.05324669\n 0.03938958 0.03150809 0.03559248 0.02661422 0.03019245 0.02838003]\n",
  "history_begin_time" : 1696832133218,
  "history_end_time" : 1696832162449,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "a0a2mqx5j1s",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    data = data.drop(\"date\", axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "",
  "history_begin_time" : 1696832093433,
  "history_end_time" : 1696832099934,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "cF1jtPsEHaZG",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    data = data.drop(\"date\", axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\ntoday date = 2023-10-09\ntest start date:  2023-02-11\ntest end date:  2023-10-09\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib\nnew_data shape:  (462204, 21)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib\n    lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0  49.0 -125.000    0   241  ...     -999.0  -999.0    -999.0     -999.0\n1  49.0 -124.964    0   241  ...     -999.0  -999.0    -999.0     -999.0\n2  49.0 -124.928    0   241  ...     -999.0  -999.0    -999.0     -999.0\n3  49.0 -124.892    0   241  ...     -999.0  -999.0    -999.0     -999.0\n4  49.0 -124.856    0   241  ...     -999.0  -999.0    -999.0     -999.0\n[5 rows x 18 columns]\nhow many rows are there? 462204\npredicted swe:  count    462204.000000\nmean          8.038037\nstd           1.831186\nmin           3.096000\n25%           6.958500\n50%           7.514500\n75%           8.742500\nmax          15.575500\nName: predicted_swe, dtype: float64\nIndex(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n[0.04633082 0.049006   0.05922091 0.00830327 0.07838727 0.11662276\n 0.10793431 0.07455769 0.07209162 0.03498797 0.10763383 0.05324669\n 0.03938958 0.03150809 0.03559248 0.02661422 0.03019245 0.02838003]\n",
  "history_begin_time" : 1696831759576,
  "history_end_time" : 1696831788567,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Mbetom6r3pmO",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    data = data.drop(\"data\", axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "today date = 2023-10-09\ntest start date:  2023-02-11\ntest end date:  2023-10-09\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib\nFile '/home/chetana/gridmet_test_run/test_data_predicted.csv' has been removed.\nnew_data shape:  (462204, 21)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/Mbetom6r3pmO/interpret_model_results.py\", line 219, in <module>\n    interpret_prediction()\n  File \"/home/chetana/gw-workspace/Mbetom6r3pmO/interpret_model_results.py\", line 159, in interpret_prediction\n    preprocessed_data = preprocess_data(new_data)\n  File \"/home/chetana/gw-workspace/Mbetom6r3pmO/interpret_model_results.py\", line 92, in preprocess_data\n    data = data.drop(\"data\", axis=1)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\", line 5399, in drop\n    return super().drop(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\", line 4505, in drop\n    obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\", line 4546, in _drop_axis\n    new_axis = axis.drop(labels, errors=errors)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 6934, in drop\n    raise KeyError(f\"{list(labels[mask])} not found in axis\")\nKeyError: \"['data'] not found in axis\"\n",
  "history_begin_time" : 1696831673167,
  "history_end_time" : 1696831711248,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "axydqyfsu54",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696830174455,
  "history_end_time" : 1696830174455,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6mekPybAkcX1",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n    explainer = shap.Explainer(current_model)\n    shap_values = explainer.shap_values(\n      preprocessed_data.drop('predicted_swe', axis=1))\n\n    # Summary plot of SHAP values\n    shap.summary_plot(shap_values, preprocessed_data)\n    plt.title('Summary Plot of SHAP Values')\n    plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\n\nStream closed",
  "history_begin_time" : 1696790756418,
  "history_end_time" : 1696790760432,
  "history_notes" : "this might take longer, just wait for a while",
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "FWLQUOMN0ZqJ",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n    explainer = shap.Explainer(current_model)\n    shap_values = explainer.shap_values(\n      preprocessed_data.drop('predicted_swe', axis=1))\n\n    # Summary plot of SHAP values\n    shap.summary_plot(shap_values, preprocessed_data)\n    plt.title('Summary Plot of SHAP Values')\n    plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "Running",
  "history_begin_time" : 1696790750029,
  "history_end_time" : 1696790758767,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "hp982v9whf5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696787542024,
  "history_end_time" : 1696787542024,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "oitwpvzkuto",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696786838353,
  "history_end_time" : 1696786838353,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "zUToy4VDHJK0",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n    explainer = shap.Explainer(current_model)\n    shap_values = explainer.shap_values(\n      preprocessed_data.drop('predicted_swe', axis=1))\n\n    # Summary plot of SHAP values\n    shap.summary_plot(shap_values, preprocessed_data)\n    plt.title('Summary Plot of SHAP Values')\n    plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\n",
  "history_begin_time" : 1696786668527,
  "history_end_time" : 1696786822275,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "zcCTtvvpLkfP",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    partial_dependence_display = PartialDependenceDisplay.from_estimator(\n    current_model, preprocessed_data.drop('predicted_swe', axis=1), features=features_to_plot, grid_resolution=50\n)\n    partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n    partial_dependence_display.figure_.subplots_adjust(top=0.9)\n    partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n    explainer = shap.Explainer(current_model)\n    shap_values = explainer.shap_values(preprocessed_data)\n\n    # Summary plot of SHAP values\n    shap.summary_plot(shap_values, preprocessed_data)\n    plt.title('Summary Plot of SHAP Values')\n    plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "Running",
  "history_begin_time" : 1696782206190,
  "history_end_time" : 1696784080434,
  "history_notes" : "it seems the partial dependence and SHAP take a while to finish",
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "KGzY1cKWR9cK",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    partial_dependence_display = PartialDependenceDisplay.from_estimator(\n    current_model, preprocessed_data, features=features_to_plot, grid_resolution=50\n)\n    partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n    partial_dependence_display.figure_.subplots_adjust(top=0.9)\n    partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n    explainer = shap.Explainer(current_model)\n    shap_values = explainer.shap_values(preprocessed_data)\n\n    # Summary plot of SHAP values\n    shap.summary_plot(shap_values, preprocessed_data)\n    plt.title('Summary Plot of SHAP Values')\n    plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\nX has feature names, but ExtraTreesRegressor was fitted without feature names\ntoday date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib\nnew_data shape:  (462204, 21)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib\n    date   lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0  44944  49.0 -125.000    0  ...     -999.0  -999.0    -999.0     -999.0\n1  44944  49.0 -124.964    0  ...     -999.0  -999.0    -999.0     -999.0\n2  44944  49.0 -124.928    0  ...     -999.0  -999.0    -999.0     -999.0\n3  44944  49.0 -124.892    0  ...     -999.0  -999.0    -999.0     -999.0\n4  44944  49.0 -124.856    0  ...     -999.0  -999.0    -999.0     -999.0\n[5 rows x 19 columns]\nhow many rows are there? 462204\npredicted swe:  count    462204.000000\nmean          7.450335\nstd           0.053167\nmin           7.049500\n25%           7.436000\n50%           7.461500\n75%           7.481000\nmax           7.500000\nName: predicted_swe, dtype: float64\nIndex(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n[9.74456085e-01 1.96865689e-04 1.65304045e-04 9.63018037e-04\n 1.12789367e-04 2.03836414e-03 6.29689328e-03 9.65969234e-03\n 7.04791262e-04 7.43298562e-04 9.42858243e-05 3.93990115e-03\n 3.29505772e-04 1.82865198e-04 2.30558544e-05 3.17364141e-05\n 1.75703188e-05 2.29915842e-05 2.09861648e-05]\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/KGzY1cKWR9cK/interpret_model_results.py\", line 211, in <module>\n    interpret_prediction()\n  File \"/home/chetana/gw-workspace/KGzY1cKWR9cK/interpret_model_results.py\", line 179, in interpret_prediction\n    partial_dependence_display = PartialDependenceDisplay.from_estimator(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/inspection/_plot/partial_dependence.py\", line 703, in from_estimator\n    pd_results = Parallel(n_jobs=n_jobs, verbose=verbose)(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 65, in __call__\n    return super().__call__(iterable_with_config)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1863, in __call__\n    return output if self.return_generator else list(output)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1792, in _get_sequential_output\n    res = func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 127, in __call__\n    return self.function(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\", line 211, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/inspection/_partial_dependence.py\", line 706, in partial_dependence\n    averaged_predictions, predictions = _partial_dependence_brute(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/inspection/_partial_dependence.py\", line 310, in _partial_dependence_brute\n    pred = prediction_method(X_eval)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 984, in predict\n    X = self._validate_X_predict(X)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 599, in _validate_X_predict\n    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 625, in _validate_data\n    self._check_n_features(X, reset=reset)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 414, in _check_n_features\n    raise ValueError(\nValueError: X has 20 features, but ExtraTreesRegressor is expecting 19 features as input.\n",
  "history_begin_time" : 1696781870532,
  "history_end_time" : 1696781882681,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "CJB3jHy6O8X4",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    partial_dependence_display = PartialDependenceDisplay.from_estimator(\n    current_model, preprocessed_data, features=features_to_plot, grid_resolution=50\n)\n    partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n    partial_dependence_display.figure_.subplots_adjust(top=0.9)\n    partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n    explainer = shap.Explainer(current_model)\n    shap_values = explainer.shap_values(preprocessed_data)\n\n    # Summary plot of SHAP values\n    shap.summary_plot(shap_values, preprocessed_data)\n    plt.title('Summary Plot of SHAP Values')\n    plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\ntoday date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib\nnew_data shape:  (462204, 21)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib\n    date   lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0  44944  49.0 -125.000    0  ...     -999.0  -999.0    -999.0     -999.0\n1  44944  49.0 -124.964    0  ...     -999.0  -999.0    -999.0     -999.0\n2  44944  49.0 -124.928    0  ...     -999.0  -999.0    -999.0     -999.0\n3  44944  49.0 -124.892    0  ...     -999.0  -999.0    -999.0     -999.0\n4  44944  49.0 -124.856    0  ...     -999.0  -999.0    -999.0     -999.0\n[5 rows x 19 columns]\nhow many rows are there? 462204\npredicted swe:  count    462204.000000\nmean          7.450335\nstd           0.053167\nmin           7.049500\n25%           7.436000\n50%           7.461500\n75%           7.481000\nmax           7.500000\nName: predicted_swe, dtype: float64\nIndex(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'predicted_swe'],\n      dtype='object')\n[9.74456085e-01 1.96865689e-04 1.65304045e-04 9.63018037e-04\n 1.12789367e-04 2.03836414e-03 6.29689328e-03 9.65969234e-03\n 7.04791262e-04 7.43298562e-04 9.42858243e-05 3.93990115e-03\n 3.29505772e-04 1.82865198e-04 2.30558544e-05 3.17364141e-05\n 1.75703188e-05 2.29915842e-05 2.09861648e-05]\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/CJB3jHy6O8X4/interpret_model_results.py\", line 211, in <module>\n    interpret_prediction()\n  File \"/home/chetana/gw-workspace/CJB3jHy6O8X4/interpret_model_results.py\", line 169, in interpret_prediction\n    plt.barh(feature_names, feature_importances)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/matplotlib/pyplot.py\", line 2771, in barh\n    return gca().barh(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/matplotlib/axes/_axes.py\", line 2698, in barh\n    patches = self.bar(x=left, height=height, width=width, bottom=y,\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/matplotlib/__init__.py\", line 1465, in inner\n    return func(ax, *map(sanitize_sequence, args), **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/matplotlib/axes/_axes.py\", line 2457, in bar\n    x, height, width, y, linewidth, hatch = np.broadcast_arrays(\n  File \"<__array_function__ internals>\", line 200, in broadcast_arrays\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/numpy/lib/stride_tricks.py\", line 540, in broadcast_arrays\n    shape = _broadcast_shape(*args)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/numpy/lib/stride_tricks.py\", line 422, in _broadcast_shape\n    b = np.broadcast(*args[:32])\nValueError: shape mismatch: objects cannot be broadcast to a single shape.  Mismatch is between arg 2 with shape (19,) and arg 3 with shape (20,).\n",
  "history_begin_time" : 1696781771056,
  "history_end_time" : 1696781782689,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "XVjeIgZKfaSc",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    partial_dependence_display = PartialDependenceDisplay.from_estimator(\n    current_model, preprocessed_data, features=features_to_plot, grid_resolution=50\n)\n    partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n    partial_dependence_display.figure_.subplots_adjust(top=0.9)\n    partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n    explainer = shap.Explainer(current_model)\n    shap_values = explainer.shap_values(preprocessed_data)\n\n    # Summary plot of SHAP values\n    shap.summary_plot(shap_values, preprocessed_data)\n    plt.title('Summary Plot of SHAP Values')\n    plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "",
  "history_begin_time" : 1696781759328,
  "history_end_time" : 1696781766761,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "fgrz9IZG15Pw",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    partial_dependence_display = PartialDependenceDisplay.from_estimator(\n    current_model, preprocessed_data, features=features_to_plot, grid_resolution=50\n)\n    partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n    partial_dependence_display.figure_.subplots_adjust(top=0.9)\n    partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n    explainer = shap.Explainer(current_model)\n    shap_values = explainer.shap_values(preprocessed_data)\n\n    # Summary plot of SHAP values\n    shap.summary_plot(shap_values, preprocessed_data)\n    plt.title('Summary Plot of SHAP Values')\n    plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\ntoday date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib\nnew_data shape:  (462204, 21)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib\n    date   lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0  44944  49.0 -125.000    0  ...     -999.0  -999.0    -999.0     -999.0\n1  44944  49.0 -124.964    0  ...     -999.0  -999.0    -999.0     -999.0\n2  44944  49.0 -124.928    0  ...     -999.0  -999.0    -999.0     -999.0\n3  44944  49.0 -124.892    0  ...     -999.0  -999.0    -999.0     -999.0\n4  44944  49.0 -124.856    0  ...     -999.0  -999.0    -999.0     -999.0\n[5 rows x 19 columns]\nhow many rows are there? 462204\npredicted swe:  count    462204.000000\nmean          7.450335\nstd           0.053167\nmin           7.049500\n25%           7.436000\n50%           7.461500\n75%           7.481000\nmax           7.500000\nName: predicted_swe, dtype: float64\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/fgrz9IZG15Pw/interpret_model_results.py\", line 209, in <module>\n    interpret_prediction()\n  File \"/home/chetana/gw-workspace/fgrz9IZG15Pw/interpret_model_results.py\", line 167, in interpret_prediction\n    plt.barh(feature_names, feature_importances)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/matplotlib/pyplot.py\", line 2771, in barh\n    return gca().barh(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/matplotlib/axes/_axes.py\", line 2698, in barh\n    patches = self.bar(x=left, height=height, width=width, bottom=y,\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/matplotlib/__init__.py\", line 1465, in inner\n    return func(ax, *map(sanitize_sequence, args), **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/matplotlib/axes/_axes.py\", line 2457, in bar\n    x, height, width, y, linewidth, hatch = np.broadcast_arrays(\n  File \"<__array_function__ internals>\", line 200, in broadcast_arrays\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/numpy/lib/stride_tricks.py\", line 540, in broadcast_arrays\n    shape = _broadcast_shape(*args)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/numpy/lib/stride_tricks.py\", line 422, in _broadcast_shape\n    b = np.broadcast(*args[:32])\nValueError: shape mismatch: objects cannot be broadcast to a single shape.  Mismatch is between arg 2 with shape (19,) and arg 3 with shape (20,).\n",
  "history_begin_time" : 1696778782807,
  "history_end_time" : 1696778794790,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "hYXuwqzSwFlM",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    partial_dependence_display = PartialDependenceDisplay.from_estimator(\n    current_model, predicted_data, features=features_to_plot, grid_resolution=50\n)\n    partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n    partial_dependence_display.figure_.subplots_adjust(top=0.9)\n    partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n    explainer = shap.Explainer(xgb_model)\n    shap_values = explainer.shap_values(X)\n\n    # Summary plot of SHAP values\n    shap.summary_plot(shap_values, X)\n    plt.title('Summary Plot of SHAP Values')\n    plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\nX has feature names, but ExtraTreesRegressor was fitted without feature names\ntoday date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib\nnew_data shape:  (462204, 21)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib\n    date   lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0  44944  49.0 -125.000    0  ...        NaN     NaN       NaN        NaN\n1  44944  49.0 -124.964    0  ...        NaN     NaN       NaN        NaN\n2  44944  49.0 -124.928    0  ...        NaN     NaN       NaN        NaN\n3  44944  49.0 -124.892    0  ...        NaN     NaN       NaN        NaN\n4  44944  49.0 -124.856    0  ...        NaN     NaN       NaN        NaN\n[5 rows x 19 columns]\nhow many rows are there? 462204\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\npredicted swe:  count    462204.000000\nmean          7.450335\nstd           0.053167\nmin           7.049500\n25%           7.436000\n50%           7.461500\n75%           7.481000\nmax           7.500000\nName: predicted_swe, dtype: float64\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/hYXuwqzSwFlM/interpret_model_results.py\", line 209, in <module>\n    interpret_prediction()\n  File \"/home/chetana/gw-workspace/hYXuwqzSwFlM/interpret_model_results.py\", line 177, in interpret_prediction\n    partial_dependence_display = PartialDependenceDisplay.from_estimator(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/inspection/_plot/partial_dependence.py\", line 703, in from_estimator\n    pd_results = Parallel(n_jobs=n_jobs, verbose=verbose)(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 65, in __call__\n    return super().__call__(iterable_with_config)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1863, in __call__\n    return output if self.return_generator else list(output)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1792, in _get_sequential_output\n    res = func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 127, in __call__\n    return self.function(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\", line 211, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/inspection/_partial_dependence.py\", line 706, in partial_dependence\n    averaged_predictions, predictions = _partial_dependence_brute(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/inspection/_partial_dependence.py\", line 310, in _partial_dependence_brute\n    pred = prediction_method(X_eval)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 984, in predict\n    X = self._validate_X_predict(X)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 599, in _validate_X_predict\n    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 625, in _validate_data\n    self._check_n_features(X, reset=reset)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 414, in _check_n_features\n    raise ValueError(\nValueError: X has 20 features, but ExtraTreesRegressor is expecting 19 features as input.\n",
  "history_begin_time" : 1696778641671,
  "history_end_time" : 1696778653978,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "8fYDWTFfeDWp",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    partial_dependence_display = PartialDependenceDisplay.from_estimator(\n    current_model, preprocessed_data, features=features_to_plot, grid_resolution=50\n)\n    partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n    partial_dependence_display.figure_.subplots_adjust(top=0.9)\n    partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n    explainer = shap.Explainer(xgb_model)\n    shap_values = explainer.shap_values(X)\n\n    # Summary plot of SHAP values\n    shap.summary_plot(shap_values, X)\n    plt.title('Summary Plot of SHAP Values')\n    plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\nX has feature names, but ExtraTreesRegressor was fitted without feature names\ntoday date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib\nnew_data shape:  (462204, 21)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib\n    date   lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0  44944  49.0 -125.000    0  ...        NaN     NaN       NaN        NaN\n1  44944  49.0 -124.964    0  ...        NaN     NaN       NaN        NaN\n2  44944  49.0 -124.928    0  ...        NaN     NaN       NaN        NaN\n3  44944  49.0 -124.892    0  ...        NaN     NaN       NaN        NaN\n4  44944  49.0 -124.856    0  ...        NaN     NaN       NaN        NaN\n[5 rows x 19 columns]\nhow many rows are there? 462204\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\npredicted swe:  count    462204.000000\nmean          7.450335\nstd           0.053167\nmin           7.049500\n25%           7.436000\n50%           7.461500\n75%           7.481000\nmax           7.500000\nName: predicted_swe, dtype: float64\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/8fYDWTFfeDWp/interpret_model_results.py\", line 209, in <module>\n    interpret_prediction()\n  File \"/home/chetana/gw-workspace/8fYDWTFfeDWp/interpret_model_results.py\", line 177, in interpret_prediction\n    partial_dependence_display = PartialDependenceDisplay.from_estimator(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/inspection/_plot/partial_dependence.py\", line 703, in from_estimator\n    pd_results = Parallel(n_jobs=n_jobs, verbose=verbose)(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 65, in __call__\n    return super().__call__(iterable_with_config)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1863, in __call__\n    return output if self.return_generator else list(output)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1792, in _get_sequential_output\n    res = func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 127, in __call__\n    return self.function(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\", line 211, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/inspection/_partial_dependence.py\", line 706, in partial_dependence\n    averaged_predictions, predictions = _partial_dependence_brute(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/inspection/_partial_dependence.py\", line 310, in _partial_dependence_brute\n    pred = prediction_method(X_eval)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 984, in predict\n    X = self._validate_X_predict(X)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 599, in _validate_X_predict\n    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 604, in _validate_data\n    out = check_array(X, input_name=\"X\", **check_params)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 959, in check_array\n    _assert_all_finite(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 124, in _assert_all_finite\n    _assert_all_finite_element_wise(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 173, in _assert_all_finite_element_wise\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nExtraTreesRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
  "history_begin_time" : 1696778602441,
  "history_end_time" : 1696778614773,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "jzu0rNJ5BTNs",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence, plot_partial_dependence\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    partial_dependence_display = PartialDependenceDisplay.from_estimator(\n    current_model, preprocessed_data, features=features_to_plot, grid_resolution=50\n)\n    partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n    partial_dependence_display.figure_.subplots_adjust(top=0.9)\n    partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n    explainer = shap.Explainer(xgb_model)\n    shap_values = explainer.shap_values(X)\n\n    # Summary plot of SHAP values\n    shap.summary_plot(shap_values, X)\n    plt.title('Summary Plot of SHAP Values')\n    plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "today date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/jzu0rNJ5BTNs/interpret_model_results.py\", line 11, in <module>\n    from sklearn.inspection import partial_dependence, plot_partial_dependence\nImportError: cannot import name 'plot_partial_dependence' from 'sklearn.inspection' (/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/inspection/__init__.py)\n",
  "history_begin_time" : 1696778557651,
  "history_end_time" : 1696778559039,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "qUBDUU7jyKda",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence, plot_partial_dependence\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = [0, 1]\n    plot_partial_dependence(xgb_model, X, features=features_to_plot, grid_resolution=50)\n    plt.suptitle('Partial Dependence Plots')\n    plt.subplots_adjust(top=0.9)\n    plt.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n    explainer = shap.Explainer(xgb_model)\n    shap_values = explainer.shap_values(X)\n\n    # Summary plot of SHAP values\n    shap.summary_plot(shap_values, X)\n    plt.title('Summary Plot of SHAP Values')\n    plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "today date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/qUBDUU7jyKda/interpret_model_results.py\", line 11, in <module>\n    from sklearn.inspection import partial_dependence, plot_partial_dependence\nImportError: cannot import name 'plot_partial_dependence' from 'sklearn.inspection' (/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/inspection/__init__.py)\n",
  "history_begin_time" : 1696778298447,
  "history_end_time" : 1696778299808,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "JW0Hch1zUB5K",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = [0, 1]\n    plot_partial_dependence(xgb_model, X, features=features_to_plot, grid_resolution=50)\n    plt.suptitle('Partial Dependence Plots')\n    plt.subplots_adjust(top=0.9)\n    plt.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n    explainer = shap.Explainer(xgb_model)\n    shap_values = explainer.shap_values(X)\n\n    # Summary plot of SHAP values\n    shap.summary_plot(shap_values, X)\n    plt.title('Summary Plot of SHAP Values')\n    plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\ntoday date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib\nnew_data shape:  (462204, 21)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib\n    date   lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0  44944  49.0 -125.000    0  ...        NaN     NaN       NaN        NaN\n1  44944  49.0 -124.964    0  ...        NaN     NaN       NaN        NaN\n2  44944  49.0 -124.928    0  ...        NaN     NaN       NaN        NaN\n3  44944  49.0 -124.892    0  ...        NaN     NaN       NaN        NaN\n4  44944  49.0 -124.856    0  ...        NaN     NaN       NaN        NaN\n[5 rows x 19 columns]\nhow many rows are there? 462204\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\npredicted swe:  count    462204.000000\nmean          7.450335\nstd           0.053167\nmin           7.049500\n25%           7.436000\n50%           7.461500\n75%           7.481000\nmax           7.500000\nName: predicted_swe, dtype: float64\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/JW0Hch1zUB5K/interpret_model_results.py\", line 207, in <module>\n    interpret_prediction()\n  File \"/home/chetana/gw-workspace/JW0Hch1zUB5K/interpret_model_results.py\", line 177, in interpret_prediction\n    plot_partial_dependence(xgb_model, X, features=features_to_plot, grid_resolution=50)\nNameError: name 'plot_partial_dependence' is not defined\n",
  "history_begin_time" : 1696778203897,
  "history_end_time" : 1696778229215,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "Xl2NxXhvVLeR",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = [0, 1]\n    plot_partial_dependence(xgb_model, X, features=features_to_plot, grid_resolution=50)\n    plt.suptitle('Partial Dependence Plots')\n    plt.subplots_adjust(top=0.9)\n    plt.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n    explainer = shap.Explainer(xgb_model)\n    shap_values = explainer.shap_values(X)\n\n    # Summary plot of SHAP values\n    shap.summary_plot(shap_values, X)\n    plt.title('Summary Plot of SHAP Values')\n    plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "today date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/Xl2NxXhvVLeR/interpret_model_results.py\", line 12, in <module>\n    import shap\nModuleNotFoundError: No module named 'shap'\n",
  "history_begin_time" : 1696778040605,
  "history_end_time" : 1696778042051,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "I4Z5NrNRma7k",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = [0, 1]\n    plot_partial_dependence(xgb_model, X, features=features_to_plot, grid_resolution=50)\n    plt.suptitle('Partial Dependence Plots')\n    plt.subplots_adjust(top=0.9)\n    plt.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n    explainer = shap.Explainer(xgb_model)\n    shap_values = explainer.shap_values(X)\n\n    # Summary plot of SHAP values\n    shap.summary_plot(shap_values, X)\n    plt.title('Summary Plot of SHAP Values')\n    plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but ExtraTreesRegressor was fitted without feature names\n  warnings.warn(\ntoday date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib\nnew_data shape:  (462204, 21)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib\n    date   lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0  44944  49.0 -125.000    0  ...        NaN     NaN       NaN        NaN\n1  44944  49.0 -124.964    0  ...        NaN     NaN       NaN        NaN\n2  44944  49.0 -124.928    0  ...        NaN     NaN       NaN        NaN\n3  44944  49.0 -124.892    0  ...        NaN     NaN       NaN        NaN\n4  44944  49.0 -124.856    0  ...        NaN     NaN       NaN        NaN\n[5 rows x 19 columns]\nhow many rows are there? 462204\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\npredicted swe:  count    462204.000000\nmean          7.450335\nstd           0.053167\nmin           7.049500\n25%           7.436000\n50%           7.461500\n75%           7.481000\nmax           7.500000\nName: predicted_swe, dtype: float64\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/I4Z5NrNRma7k/interpret_model_results.py\", line 205, in <module>\n    interpret_prediction()\n  File \"/home/chetana/gw-workspace/I4Z5NrNRma7k/interpret_model_results.py\", line 164, in interpret_prediction\n    plt.figure(figsize=(10, 6))\nNameError: name 'plt' is not defined\n",
  "history_begin_time" : 1696778004068,
  "history_end_time" : 1696778014008,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "Si0ebmlpr61i",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = xgb_model.feature_importances_\n    feature_names = X.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = [0, 1]\n    plot_partial_dependence(xgb_model, X, features=features_to_plot, grid_resolution=50)\n    plt.suptitle('Partial Dependence Plots')\n    plt.subplots_adjust(top=0.9)\n    plt.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n    explainer = shap.Explainer(xgb_model)\n    shap_values = explainer.shap_values(X)\n\n    # Summary plot of SHAP values\n    shap.summary_plot(shap_values, X)\n    plt.title('Summary Plot of SHAP Values')\n    plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but ExtraTreesRegressor was fitted without feature names\n  warnings.warn(\ntoday date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib\nFile '/home/chetana/gridmet_test_run/test_data_predicted.csv' has been removed.\nnew_data shape:  (462204, 21)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib\n    date   lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0  44944  49.0 -125.000    0  ...        NaN     NaN       NaN        NaN\n1  44944  49.0 -124.964    0  ...        NaN     NaN       NaN        NaN\n2  44944  49.0 -124.928    0  ...        NaN     NaN       NaN        NaN\n3  44944  49.0 -124.892    0  ...        NaN     NaN       NaN        NaN\n4  44944  49.0 -124.856    0  ...        NaN     NaN       NaN        NaN\n[5 rows x 19 columns]\nhow many rows are there? 462204\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\npredicted swe:  count    462204.000000\nmean          7.450335\nstd           0.053167\nmin           7.049500\n25%           7.436000\n50%           7.461500\n75%           7.481000\nmax           7.500000\nName: predicted_swe, dtype: float64\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/Si0ebmlpr61i/interpret_model_results.py\", line 205, in <module>\n    interpret_prediction()\n  File \"/home/chetana/gw-workspace/Si0ebmlpr61i/interpret_model_results.py\", line 160, in interpret_prediction\n    feature_importances = xgb_model.feature_importances_\nNameError: name 'xgb_model' is not defined\n",
  "history_begin_time" : 1696777949033,
  "history_end_time" : 1696777959385,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "QnKDc41oXamN",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.ensemble.partial_dependence import plot_partial_dependence\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = xgb_model.feature_importances_\n    feature_names = X.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = [0, 1]\n    plot_partial_dependence(xgb_model, X, features=features_to_plot, grid_resolution=50)\n    plt.suptitle('Partial Dependence Plots')\n    plt.subplots_adjust(top=0.9)\n    plt.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n    explainer = shap.Explainer(xgb_model)\n    shap_values = explainer.shap_values(X)\n\n    # Summary plot of SHAP values\n    shap.summary_plot(shap_values, X)\n    plt.title('Summary Plot of SHAP Values')\n    plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "today date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/QnKDc41oXamN/interpret_model_results.py\", line 11, in <module>\n    from sklearn.ensemble.partial_dependence import plot_partial_dependence\nModuleNotFoundError: No module named 'sklearn.ensemble.partial_dependence'\n",
  "history_begin_time" : 1696777895241,
  "history_end_time" : 1696777896744,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "zdY1K7siqP2u",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import plot_partial_dependence\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = xgb_model.feature_importances_\n    feature_names = X.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = [0, 1]\n    plot_partial_dependence(xgb_model, X, features=features_to_plot, grid_resolution=50)\n    plt.suptitle('Partial Dependence Plots')\n    plt.subplots_adjust(top=0.9)\n    plt.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n    explainer = shap.Explainer(xgb_model)\n    shap_values = explainer.shap_values(X)\n\n    # Summary plot of SHAP values\n    shap.summary_plot(shap_values, X)\n    plt.title('Summary Plot of SHAP Values')\n    plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "today date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/zdY1K7siqP2u/interpret_model_results.py\", line 11, in <module>\n    from sklearn.inspection import plot_partial_dependence\nImportError: cannot import name 'plot_partial_dependence' from 'sklearn.inspection' (/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/inspection/__init__.py)\n",
  "history_begin_time" : 1696777855410,
  "history_end_time" : 1696777857429,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "xgwybkulu05",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696771780959,
  "history_end_time" : 1696771780959,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "s2se0v2deo5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696602943971,
  "history_end_time" : 1696602943971,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "g4wg0li5w9e",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696432484364,
  "history_end_time" : 1696432484364,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "kx7x5gxn5ir",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696432299785,
  "history_end_time" : 1696432482238,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},]
