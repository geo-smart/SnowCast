[{
  "history_id" : "5yvv8goc1gp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711771209890,
  "history_end_time" : 1711771209890,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rhe9wzmu47t",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711684813911,
  "history_end_time" : 1711684813911,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tkfan4ehn3j",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711598415335,
  "history_end_time" : 1711598415335,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "kz5m4lvdz1s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711592280533,
  "history_end_time" : 1711592280533,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "o3m67crzkql",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711592081944,
  "history_end_time" : 1711592081944,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "v90paocohh4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711512016641,
  "history_end_time" : 1711512016641,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ale314uafjl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711425614401,
  "history_end_time" : 1711425614401,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "itjnpbl03pw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711339212941,
  "history_end_time" : 1711339212941,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ohxizgmmr7z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711252813723,
  "history_end_time" : 1711252813723,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "40nyqo1kjbs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711166414320,
  "history_end_time" : 1711166414320,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "m6z5e56uhvf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711080009309,
  "history_end_time" : 1711080009309,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "bpnxbkhv9b2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710993609953,
  "history_end_time" : 1710993609953,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "o8cdvho9rvv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710907209389,
  "history_end_time" : 1710907209389,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8w5vk0eojh6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710820809060,
  "history_end_time" : 1710820809060,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ur38bqge0ps",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710734409783,
  "history_end_time" : 1710734409783,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "i2e8nqfb9lg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710690214207,
  "history_end_time" : 1710690214207,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "85ag4p4h0e1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710648009480,
  "history_end_time" : 1710648009480,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ztoguuib3xa",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710561608948,
  "history_end_time" : 1710561608948,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "thj03zt3obs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710475209556,
  "history_end_time" : 1710475209556,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "hvp7607d7tf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710388809528,
  "history_end_time" : 1710388809528,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "jepjzy5d7qk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710302409631,
  "history_end_time" : 1710302409631,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "00o4l10tnbk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710216009708,
  "history_end_time" : 1710216009708,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "x6vmnw4bamr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710172046540,
  "history_end_time" : 1710172046540,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "7i569pvsnlk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710129609375,
  "history_end_time" : 1710129609375,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "41phkk1drqd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710080234076,
  "history_end_time" : 1710080234076,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "vv6dpej0kji",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710043209017,
  "history_end_time" : 1710043209017,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "sdmfrlcm5ke",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709998716295,
  "history_end_time" : 1709998716295,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "kid2o8y3u94",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709956809283,
  "history_end_time" : 1709956809283,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "mqgqnp61wxg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709924987601,
  "history_end_time" : 1709924987601,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "aq4ju0kosey",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709870410294,
  "history_end_time" : 1709870410294,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "da6ie8t0niz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709845595330,
  "history_end_time" : 1709845595330,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "mj5yhv368jv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709844621977,
  "history_end_time" : 1709844621977,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tsxrckh3nc7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709842923402,
  "history_end_time" : 1709842923402,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "re10jfai5xm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709827652623,
  "history_end_time" : 1709844621186,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "3qcas26cks2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709826138714,
  "history_end_time" : 1709826138714,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "t2apvsnh8m8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709797240852,
  "history_end_time" : 1709797240852,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "82mx5mqnm1a",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709791538004,
  "history_end_time" : 1709791538004,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9o1zpmms74s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709784009770,
  "history_end_time" : 1709784009770,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ornw7g9yw84",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709778056675,
  "history_end_time" : 1709778056675,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8zwhhx6lqp1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709774843626,
  "history_end_time" : 1709774843626,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "lw6vm0n564t",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709765244685,
  "history_end_time" : 1709774842950,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "zda7tlbva85",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709763117640,
  "history_end_time" : 1709765243449,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "azu4koymbq1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709751527394,
  "history_end_time" : 1709751527394,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "bcbpt57hsoi",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709751443198,
  "history_end_time" : 1709751495339,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "cc38e74sv1s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709697609592,
  "history_end_time" : 1709697609592,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "d2qagh5r71h",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709611209009,
  "history_end_time" : 1709611209009,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "g9afcw4q2ay",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709524809195,
  "history_end_time" : 1709524809195,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6wefnp7d6ed",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709438408939,
  "history_end_time" : 1709438408939,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ah1ra6r0g1h",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709352009366,
  "history_end_time" : 1709352009366,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "cggxtna3x9x",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709265609739,
  "history_end_time" : 1709265609739,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "e6bc5g195jk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709179209699,
  "history_end_time" : 1709179209699,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "w5ve8gkej23",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709137312987,
  "history_end_time" : 1709137312987,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "tga242jnjqw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709092809289,
  "history_end_time" : 1709092809289,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "16l0i6enz7v",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709085684222,
  "history_end_time" : 1709085684222,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "1sbkiyzu11t",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709078942165,
  "history_end_time" : 1709085672813,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "8drjlv",
  "indicator" : "Stopped"
},{
  "history_id" : "btpar80k4tg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709038894872,
  "history_end_time" : 1709038894872,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "70djt166b8c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709038872996,
  "history_end_time" : 1709038879074,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "8drjlv",
  "indicator" : "Stopped"
},{
  "history_id" : "q6vgp5wiftf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709006408902,
  "history_end_time" : 1709006408902,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "onwyjmkbgax",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708971826225,
  "history_end_time" : 1708971826225,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "wlkptof94qz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708958410896,
  "history_end_time" : 1708958410896,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "1majnxl5b3g",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708954624228,
  "history_end_time" : 1708954624228,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "0i27bkhq8ce",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708920009721,
  "history_end_time" : 1708920009721,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "459vmlvedc9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708833609414,
  "history_end_time" : 1708833609414,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "mp3zvh1ncy2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708747209160,
  "history_end_time" : 1708747209160,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "er4oh90fhsz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708660809291,
  "history_end_time" : 1708660809291,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "i1mwyt5osqa",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708574409961,
  "history_end_time" : 1708574409961,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "mcha6qp2chj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708488010461,
  "history_end_time" : 1708488010461,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "61xg3evyzma",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708401609196,
  "history_end_time" : 1708401609196,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "i03g2u6c5qx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708352215561,
  "history_end_time" : 1708352215561,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "uh7geycb4xc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708348193303,
  "history_end_time" : 1708352214877,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "8drjlv",
  "indicator" : "Stopped"
},{
  "history_id" : "1gcamc3ff2o",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708315209344,
  "history_end_time" : 1708315209344,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "7f5401ea0lx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708312689975,
  "history_end_time" : 1708312689975,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "b09cvsyhlfx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708305309944,
  "history_end_time" : 1708312689139,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "8drjlv",
  "indicator" : "Stopped"
},{
  "history_id" : "tqxpxnhn5sn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708242679328,
  "history_end_time" : 1708242679328,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "sq742wsq24e",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708240733539,
  "history_end_time" : 1708240733539,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "51v84pg9zjd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708238769959,
  "history_end_time" : 1708238769959,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "6fj82tfjr5b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708237144903,
  "history_end_time" : 1708237144903,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "yd89ob4o0bt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708235187254,
  "history_end_time" : 1708235187254,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "4a0e0m3z7ph",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708233874703,
  "history_end_time" : 1708233874703,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "dd5mveucyxd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708228809470,
  "history_end_time" : 1708228809470,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rks1gh4xg6g",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708227613831,
  "history_end_time" : 1708227613831,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "0dfe5ntylpf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708142409671,
  "history_end_time" : 1708142409671,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "uegalwuyhac",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708056009479,
  "history_end_time" : 1708056009479,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ixbj0rmi0sz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707969609306,
  "history_end_time" : 1707969609306,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "cqqzrd3fvox",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707883209091,
  "history_end_time" : 1707883209091,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "r79u5mzlgez",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707796809596,
  "history_end_time" : 1707796809596,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "kwhrw5h1vjz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707750669778,
  "history_end_time" : 1707750669778,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "hfylk886kls",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707710409648,
  "history_end_time" : 1707710409648,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6crcqidhc4r",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707624009699,
  "history_end_time" : 1707624009699,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "c2n9hifcsf8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707537608871,
  "history_end_time" : 1707537608871,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "le0riz4tixy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707491746724,
  "history_end_time" : 1707491746724,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "4x9dvohf1gb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707484657957,
  "history_end_time" : 1707484657957,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "u1nlvm4ek0b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707451209929,
  "history_end_time" : 1707451209929,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "m2qejzu37nj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707434644206,
  "history_end_time" : 1707434644206,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "i23ukj9isiq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707432571824,
  "history_end_time" : 1707432571824,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "fz5zx1swbaa",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707431129261,
  "history_end_time" : 1707432053960,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "8drjlv",
  "indicator" : "Stopped"
},{
  "history_id" : "3aton3f3rvs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707418188661,
  "history_end_time" : 1707418188661,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "mahjd7",
  "indicator" : "Skipped"
},{
  "history_id" : "b432oatpn23",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707413610361,
  "history_end_time" : 1707413610361,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "mahjd7",
  "indicator" : "Skipped"
},{
  "history_id" : "zof8fjpp7y9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707364808958,
  "history_end_time" : 1707364808958,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "sfatwbx11g6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707278409184,
  "history_end_time" : 1707278409184,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "epoil516p3o",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707192718896,
  "history_end_time" : 1707192718896,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "21426lkiymk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707192009373,
  "history_end_time" : 1707448888655,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "u5ehn3lt1lb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707189399136,
  "history_end_time" : 1707189399136,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "nxs5j150d1n",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707105609401,
  "history_end_time" : 1707750639196,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "fzeoe2dd6bj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707019209417,
  "history_end_time" : 1707750639665,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "h8h4kv19e3t",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706932809297,
  "history_end_time" : 1707750640209,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "a70711h73gz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706846409768,
  "history_end_time" : 1707750640643,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "lrlgnb3kwnc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706760010042,
  "history_end_time" : 1707750643299,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ln2p3vag8ti",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706673609569,
  "history_end_time" : 1707750643804,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "as4zdecfcv1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706587209495,
  "history_end_time" : 1707750644988,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ef84q9j8rt2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706500809083,
  "history_end_time" : 1707750645665,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "i6gr4hbt1rl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706414409411,
  "history_end_time" : 1707750646109,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "rktb02vsd0e",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706366105914,
  "history_end_time" : 1706366105914,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ssgndwi7uwk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706364888468,
  "history_end_time" : 1706364888468,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "2t90fwqk5m8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706328009598,
  "history_end_time" : 1707750646908,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "tj5px4v549j",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706280497975,
  "history_end_time" : 1706280497975,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "5dl35t9m0b5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706244881244,
  "history_end_time" : 1706244881244,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xfie5y0gyg9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706241609820,
  "history_end_time" : 1706244810657,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "vxgrg48gwew",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706155209691,
  "history_end_time" : 1706244801319,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "7rxh9yq7xq5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706068809199,
  "history_end_time" : 1706244800345,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "uzy3bqu1zr0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705982409089,
  "history_end_time" : 1706244799978,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "33zwhw9diio",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705896759284,
  "history_end_time" : 1706244799014,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "s8oxtbcpuxh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705896009886,
  "history_end_time" : 1706244798534,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "icgb15v9e6d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705849064133,
  "history_end_time" : 1706244798021,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "4tzad1631dj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705809609029,
  "history_end_time" : 1705849649605,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "s3qmul7tmxo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705793527185,
  "history_end_time" : 1705849647063,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "g8nm7hornom",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705790835103,
  "history_end_time" : 1705790835103,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "fii4xw4jqgl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705770628199,
  "history_end_time" : 1705849642250,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "tpu58856qbi",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705762760974,
  "history_end_time" : 1705849640894,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ra2cpxs1e59",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705723209608,
  "history_end_time" : 1705789738453,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "6e1g3hjamph",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705636808916,
  "history_end_time" : 1705770636235,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "geqrmk6yk4t",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705550409396,
  "history_end_time" : 1705770635552,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "r1bs5by7uxu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705464008970,
  "history_end_time" : 1705770635063,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "0cmxf2m8f2k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705422422529,
  "history_end_time" : 1705422422529,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "5s4j0gu9qtx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705377609542,
  "history_end_time" : 1705770633057,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "narohufvkob",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705291209157,
  "history_end_time" : 1705770632178,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "7dqene6kaa6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705278850593,
  "history_end_time" : 1705278850593,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "civ9sygcigz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705270952798,
  "history_end_time" : 1705270952798,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "n8r2phtoewy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705204809625,
  "history_end_time" : 1705789662009,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "x7mj60yvfef",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705169057232,
  "history_end_time" : 1705169057232,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "x8uusmr6u7s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705118409751,
  "history_end_time" : 1705789660778,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "fmd1ebqktmi",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705072448173,
  "history_end_time" : 1705072448173,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "lfajnbyptmd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705032009609,
  "history_end_time" : 1705789659550,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "avawuaablax",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704979918377,
  "history_end_time" : 1704979918377,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "gmludbw0u1i",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704945609748,
  "history_end_time" : 1705789658818,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "lzkxvsre64w",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704918977797,
  "history_end_time" : 1704918977797,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "2j05d3ig1ts",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704908919831,
  "history_end_time" : 1704908919831,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "fng7y84uvaq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704859207695,
  "history_end_time" : 1705789668263,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "uuxcclyc8e4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704775840760,
  "history_end_time" : 1704775840760,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "kxyvuvkir94",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704772806910,
  "history_end_time" : 1705789667293,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "1e267uoz1mi",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704726161281,
  "history_end_time" : 1704727049032,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "hu79rtqc38t",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704686408489,
  "history_end_time" : 1705789666651,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "m63cir6o0cp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704644803734,
  "history_end_time" : 1704644803734,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "p3m6u9s8a02",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704600008311,
  "history_end_time" : 1705789665924,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "3oedi635g98",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704566156634,
  "history_end_time" : 1704566156634,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "o5tdrolpyrv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704565587391,
  "history_end_time" : 1704565587391,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "4oaqcvrgbwl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704564424167,
  "history_end_time" : 1704564424167,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "i6cpab6b5ur",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704562992188,
  "history_end_time" : 1704562992188,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "5svy1ougvw9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704561889806,
  "history_end_time" : 1704561889806,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "v6eneoib15f",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704561861180,
  "history_end_time" : 1704561887041,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "32s9ncc6hco",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704555479216,
  "history_end_time" : 1704555479216,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "pg4x8dtfva4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704555028207,
  "history_end_time" : 1704555028207,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "s8c21e0ylmo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704553241778,
  "history_end_time" : 1704553241778,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ko6fpbcurpj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704552254640,
  "history_end_time" : 1704552254640,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "z0w1y0ctyo0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704513607476,
  "history_end_time" : 1705789671133,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "gfamp9eswej",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704427207629,
  "history_end_time" : 1705789671874,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "t6gw9odhmtw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704340807662,
  "history_end_time" : 1705789673117,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "oimyzjn52ra",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704330109325,
  "history_end_time" : 1704330109325,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "4on3ioqaymx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704329364876,
  "history_end_time" : 1704329364876,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "mea8w86echt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704254407642,
  "history_end_time" : 1705789675685,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "fr5vcuv3xbh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704208947965,
  "history_end_time" : 1704208947965,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "fav3law7o2d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704207352027,
  "history_end_time" : 1704207352027,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "g0x1wiivwt8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704205859383,
  "history_end_time" : 1704205859383,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "nt38kxs45fo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704168007494,
  "history_end_time" : 1705789676830,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ympm3u7jhy0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704081607603,
  "history_end_time" : 1705789677623,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "8f7oamz7smf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703995208525,
  "history_end_time" : 1705789678818,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "xyx9oncfmrx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703962871414,
  "history_end_time" : 1703962871414,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "j63wyamwftk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703960265452,
  "history_end_time" : 1703960265452,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ye9yzx0w45r",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703959737855,
  "history_end_time" : 1703959737855,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "q3fb32u3tkr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703958611599,
  "history_end_time" : 1703958611599,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "pf06se0hriw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703955838235,
  "history_end_time" : 1703955838235,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "df43w68y5v1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703954150385,
  "history_end_time" : 1703954150385,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "b6wnviu1qp4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703915768080,
  "history_end_time" : 1703915768080,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xj4tolijpot",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703915283496,
  "history_end_time" : 1703915283496,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vyyoi7b9afe",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703914476652,
  "history_end_time" : 1703914476652,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "lqx5o3pusb3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703912302185,
  "history_end_time" : 1703912302185,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "prweinq7umj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703908807259,
  "history_end_time" : 1705789681237,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "uzhivpvdvur",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703906215389,
  "history_end_time" : 1703906215389,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "bch2ug8oo41",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703900919153,
  "history_end_time" : 1703900919153,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "t8ehjk8ahdj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703899837776,
  "history_end_time" : 1703899837776,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "zyh75erem5r",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703897422960,
  "history_end_time" : 1703897422960,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xv64cl2niy7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703896125593,
  "history_end_time" : 1703896125593,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "zvcu0qs4m85",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703890276000,
  "history_end_time" : 1703890276000,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "cul2xfo871p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703886800818,
  "history_end_time" : 1703886800818,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "fylsqau9i7e",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703885997776,
  "history_end_time" : 1703885997776,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "yn9t5m0jbze",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703880194725,
  "history_end_time" : 1703880194725,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rvk3c1kwa0y",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703872753043,
  "history_end_time" : 1703872753043,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "x2uc458pz4l",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703869828244,
  "history_end_time" : 1703869828244,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "eb55bp0zj9o",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703868616941,
  "history_end_time" : 1703868616941,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "fodecqq6121",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703867114056,
  "history_end_time" : 1703867114056,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "4s9c5g84dgu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703864885456,
  "history_end_time" : 1703864885456,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "d1hiv12bh69",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703862637378,
  "history_end_time" : 1703862637378,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xiepdf2hq4p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703827227356,
  "history_end_time" : 1703827227356,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "l6178r4r1h6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703822411587,
  "history_end_time" : 1703822411587,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ltln98vi1w2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703786924643,
  "history_end_time" : 1703789718829,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "e7j0m7rkrkq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703786053497,
  "history_end_time" : 1703786917618,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "o8trs3lr4ij",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703778395403,
  "history_end_time" : 1703778395403,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "l30c1sf6lh6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703739034557,
  "history_end_time" : 1703739034557,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tft3r53zyej",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703738754606,
  "history_end_time" : 1703792459280,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "xxxmsios4ps",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703736166917,
  "history_end_time" : 1703737316889,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "lld0o7exsmb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703694763586,
  "history_end_time" : 1703694763586,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9shkh1ufavo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703659541209,
  "history_end_time" : 1703659541209,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tjgr0ff8i7f",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703658144712,
  "history_end_time" : 1703658144712,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0tel6dx3aq7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703650855782,
  "history_end_time" : 1703650855782,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ji2k67vshl8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703646751543,
  "history_end_time" : 1703650812447,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "fdobm4ryslb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703642120903,
  "history_end_time" : 1703646749628,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "sr8g07kj1g1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703641988953,
  "history_end_time" : 1703642074636,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ianfi31ii0j",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703629665515,
  "history_end_time" : 1703629665515,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vp9hwaqzq1b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703626687994,
  "history_end_time" : 1703627783060,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "2bcpzrppyxf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703625782108,
  "history_end_time" : 1703625782108,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "jvcc5psp85c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703624784028,
  "history_end_time" : 1703624784028,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "h79rvt7si3g",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702875592851,
  "history_end_time" : 1702875592851,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tbsb4q9t2vx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702871264384,
  "history_end_time" : 1702871264384,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "hbgpgqqtxis",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702867996403,
  "history_end_time" : 1702867996403,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ovvlpf48nme",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702866593400,
  "history_end_time" : 1702866593400,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ra5v8a8fhim",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702866137642,
  "history_end_time" : 1702866137642,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "240sv9w0pc7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702657305640,
  "history_end_time" : 1702657305640,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "aj4oez8n9pm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702633223054,
  "history_end_time" : 1702633223054,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0o41qfvlhnm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702633156937,
  "history_end_time" : 1702633163904,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "pw4xt9gsu41",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702274520892,
  "history_end_time" : 1702274520892,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "7n0s0s6da16",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702257109209,
  "history_end_time" : 1702257109209,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "sif9wpb6oj5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702253506548,
  "history_end_time" : 1702253506548,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "19u62nbz40y",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702047800946,
  "history_end_time" : 1702047800946,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "v3asrvbvzuk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702046671881,
  "history_end_time" : 1702047789489,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ephalc7yz19",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701838624075,
  "history_end_time" : 1701838624075,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "jr3oft9ay0k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701272631487,
  "history_end_time" : 1701272875116,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "mhh4xbn13hr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701272152710,
  "history_end_time" : 1701272363360,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "s40nbcvh4xi",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701269761353,
  "history_end_time" : 1701269761353,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "uge925jgnag",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701245472028,
  "history_end_time" : 1701245472028,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ifxwtoog9a6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701234300626,
  "history_end_time" : 1701234300626,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "r2hhtxh8u1n",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701232375288,
  "history_end_time" : 1701234158047,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "wi1tq5q4f84",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701231048676,
  "history_end_time" : 1701231048676,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "u8mgrsm3pu7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933473,
  "history_end_time" : 1701230952353,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "hywz1fbu30p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230796348,
  "history_end_time" : 1701230932260,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "pthcf14en2k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230384896,
  "history_end_time" : 1701230384896,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "4ebtlgb69a5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701229983516,
  "history_end_time" : 1701229983516,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "yimwileedfs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701228899379,
  "history_end_time" : 1701228899379,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rja5gte75pb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701228374813,
  "history_end_time" : 1701228374813,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6yeavpm6a6s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701228236350,
  "history_end_time" : 1701228236350,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "l1kk78bbe0c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701228118517,
  "history_end_time" : 1701228118517,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3qzf060ewx7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701228056538,
  "history_end_time" : 1701228056538,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3247lrmmulj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701227912538,
  "history_end_time" : 1701227912538,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wlnxzy16weq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701013937418,
  "history_end_time" : 1701015920042,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "tw3pic39q10",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700974688735,
  "history_end_time" : 1700974688735,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "zbb4urnkdaj",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\nfeature_names = None\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness',\n                         'cumulative_AMSR_SWE': 'cumulative_SWE',\n                         'cumulative_AMSR_Flag': 'cumulative_Flag',\n                         'cumulative_tmmn':'cumulative_air_temperature_tmmn',\n                         'cumulative_etr': 'cumulative_potential_evapotranspiration',\n                         'cumulative_vpd': 'cumulative_mean_vapor_pressure_deficit',\n                         'cumulative_rmax': 'cumulative_relative_humidity_rmax', \n                         'cumulative_rmin': 'cumulative_relative_humidity_rmin',\n                         'cumulative_pr': 'cumulative_precipitation_amount',\n                         'cumulative_tmmx': 'cumulative_air_temperature_tmmx',\n                         'cumulative_vs': 'cumulative_wind_speed',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                        }, inplace=True)\n\n    desired_order = ['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness', 'cumulative_SWE',\n'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n'cumulative_potential_evapotranspiration',\n'cumulative_mean_vapor_pressure_deficit',\n'cumulative_relative_humidity_rmax',\n'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n'cumulative_air_temperature_tmmx', 'cumulative_wind_speed']\n    \n    feature_names = desired_order\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    data = data.drop(['lat', 'lon',], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \ninterpret_prediction()\n",
  "history_output" : "today date = 2023-11-25\ntest start date:  2022-11-05\ntest end date:  2023-10-07\n/home/chetana\n2022275\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nnew_data shape:  (462204, 35)\nhow many rows are left? 462204\ndata.shape:  (462204, 28)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\n   SWE  Flag  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  0.0   241  ...                              0.0                    0.0\n1  0.0   241  ...                              0.0                    0.0\n2  0.0   241  ...                              0.0                    0.0\n3  0.0   241  ...                              0.0                    0.0\n4  0.0   241  ...                              0.0                    0.0\n[5 rows x 26 columns]\nhow many rows are there? 462204\nX has feature names, but ExtraTreesRegressor was fitted without feature names\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/zbb4urnkdaj/interpret_model_results.py\", line 254, in <module>\n    interpret_prediction()\n  File \"/home/chetana/gw-workspace/zbb4urnkdaj/interpret_model_results.py\", line 209, in interpret_prediction\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n  File \"/home/chetana/gw-workspace/zbb4urnkdaj/interpret_model_results.py\", line 131, in predict_swe\n    predictions = model.predict(data)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 984, in predict\n    X = self._validate_X_predict(X)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 599, in _validate_X_predict\n    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 625, in _validate_data\n    self._check_n_features(X, reset=reset)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 414, in _check_n_features\n    raise ValueError(\nValueError: X has 26 features, but ExtraTreesRegressor is expecting 28 features as input.\n",
  "history_begin_time" : 1700885691426,
  "history_end_time" : 1700885713818,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "8xgp1ex5e8a",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1700471599653,
  "history_end_time" : 1700471601418,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "0it6hpvwd8e",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1700468946554,
  "history_end_time" : 1700468948404,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "smp5mbqg8l3",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\nfeature_names = None\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness',\n                         'cumulative_AMSR_SWE': 'cumulative_SWE',\n                         'cumulative_AMSR_Flag': 'cumulative_Flag',\n                         'cumulative_tmmn':'cumulative_air_temperature_tmmn',\n                         'cumulative_etr': 'cumulative_potential_evapotranspiration',\n                         'cumulative_vpd': 'cumulative_mean_vapor_pressure_deficit',\n                         'cumulative_rmax': 'cumulative_relative_humidity_rmax', \n                         'cumulative_rmin': 'cumulative_relative_humidity_rmin',\n                         'cumulative_pr': 'cumulative_precipitation_amount',\n                         'cumulative_tmmx': 'cumulative_air_temperature_tmmx',\n                         'cumulative_vs': 'cumulative_wind_speed',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                        }, inplace=True)\n\n    desired_order = ['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness', 'cumulative_SWE',\n'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n'cumulative_potential_evapotranspiration',\n'cumulative_mean_vapor_pressure_deficit',\n'cumulative_relative_humidity_rmax',\n'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n'cumulative_air_temperature_tmmx', 'cumulative_wind_speed']\n    \n    feature_names = desired_order\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    data = data.drop(['lat', 'lon',], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \ninterpret_prediction()\n",
  "history_output" : "today date = 2023-11-20\ntest start date:  2022-10-15\ntest end date:  2023-10-07\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nnew_data shape:  (462204, 32)\nhow many rows are left? 462204\ndata.shape:  (462204, 28)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\n   SWE  Flag  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  0.0   241  ...                              0.0                    0.0\n1  0.0   241  ...                              0.0                    0.0\n2  0.0   241  ...                              0.0                    0.0\n3  0.0   241  ...                              0.0                    0.0\n4  0.0   241  ...                              0.0                    0.0\n[5 rows x 26 columns]\nhow many rows are there? 462204\nX has feature names, but ExtraTreesRegressor was fitted without feature names\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/smp5mbqg8l3/interpret_model_results.py\", line 254, in <module>\n    interpret_prediction()\n  File \"/home/chetana/gw-workspace/smp5mbqg8l3/interpret_model_results.py\", line 209, in interpret_prediction\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n  File \"/home/chetana/gw-workspace/smp5mbqg8l3/interpret_model_results.py\", line 131, in predict_swe\n    predictions = model.predict(data)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 984, in predict\n    X = self._validate_X_predict(X)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 599, in _validate_X_predict\n    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 625, in _validate_data\n    self._check_n_features(X, reset=reset)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 414, in _check_n_features\n    raise ValueError(\nValueError: X has 26 features, but ExtraTreesRegressor is expecting 13 features as input.\n",
  "history_begin_time" : 1700448577731,
  "history_end_time" : 1700448598473,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "qanjid1msd3",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\nfeature_names = None\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness',\n                         'cumulative_AMSR_SWE': 'cumulative_SWE',\n                         'cumulative_AMSR_Flag': 'cumulative_Flag',\n                         'cumulative_tmmn':'cumulative_air_temperature_tmmn',\n                         'cumulative_etr': 'cumulative_potential_evapotranspiration',\n                         'cumulative_vpd': 'cumulative_mean_vapor_pressure_deficit',\n                         'cumulative_rmax': 'cumulative_relative_humidity_rmax', \n                         'cumulative_rmin': 'cumulative_relative_humidity_rmin',\n                         'cumulative_pr': 'cumulative_precipitation_amount',\n                         'cumulative_tmmx': 'cumulative_air_temperature_tmmx',\n                         'cumulative_vs': 'cumulative_wind_speed',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                        }, inplace=True)\n\n    desired_order = ['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness', 'cumulative_SWE',\n'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n'cumulative_potential_evapotranspiration',\n'cumulative_mean_vapor_pressure_deficit',\n'cumulative_relative_humidity_rmax',\n'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n'cumulative_air_temperature_tmmx', 'cumulative_wind_speed']\n    \n    feature_names = desired_order\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    data = data.drop(['lat', 'lon',], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \ninterpret_prediction()\n",
  "history_output" : "today date = 2023-11-20\ntest start date:  2022-10-15\ntest end date:  2023-10-07\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nnew_data shape:  (462204, 32)\nhow many rows are left? 462204\ndata.shape:  (462204, 28)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\n   SWE  Flag  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  0.0   241  ...                              0.0                    0.0\n1  0.0   241  ...                              0.0                    0.0\n2  0.0   241  ...                              0.0                    0.0\n3  0.0   241  ...                              0.0                    0.0\n4  0.0   241  ...                              0.0                    0.0\n[5 rows x 26 columns]\nhow many rows are there? 462204\nX has feature names, but ExtraTreesRegressor was fitted without feature names\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/qanjid1msd3/interpret_model_results.py\", line 254, in <module>\n    interpret_prediction()\n  File \"/home/chetana/gw-workspace/qanjid1msd3/interpret_model_results.py\", line 209, in interpret_prediction\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n  File \"/home/chetana/gw-workspace/qanjid1msd3/interpret_model_results.py\", line 131, in predict_swe\n    predictions = model.predict(data)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 984, in predict\n    X = self._validate_X_predict(X)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 599, in _validate_X_predict\n    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 625, in _validate_data\n    self._check_n_features(X, reset=reset)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 414, in _check_n_features\n    raise ValueError(\nValueError: X has 26 features, but ExtraTreesRegressor is expecting 13 features as input.\n",
  "history_begin_time" : 1700448540583,
  "history_end_time" : 1700448575678,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "i05pbfrxzrc",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1700230076209,
  "history_end_time" : 1700230077672,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "6rkj8enukin",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1700229021599,
  "history_end_time" : 1700229023056,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "irf47z0tpz8",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1700210222553,
  "history_end_time" : 1700210224013,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "bpfxu8pcuiy",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1700209791022,
  "history_end_time" : 1700209792489,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "7i6y0u760at",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1700209736571,
  "history_end_time" : 1700209738007,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "fwweujhfld3",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\nfeature_names = None\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness',\n                         'cumulative_AMSR_SWE': 'cumulative_SWE',\n                         'cumulative_AMSR_Flag': 'cumulative_Flag',\n                         'cumulative_tmmn':'cumulative_air_temperature_tmmn',\n                         'cumulative_etr': 'cumulative_potential_evapotranspiration',\n                         'cumulative_vpd': 'cumulative_mean_vapor_pressure_deficit',\n                         'cumulative_rmax': 'cumulative_relative_humidity_rmax', \n                         'cumulative_rmin': 'cumulative_relative_humidity_rmin',\n                         'cumulative_pr': 'cumulative_precipitation_amount',\n                         'cumulative_tmmx': 'cumulative_air_temperature_tmmx',\n                         'cumulative_vs': 'cumulative_wind_speed',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                        }, inplace=True)\n\n    desired_order = ['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness', 'cumulative_SWE',\n'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n'cumulative_potential_evapotranspiration',\n'cumulative_mean_vapor_pressure_deficit',\n'cumulative_relative_humidity_rmax',\n'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n'cumulative_air_temperature_tmmx', 'cumulative_wind_speed']\n    \n    feature_names = desired_order\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    data = data.drop(['lat', 'lon',], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \ninterpret_prediction()\n",
  "history_output" : "today date = 2023-11-17\ntest start date:  2022-10-04\ntest end date:  2023-10-07\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nnew_data shape:  (462204, 32)\nhow many rows are left? 462204\ndata.shape:  (462204, 28)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\n   SWE  Flag  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  0.0   241  ...                              0.0                    0.0\n1  0.0   241  ...                              0.0                    0.0\n2  0.0   241  ...                              0.0                    0.0\n3  0.0   241  ...                              0.0                    0.0\n4  0.0   241  ...                              0.0                    0.0\n[5 rows x 26 columns]\nhow many rows are there? 462204\nX has feature names, but ExtraTreesRegressor was fitted without feature names\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/fwweujhfld3/interpret_model_results.py\", line 254, in <module>\n    interpret_prediction()\n  File \"/home/chetana/gw-workspace/fwweujhfld3/interpret_model_results.py\", line 209, in interpret_prediction\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n  File \"/home/chetana/gw-workspace/fwweujhfld3/interpret_model_results.py\", line 131, in predict_swe\n    predictions = model.predict(data)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 984, in predict\n    X = self._validate_X_predict(X)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 599, in _validate_X_predict\n    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 625, in _validate_data\n    self._check_n_features(X, reset=reset)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 414, in _check_n_features\n    raise ValueError(\nValueError: X has 26 features, but ExtraTreesRegressor is expecting 9 features as input.\n",
  "history_begin_time" : 1700201985831,
  "history_end_time" : 1700202006040,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "91rj5a0cizp",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\nfeature_names = None\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness',\n                         'cumulative_AMSR_SWE': 'cumulative_SWE',\n                         'cumulative_AMSR_Flag': 'cumulative_Flag',\n                         'cumulative_tmmn':'cumulative_air_temperature_tmmn',\n                         'cumulative_etr': 'cumulative_potential_evapotranspiration',\n                         'cumulative_vpd': 'cumulative_mean_vapor_pressure_deficit',\n                         'cumulative_rmax': 'cumulative_relative_humidity_rmax', \n                         'cumulative_rmin': 'cumulative_relative_humidity_rmin',\n                         'cumulative_pr': 'cumulative_precipitation_amount',\n                         'cumulative_tmmx': 'cumulative_air_temperature_tmmx',\n                         'cumulative_vs': 'cumulative_wind_speed',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                        }, inplace=True)\n\n    desired_order = ['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness', 'cumulative_SWE',\n'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n'cumulative_potential_evapotranspiration',\n'cumulative_mean_vapor_pressure_deficit',\n'cumulative_relative_humidity_rmax',\n'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n'cumulative_air_temperature_tmmx', 'cumulative_wind_speed']\n    \n    feature_names = desired_order\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    data = data.drop(['lat', 'lon',], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \ninterpret_prediction()\n",
  "history_output" : "today date = 2023-11-17\ntest start date:  2023-10-04\ntest end date:  2023-10-07\n/home/chetana\n",
  "history_begin_time" : 1700201114903,
  "history_end_time" : 1700201127214,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "8by60zetz1i",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1700145679235,
  "history_end_time" : 1700145680685,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "g6rl06c7nqq",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1700143305468,
  "history_end_time" : 1700143306915,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "jgll63uizuj",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1700141626804,
  "history_end_time" : 1700141628358,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "48ltctggmfw",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1700134140840,
  "history_end_time" : 1700134142312,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "4mchv7zcy0s",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1700133795308,
  "history_end_time" : 1700133796784,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "u1wsdbspvcy",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\nfeature_names = None\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness',\n                         'cumulative_AMSR_SWE': 'cumulative_SWE',\n                         'cumulative_AMSR_Flag': 'cumulative_Flag',\n                         'cumulative_tmmn':'cumulative_air_temperature_tmmn',\n                         'cumulative_etr': 'cumulative_potential_evapotranspiration',\n                         'cumulative_vpd': 'cumulative_mean_vapor_pressure_deficit',\n                         'cumulative_rmax': 'cumulative_relative_humidity_rmax', \n                         'cumulative_rmin': 'cumulative_relative_humidity_rmin',\n                         'cumulative_pr': 'cumulative_precipitation_amount',\n                         'cumulative_tmmx': 'cumulative_air_temperature_tmmx',\n                         'cumulative_vs': 'cumulative_wind_speed',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                        }, inplace=True)\n\n    desired_order = ['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness', 'cumulative_SWE',\n'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n'cumulative_potential_evapotranspiration',\n'cumulative_mean_vapor_pressure_deficit',\n'cumulative_relative_humidity_rmax',\n'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n'cumulative_air_temperature_tmmx', 'cumulative_wind_speed']\n    \n    feature_names = desired_order\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    data = data.drop(['lat', 'lon',], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \ninterpret_prediction()\n",
  "history_output" : "today date = 2023-11-14\ntest start date:  2023-03-10\ntest end date:  2023-10-07\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nnew_data shape:  (462204, 16)\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/u1wsdbspvcy/interpret_model_results.py\", line 254, in <module>\n    interpret_prediction()\n  File \"/home/chetana/gw-workspace/u1wsdbspvcy/interpret_model_results.py\", line 206, in interpret_prediction\n    preprocessed_data = preprocess_data(new_data)\n  File \"/home/chetana/gw-workspace/u1wsdbspvcy/interpret_model_results.py\", line 103, in preprocess_data\n    data = data[desired_order]\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\", line 3813, in __getitem__\n    indexer = self.columns._get_indexer_strict(key, \"columns\")[1]\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 6070, in _get_indexer_strict\n    self._raise_if_missing(keyarr, indexer, axis_name)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 6133, in _raise_if_missing\n    raise KeyError(f\"{not_found} not in index\")\nKeyError: \"['air_temperature_tmmn', 'potential_evapotranspiration', 'mean_vapor_pressure_deficit', 'relative_humidity_rmax', 'relative_humidity_rmin', 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed', 'cumulative_air_temperature_tmmn', 'cumulative_potential_evapotranspiration', 'cumulative_mean_vapor_pressure_deficit', 'cumulative_relative_humidity_rmax', 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount', 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'] not in index\"\n",
  "history_begin_time" : 1699999634227,
  "history_end_time" : 1699999648139,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "7nj5b39bu94",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\nfeature_names = None\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness',\n                         'cumulative_AMSR_SWE': 'cumulative_SWE',\n                         'cumulative_AMSR_Flag': 'cumulative_Flag',\n                         'cumulative_tmmn':'cumulative_air_temperature_tmmn',\n                         'cumulative_etr': 'cumulative_potential_evapotranspiration',\n                         'cumulative_vpd': 'cumulative_mean_vapor_pressure_deficit',\n                         'cumulative_rmax': 'cumulative_relative_humidity_rmax', \n                         'cumulative_rmin': 'cumulative_relative_humidity_rmin',\n                         'cumulative_pr': 'cumulative_precipitation_amount',\n                         'cumulative_tmmx': 'cumulative_air_temperature_tmmx',\n                         'cumulative_vs': 'cumulative_wind_speed',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                        }, inplace=True)\n\n    desired_order = ['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness', 'cumulative_SWE',\n'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n'cumulative_potential_evapotranspiration',\n'cumulative_mean_vapor_pressure_deficit',\n'cumulative_relative_humidity_rmax',\n'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n'cumulative_air_temperature_tmmx', 'cumulative_wind_speed']\n    \n    feature_names = desired_order\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    data = data.drop(['lat', 'lon',], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \ninterpret_prediction()\n",
  "history_output" : "today date = 2023-11-14\ntest start date:  2023-01-10\ntest end date:  2023-10-07\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\n",
  "history_begin_time" : 1699985471974,
  "history_end_time" : 1699985490263,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "pb2rdkb68v0",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\nfeature_names = None\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness',\n                         'cumulative_AMSR_SWE': 'cumulative_SWE',\n                         'cumulative_AMSR_Flag': 'cumulative_Flag',\n                         'cumulative_tmmn':'cumulative_air_temperature_tmmn',\n                         'cumulative_etr': 'cumulative_potential_evapotranspiration',\n                         'cumulative_vpd': 'cumulative_mean_vapor_pressure_deficit',\n                         'cumulative_rmax': 'cumulative_relative_humidity_rmax', \n                         'cumulative_rmin': 'cumulative_relative_humidity_rmin',\n                         'cumulative_pr': 'cumulative_precipitation_amount',\n                         'cumulative_tmmx': 'cumulative_air_temperature_tmmx',\n                         'cumulative_vs': 'cumulative_wind_speed',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                        }, inplace=True)\n\n    desired_order = ['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness', 'cumulative_SWE',\n'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n'cumulative_potential_evapotranspiration',\n'cumulative_mean_vapor_pressure_deficit',\n'cumulative_relative_humidity_rmax',\n'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n'cumulative_air_temperature_tmmx', 'cumulative_wind_speed']\n    \n    feature_names = desired_order\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    data = data.drop(['lat', 'lon',], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \ninterpret_prediction()\n",
  "history_output" : "today date = 2023-11-14\ntest start date:  2022-12-25\ntest end date:  2023-10-07\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nnew_data shape:  (462204, 32)\nhow many rows are left? 462204\ndata.shape:  (462204, 28)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\n   SWE  Flag  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  0.0   241  ...                              0.0                    0.0\n1  0.0   241  ...                              0.0                    0.0\n2  0.0   241  ...                              0.0                    0.0\n3  0.0   241  ...                              0.0                    0.0\n4  0.0   241  ...                              0.0                    0.0\n[5 rows x 26 columns]\nhow many rows are there? 462204\nX has feature names, but ExtraTreesRegressor was fitted without feature names\npredicted swe:  count    462204.000000\nmean          1.296673\nstd           0.847741\nmin           0.000000\n25%           0.866000\n50%           0.971000\n75%           1.666000\nmax           5.505000\nName: predicted_swe, dtype: float64\n",
  "history_begin_time" : 1699943616613,
  "history_end_time" : 1699943634303,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "9nhvsfm3msa",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\nfeature_names = None\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness',\n                         'cumulative_AMSR_SWE': 'cumulative_SWE',\n                         'cumulative_AMSR_Flag': 'cumulative_Flag',\n                         'cumulative_tmmn':'cumulative_air_temperature_tmmn',\n                         'cumulative_etr': 'cumulative_potential_evapotranspiration',\n                         'cumulative_vpd': 'cumulative_mean_vapor_pressure_deficit',\n                         'cumulative_rmax': 'cumulative_relative_humidity_rmax', \n                         'cumulative_rmin': 'cumulative_relative_humidity_rmin',\n                         'cumulative_pr': 'cumulative_precipitation_amount',\n                         'cumulative_tmmx': 'cumulative_air_temperature_tmmx',\n                         'cumulative_vs': 'cumulative_wind_speed',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                        }, inplace=True)\n\n    desired_order = ['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness', 'cumulative_SWE',\n'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n'cumulative_potential_evapotranspiration',\n'cumulative_mean_vapor_pressure_deficit',\n'cumulative_relative_humidity_rmax',\n'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n'cumulative_air_temperature_tmmx', 'cumulative_wind_speed']\n    \n    feature_names = desired_order\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    data = data.drop(['lat', 'lon',], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \ninterpret_prediction()\n",
  "history_output" : "today date = 2023-11-14\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nnew_data shape:  (462204, 32)\nhow many rows are left? 462204\ndata.shape:  (462204, 28)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\n   SWE  Flag  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  0.0   241  ...                              0.0                    0.0\n1  0.0   241  ...                              0.0                    0.0\n2  0.0   241  ...                              0.0                    0.0\n3  0.0   241  ...                              0.0                    0.0\n4  0.0   241  ...                              0.0                    0.0\n[5 rows x 26 columns]\nhow many rows are there? 462204\nX has feature names, but ExtraTreesRegressor was fitted without feature names\npredicted swe:  count    462204.000000\nmean          0.196272\nstd           0.100128\nmin           0.000000\n25%           0.135000\n50%           0.207000\n75%           0.210000\nmax           1.999000\nName: predicted_swe, dtype: float64\n",
  "history_begin_time" : 1699939894895,
  "history_end_time" : 1699939911617,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "avn1rjxzoit",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\nfeature_names = None\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness',\n                         'cumulative_AMSR_SWE': 'cumulative_SWE',\n                         'cumulative_AMSR_Flag': 'cumulative_Flag',\n                         'cumulative_tmmn':'cumulative_air_temperature_tmmn',\n                         'cumulative_etr': 'cumulative_potential_evapotranspiration',\n                         'cumulative_vpd': 'cumulative_mean_vapor_pressure_deficit',\n                         'cumulative_rmax': 'cumulative_relative_humidity_rmax', \n                         'cumulative_rmin': 'cumulative_relative_humidity_rmin',\n                         'cumulative_pr': 'cumulative_precipitation_amount',\n                         'cumulative_tmmx': 'cumulative_air_temperature_tmmx',\n                         'cumulative_vs': 'cumulative_wind_speed',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                        }, inplace=True)\n\n    desired_order = ['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness', 'cumulative_SWE',\n'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n'cumulative_potential_evapotranspiration',\n'cumulative_mean_vapor_pressure_deficit',\n'cumulative_relative_humidity_rmax',\n'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n'cumulative_air_temperature_tmmx', 'cumulative_wind_speed']\n    \n    feature_names = desired_order\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    data = data.drop(['lat', 'lon',], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \ninterpret_prediction()\n",
  "history_output" : "today date = 2023-11-14\ntest start date:  2022-12-17\ntest end date:  2023-10-07\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nnew_data shape:  (462204, 32)\nhow many rows are left? 462204\ndata.shape:  (462204, 28)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\n   SWE  Flag  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  0.0   241  ...                              0.0                    0.0\n1  0.0   241  ...                              0.0                    0.0\n2  0.0   241  ...                              0.0                    0.0\n3  0.0   241  ...                              0.0                    0.0\n4  0.0   241  ...                              0.0                    0.0\n[5 rows x 26 columns]\nhow many rows are there? 462204\nX has feature names, but ExtraTreesRegressor was fitted without feature names\npredicted swe:  count    462204.000000\nmean          1.225549\nstd           0.641715\nmin           0.005000\n25%           0.649000\n50%           1.107000\n75%           1.704000\nmax           4.182000\nName: predicted_swe, dtype: float64\n",
  "history_begin_time" : 1699938886154,
  "history_end_time" : 1699938921201,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "gsJo5inE7TOd",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\nfeature_names = None\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness',\n                         'cumulative_AMSR_SWE': 'cumulative_SWE',\n                         'cumulative_AMSR_Flag': 'cumulative_Flag',\n                         'cumulative_tmmn':'cumulative_air_temperature_tmmn',\n                         'cumulative_etr': 'cumulative_potential_evapotranspiration',\n                         'cumulative_vpd': 'cumulative_mean_vapor_pressure_deficit',\n                         'cumulative_rmax': 'cumulative_relative_humidity_rmax', \n                         'cumulative_rmin': 'cumulative_relative_humidity_rmin',\n                         'cumulative_pr': 'cumulative_precipitation_amount',\n                         'cumulative_tmmx': 'cumulative_air_temperature_tmmx',\n                         'cumulative_vs': 'cumulative_wind_speed',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                        }, inplace=True)\n\n    desired_order = ['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness', 'cumulative_SWE',\n'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n'cumulative_potential_evapotranspiration',\n'cumulative_mean_vapor_pressure_deficit',\n'cumulative_relative_humidity_rmax',\n'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n'cumulative_air_temperature_tmmx', 'cumulative_wind_speed']\n    \n    feature_names = desired_order\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    data = data.drop(['lat', 'lon',], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \ninterpret_prediction()\n",
  "history_output" : "today date = 2023-11-13\ntest start date:  2022-12-15\ntest end date:  2023-10-07\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nnew_data shape:  (462204, 32)\nhow many rows are left? 462204\ndata.shape:  (462204, 28)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\n   SWE  Flag  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0    0   241  ...                              0.0                    0.0\n1    0   241  ...                              0.0                    0.0\n2    0   241  ...                              0.0                    0.0\n3    0   241  ...                              0.0                    0.0\n4    0   241  ...                              0.0                    0.0\n[5 rows x 26 columns]\nhow many rows are there? 462204\nX has feature names, but ExtraTreesRegressor was fitted without feature names\npredicted swe:  count    462204.000000\nmean          1.594181\nstd           0.461051\nmin           0.002000\n25%           1.296000\n50%           1.737000\n75%           1.902000\nmax           4.080000\nName: predicted_swe, dtype: float64\n",
  "history_begin_time" : 1699836725356,
  "history_end_time" : 1699836741722,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "qKAnwe4n93th",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\nfeature_names = None\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness',\n                         'cumulative_AMSR_SWE': 'cumulative_SWE',\n                         'cumulative_AMSR_Flag': 'cumulative_Flag',\n                         'cumulative_tmmn':'cumulative_air_temperature_tmmn',\n                         'cumulative_etr': 'cumulative_potential_evapotranspiration',\n                         'cumulative_vpd': 'cumulative_mean_vapor_pressure_deficit',\n                         'cumulative_rmax': 'cumulative_relative_humidity_rmax', \n                         'cumulative_rmin': 'cumulative_relative_humidity_rmin',\n                         'cumulative_pr': 'cumulative_precipitation_amount',\n                         'cumulative_tmmx': 'cumulative_air_temperature_tmmx',\n                         'cumulative_vs': 'cumulative_wind_speed',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                        }, inplace=True)\n\n    desired_order = ['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness', 'cumulative_SWE',\n'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n'cumulative_potential_evapotranspiration',\n'cumulative_mean_vapor_pressure_deficit',\n'cumulative_relative_humidity_rmax',\n'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n'cumulative_air_temperature_tmmx', 'cumulative_wind_speed']\n    \n    feature_names = desired_order\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    data = data.drop(['lat', 'lon',])\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \ninterpret_prediction()\n",
  "history_output" : "today date = 2023-11-13\ntest start date:  2022-12-15\ntest end date:  2023-10-07\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nnew_data shape:  (462204, 32)\nhow many rows are left? 462204\ndata.shape:  (462204, 28)\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/qKAnwe4n93th/interpret_model_results.py\", line 254, in <module>\n    interpret_prediction()\n  File \"/home/chetana/gw-workspace/qKAnwe4n93th/interpret_model_results.py\", line 206, in interpret_prediction\n    preprocessed_data = preprocess_data(new_data)\n  File \"/home/chetana/gw-workspace/qKAnwe4n93th/interpret_model_results.py\", line 113, in preprocess_data\n    data = data.drop(['lat', 'lon',])\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\", line 5399, in drop\n    return super().drop(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\", line 4505, in drop\n    obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\", line 4546, in _drop_axis\n    new_axis = axis.drop(labels, errors=errors)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 6934, in drop\n    raise KeyError(f\"{list(labels[mask])} not found in axis\")\nKeyError: \"['lat', 'lon'] not found in axis\"\n",
  "history_begin_time" : 1699836668934,
  "history_end_time" : 1699836684963,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "vlHNoxS1vRQb",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\nfeature_names = None\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness',\n                         'cumulative_AMSR_SWE': 'cumulative_SWE',\n                         'cumulative_AMSR_Flag': 'cumulative_Flag',\n                         'cumulative_tmmn':'cumulative_air_temperature_tmmn',\n                         'cumulative_etr': 'cumulative_potential_evapotranspiration',\n                         'cumulative_vpd': 'cumulative_mean_vapor_pressure_deficit',\n                         'cumulative_rmax': 'cumulative_relative_humidity_rmax', \n                         'cumulative_rmin': 'cumulative_relative_humidity_rmin',\n                         'cumulative_pr': 'cumulative_precipitation_amount',\n                         'cumulative_tmmx': 'cumulative_air_temperature_tmmx',\n                         'cumulative_vs': 'cumulative_wind_speed',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                        }, inplace=True)\n\n    desired_order = ['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness', 'cumulative_SWE',\n'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n'cumulative_potential_evapotranspiration',\n'cumulative_mean_vapor_pressure_deficit',\n'cumulative_relative_humidity_rmax',\n'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n'cumulative_air_temperature_tmmx', 'cumulative_wind_speed']\n    \n    feature_names = desired_order\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \ninterpret_prediction()\n",
  "history_output" : "today date = 2023-11-13\ntest start date:  2022-12-15\ntest end date:  2023-10-07\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nnew_data shape:  (462204, 32)\nhow many rows are left? 462204\ndata.shape:  (462204, 28)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\n    lat      lon  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  49.0 -125.000  ...                              0.0                    0.0\n1  49.0 -124.964  ...                              0.0                    0.0\n2  49.0 -124.928  ...                              0.0                    0.0\n3  49.0 -124.892  ...                              0.0                    0.0\n4  49.0 -124.856  ...                              0.0                    0.0\n[5 rows x 28 columns]\nhow many rows are there? 462204\nX has feature names, but ExtraTreesRegressor was fitted without feature names\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/vlHNoxS1vRQb/interpret_model_results.py\", line 253, in <module>\n    interpret_prediction()\n  File \"/home/chetana/gw-workspace/vlHNoxS1vRQb/interpret_model_results.py\", line 208, in interpret_prediction\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n  File \"/home/chetana/gw-workspace/vlHNoxS1vRQb/interpret_model_results.py\", line 130, in predict_swe\n    predictions = model.predict(data)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 984, in predict\n    X = self._validate_X_predict(X)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 599, in _validate_X_predict\n    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 625, in _validate_data\n    self._check_n_features(X, reset=reset)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 414, in _check_n_features\n    raise ValueError(\nValueError: X has 28 features, but ExtraTreesRegressor is expecting 26 features as input.\n",
  "history_begin_time" : 1699836610290,
  "history_end_time" : 1699836626432,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "F0BRHFIXOBig",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\nfeature_names = None\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness',\n                         'cumulative_AMSR_SWE': 'cumulative_SWE',\n                         'cumulative_AMSR_Flag': 'cumulative_Flag',\n                         'cumulative_tmmn':'cumulative_air_temperature_tmmn',\n                         'cumulative_etr': 'cumulative_potential_evapotranspiration',\n                         'cumulative_vpd': 'cumulative_mean_vapor_pressure_deficit',\n                         'cumulative_rmax': 'cumulative_relative_humidity_rmax', \n                         'cumulative_rmin': 'cumulative_relative_humidity_rmin',\n                         'cumulative_pr': 'cumulative_precipitation_amount',\n                         'cumulative_tmmx': 'cumulative_air_temperature_tmmx',\n                         'cumulative_vs': 'cumulative_wind_speed',\n                        }, inplace=True)\n\n    desired_order = ['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness', 'cumulative_SWE',\n'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n'cumulative_potential_evapotranspiration',\n'cumulative_mean_vapor_pressure_deficit',\n'cumulative_relative_humidity_rmax',\n'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n'cumulative_air_temperature_tmmx', 'cumulative_wind_speed']\n    \n    feature_names = desired_order\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \ninterpret_prediction()\n",
  "history_output" : "today date = 2023-11-13\ntest start date:  2022-12-15\ntest end date:  2023-10-07\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nnew_data shape:  (462204, 32)\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/F0BRHFIXOBig/interpret_model_results.py\", line 251, in <module>\n    interpret_prediction()\n  File \"/home/chetana/gw-workspace/F0BRHFIXOBig/interpret_model_results.py\", line 203, in interpret_prediction\n    preprocessed_data = preprocess_data(new_data)\n  File \"/home/chetana/gw-workspace/F0BRHFIXOBig/interpret_model_results.py\", line 101, in preprocess_data\n    data = data[desired_order]\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\", line 3813, in __getitem__\n    indexer = self.columns._get_indexer_strict(key, \"columns\")[1]\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 6070, in _get_indexer_strict\n    self._raise_if_missing(keyarr, indexer, axis_name)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 6133, in _raise_if_missing\n    raise KeyError(f\"{not_found} not in index\")\nKeyError: \"['SWE', 'Flag'] not in index\"\n",
  "history_begin_time" : 1699836579911,
  "history_end_time" : 1699836588558,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "jCI1lzcrJB17",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\nfeature_names = None\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness',\n                         'cumulative_AMSR_SWE': 'cumulative_SWE',\n                         'cumulative_AMSR_Flag': 'cumulative_Flag',\n                         'cumulative_tmmn':'cumulative_air_temperature_tmmn',\n                         'cumulative_etr': 'cumulative_potential_evapotranspiration',\n                         'cumulative_vpd': 'cumulative_mean_vapor_pressure_deficit',\n                         'cumulative_rmax': 'cumulative_relative_humidity_rmax', \n                         'cumulative_rmin': 'cumulative_relative_humidity_rmin',\n                         'cumulative_pr': 'cumulative_precipitation_amount',\n                         'cumulative_tmmx': 'cumulative_air_temperature_tmmx',\n                         'cumulative_vs': 'cumulative_wind_speed',\n                        }, inplace=True)\n\n    desired_order = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\",]\n    \n    feature_names = desired_order\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \ninterpret_prediction()\n",
  "history_output" : "today date = 2023-11-13\ntest start date:  2022-12-15\ntest end date:  2023-10-07\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nnew_data shape:  (462204, 32)\nhow many rows are left? 462204\ndata.shape:  (462204, 18)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\n    lat      lon  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  49.0 -125.000  ...                              0.0                    0.0\n1  49.0 -124.964  ...                              0.0                    0.0\n2  49.0 -124.928  ...                              0.0                    0.0\n3  49.0 -124.892  ...                              0.0                    0.0\n4  49.0 -124.856  ...                              0.0                    0.0\n[5 rows x 18 columns]\nhow many rows are there? 462204\nX has feature names, but ExtraTreesRegressor was fitted without feature names\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/jCI1lzcrJB17/interpret_model_results.py\", line 241, in <module>\n    interpret_prediction()\n  File \"/home/chetana/gw-workspace/jCI1lzcrJB17/interpret_model_results.py\", line 196, in interpret_prediction\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n  File \"/home/chetana/gw-workspace/jCI1lzcrJB17/interpret_model_results.py\", line 118, in predict_swe\n    predictions = model.predict(data)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 984, in predict\n    X = self._validate_X_predict(X)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 599, in _validate_X_predict\n    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 625, in _validate_data\n    self._check_n_features(X, reset=reset)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 414, in _check_n_features\n    raise ValueError(\nValueError: X has 18 features, but ExtraTreesRegressor is expecting 26 features as input.\n",
  "history_begin_time" : 1699836522736,
  "history_end_time" : 1699836539317,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "YpogNWOFS4Ua",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\nfeature_names = None\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness',\n                         'cumulative_AMSR_SWE': 'cumulative_SWE',\n                         'cumulative_AMSR_Flag': 'cumulative_Flag',\n                         'cumulative_tmmn':'cumulative_air_temperature_tmmn',\n                         'cumulative_etr': 'cumulative_potential_evapotranspiration',\n                         'cumulative_vpd': 'cumulative_mean_vapor_pressure_deficit',\n                         'cumulative_rmax': 'cumulative_relative_humidity_rmax', \n                         'cumulative_rmin': 'cumulative_relative_humidity_rmin',\n                         'cumulative_pr': 'cumulative_precipitation_amount',\n                         'cumulative_tmmx': 'cumulative_air_temperature_tmmx',\n                         'cumulative_vs': 'cumulative_wind_speed',\n                        }, inplace=True)\n\n    desired_order = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\",]\n    \n    feature_names = desired_order\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \ninterpret_prediction()\n",
  "history_output" : "today date = 2023-11-12\ntest start date:  2022-10-16\ntest end date:  2023-10-07\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nnew_data shape:  (462204, 32)\nhow many rows are left? 462204\ndata.shape:  (462204, 18)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\n    lat      lon  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  49.0 -125.000  ...                              0.0                    0.0\n1  49.0 -124.964  ...                              0.0                    0.0\n2  49.0 -124.928  ...                              0.0                    0.0\n3  49.0 -124.892  ...                              0.0                    0.0\n4  49.0 -124.856  ...                              0.0                    0.0\n[5 rows x 18 columns]\nhow many rows are there? 462204\nX has feature names, but ExtraTreesRegressor was fitted without feature names\npredicted swe:  count    462204.000000\nmean          0.023167\nstd           0.020628\nmin           0.000000\n25%           0.009000\n50%           0.017000\n75%           0.030000\nmax           0.139000\nName: predicted_swe, dtype: float64\n",
  "history_begin_time" : 1699804658678,
  "history_end_time" : 1699804672030,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "eHdQb7UuWxnK",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\nfeature_names = None\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness',\n                         'cumulative_AMSR_SWE': 'cumulative_SWE',\n                         'cumulative_AMSR_Flag': 'cumulative_Flag',\n                         'cumulative_tmmn':'cumulative_air_temperature_tmmn',\n                         'cumulative_etr': 'cumulative_potential_evapotranspiration',\n                         'cumulative_vpd': 'cumulative_mean_vapor_pressure_deficit',\n                         'cumulative_rmax': 'cumulative_relative_humidity_rmax', \n                         'cumulative_rmin': 'cumulative_relative_humidity_rmin',\n                         'cumulative_pr': 'cumulative_precipitation_amount',\n                         'cumulative_tmmx': 'cumulative_air_temperature_tmmx',\n                         'cumulative_vs': 'cumulative_wind_speed',\n                        }, inplace=True)\n\n    desired_order = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"water_year\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\",]\n    \n    feature_names = desired_order\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \ninterpret_prediction()\n",
  "history_output" : "today date = 2023-11-12\ntest start date:  2022-10-16\ntest end date:  2023-10-07\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nnew_data shape:  (462204, 32)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\n    lat      lon  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  49.0 -125.000  ...                              0.0                    0.0\n1  49.0 -124.964  ...                              0.0                    0.0\n2  49.0 -124.928  ...                              0.0                    0.0\n3  49.0 -124.892  ...                              0.0                    0.0\n4  49.0 -124.856  ...                              0.0                    0.0\n[5 rows x 19 columns]\nhow many rows are there? 462204\nX has feature names, but ExtraTreesRegressor was fitted without feature names\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/eHdQb7UuWxnK/interpret_model_results.py\", line 241, in <module>\n    interpret_prediction()\n  File \"/home/chetana/gw-workspace/eHdQb7UuWxnK/interpret_model_results.py\", line 196, in interpret_prediction\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n  File \"/home/chetana/gw-workspace/eHdQb7UuWxnK/interpret_model_results.py\", line 118, in predict_swe\n    predictions = model.predict(data)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 984, in predict\n    X = self._validate_X_predict(X)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 599, in _validate_X_predict\n    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 625, in _validate_data\n    self._check_n_features(X, reset=reset)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 414, in _check_n_features\n    raise ValueError(\nValueError: X has 19 features, but ExtraTreesRegressor is expecting 18 features as input.\n",
  "history_begin_time" : 1699804632995,
  "history_end_time" : 1699804645565,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "U6zO8yjMYDQJ",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\nfeature_names = None\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness',\n                         'cumulative_AMSR_SWE': 'cumulative_SWE',\n                         'cumulative_AMSR_Flag': 'cumulative_Flag',\n                         'cumulative_tmmn':'cumulative_air_temperature_tmmn',\n                         'cumulative_etr': 'cumulative_potential_evapotranspiration',\n                         'cumulative_vpd': 'cumulative_mean_vapor_pressure_deficit',\n                         'cumulative_rmax': 'cumulative_relative_humidity_rmax', \n                         'cumulative_rmin': 'cumulative_relative_humidity_rmin',\n                         'cumulative_pr': 'cumulative_precipitation_amount',\n                         'cumulative_tmmx': 'cumulative_air_temperature_tmmx',\n                         'cumulative_vs': 'cumulative_wind_speed',\n                        }, inplace=True)\n\n    desired_order = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"water_year\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\",]\n    \n    feature_names = desired_order\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \ninterpret_prediction()\n",
  "history_output" : "today date = 2023-11-12\ntest start date:  2022-10-16\ntest end date:  2023-10-07\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nnew_data shape:  (462204, 32)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\n    lat      lon  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  49.0 -125.000  ...                              0.0                    0.0\n1  49.0 -124.964  ...                              0.0                    0.0\n2  49.0 -124.928  ...                              0.0                    0.0\n3  49.0 -124.892  ...                              0.0                    0.0\n4  49.0 -124.856  ...                              0.0                    0.0\n[5 rows x 19 columns]\nhow many rows are there? 462204\nX has feature names, but ExtraTreesRegressor was fitted without feature names\npredicted swe:  count    462204.000000\nmean          0.037221\nstd           0.020437\nmin           0.000000\n25%           0.024000\n50%           0.033000\n75%           0.043000\nmax           0.149000\nName: predicted_swe, dtype: float64\n",
  "history_begin_time" : 1699802637914,
  "history_end_time" : 1699802657770,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "IIQqodIlh0WL",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\nfeature_names = None\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness',\n                         'cumulative_AMSR_SWE': 'cumulative_SWE',\n                         'cumulative_AMSR_Flag': 'cumulative_Flag',\n                         'cumulative_tmmn':'cumulative_air_temperature_tmmn',\n                         'cumulative_etr': 'cumulative_potential_evapotranspiration',\n                         'cumulative_vpd': 'cumulative_mean_vapor_pressure_deficit',\n                         'cumulative_rmax': 'cumulative_relative_humidity_rmax', \n                         'cumulative_rmin': 'cumulative_relative_humidity_rmin',\n                         'cumulative_pr': 'cumulative_precipitation_amount',\n                         'cumulative_tmmx': 'cumulative_air_temperature_tmmx',\n                         'cumulative_vs': 'cumulative_wind_speed',\n                        }, inplace=True)\n\n    desired_order = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"water_year\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\",]\n    \n    feature_names = desired_order\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \ninterpret_prediction()\n",
  "history_output" : "today date = 2023-11-12\ntest start date:  2022-10-16\ntest end date:  2023-10-07\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nnew_data shape:  (462204, 32)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\n    lat      lon  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  49.0 -125.000  ...                              0.0                    0.0\n1  49.0 -124.964  ...                              0.0                    0.0\n2  49.0 -124.928  ...                              0.0                    0.0\n3  49.0 -124.892  ...                              0.0                    0.0\n4  49.0 -124.856  ...                              0.0                    0.0\n[5 rows x 19 columns]\nhow many rows are there? 462204\nX has feature names, but ExtraTreesRegressor was fitted without feature names\npredicted swe:  count    462204.000000\nmean          0.047370\nstd           0.025404\nmin           0.000000\n25%           0.025000\n50%           0.047000\n75%           0.070000\nmax           0.172000\nName: predicted_swe, dtype: float64\n",
  "history_begin_time" : 1699800390346,
  "history_end_time" : 1699800419919,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "4U7q74rRdIjD",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\nfeature_names = None\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness',\n                         'cumulative_AMSR_SWE': 'cumulative_SWE',\n                         'cumulative_AMSR_Flag': 'cumulative_Flag',\n                         'cumulative_tmmn':'cumulative_air_temperature_tmmn',\n                         'cumulative_etr': 'cumulative_potential_evapotranspiration',\n                         'cumulative_vpd': 'cumulative_mean_vapor_pressure_deficit',\n                         'cumulative_rmax': 'cumulative_relative_humidity_rmax', \n                         'cumulative_rmin': 'cumulative_relative_humidity_rmin',\n                         'cumulative_pr': 'cumulative_precipitation_amount',\n                         'cumulative_tmmx': 'cumulative_air_temperature_tmmx',\n                         'cumulative_vs': 'cumulative_wind_speed',\n                        }, inplace=True)\n\n    desired_order = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"water_year\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\",]\n    \n    feature_names = desired_order\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \ninterpret_prediction()\n",
  "history_output" : "today date = 2023-11-11\ntest start date:  2022-10-16\ntest end date:  2023-10-07\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nnew_data shape:  (462204, 32)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\n    lat      lon  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  49.0 -125.000  ...                              0.0                    0.0\n1  49.0 -124.964  ...                              0.0                    0.0\n2  49.0 -124.928  ...                              0.0                    0.0\n3  49.0 -124.892  ...                              0.0                    0.0\n4  49.0 -124.856  ...                              0.0                    0.0\n[5 rows x 19 columns]\nhow many rows are there? 462204\nX has feature names, but ExtraTreesRegressor was fitted without feature names\npredicted swe:  count    462204.000000\nmean          0.047370\nstd           0.025404\nmin           0.000000\n25%           0.025000\n50%           0.047000\n75%           0.070000\nmax           0.172000\nName: predicted_swe, dtype: float64\n",
  "history_begin_time" : 1699730229618,
  "history_end_time" : 1699730243615,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "3GnR6HBkFge9",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness',\n                         'cumulative_AMSR_SWE': 'cumulative_SWE',\n                         'cumulative_AMSR_Flag': 'cumulative_Flag',\n                         'cumulative_tmmn':'cumulative_air_temperature_tmmn',\n                         'cumulative_etr': 'cumulative_potential_evapotranspiration',\n                         'cumulative_vpd': 'cumulative_mean_vapor_pressure_deficit',\n                         'cumulative_rmax': 'cumulative_relative_humidity_rmax', \n                         'cumulative_rmin': 'cumulative_relative_humidity_rmin',\n                         'cumulative_pr': 'cumulative_precipitation_amount',\n                         'cumulative_tmmx': 'cumulative_air_temperature_tmmx',\n                         'cumulative_vs': 'cumulative_wind_speed',\n                        }, inplace=True)\n\n    desired_order = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"water_year\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\",]\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \ninterpret_prediction()\n",
  "history_output" : "today date = 2023-11-11\ntest start date:  2022-10-16\ntest end date:  2023-10-07\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nnew_data shape:  (462204, 32)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\n    lat      lon  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  49.0 -125.000  ...                              0.0                    0.0\n1  49.0 -124.964  ...                              0.0                    0.0\n2  49.0 -124.928  ...                              0.0                    0.0\n3  49.0 -124.892  ...                              0.0                    0.0\n4  49.0 -124.856  ...                              0.0                    0.0\n[5 rows x 19 columns]\nhow many rows are there? 462204\nX has feature names, but ExtraTreesRegressor was fitted without feature names\npredicted swe:  count    462204.000000\nmean          0.047370\nstd           0.025404\nmin           0.000000\n25%           0.025000\n50%           0.047000\n75%           0.070000\nmax           0.172000\nName: predicted_swe, dtype: float64\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/3GnR6HBkFge9/interpret_model_results.py\", line 238, in <module>\n    interpret_prediction()\n  File \"/home/chetana/gw-workspace/3GnR6HBkFge9/interpret_model_results.py\", line 199, in interpret_prediction\n    features_to_plot = feature_names\nNameError: name 'feature_names' is not defined\n",
  "history_begin_time" : 1699730053381,
  "history_end_time" : 1699730067468,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "xtN9vmfDus0s",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness',\n                         'cumulative_AMSR_SWE': 'cumulative_SWE',\n                         'cumulative_AMSR_Flag': 'cumulative_Flag',\n                         'cumulative_tmmn':'cumulative_air_temperature_tmmn',\n                         'cumulative_etr': 'cumulative_potential_evapotranspiration',\n                         'cumulative_vpd': 'cumulative_mean_vapor_pressure_deficit',\n                         'cumulative_rmax': 'cumulative_relative_humidity_rmax', \n                         'cumulative_rmin': 'cumulative_relative_humidity_rmin',\n                         'cumulative_pr': 'cumulative_precipitation_amount',\n                         'cumulative_tmmx': 'cumulative_air_temperature_tmmx',\n                         'cumulative_vs': 'cumulative_wind_speed',\n                        }, inplace=True)\n\n    desired_order = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"water_year\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \ninterpret_prediction()\n",
  "history_output" : "today date = 2023-11-11\ntest start date:  2022-10-16\ntest end date:  2023-10-07\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nnew_data shape:  (462204, 32)\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/xtN9vmfDus0s/interpret_model_results.py\", line 238, in <module>\n    interpret_prediction()\n  File \"/home/chetana/gw-workspace/xtN9vmfDus0s/interpret_model_results.py\", line 189, in interpret_prediction\n    preprocessed_data = preprocess_data(new_data)\n  File \"/home/chetana/gw-workspace/xtN9vmfDus0s/interpret_model_results.py\", line 87, in preprocess_data\n    data = data[desired_order]\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\", line 3813, in __getitem__\n    indexer = self.columns._get_indexer_strict(key, \"columns\")[1]\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 6070, in _get_indexer_strict\n    self._raise_if_missing(keyarr, indexer, axis_name)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 6133, in _raise_if_missing\n    raise KeyError(f\"{not_found} not in index\")\nKeyError: \"['swe_value'] not in index\"\n",
  "history_begin_time" : 1699730009026,
  "history_end_time" : 1699730019671,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "agsRFKAzw6vk",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"water_year\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \ninterpret_prediction()\n",
  "history_output" : "today date = 2023-11-11\ntest start date:  2022-10-16\ntest end date:  2023-10-07\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFile '/home/chetana/gridmet_test_run/test_data_predicted.csv' has been removed.\nnew_data shape:  (462204, 32)\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/agsRFKAzw6vk/interpret_model_results.py\", line 230, in <module>\n    interpret_prediction()\n  File \"/home/chetana/gw-workspace/agsRFKAzw6vk/interpret_model_results.py\", line 181, in interpret_prediction\n    preprocessed_data = preprocess_data(new_data)\n  File \"/home/chetana/gw-workspace/agsRFKAzw6vk/interpret_model_results.py\", line 79, in preprocess_data\n    data = data[desired_order]\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\", line 3813, in __getitem__\n    indexer = self.columns._get_indexer_strict(key, \"columns\")[1]\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 6070, in _get_indexer_strict\n    self._raise_if_missing(keyarr, indexer, axis_name)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 6133, in _raise_if_missing\n    raise KeyError(f\"{not_found} not in index\")\nKeyError: \"['cumulative_SWE', 'cumulative_Flag', 'cumulative_air_temperature_tmmn', 'cumulative_potential_evapotranspiration', 'cumulative_mean_vapor_pressure_deficit', 'cumulative_relative_humidity_rmax', 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount', 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value'] not in index\"\n",
  "history_begin_time" : 1699710743412,
  "history_end_time" : 1699710757786,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "sBY1nDg5HRsS",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"water_year\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \n#interpret_prediction()\n",
  "history_output" : "today date = 2023-11-11\ntest start date:  2022-10-16\ntest end date:  2023-10-07\n/home/chetana\n",
  "history_begin_time" : 1699710716868,
  "history_end_time" : 1699710737333,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "79wqo1ajbnu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699684154075,
  "history_end_time" : 1705789690245,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "nz9k00a7pwm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699681071374,
  "history_end_time" : 1699681071374,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ner247i1m7l",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698762678708,
  "history_end_time" : 1698762678708,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "cfkhbujt954",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698762637992,
  "history_end_time" : 1698762637992,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "r0u81d5n1zi",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \n#interpret_prediction()\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2018-01-01\ntest end date:  2023-10-25\n/home/ubuntu\n",
  "history_begin_time" : 1698276641320,
  "history_end_time" : 1698276642687,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "dd8ffsrwqwu",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \n#interpret_prediction()\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-03-02\ntest end date:  2023-10-25\n",
  "history_begin_time" : 1698252392876,
  "history_end_time" : 1698252394339,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "d0wmmbfgv2n",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \n#interpret_prediction()\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n",
  "history_begin_time" : 1698251443707,
  "history_end_time" : 1698251445155,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "u55b5dg0vkq",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \n#interpret_prediction()\n\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-26\ntest end date:  2023-10-25\n/home/chetana\n",
  "history_begin_time" : 1698228355447,
  "history_end_time" : 1698228372499,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "yezjp7rqlh4",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \n#interpret_prediction()\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\n",
  "history_begin_time" : 1698227950658,
  "history_end_time" : 1698227952130,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "zpuvtb082kd",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \n#interpret_prediction()\n",
  "history_output" : "today date = 2023-10-24\ntest start date:  2023-05-29\ntest end date:  2023-10-24\n/home/ubuntu\n",
  "history_begin_time" : 1698163869319,
  "history_end_time" : 1698163870663,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "6kbxrxyr0a5",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \n#interpret_prediction()\n",
  "history_output" : "today date = 2023-10-24\ntest start date:  2023-05-28\ntest end date:  2023-10-24\n/home/ubuntu\n",
  "history_begin_time" : 1698163579571,
  "history_end_time" : 1698163581274,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "WmIroMM5W8sI",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \n#interpret_prediction()\n",
  "history_output" : "today date = 2023-10-24\ntest start date:  2023-05-26\ntest end date:  2023-10-24\n/home/ubuntu\n",
  "history_begin_time" : 1698163324019,
  "history_end_time" : 1698163326012,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ho266bn7ba8",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \n#interpret_prediction()\n",
  "history_output" : "today date = 2023-10-24\ntest start date:  2023-05-26\ntest end date:  2023-10-24\n/home/ubuntu\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/ho266bn7ba8/interpret_model_results.py\", line 12, in <module>\n    import shap\nModuleNotFoundError: No module named 'shap'\n",
  "history_begin_time" : 1698163257058,
  "history_end_time" : 1698163257884,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "frw8hix0mv0",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \n#interpret_prediction()\n",
  "history_output" : "today date = 2023-10-24\ntest start date:  2023-05-26\ntest end date:  2023-10-24\n/home/chetana\n",
  "history_begin_time" : 1698161152419,
  "history_end_time" : 1698161168499,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "o4dgwzbm4xv",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \n#interpret_prediction()\n",
  "history_output" : "today date = 2023-10-24\ntest start date:  2023-05-25\ntest end date:  2023-10-24\n/home/ubuntu\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/o4dgwzbm4xv/interpret_model_results.py\", line 12, in <module>\n    import shap\nModuleNotFoundError: No module named 'shap'\n",
  "history_begin_time" : 1698157859239,
  "history_end_time" : 1698157860434,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "lo2hwzeay1z",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \n#interpret_prediction()\n",
  "history_output" : "today date = 2023-10-24\ntest start date:  2023-05-17\ntest end date:  2023-10-24\n/home/ubuntu\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/lo2hwzeay1z/interpret_model_results.py\", line 12, in <module>\n    import shap\nModuleNotFoundError: No module named 'shap'\n",
  "history_begin_time" : 1698152113540,
  "history_end_time" : 1698152114407,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "9easlzy69nm",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \n#interpret_prediction()\n",
  "history_output" : "today date = 2023-10-23\ntest start date:  2023-05-18\ntest end date:  2023-10-23\n/home/chetana\n",
  "history_begin_time" : 1698095651509,
  "history_end_time" : 1698095667380,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "qdb5yfffjzr",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \n#interpret_prediction()\n",
  "history_output" : "today date = 2023-10-23\ntest start date:  2023-05-18\ntest end date:  2023-10-23\n/home/chetana\n",
  "history_begin_time" : 1698075753143,
  "history_end_time" : 1698075772240,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "JSTe6t7LLuEH",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \n#interpret_prediction()\n",
  "history_output" : "today date = 2023-10-18\ntest start date:  2023-05-17\ntest end date:  2023-10-18\n/home/chetana\n",
  "history_begin_time" : 1697639578020,
  "history_end_time" : 1697639596624,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "SbaSbs6VrSjw",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\nplot_feature_importance()\n#interpret_prediction()\n",
  "history_output" : "today date = 2023-10-16\ntest start date:  2023-05-17\ntest end date:  2023-10-16\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v3_time_series.csv\n(89,)\n(91,)\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/SbaSbs6VrSjw/interpret_model_results.py\", line 234, in <module>\n    plot_feature_importance()\n  File \"/home/chetana/gw-workspace/SbaSbs6VrSjw/interpret_model_results.py\", line 156, in plot_feature_importance\n    plt.barh(feature_names, feature_importances)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/matplotlib/pyplot.py\", line 2771, in barh\n    return gca().barh(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/matplotlib/axes/_axes.py\", line 2698, in barh\n    patches = self.bar(x=left, height=height, width=width, bottom=y,\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/matplotlib/__init__.py\", line 1465, in inner\n    return func(ax, *map(sanitize_sequence, args), **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/matplotlib/axes/_axes.py\", line 2457, in bar\n    x, height, width, y, linewidth, hatch = np.broadcast_arrays(\n  File \"<__array_function__ internals>\", line 200, in broadcast_arrays\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/numpy/lib/stride_tricks.py\", line 540, in broadcast_arrays\n    shape = _broadcast_shape(*args)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/numpy/lib/stride_tricks.py\", line 422, in _broadcast_shape\n    b = np.broadcast(*args[:32])\nValueError: shape mismatch: objects cannot be broadcast to a single shape.  Mismatch is between arg 2 with shape (91,) and arg 3 with shape (89,).\n",
  "history_begin_time" : 1697424086137,
  "history_end_time" : 1697424124389,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "dz4AxtgJ1Jvn",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\nplot_feature_importance()\n#interpret_prediction()\n",
  "history_output" : "today date = 2023-10-15\ntest start date:  2023-05-17\ntest end date:  2023-10-15\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v3_time_series.csv\n(89,)\n(89,)\n",
  "history_begin_time" : 1697402766119,
  "history_end_time" : 1697402796347,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "rfMW8AQAkLcc",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\nplot_feature_importance()\n#interpret_prediction()\n",
  "history_output" : "today date = 2023-10-15\ntest start date:  2023-05-17\ntest end date:  2023-10-15\n/home/chetana\npreparing training data from csv:  /home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v3_time_series.csv\nIndex(['date', 'lat', 'lon', 'SWE', 'Flag', 'Unnamed: 0',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'SWE_1', 'Flag_1',\n       'air_temperature_tmmn_1', 'potential_evapotranspiration_1',\n       'mean_vapor_pressure_deficit_1', 'relative_humidity_rmax_1',\n       'relative_humidity_rmin_1', 'precipitation_amount_1',\n       'air_temperature_tmmx_1', 'wind_speed_1', 'SWE_2', 'Flag_2',\n       'air_temperature_tmmn_2', 'potential_evapotranspiration_2',\n       'mean_vapor_pressure_deficit_2', 'relative_humidity_rmax_2',\n       'relative_humidity_rmin_2', 'precipitation_amount_2',\n       'air_temperature_tmmx_2', 'wind_speed_2', 'SWE_3', 'Flag_3',\n       'air_temperature_tmmn_3', 'potential_evapotranspiration_3',\n       'mean_vapor_pressure_deficit_3', 'relative_humidity_rmax_3',\n       'relative_humidity_rmin_3', 'precipitation_amount_3',\n       'air_temperature_tmmx_3', 'wind_speed_3', 'SWE_4', 'Flag_4',\n       'air_temperature_tmmn_4', 'potential_evapotranspiration_4',\n       'mean_vapor_pressure_deficit_4', 'relative_humidity_rmax_4',\n       'relative_humidity_rmin_4', 'precipitation_amount_4',\n       'air_temperature_tmmx_4', 'wind_speed_4', 'SWE_5', 'Flag_5',\n       'air_temperature_tmmn_5', 'potential_evapotranspiration_5',\n       'mean_vapor_pressure_deficit_5', 'relative_humidity_rmax_5',\n       'relative_humidity_rmin_5', 'precipitation_amount_5',\n       'air_temperature_tmmx_5', 'wind_speed_5', 'SWE_6', 'Flag_6',\n       'air_temperature_tmmn_6', 'potential_evapotranspiration_6',\n       'mean_vapor_pressure_deficit_6', 'relative_humidity_rmax_6',\n       'relative_humidity_rmin_6', 'precipitation_amount_6',\n       'air_temperature_tmmx_6', 'wind_speed_6', 'SWE_7', 'Flag_7',\n       'air_temperature_tmmn_7', 'potential_evapotranspiration_7',\n       'mean_vapor_pressure_deficit_7', 'relative_humidity_rmax_7',\n       'relative_humidity_rmin_7', 'precipitation_amount_7',\n       'air_temperature_tmmx_7', 'wind_speed_7'],\n      dtype='object')\n[0.25497571 0.01098041 0.01731336 0.00534393 0.00168931 0.01069841\n 0.01691357 0.02194472 0.00815427 0.00712048 0.0032472  0.01812428\n 0.0042528  0.01007254 0.00597691 0.00659383 0.00470674 0.00557513\n 0.00458301 0.00377496 0.00145056 0.00799624 0.01338926 0.01839849\n 0.00675435 0.0071384  0.00317518 0.01715636 0.00416799 0.0035729\n 0.00131883 0.00786631 0.01279631 0.01912706 0.00573626 0.00723549\n 0.0030853  0.01774938 0.00405792 0.00331793 0.00128613 0.00778931\n 0.01347009 0.01990534 0.00563495 0.00713673 0.00300723 0.01897349\n 0.00405158 0.00368084 0.00138643 0.00798225 0.01370399 0.020352\n 0.00558865 0.00719759 0.00307287 0.01863393 0.00406659 0.004012\n 0.00133568 0.00783714 0.01234209 0.01797572 0.00562835 0.00686195\n 0.00308649 0.01657908 0.0040865  0.00404839 0.00153564 0.00792966\n 0.0120827  0.01855957 0.0060164  0.00679131 0.00304828 0.01595397\n 0.004163   0.00494118 0.00177298 0.00887887 0.01259538 0.01965502\n 0.00651793 0.00691735 0.00320772 0.01687973 0.00430787]\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/rfMW8AQAkLcc/interpret_model_results.py\", line 232, in <module>\n    plot_feature_importance()\n  File \"/home/chetana/gw-workspace/rfMW8AQAkLcc/interpret_model_results.py\", line 154, in plot_feature_importance\n    plt.barh(feature_names, feature_importances)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/matplotlib/pyplot.py\", line 2771, in barh\n    return gca().barh(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/matplotlib/axes/_axes.py\", line 2698, in barh\n    patches = self.bar(x=left, height=height, width=width, bottom=y,\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/matplotlib/__init__.py\", line 1465, in inner\n    return func(ax, *map(sanitize_sequence, args), **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/matplotlib/axes/_axes.py\", line 2457, in bar\n    x, height, width, y, linewidth, hatch = np.broadcast_arrays(\n  File \"<__array_function__ internals>\", line 200, in broadcast_arrays\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/numpy/lib/stride_tricks.py\", line 540, in broadcast_arrays\n    shape = _broadcast_shape(*args)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/numpy/lib/stride_tricks.py\", line 422, in _broadcast_shape\n    b = np.broadcast(*args[:32])\nValueError: shape mismatch: objects cannot be broadcast to a single shape.  Mismatch is between arg 2 with shape (89,) and arg 3 with shape (90,).\n",
  "history_begin_time" : 1697402522179,
  "history_end_time" : 1697402561245,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "QcmKnOryIDS3",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\nplot_feature_importance()\n#interpret_prediction()\n",
  "history_output" : "today date = 2023-10-15\ntest start date:  2023-05-17\ntest end date:  2023-10-15\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/QcmKnOryIDS3/interpret_model_results.py\", line 232, in <module>\n    plot_feature_importance()\n  File \"/home/chetana/gw-workspace/QcmKnOryIDS3/interpret_model_results.py\", line 140, in plot_feature_importance\n    training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series.csv'\nNameError: name 'working_dir' is not defined\n",
  "history_begin_time" : 1697402399934,
  "history_end_time" : 1697402451230,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "b1062pjcfcj",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\ntoday date = 2023-10-15\ntest start date:  2023-05-17\ntest end date:  2023-10-15\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFile '/home/chetana/gridmet_test_run/test_data_predicted.csv' has been removed.\nnew_data shape:  (462204, 21)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\n   date   lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0     1  49.0 -125.000    0  ...     -999.0  -999.0    -999.0     -999.0\n1     1  49.0 -124.964    0  ...     -999.0  -999.0    -999.0     -999.0\n2     1  49.0 -124.928    0  ...     -999.0  -999.0    -999.0     -999.0\n3     1  49.0 -124.892    0  ...     -999.0  -999.0    -999.0     -999.0\n4     1  49.0 -124.856    0  ...     -999.0  -999.0    -999.0     -999.0\n[5 rows x 19 columns]\nhow many rows are there? 462204\npredicted swe:  count    462204.000000\nmean          9.316173\nstd           2.202320\nmin           4.671500\n25%           7.188500\n50%           9.179500\n75%          11.733000\nmax          13.806500\nName: predicted_swe, dtype: float64\nIndex(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n[0.25605139 0.03419947 0.03890676 0.03505011 0.0081191  0.07097952\n 0.07288071 0.08165229 0.0562714  0.05387592 0.02544186 0.08538923\n 0.03747726 0.03169901 0.02372949 0.02630215 0.01943233 0.02209695\n 0.02044504]\n",
  "history_begin_time" : 1697349768455,
  "history_end_time" : 1697349810828,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "ee401bjifc0",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\ntoday date = 2023-10-15\ntest start date:  2023-05-16\ntest end date:  2023-10-15\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFile '/home/chetana/gridmet_test_run/test_data_predicted.csv' has been removed.\nnew_data shape:  (462204, 21)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\n   date   lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0     5  49.0 -125.000    0  ...     -999.0  -999.0    -999.0     -999.0\n1     5  49.0 -124.964    0  ...     -999.0  -999.0    -999.0     -999.0\n2     5  49.0 -124.928    0  ...     -999.0  -999.0    -999.0     -999.0\n3     5  49.0 -124.892    0  ...     -999.0  -999.0    -999.0     -999.0\n4     5  49.0 -124.856    0  ...     -999.0  -999.0    -999.0     -999.0\n[5 rows x 19 columns]\nhow many rows are there? 462204\npredicted swe:  count    462204.000000\nmean          3.340447\nstd           1.254240\nmin           0.457000\n25%           2.231000\n50%           3.323000\n75%           4.435000\nmax           7.354500\nName: predicted_swe, dtype: float64\nIndex(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n[0.37746971 0.03045268 0.03165519 0.01750784 0.00681925 0.05540429\n 0.06165659 0.08024015 0.05017038 0.04535937 0.01938001 0.08335318\n 0.02902532 0.02480191 0.01865674 0.02018943 0.01501929 0.01714384\n 0.01569483]\n",
  "history_begin_time" : 1697349078956,
  "history_end_time" : 1697349105354,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "GyYiPAjudnbq",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\ntoday date = 2023-10-15\ntest start date:  2023-03-16\ntest end date:  2023-10-15\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nFile '/home/chetana/gridmet_test_run/test_data_predicted.csv' has been removed.\nnew_data shape:  (462204, 21)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\n   date   lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0     3  49.0 -125.000    0  ...     -999.0  -999.0    -999.0     -999.0\n1     3  49.0 -124.964    0  ...     -999.0  -999.0    -999.0     -999.0\n2     3  49.0 -124.928    0  ...     -999.0  -999.0    -999.0     -999.0\n3     3  49.0 -124.892    0  ...     -999.0  -999.0    -999.0     -999.0\n4     3  49.0 -124.856    0  ...     -999.0  -999.0    -999.0     -999.0\n[5 rows x 19 columns]\nhow many rows are there? 462204\npredicted swe:  count    462204.000000\nmean         12.543785\nstd           1.348075\nmin           7.766000\n25%          11.678000\n50%          12.685000\n75%          13.765000\nmax          17.199000\nName: predicted_swe, dtype: float64\nIndex(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n[0.37746971 0.03045268 0.03165519 0.01750784 0.00681925 0.05540429\n 0.06165659 0.08024015 0.05017038 0.04535937 0.01938001 0.08335318\n 0.02902532 0.02480191 0.01865674 0.02018943 0.01501929 0.01714384\n 0.01569483]\n",
  "history_begin_time" : 1697348733600,
  "history_end_time" : 1697348765045,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "I67wrXYnsyuP",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231510051348.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\ntoday date = 2023-10-15\ntest start date:  2023-03-16\ntest end date:  2023-10-15\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231510051348.joblib\nnew_data shape:  (462204, 21)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231510051348.joblib\n   date   lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0     3  49.0 -125.000    0  ...     -999.0  -999.0    -999.0     -999.0\n1     3  49.0 -124.964    0  ...     -999.0  -999.0    -999.0     -999.0\n2     3  49.0 -124.928    0  ...     -999.0  -999.0    -999.0     -999.0\n3     3  49.0 -124.892    0  ...     -999.0  -999.0    -999.0     -999.0\n4     3  49.0 -124.856    0  ...     -999.0  -999.0    -999.0     -999.0\n[5 rows x 19 columns]\nhow many rows are there? 462204\npredicted swe:  count    462204.000000\nmean         12.543785\nstd           1.348075\nmin           7.766000\n25%          11.678000\n50%          12.685000\n75%          13.765000\nmax          17.199000\nName: predicted_swe, dtype: float64\nIndex(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n[0.37746971 0.03045268 0.03165519 0.01750784 0.00681925 0.05540429\n 0.06165659 0.08024015 0.05017038 0.04535937 0.01938001 0.08335318\n 0.02902532 0.02480191 0.01865674 0.02018943 0.01501929 0.01714384\n 0.01569483]\n",
  "history_begin_time" : 1697347537180,
  "history_end_time" : 1697347563255,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "HnHlXLHTEVmK",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231310093755.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\ntoday date = 2023-10-15\ntest start date:  2023-03-16\ntest end date:  2023-10-15\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231310093755.joblib\nnew_data shape:  (462204, 21)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231310093755.joblib\n   date   lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0     3  49.0 -125.000    0  ...     -999.0  -999.0    -999.0     -999.0\n1     3  49.0 -124.964    0  ...     -999.0  -999.0    -999.0     -999.0\n2     3  49.0 -124.928    0  ...     -999.0  -999.0    -999.0     -999.0\n3     3  49.0 -124.892    0  ...     -999.0  -999.0    -999.0     -999.0\n4     3  49.0 -124.856    0  ...     -999.0  -999.0    -999.0     -999.0\n[5 rows x 19 columns]\nhow many rows are there? 462204\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/HnHlXLHTEVmK/interpret_model_results.py\", line 220, in <module>\n    interpret_prediction()\n  File \"/home/chetana/gw-workspace/HnHlXLHTEVmK/interpret_model_results.py\", line 163, in interpret_prediction\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n  File \"/home/chetana/gw-workspace/HnHlXLHTEVmK/interpret_model_results.py\", line 111, in predict_swe\n    predictions = model.predict(data)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 984, in predict\n    X = self._validate_X_predict(X)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 599, in _validate_X_predict\n    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 625, in _validate_data\n    self._check_n_features(X, reset=reset)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 414, in _check_n_features\n    raise ValueError(\nValueError: X has 19 features, but ExtraTreesRegressor is expecting 11 features as input.\n",
  "history_begin_time" : 1697347474677,
  "history_end_time" : 1697347496035,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "XEtvP2owgxAJ",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231310093755.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\ntoday date = 2023-10-15\ntest start date:  2023-03-16\ntest end date:  2023-10-15\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231310093755.joblib\nFile '/home/chetana/gridmet_test_run/test_data_predicted.csv' has been removed.\nnew_data shape:  (462204, 21)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231310093755.joblib\n    lat      lon  air_temperature_tmmn  ...  aspect  eastness  northness\n0  49.0 -125.000                -999.0  ...  -999.0    -999.0     -999.0\n1  49.0 -124.964                -999.0  ...  -999.0    -999.0     -999.0\n2  49.0 -124.928                -999.0  ...  -999.0    -999.0     -999.0\n3  49.0 -124.892                -999.0  ...  -999.0    -999.0     -999.0\n4  49.0 -124.856                -999.0  ...  -999.0    -999.0     -999.0\n[5 rows x 11 columns]\nhow many rows are there? 462204\npredicted swe:  count    462204.000000\nmean          9.257585\nstd           2.635650\nmin           2.119125\n25%           7.385853\n50%           8.447779\n75%          11.608922\nmax          14.834006\nName: predicted_swe, dtype: float64\nIndex(['lat', 'lon', 'air_temperature_tmmn', 'precipitation_amount',\n       'wind_speed', 'elevation', 'slope', 'curvature', 'aspect', 'eastness',\n       'northness'],\n      dtype='object')\n[0.02724017 0.02768547 0.38693487 0.1269662  0.30026989 0.02636709\n 0.02238656 0.02313604 0.01899226 0.02016788 0.01985358]\n",
  "history_begin_time" : 1697347267270,
  "history_end_time" : 1697347317979,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "uhDe9SSxvwMc",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231310093755.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\ntoday date = 2023-10-13\ntest start date:  2023-03-16\ntest end date:  2023-10-13\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231310093755.joblib\nnew_data shape:  (462204, 21)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231310093755.joblib\n    lat      lon  air_temperature_tmmn  ...  aspect  eastness  northness\n0  49.0 -125.000                -999.0  ...  -999.0    -999.0     -999.0\n1  49.0 -124.964                -999.0  ...  -999.0    -999.0     -999.0\n2  49.0 -124.928                -999.0  ...  -999.0    -999.0     -999.0\n3  49.0 -124.892                -999.0  ...  -999.0    -999.0     -999.0\n4  49.0 -124.856                -999.0  ...  -999.0    -999.0     -999.0\n[5 rows x 11 columns]\nhow many rows are there? 462204\npredicted swe:  count    462204.000000\nmean          9.257585\nstd           2.635650\nmin           2.119125\n25%           7.385853\n50%           8.447779\n75%          11.608922\nmax          14.834006\nName: predicted_swe, dtype: float64\nIndex(['lat', 'lon', 'air_temperature_tmmn', 'precipitation_amount',\n       'wind_speed', 'elevation', 'slope', 'curvature', 'aspect', 'eastness',\n       'northness'],\n      dtype='object')\n[0.02724017 0.02768547 0.38693487 0.1269662  0.30026989 0.02636709\n 0.02238656 0.02313604 0.01899226 0.02016788 0.01985358]\n",
  "history_begin_time" : 1697190188640,
  "history_end_time" : 1697190214706,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "kmpANDJtaobX",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    data = data.drop([\"date\", \"SWE\", \"Flag\"], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231310093755.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\ntoday date = 2023-10-13\ntest start date:  2023-03-16\ntest end date:  2023-10-13\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231310093755.joblib\nFile '/home/chetana/gridmet_test_run/test_data_predicted.csv' has been removed.\nnew_data shape:  (462204, 21)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231310093755.joblib\n    lat      lon  air_temperature_tmmn  ...  aspect  eastness  northness\n0  49.0 -125.000                -999.0  ...  -999.0    -999.0     -999.0\n1  49.0 -124.964                -999.0  ...  -999.0    -999.0     -999.0\n2  49.0 -124.928                -999.0  ...  -999.0    -999.0     -999.0\n3  49.0 -124.892                -999.0  ...  -999.0    -999.0     -999.0\n4  49.0 -124.856                -999.0  ...  -999.0    -999.0     -999.0\n[5 rows x 16 columns]\nhow many rows are there? 462204\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/kmpANDJtaobX/interpret_model_results.py\", line 219, in <module>\n    interpret_prediction()\n  File \"/home/chetana/gw-workspace/kmpANDJtaobX/interpret_model_results.py\", line 162, in interpret_prediction\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n  File \"/home/chetana/gw-workspace/kmpANDJtaobX/interpret_model_results.py\", line 110, in predict_swe\n    predictions = model.predict(data)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 984, in predict\n    X = self._validate_X_predict(X)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 599, in _validate_X_predict\n    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 625, in _validate_data\n    self._check_n_features(X, reset=reset)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 414, in _check_n_features\n    raise ValueError(\nValueError: X has 16 features, but ExtraTreesRegressor is expecting 11 features as input.\n",
  "history_begin_time" : 1697190160136,
  "history_end_time" : 1697190182135,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "z5kgnrftrir",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    data = data.drop([\"date\", \"SWE\", \"Flag\"], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231310093755.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\ntoday date = 2023-10-13\ntest start date:  2023-03-16\ntest end date:  2023-10-13\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231310093755.joblib\nnew_data shape:  (462204, 21)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231310093755.joblib\n    lat      lon  air_temperature_tmmn  ...  aspect  eastness  northness\n0  49.0 -125.000                -999.0  ...  -999.0    -999.0     -999.0\n1  49.0 -124.964                -999.0  ...  -999.0    -999.0     -999.0\n2  49.0 -124.928                -999.0  ...  -999.0    -999.0     -999.0\n3  49.0 -124.892                -999.0  ...  -999.0    -999.0     -999.0\n4  49.0 -124.856                -999.0  ...  -999.0    -999.0     -999.0\n[5 rows x 16 columns]\nhow many rows are there? 462204\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/z5kgnrftrir/interpret_model_results.py\", line 219, in <module>\n    interpret_prediction()\n  File \"/home/chetana/gw-workspace/z5kgnrftrir/interpret_model_results.py\", line 162, in interpret_prediction\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n  File \"/home/chetana/gw-workspace/z5kgnrftrir/interpret_model_results.py\", line 110, in predict_swe\n    predictions = model.predict(data)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 984, in predict\n    X = self._validate_X_predict(X)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 599, in _validate_X_predict\n    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 625, in _validate_data\n    self._check_n_features(X, reset=reset)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 414, in _check_n_features\n    raise ValueError(\nValueError: X has 16 features, but ExtraTreesRegressor is expecting 11 features as input.\n",
  "history_begin_time" : 1697190021093,
  "history_end_time" : 1697190044665,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "vfpKQKHkVRWw",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    data = data.drop([\"date\", \"SWE\", \"Flag\"], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231310091310.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\ntoday date = 2023-10-13\ntest start date:  2023-03-16\ntest end date:  2023-10-13\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231310091310.joblib\nnew_data shape:  (462204, 21)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231310091310.joblib\n    lat      lon  air_temperature_tmmn  ...  aspect  eastness  northness\n0  49.0 -125.000                -999.0  ...  -999.0    -999.0     -999.0\n1  49.0 -124.964                -999.0  ...  -999.0    -999.0     -999.0\n2  49.0 -124.928                -999.0  ...  -999.0    -999.0     -999.0\n3  49.0 -124.892                -999.0  ...  -999.0    -999.0     -999.0\n4  49.0 -124.856                -999.0  ...  -999.0    -999.0     -999.0\n[5 rows x 16 columns]\nhow many rows are there? 462204\npredicted swe:  count    462204.000000\nmean          7.929550\nstd           1.055634\nmin           4.307000\n25%           7.162500\n50%           7.694000\n75%           8.630500\nmax          14.828500\nName: predicted_swe, dtype: float64\nIndex(['lat', 'lon', 'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness'],\n      dtype='object')\n[0.04697698 0.04798496 0.08178774 0.12859049 0.11840653 0.07837884\n 0.08208776 0.04000535 0.11386358 0.06246501 0.04016314 0.03377809\n 0.03626641 0.0277447  0.03189248 0.02960795]\n",
  "history_begin_time" : 1697188918651,
  "history_end_time" : 1697188959947,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "b86vpc6zlfu",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    data = data.drop(\"date\", axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231310091310.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\ntoday date = 2023-10-13\ntest start date:  2023-03-16\ntest end date:  2023-10-13\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231310091310.joblib\nFile '/home/chetana/gridmet_test_run/test_data_predicted.csv' has been removed.\nnew_data shape:  (462204, 21)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231310091310.joblib\n    lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0  49.0 -125.000    0   241  ...     -999.0  -999.0    -999.0     -999.0\n1  49.0 -124.964    0   241  ...     -999.0  -999.0    -999.0     -999.0\n2  49.0 -124.928    0   241  ...     -999.0  -999.0    -999.0     -999.0\n3  49.0 -124.892    0   241  ...     -999.0  -999.0    -999.0     -999.0\n4  49.0 -124.856    0   241  ...     -999.0  -999.0    -999.0     -999.0\n[5 rows x 18 columns]\nhow many rows are there? 462204\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/b86vpc6zlfu/interpret_model_results.py\", line 219, in <module>\n    interpret_prediction()\n  File \"/home/chetana/gw-workspace/b86vpc6zlfu/interpret_model_results.py\", line 162, in interpret_prediction\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n  File \"/home/chetana/gw-workspace/b86vpc6zlfu/interpret_model_results.py\", line 110, in predict_swe\n    predictions = model.predict(data)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 984, in predict\n    X = self._validate_X_predict(X)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 599, in _validate_X_predict\n    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 625, in _validate_data\n    self._check_n_features(X, reset=reset)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 414, in _check_n_features\n    raise ValueError(\nValueError: X has 18 features, but ExtraTreesRegressor is expecting 16 features as input.\n",
  "history_begin_time" : 1697188768518,
  "history_end_time" : 1697188815656,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "sn2w8wfgl34",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    data = data.drop(\"date\", axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\ntoday date = 2023-10-13\ntest start date:  2023-07-15\ntest end date:  2023-10-13\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib\nFile '/home/chetana/gridmet_test_run/test_data_predicted.csv' has been removed.\nnew_data shape:  (462204, 21)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib\n    lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0  49.0 -125.000    0   241  ...     -999.0  -999.0    -999.0     -999.0\n1  49.0 -124.964    0   241  ...     -999.0  -999.0    -999.0     -999.0\n2  49.0 -124.928    0   241  ...     -999.0  -999.0    -999.0     -999.0\n3  49.0 -124.892    0   241  ...     -999.0  -999.0    -999.0     -999.0\n4  49.0 -124.856    0   241  ...     -999.0  -999.0    -999.0     -999.0\n[5 rows x 18 columns]\nhow many rows are there? 462204\npredicted swe:  count    462204.000000\nmean          6.154438\nstd           1.112104\nmin           3.090500\n25%           5.196500\n50%           6.065000\n75%           7.020500\nmax          12.409500\nName: predicted_swe, dtype: float64\nIndex(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n[0.04633082 0.049006   0.05922091 0.00830327 0.07838727 0.11662276\n 0.10793431 0.07455769 0.07209162 0.03498797 0.10763383 0.05324669\n 0.03938958 0.03150809 0.03559248 0.02661422 0.03019245 0.02838003]\n",
  "history_begin_time" : 1697188144587,
  "history_end_time" : 1697188195377,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "nbyq5ci2ohv",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    data = data.drop(\"date\", axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\ntoday date = 2023-10-13\ntest start date:  2023-03-15\ntest end date:  2023-10-13\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib\nFile '/home/chetana/gridmet_test_run/test_data_predicted.csv' has been removed.\nnew_data shape:  (462204, 21)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib\n    lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0  49.0 -125.000    0   241  ...     -999.0  -999.0    -999.0     -999.0\n1  49.0 -124.964    0   241  ...     -999.0  -999.0    -999.0     -999.0\n2  49.0 -124.928    0   241  ...     -999.0  -999.0    -999.0     -999.0\n3  49.0 -124.892    0   241  ...     -999.0  -999.0    -999.0     -999.0\n4  49.0 -124.856    0   241  ...     -999.0  -999.0    -999.0     -999.0\n[5 rows x 18 columns]\nhow many rows are there? 462204\npredicted swe:  count    462204.000000\nmean          7.405248\nstd           1.398622\nmin           2.781500\n25%           6.782000\n50%           7.362500\n75%           7.960500\nmax          14.282000\nName: predicted_swe, dtype: float64\nIndex(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n[0.04633082 0.049006   0.05922091 0.00830327 0.07838727 0.11662276\n 0.10793431 0.07455769 0.07209162 0.03498797 0.10763383 0.05324669\n 0.03938958 0.03150809 0.03559248 0.02661422 0.03019245 0.02838003]\n",
  "history_begin_time" : 1697187637880,
  "history_end_time" : 1697187705116,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "ODKN3ZNJLIRQ",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    data = data.drop(\"date\", axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\ntoday date = 2023-10-13\ntest start date:  2023-09-16\ntest end date:  2023-10-13\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib\nFile '/home/chetana/gridmet_test_run/test_data_predicted.csv' has been removed.\nnew_data shape:  (462204, 21)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib\n    lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0  49.0 -125.000    0   241  ...     -999.0  -999.0    -999.0     -999.0\n1  49.0 -124.964    0   241  ...     -999.0  -999.0    -999.0     -999.0\n2  49.0 -124.928    0   241  ...     -999.0  -999.0    -999.0     -999.0\n3  49.0 -124.892    0   241  ...     -999.0  -999.0    -999.0     -999.0\n4  49.0 -124.856    0   241  ...     -999.0  -999.0    -999.0     -999.0\n[5 rows x 18 columns]\nhow many rows are there? 462204\npredicted swe:  count    462204.000000\nmean          6.633628\nstd           1.415936\nmin           3.486000\n25%           5.415000\n50%           6.884500\n75%           7.514500\nmax          13.949500\nName: predicted_swe, dtype: float64\nIndex(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n[0.04633082 0.049006   0.05922091 0.00830327 0.07838727 0.11662276\n 0.10793431 0.07455769 0.07209162 0.03498797 0.10763383 0.05324669\n 0.03938958 0.03150809 0.03559248 0.02661422 0.03019245 0.02838003]\n",
  "history_begin_time" : 1697187123737,
  "history_end_time" : 1697187195920,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "jnn34vo22j7",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    data = data.drop(\"date\", axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\ntoday date = 2023-10-09\ntest start date:  2023-09-16\ntest end date:  2023-10-09\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib\nFile '/home/chetana/gridmet_test_run/test_data_predicted.csv' has been removed.\nnew_data shape:  (462204, 21)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib\n    lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0  49.0 -125.000    0   241  ...     -999.0  -999.0    -999.0     -999.0\n1  49.0 -124.964    0   241  ...     -999.0  -999.0    -999.0     -999.0\n2  49.0 -124.928    0   241  ...     -999.0  -999.0    -999.0     -999.0\n3  49.0 -124.892    0   241  ...     -999.0  -999.0    -999.0     -999.0\n4  49.0 -124.856    0   241  ...     -999.0  -999.0    -999.0     -999.0\n[5 rows x 18 columns]\nhow many rows are there? 462204\npredicted swe:  count    462204.000000\nmean          6.633628\nstd           1.415936\nmin           3.486000\n25%           5.415000\n50%           6.884500\n75%           7.514500\nmax          13.949500\nName: predicted_swe, dtype: float64\nIndex(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n[0.04633082 0.049006   0.05922091 0.00830327 0.07838727 0.11662276\n 0.10793431 0.07455769 0.07209162 0.03498797 0.10763383 0.05324669\n 0.03938958 0.03150809 0.03559248 0.02661422 0.03019245 0.02838003]\n",
  "history_begin_time" : 1696864201207,
  "history_end_time" : 1696864243272,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "slhd03iqugl",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    data = data.drop(\"date\", axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\ntoday date = 2023-10-09\ntest start date:  2023-09-15\ntest end date:  2023-10-09\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib\nFile '/home/chetana/gridmet_test_run/test_data_predicted.csv' has been removed.\nnew_data shape:  (462204, 21)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib\n    lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0  49.0 -125.000    0   241  ...     -999.0  -999.0    -999.0     -999.0\n1  49.0 -124.964    0   241  ...     -999.0  -999.0    -999.0     -999.0\n2  49.0 -124.928    0   241  ...     -999.0  -999.0    -999.0     -999.0\n3  49.0 -124.892    0   241  ...     -999.0  -999.0    -999.0     -999.0\n4  49.0 -124.856    0   241  ...     -999.0  -999.0    -999.0     -999.0\n[5 rows x 18 columns]\nhow many rows are there? 462204\npredicted swe:  count    462204.000000\nmean          7.079997\nstd           1.527712\nmin           3.739000\n25%           5.930500\n50%           6.967000\n75%           7.775000\nmax          13.356500\nName: predicted_swe, dtype: float64\nIndex(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n[0.04633082 0.049006   0.05922091 0.00830327 0.07838727 0.11662276\n 0.10793431 0.07455769 0.07209162 0.03498797 0.10763383 0.05324669\n 0.03938958 0.03150809 0.03559248 0.02661422 0.03019245 0.02838003]\n",
  "history_begin_time" : 1696862681200,
  "history_end_time" : 1696862735745,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "rnzdlwmbrtq",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    data = data.drop(\"date\", axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\ntoday date = 2023-10-09\ntest start date:  2023-06-15\ntest end date:  2023-10-09\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib\nFile '/home/chetana/gridmet_test_run/test_data_predicted.csv' has been removed.\nnew_data shape:  (462204, 21)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib\n    lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0  49.0 -125.000    0   241  ...     -999.0  -999.0    -999.0     -999.0\n1  49.0 -124.964    0   241  ...     -999.0  -999.0    -999.0     -999.0\n2  49.0 -124.928    0   241  ...     -999.0  -999.0    -999.0     -999.0\n3  49.0 -124.892    0   241  ...     -999.0  -999.0    -999.0     -999.0\n4  49.0 -124.856    0   241  ...     -999.0  -999.0    -999.0     -999.0\n[5 rows x 18 columns]\nhow many rows are there? 462204\npredicted swe:  count    462204.000000\nmean          7.097843\nstd           1.343252\nmin           3.136000\n25%           6.070000\n50%           7.022500\n75%           7.771500\nmax          12.641500\nName: predicted_swe, dtype: float64\nIndex(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n[0.04633082 0.049006   0.05922091 0.00830327 0.07838727 0.11662276\n 0.10793431 0.07455769 0.07209162 0.03498797 0.10763383 0.05324669\n 0.03938958 0.03150809 0.03559248 0.02661422 0.03019245 0.02838003]\n",
  "history_begin_time" : 1696832501356,
  "history_end_time" : 1696832529964,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "0Sc91JbgAPF6",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    data = data.drop(\"date\", axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\ntoday date = 2023-10-09\ntest start date:  2023-02-12\ntest end date:  2023-10-09\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib\nnew_data shape:  (462204, 21)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib\n    lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0  49.0 -125.000    0   241  ...     -999.0  -999.0    -999.0     -999.0\n1  49.0 -124.964    0   241  ...     -999.0  -999.0    -999.0     -999.0\n2  49.0 -124.928    0   241  ...     -999.0  -999.0    -999.0     -999.0\n3  49.0 -124.892    0   241  ...     -999.0  -999.0    -999.0     -999.0\n4  49.0 -124.856    0   241  ...     -999.0  -999.0    -999.0     -999.0\n[5 rows x 18 columns]\nhow many rows are there? 462204\npredicted swe:  count    462204.000000\nmean          8.052420\nstd           1.940919\nmin           3.401000\n25%           6.861500\n50%           7.556500\n75%           8.707500\nmax          16.004000\nName: predicted_swe, dtype: float64\nIndex(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n[0.04633082 0.049006   0.05922091 0.00830327 0.07838727 0.11662276\n 0.10793431 0.07455769 0.07209162 0.03498797 0.10763383 0.05324669\n 0.03938958 0.03150809 0.03559248 0.02661422 0.03019245 0.02838003]\n",
  "history_begin_time" : 1696832133218,
  "history_end_time" : 1696832162449,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "a0a2mqx5j1s",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    data = data.drop(\"date\", axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "",
  "history_begin_time" : 1696832093433,
  "history_end_time" : 1696832099934,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "cF1jtPsEHaZG",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    data = data.drop(\"date\", axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\ntoday date = 2023-10-09\ntest start date:  2023-02-11\ntest end date:  2023-10-09\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib\nnew_data shape:  (462204, 21)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib\n    lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0  49.0 -125.000    0   241  ...     -999.0  -999.0    -999.0     -999.0\n1  49.0 -124.964    0   241  ...     -999.0  -999.0    -999.0     -999.0\n2  49.0 -124.928    0   241  ...     -999.0  -999.0    -999.0     -999.0\n3  49.0 -124.892    0   241  ...     -999.0  -999.0    -999.0     -999.0\n4  49.0 -124.856    0   241  ...     -999.0  -999.0    -999.0     -999.0\n[5 rows x 18 columns]\nhow many rows are there? 462204\npredicted swe:  count    462204.000000\nmean          8.038037\nstd           1.831186\nmin           3.096000\n25%           6.958500\n50%           7.514500\n75%           8.742500\nmax          15.575500\nName: predicted_swe, dtype: float64\nIndex(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n[0.04633082 0.049006   0.05922091 0.00830327 0.07838727 0.11662276\n 0.10793431 0.07455769 0.07209162 0.03498797 0.10763383 0.05324669\n 0.03938958 0.03150809 0.03559248 0.02661422 0.03019245 0.02838003]\n",
  "history_begin_time" : 1696831759576,
  "history_end_time" : 1696831788567,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Mbetom6r3pmO",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    data = data.drop(\"data\", axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "today date = 2023-10-09\ntest start date:  2023-02-11\ntest end date:  2023-10-09\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20230910045039.joblib\nFile '/home/chetana/gridmet_test_run/test_data_predicted.csv' has been removed.\nnew_data shape:  (462204, 21)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/Mbetom6r3pmO/interpret_model_results.py\", line 219, in <module>\n    interpret_prediction()\n  File \"/home/chetana/gw-workspace/Mbetom6r3pmO/interpret_model_results.py\", line 159, in interpret_prediction\n    preprocessed_data = preprocess_data(new_data)\n  File \"/home/chetana/gw-workspace/Mbetom6r3pmO/interpret_model_results.py\", line 92, in preprocess_data\n    data = data.drop(\"data\", axis=1)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\", line 5399, in drop\n    return super().drop(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\", line 4505, in drop\n    obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\", line 4546, in _drop_axis\n    new_axis = axis.drop(labels, errors=errors)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 6934, in drop\n    raise KeyError(f\"{list(labels[mask])} not found in axis\")\nKeyError: \"['data'] not found in axis\"\n",
  "history_begin_time" : 1696831673167,
  "history_end_time" : 1696831711248,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "axydqyfsu54",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696830174455,
  "history_end_time" : 1696830174455,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6mekPybAkcX1",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n    explainer = shap.Explainer(current_model)\n    shap_values = explainer.shap_values(\n      preprocessed_data.drop('predicted_swe', axis=1))\n\n    # Summary plot of SHAP values\n    shap.summary_plot(shap_values, preprocessed_data)\n    plt.title('Summary Plot of SHAP Values')\n    plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\n\nStream closed",
  "history_begin_time" : 1696790756418,
  "history_end_time" : 1696790760432,
  "history_notes" : "this might take longer, just wait for a while",
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "FWLQUOMN0ZqJ",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n    explainer = shap.Explainer(current_model)\n    shap_values = explainer.shap_values(\n      preprocessed_data.drop('predicted_swe', axis=1))\n\n    # Summary plot of SHAP values\n    shap.summary_plot(shap_values, preprocessed_data)\n    plt.title('Summary Plot of SHAP Values')\n    plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "Running",
  "history_begin_time" : 1696790750029,
  "history_end_time" : 1696790758767,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "hp982v9whf5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696787542024,
  "history_end_time" : 1696787542024,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "oitwpvzkuto",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696786838353,
  "history_end_time" : 1696786838353,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "zUToy4VDHJK0",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n    explainer = shap.Explainer(current_model)\n    shap_values = explainer.shap_values(\n      preprocessed_data.drop('predicted_swe', axis=1))\n\n    # Summary plot of SHAP values\n    shap.summary_plot(shap_values, preprocessed_data)\n    plt.title('Summary Plot of SHAP Values')\n    plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\n",
  "history_begin_time" : 1696786668527,
  "history_end_time" : 1696786822275,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "zcCTtvvpLkfP",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    partial_dependence_display = PartialDependenceDisplay.from_estimator(\n    current_model, preprocessed_data.drop('predicted_swe', axis=1), features=features_to_plot, grid_resolution=50\n)\n    partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n    partial_dependence_display.figure_.subplots_adjust(top=0.9)\n    partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n    explainer = shap.Explainer(current_model)\n    shap_values = explainer.shap_values(preprocessed_data)\n\n    # Summary plot of SHAP values\n    shap.summary_plot(shap_values, preprocessed_data)\n    plt.title('Summary Plot of SHAP Values')\n    plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "Running",
  "history_begin_time" : 1696782206190,
  "history_end_time" : 1696784080434,
  "history_notes" : "it seems the partial dependence and SHAP take a while to finish",
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "KGzY1cKWR9cK",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns[:-1]\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    partial_dependence_display = PartialDependenceDisplay.from_estimator(\n    current_model, preprocessed_data, features=features_to_plot, grid_resolution=50\n)\n    partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n    partial_dependence_display.figure_.subplots_adjust(top=0.9)\n    partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n    explainer = shap.Explainer(current_model)\n    shap_values = explainer.shap_values(preprocessed_data)\n\n    # Summary plot of SHAP values\n    shap.summary_plot(shap_values, preprocessed_data)\n    plt.title('Summary Plot of SHAP Values')\n    plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\nX has feature names, but ExtraTreesRegressor was fitted without feature names\ntoday date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib\nnew_data shape:  (462204, 21)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib\n    date   lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0  44944  49.0 -125.000    0  ...     -999.0  -999.0    -999.0     -999.0\n1  44944  49.0 -124.964    0  ...     -999.0  -999.0    -999.0     -999.0\n2  44944  49.0 -124.928    0  ...     -999.0  -999.0    -999.0     -999.0\n3  44944  49.0 -124.892    0  ...     -999.0  -999.0    -999.0     -999.0\n4  44944  49.0 -124.856    0  ...     -999.0  -999.0    -999.0     -999.0\n[5 rows x 19 columns]\nhow many rows are there? 462204\npredicted swe:  count    462204.000000\nmean          7.450335\nstd           0.053167\nmin           7.049500\n25%           7.436000\n50%           7.461500\n75%           7.481000\nmax           7.500000\nName: predicted_swe, dtype: float64\nIndex(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n[9.74456085e-01 1.96865689e-04 1.65304045e-04 9.63018037e-04\n 1.12789367e-04 2.03836414e-03 6.29689328e-03 9.65969234e-03\n 7.04791262e-04 7.43298562e-04 9.42858243e-05 3.93990115e-03\n 3.29505772e-04 1.82865198e-04 2.30558544e-05 3.17364141e-05\n 1.75703188e-05 2.29915842e-05 2.09861648e-05]\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/KGzY1cKWR9cK/interpret_model_results.py\", line 211, in <module>\n    interpret_prediction()\n  File \"/home/chetana/gw-workspace/KGzY1cKWR9cK/interpret_model_results.py\", line 179, in interpret_prediction\n    partial_dependence_display = PartialDependenceDisplay.from_estimator(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/inspection/_plot/partial_dependence.py\", line 703, in from_estimator\n    pd_results = Parallel(n_jobs=n_jobs, verbose=verbose)(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 65, in __call__\n    return super().__call__(iterable_with_config)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1863, in __call__\n    return output if self.return_generator else list(output)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1792, in _get_sequential_output\n    res = func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 127, in __call__\n    return self.function(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\", line 211, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/inspection/_partial_dependence.py\", line 706, in partial_dependence\n    averaged_predictions, predictions = _partial_dependence_brute(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/inspection/_partial_dependence.py\", line 310, in _partial_dependence_brute\n    pred = prediction_method(X_eval)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 984, in predict\n    X = self._validate_X_predict(X)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 599, in _validate_X_predict\n    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 625, in _validate_data\n    self._check_n_features(X, reset=reset)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 414, in _check_n_features\n    raise ValueError(\nValueError: X has 20 features, but ExtraTreesRegressor is expecting 19 features as input.\n",
  "history_begin_time" : 1696781870532,
  "history_end_time" : 1696781882681,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "CJB3jHy6O8X4",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    partial_dependence_display = PartialDependenceDisplay.from_estimator(\n    current_model, preprocessed_data, features=features_to_plot, grid_resolution=50\n)\n    partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n    partial_dependence_display.figure_.subplots_adjust(top=0.9)\n    partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n    explainer = shap.Explainer(current_model)\n    shap_values = explainer.shap_values(preprocessed_data)\n\n    # Summary plot of SHAP values\n    shap.summary_plot(shap_values, preprocessed_data)\n    plt.title('Summary Plot of SHAP Values')\n    plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\ntoday date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib\nnew_data shape:  (462204, 21)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib\n    date   lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0  44944  49.0 -125.000    0  ...     -999.0  -999.0    -999.0     -999.0\n1  44944  49.0 -124.964    0  ...     -999.0  -999.0    -999.0     -999.0\n2  44944  49.0 -124.928    0  ...     -999.0  -999.0    -999.0     -999.0\n3  44944  49.0 -124.892    0  ...     -999.0  -999.0    -999.0     -999.0\n4  44944  49.0 -124.856    0  ...     -999.0  -999.0    -999.0     -999.0\n[5 rows x 19 columns]\nhow many rows are there? 462204\npredicted swe:  count    462204.000000\nmean          7.450335\nstd           0.053167\nmin           7.049500\n25%           7.436000\n50%           7.461500\n75%           7.481000\nmax           7.500000\nName: predicted_swe, dtype: float64\nIndex(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'predicted_swe'],\n      dtype='object')\n[9.74456085e-01 1.96865689e-04 1.65304045e-04 9.63018037e-04\n 1.12789367e-04 2.03836414e-03 6.29689328e-03 9.65969234e-03\n 7.04791262e-04 7.43298562e-04 9.42858243e-05 3.93990115e-03\n 3.29505772e-04 1.82865198e-04 2.30558544e-05 3.17364141e-05\n 1.75703188e-05 2.29915842e-05 2.09861648e-05]\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/CJB3jHy6O8X4/interpret_model_results.py\", line 211, in <module>\n    interpret_prediction()\n  File \"/home/chetana/gw-workspace/CJB3jHy6O8X4/interpret_model_results.py\", line 169, in interpret_prediction\n    plt.barh(feature_names, feature_importances)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/matplotlib/pyplot.py\", line 2771, in barh\n    return gca().barh(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/matplotlib/axes/_axes.py\", line 2698, in barh\n    patches = self.bar(x=left, height=height, width=width, bottom=y,\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/matplotlib/__init__.py\", line 1465, in inner\n    return func(ax, *map(sanitize_sequence, args), **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/matplotlib/axes/_axes.py\", line 2457, in bar\n    x, height, width, y, linewidth, hatch = np.broadcast_arrays(\n  File \"<__array_function__ internals>\", line 200, in broadcast_arrays\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/numpy/lib/stride_tricks.py\", line 540, in broadcast_arrays\n    shape = _broadcast_shape(*args)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/numpy/lib/stride_tricks.py\", line 422, in _broadcast_shape\n    b = np.broadcast(*args[:32])\nValueError: shape mismatch: objects cannot be broadcast to a single shape.  Mismatch is between arg 2 with shape (19,) and arg 3 with shape (20,).\n",
  "history_begin_time" : 1696781771056,
  "history_end_time" : 1696781782689,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "XVjeIgZKfaSc",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    partial_dependence_display = PartialDependenceDisplay.from_estimator(\n    current_model, preprocessed_data, features=features_to_plot, grid_resolution=50\n)\n    partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n    partial_dependence_display.figure_.subplots_adjust(top=0.9)\n    partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n    explainer = shap.Explainer(current_model)\n    shap_values = explainer.shap_values(preprocessed_data)\n\n    # Summary plot of SHAP values\n    shap.summary_plot(shap_values, preprocessed_data)\n    plt.title('Summary Plot of SHAP Values')\n    plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "",
  "history_begin_time" : 1696781759328,
  "history_end_time" : 1696781766761,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "fgrz9IZG15Pw",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    partial_dependence_display = PartialDependenceDisplay.from_estimator(\n    current_model, preprocessed_data, features=features_to_plot, grid_resolution=50\n)\n    partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n    partial_dependence_display.figure_.subplots_adjust(top=0.9)\n    partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n    explainer = shap.Explainer(current_model)\n    shap_values = explainer.shap_values(preprocessed_data)\n\n    # Summary plot of SHAP values\n    shap.summary_plot(shap_values, preprocessed_data)\n    plt.title('Summary Plot of SHAP Values')\n    plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\ntoday date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib\nnew_data shape:  (462204, 21)\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib\n    date   lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0  44944  49.0 -125.000    0  ...     -999.0  -999.0    -999.0     -999.0\n1  44944  49.0 -124.964    0  ...     -999.0  -999.0    -999.0     -999.0\n2  44944  49.0 -124.928    0  ...     -999.0  -999.0    -999.0     -999.0\n3  44944  49.0 -124.892    0  ...     -999.0  -999.0    -999.0     -999.0\n4  44944  49.0 -124.856    0  ...     -999.0  -999.0    -999.0     -999.0\n[5 rows x 19 columns]\nhow many rows are there? 462204\npredicted swe:  count    462204.000000\nmean          7.450335\nstd           0.053167\nmin           7.049500\n25%           7.436000\n50%           7.461500\n75%           7.481000\nmax           7.500000\nName: predicted_swe, dtype: float64\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/fgrz9IZG15Pw/interpret_model_results.py\", line 209, in <module>\n    interpret_prediction()\n  File \"/home/chetana/gw-workspace/fgrz9IZG15Pw/interpret_model_results.py\", line 167, in interpret_prediction\n    plt.barh(feature_names, feature_importances)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/matplotlib/pyplot.py\", line 2771, in barh\n    return gca().barh(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/matplotlib/axes/_axes.py\", line 2698, in barh\n    patches = self.bar(x=left, height=height, width=width, bottom=y,\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/matplotlib/__init__.py\", line 1465, in inner\n    return func(ax, *map(sanitize_sequence, args), **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/matplotlib/axes/_axes.py\", line 2457, in bar\n    x, height, width, y, linewidth, hatch = np.broadcast_arrays(\n  File \"<__array_function__ internals>\", line 200, in broadcast_arrays\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/numpy/lib/stride_tricks.py\", line 540, in broadcast_arrays\n    shape = _broadcast_shape(*args)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/numpy/lib/stride_tricks.py\", line 422, in _broadcast_shape\n    b = np.broadcast(*args[:32])\nValueError: shape mismatch: objects cannot be broadcast to a single shape.  Mismatch is between arg 2 with shape (19,) and arg 3 with shape (20,).\n",
  "history_begin_time" : 1696778782807,
  "history_end_time" : 1696778794790,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "hYXuwqzSwFlM",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    partial_dependence_display = PartialDependenceDisplay.from_estimator(\n    current_model, predicted_data, features=features_to_plot, grid_resolution=50\n)\n    partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n    partial_dependence_display.figure_.subplots_adjust(top=0.9)\n    partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n    explainer = shap.Explainer(xgb_model)\n    shap_values = explainer.shap_values(X)\n\n    # Summary plot of SHAP values\n    shap.summary_plot(shap_values, X)\n    plt.title('Summary Plot of SHAP Values')\n    plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\nX has feature names, but ExtraTreesRegressor was fitted without feature names\ntoday date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib\nnew_data shape:  (462204, 21)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib\n    date   lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0  44944  49.0 -125.000    0  ...        NaN     NaN       NaN        NaN\n1  44944  49.0 -124.964    0  ...        NaN     NaN       NaN        NaN\n2  44944  49.0 -124.928    0  ...        NaN     NaN       NaN        NaN\n3  44944  49.0 -124.892    0  ...        NaN     NaN       NaN        NaN\n4  44944  49.0 -124.856    0  ...        NaN     NaN       NaN        NaN\n[5 rows x 19 columns]\nhow many rows are there? 462204\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\npredicted swe:  count    462204.000000\nmean          7.450335\nstd           0.053167\nmin           7.049500\n25%           7.436000\n50%           7.461500\n75%           7.481000\nmax           7.500000\nName: predicted_swe, dtype: float64\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/hYXuwqzSwFlM/interpret_model_results.py\", line 209, in <module>\n    interpret_prediction()\n  File \"/home/chetana/gw-workspace/hYXuwqzSwFlM/interpret_model_results.py\", line 177, in interpret_prediction\n    partial_dependence_display = PartialDependenceDisplay.from_estimator(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/inspection/_plot/partial_dependence.py\", line 703, in from_estimator\n    pd_results = Parallel(n_jobs=n_jobs, verbose=verbose)(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 65, in __call__\n    return super().__call__(iterable_with_config)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1863, in __call__\n    return output if self.return_generator else list(output)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1792, in _get_sequential_output\n    res = func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 127, in __call__\n    return self.function(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\", line 211, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/inspection/_partial_dependence.py\", line 706, in partial_dependence\n    averaged_predictions, predictions = _partial_dependence_brute(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/inspection/_partial_dependence.py\", line 310, in _partial_dependence_brute\n    pred = prediction_method(X_eval)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 984, in predict\n    X = self._validate_X_predict(X)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 599, in _validate_X_predict\n    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 625, in _validate_data\n    self._check_n_features(X, reset=reset)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 414, in _check_n_features\n    raise ValueError(\nValueError: X has 20 features, but ExtraTreesRegressor is expecting 19 features as input.\n",
  "history_begin_time" : 1696778641671,
  "history_end_time" : 1696778653978,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "8fYDWTFfeDWp",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    partial_dependence_display = PartialDependenceDisplay.from_estimator(\n    current_model, preprocessed_data, features=features_to_plot, grid_resolution=50\n)\n    partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n    partial_dependence_display.figure_.subplots_adjust(top=0.9)\n    partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n    explainer = shap.Explainer(xgb_model)\n    shap_values = explainer.shap_values(X)\n\n    # Summary plot of SHAP values\n    shap.summary_plot(shap_values, X)\n    plt.title('Summary Plot of SHAP Values')\n    plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\nX has feature names, but ExtraTreesRegressor was fitted without feature names\ntoday date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib\nnew_data shape:  (462204, 21)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib\n    date   lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0  44944  49.0 -125.000    0  ...        NaN     NaN       NaN        NaN\n1  44944  49.0 -124.964    0  ...        NaN     NaN       NaN        NaN\n2  44944  49.0 -124.928    0  ...        NaN     NaN       NaN        NaN\n3  44944  49.0 -124.892    0  ...        NaN     NaN       NaN        NaN\n4  44944  49.0 -124.856    0  ...        NaN     NaN       NaN        NaN\n[5 rows x 19 columns]\nhow many rows are there? 462204\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\npredicted swe:  count    462204.000000\nmean          7.450335\nstd           0.053167\nmin           7.049500\n25%           7.436000\n50%           7.461500\n75%           7.481000\nmax           7.500000\nName: predicted_swe, dtype: float64\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/8fYDWTFfeDWp/interpret_model_results.py\", line 209, in <module>\n    interpret_prediction()\n  File \"/home/chetana/gw-workspace/8fYDWTFfeDWp/interpret_model_results.py\", line 177, in interpret_prediction\n    partial_dependence_display = PartialDependenceDisplay.from_estimator(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/inspection/_plot/partial_dependence.py\", line 703, in from_estimator\n    pd_results = Parallel(n_jobs=n_jobs, verbose=verbose)(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 65, in __call__\n    return super().__call__(iterable_with_config)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1863, in __call__\n    return output if self.return_generator else list(output)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1792, in _get_sequential_output\n    res = func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 127, in __call__\n    return self.function(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\", line 211, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/inspection/_partial_dependence.py\", line 706, in partial_dependence\n    averaged_predictions, predictions = _partial_dependence_brute(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/inspection/_partial_dependence.py\", line 310, in _partial_dependence_brute\n    pred = prediction_method(X_eval)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 984, in predict\n    X = self._validate_X_predict(X)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 599, in _validate_X_predict\n    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 604, in _validate_data\n    out = check_array(X, input_name=\"X\", **check_params)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 959, in check_array\n    _assert_all_finite(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 124, in _assert_all_finite\n    _assert_all_finite_element_wise(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 173, in _assert_all_finite_element_wise\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nExtraTreesRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
  "history_begin_time" : 1696778602441,
  "history_end_time" : 1696778614773,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "jzu0rNJ5BTNs",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence, plot_partial_dependence\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    partial_dependence_display = PartialDependenceDisplay.from_estimator(\n    current_model, preprocessed_data, features=features_to_plot, grid_resolution=50\n)\n    partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n    partial_dependence_display.figure_.subplots_adjust(top=0.9)\n    partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n    explainer = shap.Explainer(xgb_model)\n    shap_values = explainer.shap_values(X)\n\n    # Summary plot of SHAP values\n    shap.summary_plot(shap_values, X)\n    plt.title('Summary Plot of SHAP Values')\n    plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "today date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/jzu0rNJ5BTNs/interpret_model_results.py\", line 11, in <module>\n    from sklearn.inspection import partial_dependence, plot_partial_dependence\nImportError: cannot import name 'plot_partial_dependence' from 'sklearn.inspection' (/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/inspection/__init__.py)\n",
  "history_begin_time" : 1696778557651,
  "history_end_time" : 1696778559039,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "qUBDUU7jyKda",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence, plot_partial_dependence\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = [0, 1]\n    plot_partial_dependence(xgb_model, X, features=features_to_plot, grid_resolution=50)\n    plt.suptitle('Partial Dependence Plots')\n    plt.subplots_adjust(top=0.9)\n    plt.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n    explainer = shap.Explainer(xgb_model)\n    shap_values = explainer.shap_values(X)\n\n    # Summary plot of SHAP values\n    shap.summary_plot(shap_values, X)\n    plt.title('Summary Plot of SHAP Values')\n    plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "today date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/qUBDUU7jyKda/interpret_model_results.py\", line 11, in <module>\n    from sklearn.inspection import partial_dependence, plot_partial_dependence\nImportError: cannot import name 'plot_partial_dependence' from 'sklearn.inspection' (/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/inspection/__init__.py)\n",
  "history_begin_time" : 1696778298447,
  "history_end_time" : 1696778299808,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "JW0Hch1zUB5K",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = [0, 1]\n    plot_partial_dependence(xgb_model, X, features=features_to_plot, grid_resolution=50)\n    plt.suptitle('Partial Dependence Plots')\n    plt.subplots_adjust(top=0.9)\n    plt.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n    explainer = shap.Explainer(xgb_model)\n    shap_values = explainer.shap_values(X)\n\n    # Summary plot of SHAP values\n    shap.summary_plot(shap_values, X)\n    plt.title('Summary Plot of SHAP Values')\n    plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "X has feature names, but ExtraTreesRegressor was fitted without feature names\ntoday date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib\nnew_data shape:  (462204, 21)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib\n    date   lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0  44944  49.0 -125.000    0  ...        NaN     NaN       NaN        NaN\n1  44944  49.0 -124.964    0  ...        NaN     NaN       NaN        NaN\n2  44944  49.0 -124.928    0  ...        NaN     NaN       NaN        NaN\n3  44944  49.0 -124.892    0  ...        NaN     NaN       NaN        NaN\n4  44944  49.0 -124.856    0  ...        NaN     NaN       NaN        NaN\n[5 rows x 19 columns]\nhow many rows are there? 462204\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\npredicted swe:  count    462204.000000\nmean          7.450335\nstd           0.053167\nmin           7.049500\n25%           7.436000\n50%           7.461500\n75%           7.481000\nmax           7.500000\nName: predicted_swe, dtype: float64\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/JW0Hch1zUB5K/interpret_model_results.py\", line 207, in <module>\n    interpret_prediction()\n  File \"/home/chetana/gw-workspace/JW0Hch1zUB5K/interpret_model_results.py\", line 177, in interpret_prediction\n    plot_partial_dependence(xgb_model, X, features=features_to_plot, grid_resolution=50)\nNameError: name 'plot_partial_dependence' is not defined\n",
  "history_begin_time" : 1696778203897,
  "history_end_time" : 1696778229215,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "Xl2NxXhvVLeR",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = [0, 1]\n    plot_partial_dependence(xgb_model, X, features=features_to_plot, grid_resolution=50)\n    plt.suptitle('Partial Dependence Plots')\n    plt.subplots_adjust(top=0.9)\n    plt.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n    explainer = shap.Explainer(xgb_model)\n    shap_values = explainer.shap_values(X)\n\n    # Summary plot of SHAP values\n    shap.summary_plot(shap_values, X)\n    plt.title('Summary Plot of SHAP Values')\n    plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "today date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/Xl2NxXhvVLeR/interpret_model_results.py\", line 12, in <module>\n    import shap\nModuleNotFoundError: No module named 'shap'\n",
  "history_begin_time" : 1696778040605,
  "history_end_time" : 1696778042051,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "I4Z5NrNRma7k",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = current_model.feature_importances_\n    feature_names = preprocessed_data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = [0, 1]\n    plot_partial_dependence(xgb_model, X, features=features_to_plot, grid_resolution=50)\n    plt.suptitle('Partial Dependence Plots')\n    plt.subplots_adjust(top=0.9)\n    plt.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n    explainer = shap.Explainer(xgb_model)\n    shap_values = explainer.shap_values(X)\n\n    # Summary plot of SHAP values\n    shap.summary_plot(shap_values, X)\n    plt.title('Summary Plot of SHAP Values')\n    plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but ExtraTreesRegressor was fitted without feature names\n  warnings.warn(\ntoday date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib\nnew_data shape:  (462204, 21)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib\n    date   lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0  44944  49.0 -125.000    0  ...        NaN     NaN       NaN        NaN\n1  44944  49.0 -124.964    0  ...        NaN     NaN       NaN        NaN\n2  44944  49.0 -124.928    0  ...        NaN     NaN       NaN        NaN\n3  44944  49.0 -124.892    0  ...        NaN     NaN       NaN        NaN\n4  44944  49.0 -124.856    0  ...        NaN     NaN       NaN        NaN\n[5 rows x 19 columns]\nhow many rows are there? 462204\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\npredicted swe:  count    462204.000000\nmean          7.450335\nstd           0.053167\nmin           7.049500\n25%           7.436000\n50%           7.461500\n75%           7.481000\nmax           7.500000\nName: predicted_swe, dtype: float64\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/I4Z5NrNRma7k/interpret_model_results.py\", line 205, in <module>\n    interpret_prediction()\n  File \"/home/chetana/gw-workspace/I4Z5NrNRma7k/interpret_model_results.py\", line 164, in interpret_prediction\n    plt.figure(figsize=(10, 6))\nNameError: name 'plt' is not defined\n",
  "history_begin_time" : 1696778004068,
  "history_end_time" : 1696778014008,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "Si0ebmlpr61i",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import partial_dependence\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = xgb_model.feature_importances_\n    feature_names = X.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = [0, 1]\n    plot_partial_dependence(xgb_model, X, features=features_to_plot, grid_resolution=50)\n    plt.suptitle('Partial Dependence Plots')\n    plt.subplots_adjust(top=0.9)\n    plt.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n    explainer = shap.Explainer(xgb_model)\n    shap_values = explainer.shap_values(X)\n\n    # Summary plot of SHAP values\n    shap.summary_plot(shap_values, X)\n    plt.title('Summary Plot of SHAP Values')\n    plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but ExtraTreesRegressor was fitted without feature names\n  warnings.warn(\ntoday date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib\nFile '/home/chetana/gridmet_test_run/test_data_predicted.csv' has been removed.\nnew_data shape:  (462204, 21)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib\n    date   lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0  44944  49.0 -125.000    0  ...        NaN     NaN       NaN        NaN\n1  44944  49.0 -124.964    0  ...        NaN     NaN       NaN        NaN\n2  44944  49.0 -124.928    0  ...        NaN     NaN       NaN        NaN\n3  44944  49.0 -124.892    0  ...        NaN     NaN       NaN        NaN\n4  44944  49.0 -124.856    0  ...        NaN     NaN       NaN        NaN\n[5 rows x 19 columns]\nhow many rows are there? 462204\nhow many rows are left? 462204\ndata.shape:  (462204, 19)\npredicted swe:  count    462204.000000\nmean          7.450335\nstd           0.053167\nmin           7.049500\n25%           7.436000\n50%           7.461500\n75%           7.481000\nmax           7.500000\nName: predicted_swe, dtype: float64\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/Si0ebmlpr61i/interpret_model_results.py\", line 205, in <module>\n    interpret_prediction()\n  File \"/home/chetana/gw-workspace/Si0ebmlpr61i/interpret_model_results.py\", line 160, in interpret_prediction\n    feature_importances = xgb_model.feature_importances_\nNameError: name 'xgb_model' is not defined\n",
  "history_begin_time" : 1696777949033,
  "history_end_time" : 1696777959385,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "QnKDc41oXamN",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.ensemble.partial_dependence import plot_partial_dependence\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = xgb_model.feature_importances_\n    feature_names = X.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = [0, 1]\n    plot_partial_dependence(xgb_model, X, features=features_to_plot, grid_resolution=50)\n    plt.suptitle('Partial Dependence Plots')\n    plt.subplots_adjust(top=0.9)\n    plt.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n    explainer = shap.Explainer(xgb_model)\n    shap_values = explainer.shap_values(X)\n\n    # Summary plot of SHAP values\n    shap.summary_plot(shap_values, X)\n    plt.title('Summary Plot of SHAP Values')\n    plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "today date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/QnKDc41oXamN/interpret_model_results.py\", line 11, in <module>\n    from sklearn.ensemble.partial_dependence import plot_partial_dependence\nModuleNotFoundError: No module named 'sklearn.ensemble.partial_dependence'\n",
  "history_begin_time" : 1696777895241,
  "history_end_time" : 1696777896744,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "zdY1K7siqP2u",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nfrom sklearn.inspection import plot_partial_dependence\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20232409194643.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = xgb_model.feature_importances_\n    feature_names = X.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_{test_start_date}.png')\n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = [0, 1]\n    plot_partial_dependence(xgb_model, X, features=features_to_plot, grid_resolution=50)\n    plt.suptitle('Partial Dependence Plots')\n    plt.subplots_adjust(top=0.9)\n    plt.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n    explainer = shap.Explainer(xgb_model)\n    shap_values = explainer.shap_values(X)\n\n    # Summary plot of SHAP values\n    shap.summary_plot(shap_values, X)\n    plt.title('Summary Plot of SHAP Values')\n    plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\ninterpret_prediction()\n",
  "history_output" : "today date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/zdY1K7siqP2u/interpret_model_results.py\", line 11, in <module>\n    from sklearn.inspection import plot_partial_dependence\nImportError: cannot import name 'plot_partial_dependence' from 'sklearn.inspection' (/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/inspection/__init__.py)\n",
  "history_begin_time" : 1696777855410,
  "history_end_time" : 1696777857429,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "xgwybkulu05",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696771780959,
  "history_end_time" : 1696771780959,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "s2se0v2deo5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696602943971,
  "history_end_time" : 1696602943971,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "g4wg0li5w9e",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696432484364,
  "history_end_time" : 1696432484364,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "kx7x5gxn5ir",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696432299785,
  "history_end_time" : 1696432482238,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "zz391mhmpye",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1699806085200,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "w6x8d98e6rg",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1700204245687,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "vj0h21o1qj2",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1700462913693,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},]
