[{
  "history_id" : "2a5vn2k0ht3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717473609621,
  "history_end_time" : 1717473609621,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "cc872pi03c3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717387237114,
  "history_end_time" : 1717387237114,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8ssbfj1qjw6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717300810099,
  "history_end_time" : 1717300810099,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "73nnzcez5px",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717214410194,
  "history_end_time" : 1717214410194,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "graavzew8dw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717128010125,
  "history_end_time" : 1717128010125,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xyf274d9e8f",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717041609851,
  "history_end_time" : 1717041609851,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0pvw278pntu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716955209867,
  "history_end_time" : 1716955209867,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "zk0013vp9vt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716868810739,
  "history_end_time" : 1716868810739,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "da8pzg3cq6s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716817415333,
  "history_end_time" : 1716817415333,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "nd709jla3qk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716797560620,
  "history_end_time" : 1716797560620,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "t2a9rda7el9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716782411138,
  "history_end_time" : 1716782411138,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "g5ujdt55t2c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716777609612,
  "history_end_time" : 1716777609612,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8k83mk3xaek",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716774915343,
  "history_end_time" : 1716774915343,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "x7eqk34j1oh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716754065190,
  "history_end_time" : 1716754065190,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "eq6jixbrh9q",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716751217902,
  "history_end_time" : 1716751217902,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tj0vswoy64s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716750881058,
  "history_end_time" : 1716751199716,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "41n8bhgzhjt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716746640541,
  "history_end_time" : 1716746640541,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "x34evoiz7lt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716696010681,
  "history_end_time" : 1716696010681,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "drj743uu12x",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716618219019,
  "history_end_time" : 1716618219019,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "lnivpbausj8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716609611046,
  "history_end_time" : 1716609611046,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "m4mdk89b7do",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716563756541,
  "history_end_time" : 1716563756541,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "nco4kqql41b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716524704898,
  "history_end_time" : 1716524704898,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "f3rkpf18xn9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716523210955,
  "history_end_time" : 1716523210955,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3iecob26ouw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716521650224,
  "history_end_time" : 1716521650224,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "g67ohnwg0dh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716519590207,
  "history_end_time" : 1716519590207,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tj0l7b2eark",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716513818332,
  "history_end_time" : 1716513818332,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "fwpr2qwethl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716471261932,
  "history_end_time" : 1716471261932,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "hb48j7tnucz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716436814138,
  "history_end_time" : 1716799400438,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "los0fc06gvl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716422698064,
  "history_end_time" : 1716422827901,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ksgk3ylylbl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716422518601,
  "history_end_time" : 1716422518601,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9wtdw15bfct",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716418166722,
  "history_end_time" : 1716418166722,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "5ns2tsovy3o",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716350423469,
  "history_end_time" : 1716418192213,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "5pjt6yycsbc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716264024281,
  "history_end_time" : 1716418192345,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "r269vaj3qe0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716215991412,
  "history_end_time" : 1716220261656,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "y57zilyk9ua",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716184844723,
  "history_end_time" : 1716216036779,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "6s4gtgf7ax9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716177613807,
  "history_end_time" : 1716418192100,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "psgrn6yyadg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716091214021,
  "history_end_time" : 1716418191184,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "p7lopjtygh2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716062449141,
  "history_end_time" : 1716124523824,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "e0hhb2zgkh0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716043609986,
  "history_end_time" : 1716057482685,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "aagebu4d2li",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716035693351,
  "history_end_time" : 1716036846661,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "z128hubz461",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716004809901,
  "history_end_time" : 1716418194181,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ef6g0b6bhrt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715982779442,
  "history_end_time" : 1716035691975,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ik9jqqmhoav",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715980750504,
  "history_end_time" : 1715982778744,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "da0tpu38xz9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715972396935,
  "history_end_time" : 1715980749622,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "whgouzg0psf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715967265181,
  "history_end_time" : 1715972396080,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "0yy1mcalqie",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715960695198,
  "history_end_time" : 1715967264437,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "sk9pwk0047t",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715957532886,
  "history_end_time" : 1715960694157,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "wjxea6pyy8t",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715952722394,
  "history_end_time" : 1715957517341,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "id2tp8jr7i5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715939821439,
  "history_end_time" : 1716418196282,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "crcr4ofdj34",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715937541743,
  "history_end_time" : 1715937541743,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "bohiupure9g",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715937326829,
  "history_end_time" : 1715937540114,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "gux9u2ozvhl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715927549695,
  "history_end_time" : 1715927549695,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ccfb9n4lrdu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715918413323,
  "history_end_time" : 1716418198300,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "fisu4lznpdi",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715832029683,
  "history_end_time" : 1716418197828,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ao6dcb62kzu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715799528273,
  "history_end_time" : 1716418199552,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "a6hr4yt6am5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715798137986,
  "history_end_time" : 1715799518107,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "jnlkxjwp7eg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715780310582,
  "history_end_time" : 1715780310582,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8zkjvxsjuvh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715755079573,
  "history_end_time" : 1715780309807,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "udqps0yxtoq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715745613657,
  "history_end_time" : 1716418200333,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "qw8oyyqk9tj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715670775304,
  "history_end_time" : 1715670775304,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "osa0fi3wb18",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715668379772,
  "history_end_time" : 1715668379772,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "lv900gc7mep",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715660606896,
  "history_end_time" : 1715660606896,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "j7yorwbf6kj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715659214466,
  "history_end_time" : 1716418201743,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "liilyh719vm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715657268365,
  "history_end_time" : 1715657268365,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "hsbcu7vj34k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715654494311,
  "history_end_time" : 1715654494311,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "i24mza2jgxw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715651069867,
  "history_end_time" : 1715654493199,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "7oide5a6n04",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715635694063,
  "history_end_time" : 1715635694063,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ajlzbmgd0xj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715631991566,
  "history_end_time" : 1715631991566,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "71ibkcyw0kb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715593112730,
  "history_end_time" : 1715593112730,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "x2sj7r8fyhd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715588598027,
  "history_end_time" : 1715588598027,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3yxh0e9zhgq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715580981943,
  "history_end_time" : 1715588593039,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "12l9gdxjtwf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715577232556,
  "history_end_time" : 1715577232556,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "5mssji1jsh0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715572814027,
  "history_end_time" : 1716418204196,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "9ramr12zzfm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715569180291,
  "history_end_time" : 1715569180291,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "icghwdx051l",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715566644972,
  "history_end_time" : 1715569179349,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "jor1o3qx8ua",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715564158995,
  "history_end_time" : 1715566643896,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "2vbpb1d1yar",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715553269423,
  "history_end_time" : 1715553269423,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "teotnyqd3e3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715551757447,
  "history_end_time" : 1716418204469,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "gjw6dv843lp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715550036509,
  "history_end_time" : 1715551587887,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "rdiAXGNpdeJF",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir, homedir\nimport pandas as pd\nimport time\n\nworking_dir = work_dir\nfinal_output_name = \"final_merged_data_4yrs_snotel_and_ghcnd_stations.csv\"\nchunk_size = '10MB'  # You can adjust this chunk size based on your hardware and data size\namsr_file = f'{working_dir}/all_training_points_in_westus.csv_amsr_dask_all_training_ponits.csv'\nsnotel_file = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_swe_restored_dask_all_vars.csv'\nghcnd_file = f'{working_dir}/active_station_only_list.csv_all_vars_masked_non_snow.csv'\nall_station_obs_file = f'{working_dir}/snotel_ghcnd_all_obs.csv'\ngridmet_file = f'{working_dir}/training_all_point_gridmet_with_snotel_ghcnd.csv'\nterrain_file = f'{working_dir}/all_training_points_with_ghcnd_in_westus.csv_terrain_4km_grid_shift.csv'\nfsca_file = f'{homedir}/fsca/fsca_final_training_all.csv'\nfinal_final_output_file = f'{work_dir}/{final_output_name}'\n\n\ndef merge_snotel_ghcnd_together():\n    snotel_df = pd.read_csv(snotel_file)\n    ghcnd_df = pd.read_csv(ghcnd_file)\n    print(snotel_df.columns)\n    print(ghcnd_df.columns)\n    ghcnd_df = ghcnd_df.rename(columns={'STATION': 'station_name',\n                                       'DATE': 'date',\n                                       'LATITUDE': 'lat',\n                                       'LONGITUDE': 'lon',\n                                       'SNWD': 'snow_depth',})\n    df_combined = pd.concat([snotel_df, ghcnd_df], axis=0, ignore_index=True)\n    df_combined.to_csv(all_station_obs_file, index=False)\n    print(f\"All snotel ang ghcnd are saved to {all_station_obs_file}\")\n    \n\ndef merge_all_data_together():\n    \n    if os.path.exists(final_final_output_file):\n      print(f\"The file '{final_final_output_file}' exists. Skipping\")\n      return final_final_output_file\n    \n    # merge the snotel and ghcnd together first\n    \n      \n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(amsr_file, blocksize=chunk_size)\n    print(\"amsr.columns = \", amsr.columns)\n    ground_truth = dd.read_csv(all_station_obs_file, blocksize=chunk_size)\n    print(\"ground_truth.columns = \", ground_truth.columns)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(gridmet_file, blocksize=chunk_size)\n    gridmet = gridmet.drop(columns=[\"Unnamed: 0\"])\n    print(\"gridmet.columns = \", gridmet.columns)\n    terrain = dd.read_csv(terrain_file, blocksize=chunk_size)\n    terrain = terrain.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    terrain = terrain[[\"lat\", \"lon\", 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness']]\n    print(\"terrain.columns = \", terrain.columns)\n    snowcover = dd.read_csv(fsca_file, blocksize=chunk_size)\n    snowcover = snowcover.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    print(\"snowcover.columns = \", snowcover.columns)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    ground_truth = ground_truth.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n    print(\"all the dataframes are partitioned\")\n\n    # Merge DataFrames based on specified columns\n    print(\"start to merge amsr and ground_truth\")\n    merged_df = dd.merge(amsr, ground_truth, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_ground_truth.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge gridmet\")\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_gridmet.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge terrain\")\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge snowcover\")\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f'Merge completed. {output_file}')\n\ndef cleanup_dataframe():\n    \"\"\"\n    Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    \"\"\"\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    dtype = {'station_name': 'object'}  # 'object' dtype represents strings\n    df = dd.read_csv(final_final_output_file, dtype=dtype)\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(final_final_output_file, single_file=True, index=False)\n    print('Data cleaning completed.')\n    return final_final_output_file\n\n  \ndef sort_training_data(input_training_csv, sorted_training_csv):\n    # Read Dask DataFrame from CSV with increased blocksize and assuming missing data\n    dtype = {'station_name': 'object'}  # 'object' dtype represents strings\n    ddf = dd.read_csv(input_training_csv, assume_missing=True, blocksize='10MB', dtype=dtype)\n\n    # Persist the Dask DataFrame in memory\n    ddf = ddf.persist()\n\n    # Sort Dask DataFrame by three columns: date, lat, and Lon\n    sorted_ddf = ddf.sort_values(by=['date', 'lat', 'lon'])\n\n    # Save the sorted Dask DataFrame to a new CSV file\n    sorted_ddf.to_csv(sorted_training_csv, index=False, single_file=True)\n    print(f\"sorted training data is saved to {sorted_training_csv}\")\n  \nif __name__ == \"__main__\":\n  \n#     merge_snotel_ghcnd_together()\n  \n#     merge_all_data_together()\n#     cleanup_dataframe()\n#     final_final_output_file = f'{work_dir}/{final_output_name}'\n#     sort_training_data(final_final_output_file, f'{work_dir}/{final_output_name}_sorted.csv')\n    \n    start_time = time.time()\n    \n    merge_all_data_together()\n    print(f\"Time taken for merge_all_data_together: {time.time() - start_time} seconds\")\n    \n    start_time = time.time()\n    cleanup_dataframe()\n    print(f\"Time taken for cleanup_dataframe: {time.time() - start_time} seconds\")\n    \n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    sorted_output_file = f'{work_dir}/{final_output_name}_sorted.csv'\n    \n    start_time = time.time()\n    sort_training_data(final_final_output_file, sorted_output_file)\n    print(f\"Time taken for sort_training_data: {time.time() - start_time} seconds\")\n    ",
  "history_output" : "today date = 2024-05-12\ntest start date:  2024-02-07\ntest end date:  2023-10-11\n/home/chetana\namsr.columns =  Index(['date', 'lat', 'lon', 'AMSR_SWE'], dtype='object')\nground_truth.columns =  Index(['station_name', 'date', 'lat', 'lon', 'swe_value', 'change_in_swe_inch',\n       'snow_depth', 'air_temperature_observed_f'],\n      dtype='object')\ngridmet.columns =  Index(['day', 'lon', 'lat', 'precipitation_amount', 'relative_humidity_rmin',\n       'potential_evapotranspiration', 'air_temperature_tmmx',\n       'relative_humidity_rmax', 'mean_vapor_pressure_deficit',\n       'air_temperature_tmmn', 'wind_speed'],\n      dtype='object')\nterrain.columns =  Index(['lat', 'lon', 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness',\n       'Eastness'],\n      dtype='object')\nsnowcover.columns =  Index(['date', 'lat', 'lon', 'fsca'], dtype='object')\nall the dataframes are partitioned\nstart to merge amsr and ground_truth\nintermediate file saved to /home/chetana/gridmet_test_run/final_merged_data_4yrs_snotel_and_ghcnd_stations.csv_ground_truth.csv\nstart to merge gridmet\nintermediate file saved to /home/chetana/gridmet_test_run/final_merged_data_4yrs_snotel_and_ghcnd_stations.csv_gridmet.csv\nstart to merge terrain\nintermediate file saved to /home/chetana/gridmet_test_run/final_merged_data_4yrs_snotel_and_ghcnd_stations.csv_terrain.csv\nstart to merge snowcover\nintermediate file saved to /home/chetana/gridmet_test_run/final_merged_data_4yrs_snotel_and_ghcnd_stations.csv_snow_cover.csv\nMerge completed. /home/chetana/gridmet_test_run/final_merged_data_4yrs_snotel_and_ghcnd_stations.csv\nTime taken for merge_all_data_together: 6032.321589708328 seconds\nData cleaning completed.\nTime taken for cleanup_dataframe: 608.7538442611694 seconds\nsorted training data is saved to /home/chetana/gridmet_test_run/final_merged_data_4yrs_snotel_and_ghcnd_stations.csv_sorted.csv\nTime taken for sort_training_data: 841.3962206840515 seconds\n",
  "history_begin_time" : 1715548783155,
  "history_end_time" : 1715556274953,
  "history_notes" : "fully integrated snotel and ghcnd",
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ly5jpspxxho",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715548411106,
  "history_end_time" : 1715551556724,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "hx7MEO3foShd",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir, homedir\nimport pandas as pd\n\nworking_dir = work_dir\nfinal_output_name = \"final_merged_data_4yrs_snotel_and_ghcnd_stations.csv\"\nchunk_size = '10MB'  # You can adjust this chunk size based on your hardware and data size\namsr_file = f'{working_dir}/all_training_points_in_westus.csv_amsr_dask_all_training_ponits.csv'\nsnotel_file = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_swe_restored_dask_all_vars.csv'\nghcnd_file = f'{working_dir}/active_station_only_list.csv_all_vars_masked_non_snow.csv'\nall_station_obs_file = f'{working_dir}/snotel_ghcnd_all_obs.csv'\ngridmet_file = f'{working_dir}/training_all_point_gridmet_with_snotel_ghcnd.csv'\nterrain_file = f'{working_dir}/all_training_points_with_ghcnd_in_westus.csv_terrain_4km_grid_shift.csv'\nfsca_file = f'{homedir}/fsca/fsca_final_training_all.csv'\nfinal_final_output_file = f'{work_dir}/{final_output_name}'\n\n\ndef merge_snotel_ghcnd_together():\n    snotel_df = pd.read_csv(snotel_file)\n    ghcnd_df = pd.read_csv(ghcnd_file)\n    print(snotel_df.columns)\n    print(ghcnd_df.columns)\n    ghcnd_df = ghcnd_df.rename(columns={'STATION': 'station_name',\n                                       'DATE': 'date',\n                                       'LATITUDE': 'lat',\n                                       'LONGITUDE': 'lon',\n                                       'SNWD': 'snow_depth',})\n    df_combined = pd.concat([snotel_df, ghcnd_df], axis=0, ignore_index=True)\n    df_combined.to_csv(all_station_obs_file, index=False)\n    print(f\"All snotel ang ghcnd are saved to {all_station_obs_file}\")\n    \n\ndef merge_all_data_together():\n    \n    if os.path.exists(final_final_output_file):\n      print(f\"The file '{final_final_output_file}' exists. Skipping\")\n      return final_final_output_file\n    \n    # merge the snotel and ghcnd together first\n    \n      \n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(amsr_file, blocksize=chunk_size)\n    print(\"amsr.columns = \", amsr.columns)\n    ground_truth = dd.read_csv(all_station_obs_file, blocksize=chunk_size)\n    print(\"ground_truth.columns = \", ground_truth.columns)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(gridmet_file, blocksize=chunk_size)\n    gridmet = gridmet.drop(columns=[\"Unnamed: 0\"])\n    print(\"gridmet.columns = \", gridmet.columns)\n    terrain = dd.read_csv(terrain_file, blocksize=chunk_size)\n    terrain = terrain.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    terrain = terrain[[\"lat\", \"lon\", 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness']]\n    print(\"terrain.columns = \", terrain.columns)\n    snowcover = dd.read_csv(fsca_file, blocksize=chunk_size)\n    snowcover = snowcover.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    print(\"snowcover.columns = \", snowcover.columns)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    ground_truth = ground_truth.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n    print(\"all the dataframes are partitioned\")\n\n    # Merge DataFrames based on specified columns\n    print(\"start to merge amsr and ground_truth\")\n    merged_df = dd.merge(amsr, ground_truth, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_ground_truth.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge gridmet\")\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_gridmet.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge terrain\")\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge snowcover\")\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f'Merge completed. {output_file}')\n\ndef cleanup_dataframe():\n    \"\"\"\n    Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    \"\"\"\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    dtype = {'station_name': 'object'}  # 'object' dtype represents strings\n    df = dd.read_csv(final_final_output_file, dtype=dtype)\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(final_final_output_file, single_file=True, index=False)\n    print('Data cleaning completed.')\n    return final_final_output_file\n\n  \ndef sort_training_data(input_training_csv, sorted_training_csv):\n    # Read Dask DataFrame from CSV with increased blocksize and assuming missing data\n    dtype = {'station_name': 'object'}  # 'object' dtype represents strings\n    ddf = dd.read_csv(input_training_csv, assume_missing=True, blocksize='10MB', dtype=dtype)\n\n    # Persist the Dask DataFrame in memory\n    ddf = ddf.persist()\n\n    # Sort Dask DataFrame by three columns: date, lat, and Lon\n    sorted_ddf = ddf.sort_values(by=['date', 'lat', 'lon'])\n\n    # Save the sorted Dask DataFrame to a new CSV file\n    sorted_ddf.to_csv(sorted_training_csv, index=False, single_file=True)\n    print(f\"sorted training data is saved to {sorted_training_csv}\")\n  \nif __name__ == \"__main__\":\n  \n#     merge_snotel_ghcnd_together()\n  \n#     merge_all_data_together()\n#     cleanup_dataframe()\n#     final_final_output_file = f'{work_dir}/{final_output_name}'\n#     sort_training_data(final_final_output_file, f'{work_dir}/{final_output_name}_sorted.csv')\n    \n    start_time = time.time()\n    \n    merge_all_data_together()\n    print(f\"Time taken for merge_all_data_together: {time.time() - start_time} seconds\")\n    \n    start_time = time.time()\n    cleanup_dataframe()\n    print(f\"Time taken for cleanup_dataframe: {time.time() - start_time} seconds\")\n    \n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    sorted_output_file = f'{work_dir}/{final_output_name}_sorted.csv'\n    \n    start_time = time.time()\n    sort_training_data(final_final_output_file, sorted_output_file)\n    print(f\"Time taken for sort_training_data: {time.time() - start_time} seconds\")\n    ",
  "history_output" : "today date = 2024-05-12\n2024-05-09\ntest start date:  2024-05-09\ntest end date:  2023-10-11\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/hx7MEO3foShd/merge_custom_traning_range.py\", line 162, in <module>\n    start_time = time.time()\nNameError: name 'time' is not defined\n",
  "history_begin_time" : 1715548225704,
  "history_end_time" : 1715548227722,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "sru8QS1Bbb8t",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir, homedir\nimport pandas as pd\n\nworking_dir = work_dir\nfinal_output_name = \"final_merged_data_4yrs_snotel_and_ghcnd_stations.csv\"\nchunk_size = '10MB'  # You can adjust this chunk size based on your hardware and data size\namsr_file = f'{working_dir}/all_training_points_in_westus.csv_amsr_dask_all_training_ponits.csv'\nsnotel_file = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_swe_restored_dask_all_vars.csv'\nghcnd_file = f'{working_dir}/active_station_only_list.csv_all_vars_masked_non_snow.csv'\nall_station_obs_file = f'{working_dir}/snotel_ghcnd_all_obs.csv'\ngridmet_file = f'{working_dir}/training_all_point_gridmet_with_snotel_ghcnd.csv'\nterrain_file = f'{working_dir}/all_training_points_with_ghcnd_in_westus.csv_terrain_4km_grid_shift.csv'\nfsca_file = f'{homedir}/fsca/fsca_final_training_all.csv'\nfinal_final_output_file = f'{work_dir}/{final_output_name}'\n\n\ndef merge_snotel_ghcnd_together():\n    snotel_df = pd.read_csv(snotel_file)\n    ghcnd_df = pd.read_csv(ghcnd_file)\n    print(snotel_df.columns)\n    print(ghcnd_df.columns)\n    ghcnd_df = ghcnd_df.rename(columns={'STATION': 'station_name',\n                                       'DATE': 'date',\n                                       'LATITUDE': 'lat',\n                                       'LONGITUDE': 'lon',\n                                       'SNWD': 'snow_depth',})\n    df_combined = pd.concat([snotel_df, ghcnd_df], axis=0, ignore_index=True)\n    df_combined.to_csv(all_station_obs_file, index=False)\n    print(f\"All snotel ang ghcnd are saved to {all_station_obs_file}\")\n    \n\ndef merge_all_data_together():\n    \n    if os.path.exists(final_final_output_file):\n      print(f\"The file '{final_final_output_file}' exists. Skipping\")\n      return final_final_output_file\n    \n    # merge the snotel and ghcnd together first\n    \n      \n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(amsr_file, blocksize=chunk_size)\n    print(\"amsr.columns = \", amsr.columns)\n    snotel = dd.read_csv(snotel_file, blocksize=chunk_size)\n    print(\"snotel.columns = \", snotel.columns)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(gridmet_file, blocksize=chunk_size)\n    gridmet = gridmet.drop(columns=[\"Unnamed: 0\"])\n    print(\"gridmet.columns = \", gridmet.columns)\n    terrain = dd.read_csv(terrain_file, blocksize=chunk_size)\n    terrain = terrain.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    terrain = terrain[[\"lat\", \"lon\", 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness']]\n    print(\"terrain.columns = \", terrain.columns)\n    snowcover = dd.read_csv(fsca_file, blocksize=chunk_size)\n    snowcover = snowcover.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    print(\"snowcover.columns = \", snowcover.columns)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n    print(\"all the dataframes are partitioned\")\n\n    # Merge DataFrames based on specified columns\n    print(\"start to merge amsr and snotel\")\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snotel.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge gridmet\")\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_gridmet.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge terrain\")\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge snowcover\")\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f'Merge completed. {output_file}')\n\ndef cleanup_dataframe():\n    \"\"\"\n    Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    \"\"\"\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    dtype = {'station_name': 'object'}  # 'object' dtype represents strings\n    df = dd.read_csv(final_final_output_file, dtype=dtype)\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(final_final_output_file, single_file=True, index=False)\n    print('Data cleaning completed.')\n    return final_final_output_file\n\n  \ndef sort_training_data(input_training_csv, sorted_training_csv):\n    # Read Dask DataFrame from CSV with increased blocksize and assuming missing data\n    dtype = {'station_name': 'object'}  # 'object' dtype represents strings\n    ddf = dd.read_csv(input_training_csv, assume_missing=True, blocksize='10MB', dtype=dtype)\n\n    # Persist the Dask DataFrame in memory\n    ddf = ddf.persist()\n\n    # Sort Dask DataFrame by three columns: date, lat, and Lon\n    sorted_ddf = ddf.sort_values(by=['date', 'lat', 'lon'])\n\n    # Save the sorted Dask DataFrame to a new CSV file\n    sorted_ddf.to_csv(sorted_training_csv, index=False, single_file=True)\n    print(f\"sorted training data is saved to {sorted_training_csv}\")\n  \nif __name__ == \"__main__\":\n  \n    merge_snotel_ghcnd_together()\n  \n    # merge_all_data_together()\n    #cleanup_dataframe()\n    #final_final_output_file = f'{work_dir}/{final_output_name}'\n    #sort_training_data(final_final_output_file, f'{work_dir}/{final_output_name}_sorted.csv')\n    ",
  "history_output" : "today date = 2024-05-12\n2024-05-09\ntest start date:  2024-05-09\ntest end date:  2023-10-11\n/home/chetana\nIndex(['station_name', 'date', 'lat', 'lon', 'swe_value', 'change_in_swe_inch',\n       'snow_depth', 'air_temperature_observed_f'],\n      dtype='object')\nIndex(['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD', 'swe_value'], dtype='object')\nAll snotel ang ghcnd are saved to /home/chetana/gridmet_test_run/snotel_ghcnd_all_obs.csv\n",
  "history_begin_time" : 1715547806423,
  "history_end_time" : 1715547877053,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "gw7nTEH6unXU",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir, homedir\nimport pandas as pd\n\nworking_dir = work_dir\nfinal_output_name = \"final_merged_data_4yrs_snotel_and_ghcnd_stations.csv\"\nchunk_size = '10MB'  # You can adjust this chunk size based on your hardware and data size\namsr_file = f'{working_dir}/all_training_points_in_westus.csv_amsr_dask_all_training_ponits.csv'\nsnotel_file = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_swe_restored_dask_all_vars.csv'\nghcnd_file = f'{working_dir}/active_station_only_list.csv_all_vars_masked_non_snow.csv'\nall_station_obs_file = f'{working_dir}/snotel_ghcnd_all_obs.csv'\ngridmet_file = f'{working_dir}/training_all_point_gridmet_with_snotel_ghcnd.csv'\nterrain_file = f'{working_dir}/all_training_points_with_ghcnd_in_westus.csv_terrain_4km_grid_shift.csv'\nfsca_file = f'{homedir}/fsca/fsca_final_training_all.csv'\nfinal_final_output_file = f'{work_dir}/{final_output_name}'\n\n\ndef merge_snotel_ghcnd_together():\n    snotel_df = pd.read_csv(snotel_file)\n    ghcnd_df = pd.read_csv(ghcnd_file)\n    print(snotel_df.columns)\n    print(ghcnd_df.columns)\n    ghcnd_df = ghcnd_df.rename(columns={'STATION': 'station_name',\n                                       'DATE': 'date',\n                                       'LATITUDE': 'latitude',\n                                       'LONGITUDE': 'longitude',\n                                       'SNWD': 'snow_depth',})\n    df_combined = pd.concat([snotel_df, ghcnd_df], axis=0, ignore_index=True)\n    df_combined.to_csv(all_station_obs_file, index=False)\n    print(f\"All snotel ang ghcnd are saved to {all_station_obs_file}\")\n    \n\ndef merge_all_data_together():\n    \n    if os.path.exists(final_final_output_file):\n      print(f\"The file '{final_final_output_file}' exists. Skipping\")\n      return final_final_output_file\n    \n    # merge the snotel and ghcnd together first\n    \n      \n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(amsr_file, blocksize=chunk_size)\n    print(\"amsr.columns = \", amsr.columns)\n    snotel = dd.read_csv(snotel_file, blocksize=chunk_size)\n    print(\"snotel.columns = \", snotel.columns)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(gridmet_file, blocksize=chunk_size)\n    gridmet = gridmet.drop(columns=[\"Unnamed: 0\"])\n    print(\"gridmet.columns = \", gridmet.columns)\n    terrain = dd.read_csv(terrain_file, blocksize=chunk_size)\n    terrain = terrain.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    terrain = terrain[[\"lat\", \"lon\", 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness']]\n    print(\"terrain.columns = \", terrain.columns)\n    snowcover = dd.read_csv(fsca_file, blocksize=chunk_size)\n    snowcover = snowcover.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    print(\"snowcover.columns = \", snowcover.columns)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n    print(\"all the dataframes are partitioned\")\n\n    # Merge DataFrames based on specified columns\n    print(\"start to merge amsr and snotel\")\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snotel.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge gridmet\")\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_gridmet.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge terrain\")\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge snowcover\")\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f'Merge completed. {output_file}')\n\ndef cleanup_dataframe():\n    \"\"\"\n    Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    \"\"\"\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    dtype = {'station_name': 'object'}  # 'object' dtype represents strings\n    df = dd.read_csv(final_final_output_file, dtype=dtype)\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(final_final_output_file, single_file=True, index=False)\n    print('Data cleaning completed.')\n    return final_final_output_file\n\n  \ndef sort_training_data(input_training_csv, sorted_training_csv):\n    # Read Dask DataFrame from CSV with increased blocksize and assuming missing data\n    dtype = {'station_name': 'object'}  # 'object' dtype represents strings\n    ddf = dd.read_csv(input_training_csv, assume_missing=True, blocksize='10MB', dtype=dtype)\n\n    # Persist the Dask DataFrame in memory\n    ddf = ddf.persist()\n\n    # Sort Dask DataFrame by three columns: date, lat, and Lon\n    sorted_ddf = ddf.sort_values(by=['date', 'lat', 'lon'])\n\n    # Save the sorted Dask DataFrame to a new CSV file\n    sorted_ddf.to_csv(sorted_training_csv, index=False, single_file=True)\n    print(f\"sorted training data is saved to {sorted_training_csv}\")\n  \nif __name__ == \"__main__\":\n  \n    merge_snotel_ghcnd_together()\n  \n    # merge_all_data_together()\n    #cleanup_dataframe()\n    #final_final_output_file = f'{work_dir}/{final_output_name}'\n    #sort_training_data(final_final_output_file, f'{work_dir}/{final_output_name}_sorted.csv')\n    ",
  "history_output" : "today date = 2024-05-12\n2024-05-09\ntest start date:  2024-05-09\ntest end date:  2023-10-11\n/home/chetana\nIndex(['station_name', 'date', 'lat', 'lon', 'swe_value', 'change_in_swe_inch',\n       'snow_depth', 'air_temperature_observed_f'],\n      dtype='object')\nIndex(['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD', 'swe_value'], dtype='object')\nAll snotel ang ghcnd are saved to /home/chetana/gridmet_test_run/snotel_ghcnd_all_obs.csv\n",
  "history_begin_time" : 1715534606614,
  "history_end_time" : 1715534685392,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Ic5yQt0aHQjV",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir, homedir\nimport pandas as pd\n\nworking_dir = work_dir\nfinal_output_name = \"final_merged_data_4yrs_snotel_and_ghcnd_stations.csv\"\nchunk_size = '10MB'  # You can adjust this chunk size based on your hardware and data size\namsr_file = f'{working_dir}/all_training_points_in_westus.csv_amsr_dask_all_training_ponits.csv'\nsnotel_file = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_swe_restored_dask_all_vars.csv'\nghcnd_file = f'{working_dir}/active_station_only_list.csv_all_vars_masked_non_snow.csv'\nall_station_obs_file = f'{working_dir}/snotel_ghcnd_all_obs.csv'\ngridmet_file = f'{working_dir}/training_all_point_gridmet_with_snotel_ghcnd.csv'\nterrain_file = f'{working_dir}/all_training_points_with_ghcnd_in_westus.csv_terrain_4km_grid_shift.csv'\nfsca_file = f'{homedir}/fsca/fsca_final_training_all.csv'\nfinal_final_output_file = f'{work_dir}/{final_output_name}'\n\n\ndef merge_snotel_ghcnd_together():\n    snotel_df = pd.read_csv(snotel_file)\n    ghcnd_df = pd.read_csv(ghcnd_file)\n    print(snotel_df.columns)\n    print(ghcnd_df.columns)\n    ghcnd_df = ghcnd_df.rename(columns={'STATION': 'station_name',\n                                       'DATE': 'date',\n                                       'LATITUDE': 'latitude',\n                                       'LONGITUDE': 'longitude',\n                                       'SNWD': 'snow_depth',})\n    \n    \n\ndef merge_all_data_together():\n    \n    if os.path.exists(final_final_output_file):\n      print(f\"The file '{final_final_output_file}' exists. Skipping\")\n      return final_final_output_file\n    \n    # merge the snotel and ghcnd together first\n    \n      \n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(amsr_file, blocksize=chunk_size)\n    print(\"amsr.columns = \", amsr.columns)\n    snotel = dd.read_csv(snotel_file, blocksize=chunk_size)\n    print(\"snotel.columns = \", snotel.columns)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(gridmet_file, blocksize=chunk_size)\n    gridmet = gridmet.drop(columns=[\"Unnamed: 0\"])\n    print(\"gridmet.columns = \", gridmet.columns)\n    terrain = dd.read_csv(terrain_file, blocksize=chunk_size)\n    terrain = terrain.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    terrain = terrain[[\"lat\", \"lon\", 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness']]\n    print(\"terrain.columns = \", terrain.columns)\n    snowcover = dd.read_csv(fsca_file, blocksize=chunk_size)\n    snowcover = snowcover.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    print(\"snowcover.columns = \", snowcover.columns)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n    print(\"all the dataframes are partitioned\")\n\n    # Merge DataFrames based on specified columns\n    print(\"start to merge amsr and snotel\")\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snotel.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge gridmet\")\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_gridmet.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge terrain\")\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge snowcover\")\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f'Merge completed. {output_file}')\n\ndef cleanup_dataframe():\n    \"\"\"\n    Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    \"\"\"\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    dtype = {'station_name': 'object'}  # 'object' dtype represents strings\n    df = dd.read_csv(final_final_output_file, dtype=dtype)\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(final_final_output_file, single_file=True, index=False)\n    print('Data cleaning completed.')\n    return final_final_output_file\n\n  \ndef sort_training_data(input_training_csv, sorted_training_csv):\n    # Read Dask DataFrame from CSV with increased blocksize and assuming missing data\n    dtype = {'station_name': 'object'}  # 'object' dtype represents strings\n    ddf = dd.read_csv(input_training_csv, assume_missing=True, blocksize='10MB', dtype=dtype)\n\n    # Persist the Dask DataFrame in memory\n    ddf = ddf.persist()\n\n    # Sort Dask DataFrame by three columns: date, lat, and Lon\n    sorted_ddf = ddf.sort_values(by=['date', 'lat', 'lon'])\n\n    # Save the sorted Dask DataFrame to a new CSV file\n    sorted_ddf.to_csv(sorted_training_csv, index=False, single_file=True)\n    print(f\"sorted training data is saved to {sorted_training_csv}\")\n  \nif __name__ == \"__main__\":\n  \n    merge_snotel_ghcnd_together()\n  \n    # merge_all_data_together()\n    #cleanup_dataframe()\n    #final_final_output_file = f'{work_dir}/{final_output_name}'\n    #sort_training_data(final_final_output_file, f'{work_dir}/{final_output_name}_sorted.csv')\n    ",
  "history_output" : "today date = 2024-05-12\n2024-05-09\ntest start date:  2024-05-09\ntest end date:  2023-10-11\n/home/chetana\nIndex(['station_name', 'date', 'lat', 'lon', 'swe_value', 'change_in_swe_inch',\n       'snow_depth', 'air_temperature_observed_f'],\n      dtype='object')\nIndex(['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD', 'swe_value'], dtype='object')\n",
  "history_begin_time" : 1715534524268,
  "history_end_time" : 1715534534294,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "fPwEB4hd0uRl",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir, homedir\nimport pandas as pd\n\nworking_dir = work_dir\nfinal_output_name = \"final_merged_data_4yrs_snotel_and_ghcnd_stations.csv\"\nchunk_size = '10MB'  # You can adjust this chunk size based on your hardware and data size\namsr_file = f'{working_dir}/all_training_points_in_westus.csv_amsr_dask_all_training_ponits.csv'\nsnotel_file = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_swe_restored_dask_all_vars.csv'\nghcnd_file = f'{working_dir}/active_station_only_list.csv_all_vars_masked_non_snow.csv'\nall_station_obs_file = f'{working_dir}/snotel_ghcnd_all_obs.csv'\ngridmet_file = f'{working_dir}/training_all_point_gridmet_with_snotel_ghcnd.csv'\nterrain_file = f'{working_dir}/all_training_points_with_ghcnd_in_westus.csv_terrain_4km_grid_shift.csv'\nfsca_file = f'{homedir}/fsca/fsca_final_training_all.csv'\nfinal_final_output_file = f'{work_dir}/{final_output_name}'\n\n\ndef merge_snotel_ghcnd_together():\n    snotel_df = pd.read_csv(snotel_file)\n    ghcnd_df = pd.read_csv(ghcnd_file)\n    print(snotel_df.columns())\n    print(ghcnd_df.columns())\n    ghcnd_df = ghcnd_df.rename(columns={'STATION': 'station_name',\n                                       'DATE': 'date',\n                                       'LATITUDE': 'latitude',\n                                       'LONGITUDE': 'longitude',\n                                       'SNWD': 'snow_depth',})\n    \n    \n\ndef merge_all_data_together():\n    \n    if os.path.exists(final_final_output_file):\n      print(f\"The file '{final_final_output_file}' exists. Skipping\")\n      return final_final_output_file\n    \n    # merge the snotel and ghcnd together first\n    \n      \n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(amsr_file, blocksize=chunk_size)\n    print(\"amsr.columns = \", amsr.columns)\n    snotel = dd.read_csv(snotel_file, blocksize=chunk_size)\n    print(\"snotel.columns = \", snotel.columns)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(gridmet_file, blocksize=chunk_size)\n    gridmet = gridmet.drop(columns=[\"Unnamed: 0\"])\n    print(\"gridmet.columns = \", gridmet.columns)\n    terrain = dd.read_csv(terrain_file, blocksize=chunk_size)\n    terrain = terrain.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    terrain = terrain[[\"lat\", \"lon\", 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness']]\n    print(\"terrain.columns = \", terrain.columns)\n    snowcover = dd.read_csv(fsca_file, blocksize=chunk_size)\n    snowcover = snowcover.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    print(\"snowcover.columns = \", snowcover.columns)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n    print(\"all the dataframes are partitioned\")\n\n    # Merge DataFrames based on specified columns\n    print(\"start to merge amsr and snotel\")\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snotel.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge gridmet\")\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_gridmet.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge terrain\")\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge snowcover\")\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f'Merge completed. {output_file}')\n\ndef cleanup_dataframe():\n    \"\"\"\n    Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    \"\"\"\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    dtype = {'station_name': 'object'}  # 'object' dtype represents strings\n    df = dd.read_csv(final_final_output_file, dtype=dtype)\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(final_final_output_file, single_file=True, index=False)\n    print('Data cleaning completed.')\n    return final_final_output_file\n\n  \ndef sort_training_data(input_training_csv, sorted_training_csv):\n    # Read Dask DataFrame from CSV with increased blocksize and assuming missing data\n    dtype = {'station_name': 'object'}  # 'object' dtype represents strings\n    ddf = dd.read_csv(input_training_csv, assume_missing=True, blocksize='10MB', dtype=dtype)\n\n    # Persist the Dask DataFrame in memory\n    ddf = ddf.persist()\n\n    # Sort Dask DataFrame by three columns: date, lat, and Lon\n    sorted_ddf = ddf.sort_values(by=['date', 'lat', 'lon'])\n\n    # Save the sorted Dask DataFrame to a new CSV file\n    sorted_ddf.to_csv(sorted_training_csv, index=False, single_file=True)\n    print(f\"sorted training data is saved to {sorted_training_csv}\")\n  \nif __name__ == \"__main__\":\n  \n    merge_snotel_ghcnd_together()\n  \n    # merge_all_data_together()\n    #cleanup_dataframe()\n    #final_final_output_file = f'{work_dir}/{final_output_name}'\n    #sort_training_data(final_final_output_file, f'{work_dir}/{final_output_name}_sorted.csv')\n    ",
  "history_output" : "today date = 2024-05-12\n2024-05-09\ntest start date:  2024-05-09\ntest end date:  2023-10-11\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/fPwEB4hd0uRl/merge_custom_traning_range.py\", line 153, in <module>\n    merge_snotel_ghcnd_together()\n  File \"/home/chetana/gw-workspace/fPwEB4hd0uRl/merge_custom_traning_range.py\", line 38, in merge_snotel_ghcnd_together\n    print(snotel_df.columns())\nTypeError: 'Index' object is not callable\n",
  "history_begin_time" : 1715534473891,
  "history_end_time" : 1715534483730,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "X9J4ICnorUO5",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir, homedir\nimport pandas as pd\n\nworking_dir = work_dir\nfinal_output_name = \"final_merged_data_4yrs_snotel_and_ghcnd_stations.csv\"\nchunk_size = '10MB'  # You can adjust this chunk size based on your hardware and data size\namsr_file = f'{working_dir}/all_training_points_in_westus.csv_amsr_dask_all_training_ponits.csv'\nsnotel_file = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_swe_restored_dask_all_vars.csv'\nghcnd_file = f'{working_dir}/active_station_only_list.csv_all_vars_masked_non_snow.csv'\nall_station_obs_file = f'{working_dir}/snotel_ghcnd_all_obs.csv'\ngridmet_file = f'{working_dir}/training_all_point_gridmet_with_snotel_ghcnd.csv'\nterrain_file = f'{working_dir}/all_training_points_with_ghcnd_in_westus.csv_terrain_4km_grid_shift.csv'\nfsca_file = f'{homedir}/fsca/fsca_final_training_all.csv'\nfinal_final_output_file = f'{work_dir}/{final_output_name}'\n\n\ndef merge_snotel_ghcnd_together():\n    snotel_df = pd.read_csv(snotel_file)\n    ghcnd_df = pd.read_csv(ghcnd_file)\n    print(snotel_df.head())\n    print(ghcnd_df.head())\n\ndef merge_all_data_together():\n    \n    if os.path.exists(final_final_output_file):\n      print(f\"The file '{final_final_output_file}' exists. Skipping\")\n      return final_final_output_file\n    \n    # merge the snotel and ghcnd together first\n    \n      \n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(amsr_file, blocksize=chunk_size)\n    print(\"amsr.columns = \", amsr.columns)\n    snotel = dd.read_csv(snotel_file, blocksize=chunk_size)\n    print(\"snotel.columns = \", snotel.columns)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(gridmet_file, blocksize=chunk_size)\n    gridmet = gridmet.drop(columns=[\"Unnamed: 0\"])\n    print(\"gridmet.columns = \", gridmet.columns)\n    terrain = dd.read_csv(terrain_file, blocksize=chunk_size)\n    terrain = terrain.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    terrain = terrain[[\"lat\", \"lon\", 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness']]\n    print(\"terrain.columns = \", terrain.columns)\n    snowcover = dd.read_csv(fsca_file, blocksize=chunk_size)\n    snowcover = snowcover.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    print(\"snowcover.columns = \", snowcover.columns)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n    print(\"all the dataframes are partitioned\")\n\n    # Merge DataFrames based on specified columns\n    print(\"start to merge amsr and snotel\")\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snotel.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge gridmet\")\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_gridmet.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge terrain\")\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge snowcover\")\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f'Merge completed. {output_file}')\n\ndef cleanup_dataframe():\n    \"\"\"\n    Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    \"\"\"\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    dtype = {'station_name': 'object'}  # 'object' dtype represents strings\n    df = dd.read_csv(final_final_output_file, dtype=dtype)\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(final_final_output_file, single_file=True, index=False)\n    print('Data cleaning completed.')\n    return final_final_output_file\n\n  \ndef sort_training_data(input_training_csv, sorted_training_csv):\n    # Read Dask DataFrame from CSV with increased blocksize and assuming missing data\n    dtype = {'station_name': 'object'}  # 'object' dtype represents strings\n    ddf = dd.read_csv(input_training_csv, assume_missing=True, blocksize='10MB', dtype=dtype)\n\n    # Persist the Dask DataFrame in memory\n    ddf = ddf.persist()\n\n    # Sort Dask DataFrame by three columns: date, lat, and Lon\n    sorted_ddf = ddf.sort_values(by=['date', 'lat', 'lon'])\n\n    # Save the sorted Dask DataFrame to a new CSV file\n    sorted_ddf.to_csv(sorted_training_csv, index=False, single_file=True)\n    print(f\"sorted training data is saved to {sorted_training_csv}\")\n  \nif __name__ == \"__main__\":\n  \n    merge_snotel_ghcnd_together()\n  \n    # merge_all_data_together()\n    #cleanup_dataframe()\n    #final_final_output_file = f'{work_dir}/{final_output_name}'\n    #sort_training_data(final_final_output_file, f'{work_dir}/{final_output_name}_sorted.csv')\n    ",
  "history_output" : "today date = 2024-05-12\n2024-05-09\ntest start date:  2024-05-09\ntest end date:  2023-10-11\n/home/chetana\n     station_name        date  ...  snow_depth  air_temperature_observed_f\n0  Adams Ranch #1  2018-01-01  ...         NaN                         8.6\n1  Adams Ranch #1  2018-01-02  ...         NaN                        31.3\n2  Adams Ranch #1  2018-01-03  ...         NaN                        21.4\n3  Adams Ranch #1  2018-01-04  ...         NaN                        39.0\n4  Adams Ranch #1  2018-01-05  ...         NaN                        29.5\n[5 rows x 8 columns]\n       STATION        DATE  LATITUDE  LONGITUDE  SNWD  swe_value\n0  CA001011500  2018-01-03   48.9333    -123.75   0.0          0\n1  CA001011500  2018-01-04   48.9333    -123.75   0.0          0\n2  CA001011500  2018-01-05   48.9333    -123.75   0.0          0\n3  CA001011500  2018-01-06   48.9333    -123.75   0.0          0\n4  CA001011500  2018-01-07   48.9333    -123.75   0.0          0\n",
  "history_begin_time" : 1715534237353,
  "history_end_time" : 1715534247822,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "t7og00493wr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715526734767,
  "history_end_time" : 1715548400462,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "yddes9xjdrd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715486409477,
  "history_end_time" : 1715486409477,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vd6iu2xqljf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715400010075,
  "history_end_time" : 1715549424580,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "mkk7rinxwja",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715313616751,
  "history_end_time" : 1715549423034,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "rmwjdd3we2q",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715227215059,
  "history_end_time" : 1715549422497,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "j5jo69iss7a",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715140819795,
  "history_end_time" : 1715549421788,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "z1j228x3uam",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715054414393,
  "history_end_time" : 1715549421332,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "q8xk69p9bcw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715045815899,
  "history_end_time" : 1715045815899,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "380nl14qfu5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715045635649,
  "history_end_time" : 1715045800481,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "woap30z2mfa",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714998835561,
  "history_end_time" : 1714998835561,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "z1oskzfvxc3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714977335371,
  "history_end_time" : 1714977335371,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "02k472kicjk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714968023410,
  "history_end_time" : 1715549419193,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "w20vc1kbiiz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714956083472,
  "history_end_time" : 1714956343541,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "vbx6zc",
  "indicator" : "Stopped"
},{
  "history_id" : "6uwecaxhhm0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714944534015,
  "history_end_time" : 1714944767138,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "vbx6zc",
  "indicator" : "Stopped"
},{
  "history_id" : "6shuk8f9u24",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714943546216,
  "history_end_time" : 1715549417788,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "44eakhn1ko4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714897025202,
  "history_end_time" : 1714897025202,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vzx5w07hmc0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714894044156,
  "history_end_time" : 1714895139378,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "vdude6yt0ms",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714881609784,
  "history_end_time" : 1715549416946,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "A47cBnFmauZW",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir, homedir\nimport pandas as pd\n\nworking_dir = work_dir\nfinal_output_name = \"final_merged_data_3yrs_all_stations_with_non_stations.csv\"\nchunk_size = '10MB'  # You can adjust this chunk size based on your hardware and data size\n\n\ndef merge_all_data_together():\n    amsr_file = f'{working_dir}/all_training_points_in_westus.csv_amsr_dask_all_training_ponits.csv'\n    snotel_file = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_swe_restored_dask_all_vars.csv'\n    gridmet_file = f'{working_dir}/training_all_point_gridmet_with_non_station.csv'\n    terrain_file = f'{working_dir}/all_training_points_in_westus.csv_terrain_4km_grid_shift.csv'\n    fsca_file = f'{homedir}/fsca/fsca_final_training_all.csv'\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    \n    if os.path.exists(final_final_output_file):\n      print(f\"The file '{final_final_output_file}' exists. Skipping\")\n      return final_final_output_file\n      \n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(amsr_file, blocksize=chunk_size)\n    print(\"amsr.columns = \", amsr.columns)\n    snotel = dd.read_csv(snotel_file, blocksize=chunk_size)\n    print(\"snotel.columns = \", snotel.columns)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(gridmet_file, blocksize=chunk_size)\n    gridmet = gridmet.drop(columns=[\"Unnamed: 0\"])\n    print(\"gridmet.columns = \", gridmet.columns)\n    terrain = dd.read_csv(terrain_file, blocksize=chunk_size)\n    terrain = terrain.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    terrain = terrain[[\"lat\", \"lon\", 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness']]\n    print(\"terrain.columns = \", terrain.columns)\n    snowcover = dd.read_csv(fsca_file, blocksize=chunk_size)\n    snowcover = snowcover.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    print(\"snowcover.columns = \", snowcover.columns)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n    print(\"all the dataframes are partitioned\")\n\n    # Merge DataFrames based on specified columns\n    print(\"start to merge amsr and snotel\")\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snotel.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge gridmet\")\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_gridmet.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge terrain\")\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge snowcover\")\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f'Merge completed. {output_file}')\n\ndef cleanup_dataframe():\n    \"\"\"\n    Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    \"\"\"\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    dtype = {'station_name': 'object'}  # 'object' dtype represents strings\n    df = dd.read_csv(final_final_output_file, dtype=dtype)\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(final_final_output_file, single_file=True, index=False)\n    print('Data cleaning completed.')\n    return final_final_output_file\n\n  \ndef sort_training_data(input_training_csv, sorted_training_csv):\n    # Read Dask DataFrame from CSV with increased blocksize and assuming missing data\n    dtype = {'station_name': 'object'}  # 'object' dtype represents strings\n    ddf = dd.read_csv(input_training_csv, assume_missing=True, blocksize='10MB', dtype=dtype)\n\n    # Persist the Dask DataFrame in memory\n    ddf = ddf.persist()\n\n    # Sort Dask DataFrame by three columns: date, lat, and Lon\n    sorted_ddf = ddf.sort_values(by=['date', 'lat', 'lon'])\n\n    # Save the sorted Dask DataFrame to a new CSV file\n    sorted_ddf.to_csv(sorted_training_csv, index=False, single_file=True)\n    print(f\"sorted training data is saved to {sorted_training_csv}\")\n  \nif __name__ == \"__main__\":\n    # merge_all_data_together()\n    cleanup_dataframe()\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    sort_training_data(final_final_output_file, f'{work_dir}/{final_output_name}_sorted.csv')",
  "history_output" : "today date = 2024-05-02\n2024-04-29\ntest start date:  2024-04-29\ntest end date:  2023-10-11\n/home/chetana\nData cleaning completed.\nsorted training data is saved to /home/chetana/gridmet_test_run/final_merged_data_3yrs_all_stations_with_non_stations.csv_sorted.csv\n",
  "history_begin_time" : 1714663773465,
  "history_end_time" : 1714667240398,
  "history_notes" : "finally, all the data are merged and ready for training with all the station and non-station points",
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "f2WyJGyc7NxX",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir, homedir\nimport pandas as pd\n\nworking_dir = work_dir\nfinal_output_name = \"final_merged_data_3yrs_all_stations_with_non_stations.csv\"\nchunk_size = '10MB'  # You can adjust this chunk size based on your hardware and data size\n\n\ndef merge_all_data_together():\n    amsr_file = f'{working_dir}/all_training_points_in_westus.csv_amsr_dask_all_training_ponits.csv'\n    snotel_file = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_swe_restored_dask_all_vars.csv'\n    gridmet_file = f'{working_dir}/training_all_point_gridmet_with_non_station.csv'\n    terrain_file = f'{working_dir}/all_training_points_in_westus.csv_terrain_4km_grid_shift.csv'\n    fsca_file = f'{homedir}/fsca/fsca_final_training_all.csv'\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    \n    if os.path.exists(final_final_output_file):\n      print(f\"The file '{final_final_output_file}' exists. Skipping\")\n      return final_final_output_file\n      \n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(amsr_file, blocksize=chunk_size)\n    print(\"amsr.columns = \", amsr.columns)\n    snotel = dd.read_csv(snotel_file, blocksize=chunk_size)\n    print(\"snotel.columns = \", snotel.columns)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(gridmet_file, blocksize=chunk_size)\n    gridmet = gridmet.drop(columns=[\"Unnamed: 0\"])\n    print(\"gridmet.columns = \", gridmet.columns)\n    terrain = dd.read_csv(terrain_file, blocksize=chunk_size)\n    terrain = terrain.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    terrain = terrain[[\"lat\", \"lon\", 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness']]\n    print(\"terrain.columns = \", terrain.columns)\n    snowcover = dd.read_csv(fsca_file, blocksize=chunk_size)\n    snowcover = snowcover.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    print(\"snowcover.columns = \", snowcover.columns)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n    print(\"all the dataframes are partitioned\")\n\n    # Merge DataFrames based on specified columns\n    print(\"start to merge amsr and snotel\")\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snotel.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge gridmet\")\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_gridmet.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge terrain\")\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge snowcover\")\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f'Merge completed. {output_file}')\n\ndef cleanup_dataframe():\n    \"\"\"\n    Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    \"\"\"\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    dtype = {'station_name': 'object'}  # 'object' dtype represents strings\n    df = dd.read_csv(final_final_output_file, dtype=dtype)\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(final_final_output_file, single_file=True, index=False)\n    print('Data cleaning completed.')\n    return final_final_output_file\n\n  \ndef sort_training_data(input_training_csv, sorted_training_csv):\n    # Read Dask DataFrame from CSV with increased blocksize and assuming missing data\n    ddf = dd.read_csv(input_training_csv, assume_missing=True, blocksize='10MB')\n\n    # Persist the Dask DataFrame in memory\n    ddf = ddf.persist()\n\n    # Sort Dask DataFrame by three columns: date, lat, and Lon\n    sorted_ddf = ddf.sort_values(by=['date', 'lat', 'lon'])\n\n    # Save the sorted Dask DataFrame to a new CSV file\n    sorted_ddf.to_csv(sorted_training_csv, index=False, single_file=True)\n    print(f\"sorted training data is saved to {sorted_training_csv}\")\n  \nif __name__ == \"__main__\":\n    # merge_all_data_together()\n    cleanup_dataframe()\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    sort_training_data(final_final_output_file, f'{work_dir}/{final_output_name}_sorted.csv')",
  "history_output" : "today date = 2024-05-02\n2024-04-29\ntest start date:  2024-04-29\ntest end date:  2023-10-11\n/home/chetana\nData cleaning completed.\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/f2WyJGyc7NxX/merge_custom_traning_range.py\", line 136, in <module>\n    sort_training_data(final_final_output_file, f'{work_dir}/{final_output_name}_sorted.csv')\n  File \"/home/chetana/gw-workspace/f2WyJGyc7NxX/merge_custom_traning_range.py\", line 123, in sort_training_data\n    ddf = ddf.persist()\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/dask/base.py\", line 315, in persist\n    (result,) = persist(self, traverse=False, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/dask/base.py\", line 931, in persist\n    results = schedule(dsk, keys, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 142, in __call__\n    df = pandas_read_text(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 197, in pandas_read_text\n    coerce_dtypes(df, dtypes)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 298, in coerce_dtypes\n    raise ValueError(msg)\nValueError: Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\n+--------------+--------+----------+\n| Column       | Found  | Expected |\n+--------------+--------+----------+\n| station_name | object | float64  |\n+--------------+--------+----------+\nThe following columns also raised exceptions on conversion:\n- station_name\n  ValueError(\"could not convert string to float: 'Tony Grove Lake'\")\nUsually this is due to dask's dtype inference failing, and\n*may* be fixed by specifying dtypes manually by adding:\ndtype={'station_name': 'object'}\nto the call to `read_csv`/`read_table`.\n",
  "history_begin_time" : 1714660108133,
  "history_end_time" : 1714660662174,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "eUcOnmU9L9VG",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir, homedir\nimport pandas as pd\n\nworking_dir = work_dir\nfinal_output_name = \"final_merged_data_3yrs_all_stations_with_non_stations.csv\"\nchunk_size = '10MB'  # You can adjust this chunk size based on your hardware and data size\n\n\ndef merge_all_data_together():\n    amsr_file = f'{working_dir}/all_training_points_in_westus.csv_amsr_dask_all_training_ponits.csv'\n    snotel_file = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_swe_restored_dask_all_vars.csv'\n    gridmet_file = f'{working_dir}/training_all_point_gridmet_with_non_station.csv'\n    terrain_file = f'{working_dir}/all_training_points_in_westus.csv_terrain_4km_grid_shift.csv'\n    fsca_file = f'{homedir}/fsca/fsca_final_training_all.csv'\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    \n    if os.path.exists(final_final_output_file):\n      print(f\"The file '{final_final_output_file}' exists. Skipping\")\n      return final_final_output_file\n      \n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(amsr_file, blocksize=chunk_size)\n    print(\"amsr.columns = \", amsr.columns)\n    snotel = dd.read_csv(snotel_file, blocksize=chunk_size)\n    print(\"snotel.columns = \", snotel.columns)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(gridmet_file, blocksize=chunk_size)\n    gridmet = gridmet.drop(columns=[\"Unnamed: 0\"])\n    print(\"gridmet.columns = \", gridmet.columns)\n    terrain = dd.read_csv(terrain_file, blocksize=chunk_size)\n    terrain = terrain.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    terrain = terrain[[\"lat\", \"lon\", 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness']]\n    print(\"terrain.columns = \", terrain.columns)\n    snowcover = dd.read_csv(fsca_file, blocksize=chunk_size)\n    snowcover = snowcover.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    print(\"snowcover.columns = \", snowcover.columns)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n    print(\"all the dataframes are partitioned\")\n\n    # Merge DataFrames based on specified columns\n    print(\"start to merge amsr and snotel\")\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snotel.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge gridmet\")\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_gridmet.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge terrain\")\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge snowcover\")\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f'Merge completed. {output_file}')\n\ndef cleanup_dataframe():\n    \"\"\"\n    Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    \"\"\"\n    dtype = {'station_name': 'object'}  # 'object' dtype represents strings\n    df = dd.read_csv(f'{work_dir}/{final_output_name}', dtype=dtype)\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/{final_output_name}', single_file=True, index=False)\n    print('Data cleaning completed.')\n    return final_final_output_file\n\n  \ndef sort_training_data(input_training_csv, sorted_training_csv):\n    # Read Dask DataFrame from CSV with increased blocksize and assuming missing data\n    ddf = dd.read_csv(input_training_csv, assume_missing=True, blocksize='10MB')\n\n    # Persist the Dask DataFrame in memory\n    ddf = ddf.persist()\n\n    # Sort Dask DataFrame by three columns: date, lat, and Lon\n    sorted_ddf = ddf.sort_values(by=['date', 'lat', 'lon'])\n\n    # Save the sorted Dask DataFrame to a new CSV file\n    sorted_ddf.to_csv(sorted_training_csv, index=False, single_file=True)\n    print(f\"sorted training data is saved to {sorted_training_csv}\")\n  \nif __name__ == \"__main__\":\n    # merge_all_data_together()\n    cleanup_dataframe()\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    sort_training_data(final_final_output_file, f'{work_dir}/{final_output_name}_sorted.csv')",
  "history_output" : "today date = 2024-05-02\n2024-04-29\ntest start date:  2024-04-29\ntest end date:  2023-10-11\n/home/chetana\nData cleaning completed.\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/eUcOnmU9L9VG/merge_custom_traning_range.py\", line 133, in <module>\n    cleanup_dataframe()\n  File \"/home/chetana/gw-workspace/eUcOnmU9L9VG/merge_custom_traning_range.py\", line 114, in cleanup_dataframe\n    return final_final_output_file\nNameError: name 'final_final_output_file' is not defined\n",
  "history_begin_time" : 1714657604889,
  "history_end_time" : 1714659724516,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "EO4xAkFwgpDA",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir, homedir\nimport pandas as pd\n\nworking_dir = work_dir\nfinal_output_name = \"final_merged_data_3yrs_all_stations_with_non_stations.csv\"\nchunk_size = '10MB'  # You can adjust this chunk size based on your hardware and data size\n\n\ndef merge_all_data_together():\n    amsr_file = f'{working_dir}/all_training_points_in_westus.csv_amsr_dask_all_training_ponits.csv'\n    print(f\"amsr_file = {amsr_file}\")\n    snotel_file = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_swe_restored_dask_all_vars.csv'\n    print(f\"snotel_file = {snotel_file}\")\n    gridmet_file = f'{working_dir}/training_all_point_gridmet_with_non_station.csv'\n    print(f\"gridmet_file = {gridmet_file}\")\n    terrain_file = f'{working_dir}/all_training_points_in_westus.csv_terrain_4km_grid_shift.csv'\n    print(f\"terrain_file = {terrain_file}\")\n    fsca_file = f'{homedir}/fsca/fsca_final_training_all.csv'\n    print(f\"fsca_file = {fsca_file}\")\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    print(f\"final_final_output_file = {final_final_output_file}\")\n    \n    if os.path.exists(final_final_output_file):\n      print(f\"The file '{final_final_output_file}' exists. Skipping\")\n      return final_final_output_file\n      \n    \n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(amsr_file, blocksize=chunk_size)\n    print(\"amsr.columns = \", amsr.columns)\n    snotel = dd.read_csv(snotel_file, blocksize=chunk_size)\n    print(\"snotel.columns = \", snotel.columns)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(gridmet_file, blocksize=chunk_size)\n    gridmet = gridmet.drop(columns=[\"Unnamed: 0\"])\n    print(\"gridmet.columns = \", gridmet.columns)\n    terrain = dd.read_csv(terrain_file, blocksize=chunk_size)\n    terrain = terrain.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    terrain = terrain[[\"lat\", \"lon\", 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness']]\n    print(\"terrain.columns = \", terrain.columns)\n    snowcover = dd.read_csv(fsca_file, blocksize=chunk_size)\n    snowcover = snowcover.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    print(\"snowcover.columns = \", snowcover.columns)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n    print(\"all the dataframes are partitioned\")\n\n    # Merge DataFrames based on specified columns\n    print(\"start to merge amsr and snotel\")\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snotel.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge gridmet\")\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_gridmet.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge terrain\")\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge snowcover\")\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f'Merge completed. {output_file}')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/{final_output_name}')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/{final_output_name}', single_file=True, index=False)\n    print('Data cleaning completed.')\n    return final_final_output_file\n\n  \ndef sort_training_data(input_training_csv, sorted_training_csv):\n    # Read Dask DataFrame from CSV with increased blocksize and assuming missing data\n    ddf = dd.read_csv(input_training_csv, assume_missing=True, blocksize='10MB')\n\n    # Persist the Dask DataFrame in memory\n    ddf = ddf.persist()\n\n    # Sort Dask DataFrame by three columns: date, lat, and Lon\n    sorted_ddf = ddf.sort_values(by=['date', 'lat', 'lon'])\n\n    # Save the sorted Dask DataFrame to a new CSV file\n    sorted_ddf.to_csv(sorted_training_csv, index=False, single_file=True)\n    print(f\"sorted training data is saved to {sorted_training_csv}\")\n  \nif __name__ == \"__main__\":\n    merge_all_data_together()\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    sort_training_data(final_final_output_file, f'{work_dir}/{final_output_name}_sorted.csv')",
  "history_output" : "today date = 2024-05-02\n2024-04-29\ntest start date:  2024-04-29\ntest end date:  2023-10-11\n/home/zsun\namsr_file = /home/zsun/gridmet_test_run/all_training_points_in_westus.csv_amsr_dask_all_training_ponits.csv\nsnotel_file = /home/zsun/gridmet_test_run/all_snotel_cdec_stations_active_in_westus.csv_swe_restored_dask_all_vars.csv\ngridmet_file = /home/zsun/gridmet_test_run/training_all_point_gridmet_with_non_station.csv\nterrain_file = /home/zsun/gridmet_test_run/all_training_points_in_westus.csv_terrain_4km_grid_shift.csv\nfsca_file = /home/zsun/fsca/fsca_final_training_all.csv\nfinal_final_output_file = /home/zsun/gridmet_test_run/final_merged_data_3yrs_all_stations_with_non_stations.csv\namsr.columns =  Index(['date', 'lat', 'lon', 'AMSR_SWE'], dtype='object')\nsnotel.columns =  Index(['station_name', 'date', 'lat', 'lon', 'swe_value', 'change_in_swe_inch',\n       'snow_depth', 'air_temperature_observed_f'],\n      dtype='object')\ngridmet.columns =  Index(['day', 'lon', 'lat', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed'],\n      dtype='object')\nterrain.columns =  Index(['lat', 'lon', 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness',\n       'Eastness'],\n      dtype='object')\nsnowcover.columns =  Index(['date', 'lat', 'lon', 'fsca'], dtype='object')\nall the dataframes are partitioned\nstart to merge amsr and snotel\nintermediate file saved to /home/zsun/gridmet_test_run/final_merged_data_3yrs_all_stations_with_non_stations.csv_snotel.csv\nstart to merge gridmet\nintermediate file saved to /home/zsun/gridmet_test_run/final_merged_data_3yrs_all_stations_with_non_stations.csv_gridmet.csv\nstart to merge terrain\nintermediate file saved to /home/zsun/gridmet_test_run/final_merged_data_3yrs_all_stations_with_non_stations.csv_terrain.csv\nstart to merge snowcover\nintermediate file saved to /home/zsun/gridmet_test_run/final_merged_data_3yrs_all_stations_with_non_stations.csv_snow_cover.csv\nMerge completed. /home/zsun/gridmet_test_run/final_merged_data_3yrs_all_stations_with_non_stations.csv\nData cleaning completed.\nTraceback (most recent call last):\n  File \"merge_custom_traning_range.py\", line 137, in <module>\n    sort_training_data(final_final_output_file, f'{work_dir}/{final_output_name}_sorted.csv')\n  File \"merge_custom_traning_range.py\", line 128, in sort_training_data\n    sorted_ddf = ddf.sort_values(by=['date', 'lat', 'lon'])\n  File \"/home/zsun/anaconda3/lib/python3.7/site-packages/dask/dataframe/core.py\", line 3414, in __getattr__\n    raise AttributeError(\"'DataFrame' object has no attribute %r\" % key)\nAttributeError: 'DataFrame' object has no attribute 'sort_values'\nbash: line 0: exit: $: numeric argument required\n",
  "history_begin_time" : 1714624633359,
  "history_end_time" : 1714629874650,
  "history_notes" : "running the process on 108 which is more idle as this machine is being used by others and very slow",
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Z9zspDqboMpx",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir, homedir\nimport pandas as pd\n\nworking_dir = work_dir\nfinal_output_name = \"final_merged_data_3yrs_all_stations_with_non_stations.csv\"\nchunk_size = '10MB'  # You can adjust this chunk size based on your hardware and data size\n\n\ndef merge_all_data_together():\n    amsr_file = f'{working_dir}/all_training_points_in_westus.csv_amsr_dask_all_training_ponits.csv'\n    print(f\"amsr_file = {amsr_file}\")\n    snotel_file = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_swe_restored_dask_all_vars.csv'\n    print(f\"snotel_file = {snotel_file}\")\n    gridmet_file = f'{working_dir}/training_all_point_gridmet_with_non_station.csv'\n    print(f\"gridmet_file = {gridmet_file}\")\n    terrain_file = f'{working_dir}/all_training_points_in_westus.csv_terrain_4km_grid_shift.csv'\n    print(f\"terrain_file = {terrain_file}\")\n    fsca_file = f'{homedir}/fsca/fsca_final_training_all.csv'\n    print(f\"fsca_file = {fsca_file}\")\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    print(f\"final_final_output_file = {final_final_output_file}\")\n    \n    if os.path.exists(final_final_output_file):\n      print(f\"The file '{final_final_output_file}' exists. Skipping\")\n      return final_final_output_file\n      \n    \n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(amsr_file, blocksize=chunk_size)\n    print(\"amsr.columns = \", amsr.columns)\n    snotel = dd.read_csv(snotel_file, blocksize=chunk_size)\n    print(\"snotel.columns = \", snotel.columns)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(gridmet_file, blocksize=chunk_size)\n    gridmet = gridmet.drop(columns=[\"Unnamed: 0\"])\n    print(\"gridmet.columns = \", gridmet.columns)\n    terrain = dd.read_csv(terrain_file, blocksize=chunk_size)\n    terrain = terrain.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    terrain = terrain[[\"lat\", \"lon\", 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness']]\n    print(\"terrain.columns = \", terrain.columns)\n    snowcover = dd.read_csv(fsca_file, blocksize=chunk_size)\n    snowcover = snowcover.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    print(\"snowcover.columns = \", snowcover.columns)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n    print(\"all the dataframes are partitioned\")\n\n    # Merge DataFrames based on specified columns\n    print(\"start to merge amsr and snotel\")\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snotel.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge gridmet\")\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_gridmet.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge terrain\")\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge snowcover\")\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f'Merge completed. {output_file}')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/{final_output_name}')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/{final_output_name}', single_file=True, index=False)\n    print('Data cleaning completed.')\n    return final_final_output_file\n\n  \ndef sort_training_data(input_training_csv, sorted_training_csv):\n    # Read Dask DataFrame from CSV with increased blocksize and assuming missing data\n    ddf = dd.read_csv(input_training_csv, assume_missing=True, blocksize='10MB')\n\n    # Persist the Dask DataFrame in memory\n    ddf = ddf.persist()\n\n    # Sort Dask DataFrame by three columns: date, lat, and Lon\n    sorted_ddf = ddf.sort_values(by=['date', 'lat', 'lon'])\n\n    # Save the sorted Dask DataFrame to a new CSV file\n    sorted_ddf.to_csv(sorted_training_csv, index=False, single_file=True)\n    print(f\"sorted training data is saved to {sorted_training_csv}\")\n  \nif __name__ == \"__main__\":\n    merge_all_data_together()\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    sort_training_data(final_final_output_file, f'{work_dir}/{final_output_name}_sorted.csv')",
  "history_output" : "today date = 2024-05-02\n2024-04-29\ntest start date:  2024-04-29\ntest end date:  2023-10-11\n/home/zsun\namsr_file = /home/zsun/gridmet_test_run/all_training_points_in_westus.csv_amsr_dask_all_training_ponits.csv\nsnotel_file = /home/zsun/gridmet_test_run/all_snotel_cdec_stations_active_in_westus.csv_swe_restored_dask_all_vars.csv\ngridmet_file = /home/zsun/gridmet_test_run/training_all_point_gridmet_with_non_station.csv\nterrain_file = /home/zsun/gridmet_test_run/all_training_points_in_westus.csv_terrain_4km_grid_shift.csv\nfsca_file = /home/zsun/fsca/fsca_final_training_all.csv\nfinal_final_output_file = /home/zsun/gridmet_test_run/final_merged_data_3yrs_all_stations_with_non_stations.csv\namsr.columns =  Index(['date', 'lat', 'lon', 'AMSR_SWE'], dtype='object')\nsnotel.columns =  Index(['station_name', 'date', 'lat', 'lon', 'swe_value', 'change_in_swe_inch',\n       'snow_depth', 'air_temperature_observed_f'],\n      dtype='object')\ngridmet.columns =  Index(['day', 'lon', 'lat', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed'],\n      dtype='object')\nterrain.columns =  Index(['lat', 'lon', 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness',\n       'Eastness'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"merge_custom_traning_range.py\", line 135, in <module>\n    merge_all_data_together()\n  File \"merge_custom_traning_range.py\", line 62, in merge_all_data_together\n    snowcover = dd.read_csv(fsca_file, blocksize=chunk_size)\n  File \"/home/zsun/anaconda3/lib/python3.7/site-packages/dask/dataframe/io/csv.py\", line 578, in read\n    **kwargs\n  File \"/home/zsun/anaconda3/lib/python3.7/site-packages/dask/dataframe/io/csv.py\", line 405, in read_pandas\n    **(storage_options or {})\n  File \"/home/zsun/anaconda3/lib/python3.7/site-packages/dask/bytes/core.py\", line 121, in read_bytes\n    size = fs.info(path)[\"size\"]\n  File \"/home/zsun/anaconda3/lib/python3.7/site-packages/fsspec/implementations/local.py\", line 56, in info\n    out = os.stat(path, follow_symlinks=False)\nFileNotFoundError: [Errno 2] No such file or directory: '/home/zsun/fsca/fsca_final_training_all.csv'\nbash: line 0: exit: $: numeric argument required\n",
  "history_begin_time" : 1714624519029,
  "history_end_time" : 1714624521942,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "KQweFgq6p36a",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir, homedir\nimport pandas as pd\n\nworking_dir = work_dir\nfinal_output_name = \"final_merged_data_3yrs_all_stations_with_non_stations.csv\"\nchunk_size = '10MB'  # You can adjust this chunk size based on your hardware and data size\n\n\ndef merge_all_data_together():\n    amsr_file = f'{working_dir}/all_training_points_in_westus.csv_amsr_dask_all_training_ponits.csv'\n    print(f\"amsr_file = {amsr_file}\")\n    snotel_file = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_swe_restored_dask_all_vars.csv'\n    print(f\"snotel_file = {snotel_file}\")\n    gridmet_file = f'{working_dir}/training_all_point_gridmet_with_non_station.csv'\n    print(f\"gridmet_file = {gridmet_file}\")\n    terrain_file = f'{working_dir}/all_training_points_in_westus.csv_terrain_4km_grid_shift.csv'\n    print(f\"terrain_file = {terrain_file}\")\n    fsca_file = f'{homedir}/fsca/fsca_final_training_all.csv'\n    print(f\"fsca_file = {fsca_file}\")\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    print(f\"final_final_output_file = {final_final_output_file}\")\n    \n    if os.path.exists(final_final_output_file):\n      print(f\"The file '{final_final_output_file}' exists. Skipping\")\n      return final_final_output_file\n      \n    \n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(amsr_file, blocksize=chunk_size)\n    print(\"amsr.columns = \", amsr.columns)\n    snotel = dd.read_csv(snotel_file, blocksize=chunk_size)\n    print(\"snotel.columns = \", snotel.columns)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(gridmet_file, blocksize=chunk_size)\n    gridmet = gridmet.drop(columns=[\"Unnamed: 0\"])\n    print(\"gridmet.columns = \", gridmet.columns)\n    terrain = dd.read_csv(terrain_file, blocksize=chunk_size)\n    terrain = terrain.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    terrain = terrain[[\"lat\", \"lon\", 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness']]\n    print(\"terrain.columns = \", terrain.columns)\n    snowcover = dd.read_csv(fsca_file, blocksize=chunk_size)\n    snowcover = snowcover.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    print(\"snowcover.columns = \", snowcover.columns)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n    print(\"all the dataframes are partitioned\")\n\n    # Merge DataFrames based on specified columns\n    print(\"start to merge amsr and snotel\")\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snotel.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge gridmet\")\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_gridmet.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge terrain\")\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge snowcover\")\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f'Merge completed. {output_file}')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/{final_output_name}')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/{final_output_name}', single_file=True, index=False)\n    print('Data cleaning completed.')\n    return final_final_output_file\n\n  \ndef sort_training_data(input_training_csv, sorted_training_csv):\n    # Read Dask DataFrame from CSV with increased blocksize and assuming missing data\n    ddf = dd.read_csv(input_training_csv, assume_missing=True, blocksize='10MB')\n\n    # Persist the Dask DataFrame in memory\n    ddf = ddf.persist()\n\n    # Sort Dask DataFrame by three columns: date, lat, and Lon\n    sorted_ddf = ddf.sort_values(by=['date', 'lat', 'lon'])\n\n    # Save the sorted Dask DataFrame to a new CSV file\n    sorted_ddf.to_csv(sorted_training_csv, index=False, single_file=True)\n    print(f\"sorted training data is saved to {sorted_training_csv}\")\n  \nif __name__ == \"__main__\":\n    merge_all_data_together()\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    sort_training_data(final_final_output_file, f'{work_dir}/{final_output_name}_sorted.csv')",
  "history_output" : "today date = 2024-05-02\n2024-04-29\ntest start date:  2024-04-29\ntest end date:  2023-10-11\n/home/zsun\namsr_file = /home/zsun/gridmet_test_run/all_training_points_in_westus.csv_amsr_dask_all_training_ponits.csv\nsnotel_file = /home/zsun/gridmet_test_run/all_snotel_cdec_stations_active_in_westus.csv_swe_restored_dask_all_vars.csv\ngridmet_file = /home/zsun/gridmet_test_run/training_all_point_gridmet_with_non_station.csv\nterrain_file = /home/zsun/gridmet_test_run/all_training_points_in_westus.csv_terrain_4km_grid_shift.csv\nfsca_file = /home/zsun/fsca/fsca_final_training_all.csv\nfinal_final_output_file = /home/zsun/gridmet_test_run/final_merged_data_3yrs_all_stations_with_non_stations.csv\nTraceback (most recent call last):\n  File \"merge_custom_traning_range.py\", line 135, in <module>\n    merge_all_data_together()\n  File \"merge_custom_traning_range.py\", line 47, in merge_all_data_together\n    amsr = dd.read_csv(amsr_file, blocksize=chunk_size)\n  File \"/home/zsun/anaconda3/lib/python3.7/site-packages/dask/dataframe/io/csv.py\", line 578, in read\n    **kwargs\n  File \"/home/zsun/anaconda3/lib/python3.7/site-packages/dask/dataframe/io/csv.py\", line 405, in read_pandas\n    **(storage_options or {})\n  File \"/home/zsun/anaconda3/lib/python3.7/site-packages/dask/bytes/core.py\", line 121, in read_bytes\n    size = fs.info(path)[\"size\"]\n  File \"/home/zsun/anaconda3/lib/python3.7/site-packages/fsspec/implementations/local.py\", line 56, in info\n    out = os.stat(path, follow_symlinks=False)\nFileNotFoundError: [Errno 2] No such file or directory: '/home/zsun/gridmet_test_run/all_training_points_in_westus.csv_amsr_dask_all_training_ponits.csv'\nbash: line 0: exit: $: numeric argument required\n",
  "history_begin_time" : 1714623962836,
  "history_end_time" : 1714623965078,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "muVnqP6vTRC7",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir, homedir\nimport pandas as pd\n\nworking_dir = work_dir\nfinal_output_name = \"final_merged_data_3yrs_all_stations_with_non_stations.csv\"\nchunk_size = '10MB'  # You can adjust this chunk size based on your hardware and data size\n\n\ndef merge_all_data_together():\n    amsr_file = f'{working_dir}/all_training_points_in_westus.csv_amsr_dask_all_training_ponits.csv'\n    snotel_file = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_swe_restored_dask_all_vars.csv'\n    gridmet_file = f'{working_dir}/training_all_point_gridmet_with_non_station.csv'\n    terrain_file = f'{working_dir}/all_training_points_in_westus.csv_terrain_4km_grid_shift.csv'\n    fsca_file = f'{homedir}/fsca/fsca_final_training_all.csv'\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    \n    if os.path.exists(final_final_output_file):\n      print(f\"The file '{final_final_output_file}' exists. Skipping\")\n      return final_final_output_file\n      \n    \n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(amsr_file, blocksize=chunk_size)\n    print(\"amsr.columns = \", amsr.columns)\n    snotel = dd.read_csv(snotel_file, blocksize=chunk_size)\n    print(\"snotel.columns = \", snotel.columns)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(gridmet_file, blocksize=chunk_size)\n    gridmet = gridmet.drop(columns=[\"Unnamed: 0\"])\n    print(\"gridmet.columns = \", gridmet.columns)\n    terrain = dd.read_csv(terrain_file, blocksize=chunk_size)\n    terrain = terrain.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    terrain = terrain[[\"lat\", \"lon\", 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness']]\n    print(\"terrain.columns = \", terrain.columns)\n    snowcover = dd.read_csv(fsca_file, blocksize=chunk_size)\n    snowcover = snowcover.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    print(\"snowcover.columns = \", snowcover.columns)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n    print(\"all the dataframes are partitioned\")\n\n    # Merge DataFrames based on specified columns\n    print(\"start to merge amsr and snotel\")\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snotel.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge gridmet\")\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_gridmet.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge terrain\")\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge snowcover\")\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f'Merge completed. {output_file}')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/{final_output_name}')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/{final_output_name}', single_file=True, index=False)\n    print('Data cleaning completed.')\n    return final_final_output_file\n\n  \ndef sort_training_data(input_training_csv, sorted_training_csv):\n    # Read Dask DataFrame from CSV with increased blocksize and assuming missing data\n    ddf = dd.read_csv(input_training_csv, assume_missing=True, blocksize='10MB')\n\n    # Persist the Dask DataFrame in memory\n    ddf = ddf.persist()\n\n    # Sort Dask DataFrame by three columns: date, lat, and Lon\n    sorted_ddf = ddf.sort_values(by=['date', 'lat', 'lon'])\n\n    # Save the sorted Dask DataFrame to a new CSV file\n    sorted_ddf.to_csv(sorted_training_csv, index=False, single_file=True)\n    print(f\"sorted training data is saved to {sorted_training_csv}\")\n  \nif __name__ == \"__main__\":\n    merge_all_data_together()\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    sort_training_data(final_final_output_file, f'{work_dir}/{final_output_name}_sorted.csv')",
  "history_output" : "today date = 2024-05-02\n2024-04-29\ntest start date:  2024-04-29\ntest end date:  2023-10-11\n/home/zsun\nTraceback (most recent call last):\n  File \"merge_custom_traning_range.py\", line 129, in <module>\n    merge_all_data_together()\n  File \"merge_custom_traning_range.py\", line 41, in merge_all_data_together\n    amsr = dd.read_csv(amsr_file, blocksize=chunk_size)\n  File \"/home/zsun/anaconda3/lib/python3.7/site-packages/dask/dataframe/io/csv.py\", line 578, in read\n    **kwargs\n  File \"/home/zsun/anaconda3/lib/python3.7/site-packages/dask/dataframe/io/csv.py\", line 405, in read_pandas\n    **(storage_options or {})\n  File \"/home/zsun/anaconda3/lib/python3.7/site-packages/dask/bytes/core.py\", line 121, in read_bytes\n    size = fs.info(path)[\"size\"]\n  File \"/home/zsun/anaconda3/lib/python3.7/site-packages/fsspec/implementations/local.py\", line 56, in info\n    out = os.stat(path, follow_symlinks=False)\nFileNotFoundError: [Errno 2] No such file or directory: '/home/zsun/gridmet_test_run/all_training_points_in_westus.csv_amsr_dask_all_training_ponits.csv'\nbash: line 0: exit: $: numeric argument required\n",
  "history_begin_time" : 1714623863861,
  "history_end_time" : 1714623872818,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "oOUSRB5CI1qi",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir, homedir\nimport pandas as pd\n\nworking_dir = work_dir\nfinal_output_name = \"final_merged_data_3yrs_all_stations_with_non_stations.csv\"\nchunk_size = '10MB'  # You can adjust this chunk size based on your hardware and data size\n\n\ndef merge_all_data_together():\n    amsr_file = f'{working_dir}/all_training_points_in_westus.csv_amsr_dask_all_training_ponits.csv'\n    snotel_file = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_swe_restored_dask_all_vars.csv'\n    gridmet_file = f'{working_dir}/training_all_point_gridmet_with_non_station.csv'\n    terrain_file = f'{working_dir}/all_training_points_in_westus.csv_terrain_4km_grid_shift.csv'\n    fsca_file = f'{homedir}/fsca/fsca_final_training_all.csv'\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    \n    if os.path.exists(final_final_output_file):\n      print(f\"The file '{final_final_output_file}' exists. Skipping\")\n      return final_final_output_file\n      \n    \n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(amsr_file, blocksize=chunk_size)\n    print(\"amsr.columns = \", amsr.columns)\n    snotel = dd.read_csv(snotel_file, blocksize=chunk_size)\n    print(\"snotel.columns = \", snotel.columns)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(gridmet_file, blocksize=chunk_size)\n    gridmet = gridmet.drop(columns=[\"Unnamed: 0\"])\n    print(\"gridmet.columns = \", gridmet.columns)\n    terrain = dd.read_csv(terrain_file, blocksize=chunk_size)\n    terrain = terrain.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    terrain = terrain[[\"lat\", \"lon\", 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness']]\n    print(\"terrain.columns = \", terrain.columns)\n    snowcover = dd.read_csv(fsca_file, blocksize=chunk_size)\n    snowcover = snowcover.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    print(\"snowcover.columns = \", snowcover.columns)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n    print(\"all the dataframes are partitioned\")\n\n    # Merge DataFrames based on specified columns\n    print(\"start to merge amsr and snotel\")\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snotel.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge gridmet\")\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_gridmet.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge terrain\")\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge snowcover\")\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f'Merge completed. {output_file}')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/{final_output_name}')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/{final_output_name}', single_file=True, index=False)\n    print('Data cleaning completed.')\n    return final_final_output_file\n\n  \ndef sort_training_data(input_training_csv, sorted_training_csv):\n    # Read Dask DataFrame from CSV with increased blocksize and assuming missing data\n    ddf = dd.read_csv(input_training_csv, assume_missing=True, blocksize='10MB')\n\n    # Persist the Dask DataFrame in memory\n    ddf = ddf.persist()\n\n    # Sort Dask DataFrame by three columns: date, lat, and Lon\n    sorted_ddf = ddf.sort_values(by=['date', 'lat', 'lon'])\n\n    # Save the sorted Dask DataFrame to a new CSV file\n    sorted_ddf.to_csv(sorted_training_csv, index=False, single_file=True)\n    print(f\"sorted training data is saved to {sorted_training_csv}\")\n  \nif __name__ == \"__main__\":\n    merge_all_data_together()\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    sort_training_data(final_final_output_file, f'{work_dir}/{final_output_name}_sorted.csv')",
  "history_output" : "today date = 2024-05-02\n2024-04-29\ntest start date:  2024-04-29\ntest end date:  2023-10-11\n/home/chetana\namsr.columns =  Index(['date', 'lat', 'lon', 'AMSR_SWE'], dtype='object')\nsnotel.columns =  Index(['station_name', 'date', 'lat', 'lon', 'swe_value', 'change_in_swe_inch',\n       'snow_depth', 'air_temperature_observed_f'],\n      dtype='object')\ngridmet.columns =  Index(['day', 'lon', 'lat', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed'],\n      dtype='object')\nterrain.columns =  Index(['lat', 'lon', 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness',\n       'Eastness'],\n      dtype='object')\nsnowcover.columns =  Index(['date', 'lat', 'lon', 'fsca'], dtype='object')\nall the dataframes are partitioned\nstart to merge amsr and snotel\nintermediate file saved to /home/chetana/gridmet_test_run/final_merged_data_3yrs_all_stations_with_non_stations.csv_snotel.csv\nstart to merge gridmet\nintermediate file saved to /home/chetana/gridmet_test_run/final_merged_data_3yrs_all_stations_with_non_stations.csv_gridmet.csv\nstart to merge terrain\nintermediate file saved to /home/chetana/gridmet_test_run/final_merged_data_3yrs_all_stations_with_non_stations.csv_terrain.csv\nstart to merge snowcover\nintermediate file saved to /home/chetana/gridmet_test_run/final_merged_data_3yrs_all_stations_with_non_stations.csv_snow_cover.csv\nMerge completed. /home/chetana/gridmet_test_run/final_merged_data_3yrs_all_stations_with_non_stations.csv\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/oOUSRB5CI1qi/merge_custom_traning_range.py\", line 129, in <module>\n    merge_all_data_together()\n  File \"/home/chetana/gw-workspace/oOUSRB5CI1qi/merge_custom_traning_range.py\", line 109, in merge_all_data_together\n    df.to_csv(f'{work_dir}/{final_output_name}', single_file=True, index=False)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/dask/dataframe/core.py\", line 1897, in to_csv\n    return to_csv(self, filename, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 995, in to_csv\n    return list(dask.compute(*values, **compute_kwargs))\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/dask/base.py\", line 628, in compute\n    results = schedule(dsk, keys, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 142, in __call__\n    df = pandas_read_text(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 197, in pandas_read_text\n    coerce_dtypes(df, dtypes)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 298, in coerce_dtypes\n    raise ValueError(msg)\nValueError: Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\n+--------------+--------+----------+\n| Column       | Found  | Expected |\n+--------------+--------+----------+\n| station_name | object | float64  |\n+--------------+--------+----------+\nThe following columns also raised exceptions on conversion:\n- station_name\n  ValueError(\"could not convert string to float: 'Carrot Basin'\")\nUsually this is due to dask's dtype inference failing, and\n*may* be fixed by specifying dtypes manually by adding:\ndtype={'station_name': 'object'}\nto the call to `read_csv`/`read_table`.\n",
  "history_begin_time" : 1714621194958,
  "history_end_time" : 1714635471846,
  "history_notes" : "merge all the new files with non station rows included, might have rows in 2024 and 2023",
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "GopK316nQz4c",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir, homedir\nimport pandas as pd\n\nworking_dir = work_dir\nfinal_output_name = \"final_merged_data_3yrs_all_stations_with_non_stations.csv\"\nchunk_size = '10MB'  # You can adjust this chunk size based on your hardware and data size\n\n\ndef merge_all_data_together():\n    amsr_file = f'{working_dir}/all_training_points_in_westus.csv_amsr_dask_all_training_ponits.csv'\n    snotel_file = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_swe_restored_dask_all_vars.csv'\n    gridmet_file = f'{working_dir}/training_all_point_gridmet_with_non_station.csv'\n    terrain_file = f'{working_dir}/all_training_points_in_westus.csv_terrain_4km_grid_shift.csv'\n    fsca_file = f'{homedir}/fsca/fsca_final_training_all.csv'\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    \n    if os.path.exists(final_final_output_file):\n      print(f\"The file '{final_final_output_file}' exists. Skipping\")\n      return final_final_output_file\n      \n    \n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(amsr_file, blocksize=chunk_size)\n    print(\"amsr.columns = \", amsr.columns)\n    snotel = dd.read_csv(snotel_file, blocksize=chunk_size)\n    print(\"snotel.columns = \", snotel.columns)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(gridmet_file, blocksize=chunk_size)\n    gridmet = gridmet.drop(columns=[\"Unnamed: 0\"])\n    print(\"gridmet.columns = \", gridmet.columns)\n    terrain = dd.read_csv(terrain_file, blocksize=chunk_size)\n    terrain = terrain.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    terrain = terrain[[\"stationTriplet\", \"elevation\", \"lat\", \"lon\", 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness']]\n    print(\"terrain.columns = \", terrain.columns)\n    snowcover = dd.read_csv(fsca_file, blocksize=chunk_size)\n    snowcover = snowcover.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    print(\"snowcover.columns = \", snowcover.columns)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n    print(\"all the dataframes are partitioned\")\n\n    # Merge DataFrames based on specified columns\n    print(\"start to merge amsr and snotel\")\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snotel.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge gridmet\")\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_gridmet.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge terrain\")\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge snowcover\")\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f'Merge completed. {output_file}')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/{final_output_name}')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/{final_output_name}', single_file=True, index=False)\n    print('Data cleaning completed.')\n    return final_final_output_file\n\n  \ndef sort_training_data(input_training_csv, sorted_training_csv):\n    # Read Dask DataFrame from CSV with increased blocksize and assuming missing data\n    ddf = dd.read_csv(input_training_csv, assume_missing=True, blocksize='10MB')\n\n    # Persist the Dask DataFrame in memory\n    ddf = ddf.persist()\n\n    # Sort Dask DataFrame by three columns: date, lat, and Lon\n    sorted_ddf = ddf.sort_values(by=['date', 'lat', 'lon'])\n\n    # Save the sorted Dask DataFrame to a new CSV file\n    sorted_ddf.to_csv(sorted_training_csv, index=False, single_file=True)\n    print(f\"sorted training data is saved to {sorted_training_csv}\")\n  \nif __name__ == \"__main__\":\n    merge_all_data_together()\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    sort_training_data(final_final_output_file, f'{work_dir}/{final_output_name}_sorted.csv')",
  "history_output" : "today date = 2024-05-02\n2024-04-29\ntest start date:  2024-04-29\ntest end date:  2023-10-11\n/home/chetana\namsr.columns =  Index(['date', 'lat', 'lon', 'AMSR_SWE'], dtype='object')\nsnotel.columns =  Index(['station_name', 'date', 'lat', 'lon', 'swe_value', 'change_in_swe_inch',\n       'snow_depth', 'air_temperature_observed_f'],\n      dtype='object')\ngridmet.columns =  Index(['day', 'lon', 'lat', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/GopK316nQz4c/merge_custom_traning_range.py\", line 129, in <module>\n    merge_all_data_together()\n  File \"/home/chetana/gw-workspace/GopK316nQz4c/merge_custom_traning_range.py\", line 54, in merge_all_data_together\n    terrain = terrain[[\"stationTriplet\", \"elevation\", \"lat\", \"lon\", 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness']]\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/dask/dataframe/core.py\", line 4917, in __getitem__\n    meta = self._meta[_extract_meta(key)]\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\", line 3813, in __getitem__\n    indexer = self.columns._get_indexer_strict(key, \"columns\")[1]\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 6070, in _get_indexer_strict\n    self._raise_if_missing(keyarr, indexer, axis_name)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 6133, in _raise_if_missing\n    raise KeyError(f\"{not_found} not in index\")\nKeyError: \"['stationTriplet', 'elevation'] not in index\"\n",
  "history_begin_time" : 1714620801135,
  "history_end_time" : 1714620895960,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "gsbocu5139n",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714409076782,
  "history_end_time" : 1714409076782,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "jgvo69h0cjc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714407283836,
  "history_end_time" : 1714407283836,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wti2f037orn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714404800097,
  "history_end_time" : 1714404800097,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9b85s4crrri",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714363210165,
  "history_end_time" : 1715549415917,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "pqmf1zol3ml",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714362032676,
  "history_end_time" : 1714362032676,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ptpl3u5irl6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714355595242,
  "history_end_time" : 1714355595242,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rjc02v0hqz6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714351396831,
  "history_end_time" : 1714351396831,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "j05256mji3o",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714348539284,
  "history_end_time" : 1714348539284,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6buwmtr3anm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714345973244,
  "history_end_time" : 1714345973244,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "gtyilqfxwiz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714343583853,
  "history_end_time" : 1714343583853,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "808gpb65ngc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714340841819,
  "history_end_time" : 1714340841819,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6q3s23esh9p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714332798139,
  "history_end_time" : 1714332798139,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "87wk1t7f8ob",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714328816209,
  "history_end_time" : 1714328816209,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "l10ynx090n2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714323529103,
  "history_end_time" : 1714323529103,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "087kpag072k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714320354965,
  "history_end_time" : 1714320354965,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "g2qw8ug5uv0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714315863117,
  "history_end_time" : 1714315863117,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "qk19nd64k8f",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714283543366,
  "history_end_time" : 1714315862062,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "8ibfajdzhm5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714281870980,
  "history_end_time" : 1714283515038,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "byeyeuzxcdf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714276838376,
  "history_end_time" : 1714315876911,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "we28gcc17ia",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714274662756,
  "history_end_time" : 1714281869550,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ae8n28tdcv0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714272894719,
  "history_end_time" : 1714274352443,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "z1ln19e5s3e",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714265253439,
  "history_end_time" : 1714272892780,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "udp4a6lndzh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714265112646,
  "history_end_time" : 1714265250935,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "bnptgg3v6rq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714192736297,
  "history_end_time" : 1714265111656,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "e1yc87ff23j",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714190418313,
  "history_end_time" : 1714274351051,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "cf8eiyzwybh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714104009487,
  "history_end_time" : 1714274350450,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "bxv6c0h6zmy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714017609824,
  "history_end_time" : 1714274349936,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "p9s5dwlhfrs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1713931210129,
  "history_end_time" : 1714274349391,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "l5f1bbhge5z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1713844809824,
  "history_end_time" : 1714274348792,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ttkrhd2f3gc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1713758409400,
  "history_end_time" : 1714274347355,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "moym7qz12xn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1713672009841,
  "history_end_time" : 1714274346364,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "08bpayupq4a",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1713585609637,
  "history_end_time" : 1714274345851,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "1uir1451cfq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1713499209840,
  "history_end_time" : 1714274345298,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "40ro4hw8su3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1713412809070,
  "history_end_time" : 1714274344724,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "j6z6qorszvy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1713326409181,
  "history_end_time" : 1714274343303,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "50b2fmbm04j",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1713283461320,
  "history_end_time" : 1713283461320,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rr482wm2rph",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1713240009177,
  "history_end_time" : 1714274342597,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "l62fw9uwy3l",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711771209765,
  "history_end_time" : 1711771209765,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ya390vmkm2s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711684813821,
  "history_end_time" : 1714282476116,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "mx3cmrgqkq3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711598415240,
  "history_end_time" : 1714282474708,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "9e1q0o7dv49",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711592280525,
  "history_end_time" : 1711592280525,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8m9h9mt8zfz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711592081916,
  "history_end_time" : 1711592081916,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "s11nr1ff6pu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711512016547,
  "history_end_time" : 1711512016547,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "sdsd306zj5d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711425614307,
  "history_end_time" : 1714282483186,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "fgicv0fa3ok",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711339212860,
  "history_end_time" : 1714282483865,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "dbz3b05qm1a",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711252813634,
  "history_end_time" : 1711252813634,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "5ev6ltr10nj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711166414232,
  "history_end_time" : 1711166414232,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "kcjlcrklgeu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711080009222,
  "history_end_time" : 1711080009222,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "apq2wt625s9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710993609864,
  "history_end_time" : 1714282484651,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "0wbj970fs9t",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710907209300,
  "history_end_time" : 1714282485294,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "op2lebu0wsh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710820808981,
  "history_end_time" : 1714282486306,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "orcwpw0bs2i",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710734409707,
  "history_end_time" : 1714282486973,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "xvzof4ctvam",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710690214190,
  "history_end_time" : 1710690214190,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "guipunbp6q5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710648009398,
  "history_end_time" : 1714282488978,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "d3xaowwylb2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710561608856,
  "history_end_time" : 1714282489555,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "x2yri6ose7w",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710475209452,
  "history_end_time" : 1714282490323,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "etctcl31w6y",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710388809429,
  "history_end_time" : 1714282490849,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "q4jcd1z0dep",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710302409552,
  "history_end_time" : 1714282491522,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "lvdlxy4frwr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710216009632,
  "history_end_time" : 1714282492139,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "4ijgx3py6rz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710172046533,
  "history_end_time" : 1710172046533,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "7b7p1yo99pz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710129609293,
  "history_end_time" : 1714282493751,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "68vjy01gf3w",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710080234068,
  "history_end_time" : 1710080234068,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "ir9j61nrypt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710043208949,
  "history_end_time" : 1714282495096,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "bebq7mpi8v0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709998716280,
  "history_end_time" : 1709998716280,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tfxs5613vcl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709956809187,
  "history_end_time" : 1714282495737,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "rwqx0h60r2r",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709924987586,
  "history_end_time" : 1709924987586,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "nfstje3dcs4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709870410197,
  "history_end_time" : 1714282496854,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "erqe6oy6w44",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709845595322,
  "history_end_time" : 1709845595322,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "hon54ff9mf9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709844621963,
  "history_end_time" : 1709844621963,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "kij3bnlncdw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709842923385,
  "history_end_time" : 1709842923385,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6nc4yw36oug",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709827652615,
  "history_end_time" : 1709844621177,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "le8lyzwlz56",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709826138699,
  "history_end_time" : 1709826138699,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "be6ubkbpqum",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709797240836,
  "history_end_time" : 1709797240836,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "qfjvaxoemqw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709791538001,
  "history_end_time" : 1709791538001,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "e41zxcby5m1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709784009678,
  "history_end_time" : 1714282504741,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "h03drsq18ud",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709778056667,
  "history_end_time" : 1709778056667,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "q79jjotfs58",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709774843610,
  "history_end_time" : 1709774843610,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "z81njy29rxr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709765244671,
  "history_end_time" : 1709774842948,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "pfukhwscx7u",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709763117630,
  "history_end_time" : 1709765243447,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "pcp2pjkzjsk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709751527382,
  "history_end_time" : 1709751527382,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "q12sks482ya",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709751443184,
  "history_end_time" : 1709751495337,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "eoel6wxgmqe",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709697609508,
  "history_end_time" : 1714282506687,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "eczryf9nvfd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709611208940,
  "history_end_time" : 1714282507223,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "tymn97aoomx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709524809109,
  "history_end_time" : 1714282507732,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "jvuuws5o82l",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709438408858,
  "history_end_time" : 1709438408858,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "yty52x99e3i",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709352009270,
  "history_end_time" : 1709352009270,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "hs2up09wgwm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709265609663,
  "history_end_time" : 1709265609663,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "klm0r0kkie6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709179209617,
  "history_end_time" : 1709179209617,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "39uhi6xndds",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709137312970,
  "history_end_time" : 1709137312970,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "cbkgc6ztpjy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709092809208,
  "history_end_time" : 1709092809208,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "toaw451esk5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709085684208,
  "history_end_time" : 1709085684208,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "ypv5m613lif",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709078942158,
  "history_end_time" : 1709085672811,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "8drjlv",
  "indicator" : "Stopped"
},{
  "history_id" : "ar0yj8t2n4c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709038894865,
  "history_end_time" : 1709038894865,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "uinfe9tvh2q",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709038872988,
  "history_end_time" : 1709038879072,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "8drjlv",
  "indicator" : "Stopped"
},{
  "history_id" : "cu8rby0e2zs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709006408797,
  "history_end_time" : 1709006408797,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "oz4okxxpnrj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708971826217,
  "history_end_time" : 1708971826217,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "z5fm1ieb7e3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708958410888,
  "history_end_time" : 1708958410888,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "lvy3t478wf7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708954624220,
  "history_end_time" : 1708954624220,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "c877ohyq4ck",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708920009641,
  "history_end_time" : 1708920009641,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ej7246ltvhs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708833609328,
  "history_end_time" : 1708833609328,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tap8jaumow3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708747209064,
  "history_end_time" : 1708747209064,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "fa815ru6e6x",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708660809219,
  "history_end_time" : 1708660809219,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ih1fz04odxp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708574409876,
  "history_end_time" : 1708574409876,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "e7v6a19czc1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708488010367,
  "history_end_time" : 1708488010367,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "h8ancethuxl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708401609112,
  "history_end_time" : 1708401609112,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "sst3r3b07wp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708352215546,
  "history_end_time" : 1708352215546,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "7lsyru4mq3t",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708348193296,
  "history_end_time" : 1708352214875,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "8drjlv",
  "indicator" : "Stopped"
},{
  "history_id" : "7cvscjcep76",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708315209265,
  "history_end_time" : 1708315209265,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ezfm7s4h38p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708312689960,
  "history_end_time" : 1708312689960,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "ivg77x9qd2s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708305309934,
  "history_end_time" : 1708312689136,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "8drjlv",
  "indicator" : "Stopped"
},{
  "history_id" : "ikn6akvinhg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708242679321,
  "history_end_time" : 1708242679321,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "brk9jgmd36c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708240733533,
  "history_end_time" : 1708240733533,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "qrei0hfisa6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708238769952,
  "history_end_time" : 1708238769952,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "imyefrh32m1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708237144897,
  "history_end_time" : 1708237144897,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "dye4zjxdjaz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708235187248,
  "history_end_time" : 1708235187248,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "70akrpboxtx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708233874695,
  "history_end_time" : 1708233874695,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "7w70sczy38m",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708228809404,
  "history_end_time" : 1708228809404,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "mtahkvb70ok",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708227613824,
  "history_end_time" : 1708227613824,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "5pamyokgise",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708142409587,
  "history_end_time" : 1708142409587,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0j9atwupw2d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708056009366,
  "history_end_time" : 1708056009366,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "srveksn4tg6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707969609230,
  "history_end_time" : 1707969609230,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3pdl7bzg1mm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707883209024,
  "history_end_time" : 1707883209024,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "4ruussinhf4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707796809513,
  "history_end_time" : 1707796809513,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "a5tw8498yy8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707750669764,
  "history_end_time" : 1707750669764,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "na9jn87lt6d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707710409575,
  "history_end_time" : 1707710409575,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "nmzvm7tg2tp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707624009590,
  "history_end_time" : 1707624009590,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "gsw1ih8m4w0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707537608764,
  "history_end_time" : 1707537608764,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9a913pgnqin",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707491746716,
  "history_end_time" : 1707491746716,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "fxchsagixrz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707484657942,
  "history_end_time" : 1707484657942,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "vks9en0zhuh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707451209846,
  "history_end_time" : 1707451209846,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "23uucy9i1hv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707434644197,
  "history_end_time" : 1707434644197,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "4ewnewqontu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707432571817,
  "history_end_time" : 1707432571817,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "y5ho2pfocy0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707431129255,
  "history_end_time" : 1707432053958,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "8drjlv",
  "indicator" : "Stopped"
},{
  "history_id" : "u1njiq10v15",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707418188653,
  "history_end_time" : 1707418188653,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "mahjd7",
  "indicator" : "Skipped"
},{
  "history_id" : "xbjs9um7tpj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707413610320,
  "history_end_time" : 1707413610320,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "mahjd7",
  "indicator" : "Skipped"
},{
  "history_id" : "fu6loem0fpw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707364808863,
  "history_end_time" : 1707364808863,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "mexg91iyuhf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707278409100,
  "history_end_time" : 1707278409100,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "dape01i2s1m",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707192718887,
  "history_end_time" : 1707192718887,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "4f5njhbqk2b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707192009291,
  "history_end_time" : 1707448888652,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "p9u0vfbc4pw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707189399073,
  "history_end_time" : 1707189399073,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "205v0m4etj0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707105609320,
  "history_end_time" : 1707750639193,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "xvdr5co1d5b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707019209343,
  "history_end_time" : 1707750639663,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "9w5gch8d4al",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706932809221,
  "history_end_time" : 1707750640206,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ac2mo0xlpka",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706846409688,
  "history_end_time" : 1707750640640,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "w451hb94ply",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706760009967,
  "history_end_time" : 1707750643296,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "7s1wbpdni14",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706673609492,
  "history_end_time" : 1707750643802,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "56fnhdmv24b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706587209423,
  "history_end_time" : 1707750644985,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "dtqduzygf2t",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706500809009,
  "history_end_time" : 1707750645662,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "1j3mhbwxe2h",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706414409338,
  "history_end_time" : 1707750646107,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "lf6twzogupm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706366105906,
  "history_end_time" : 1706366105906,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "4m9snjw5wqf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706364888460,
  "history_end_time" : 1706364888460,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "37mtdjjwpso",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706328009502,
  "history_end_time" : 1707750646906,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "lwan4gi9odw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706280497960,
  "history_end_time" : 1706280497960,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tno1vmpw91x",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706244881237,
  "history_end_time" : 1706244881237,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "bswomr8inh0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706241609735,
  "history_end_time" : 1706244810655,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "lazg2s270o1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706155209594,
  "history_end_time" : 1706244801317,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "nxnuf1hkqvb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706068809118,
  "history_end_time" : 1706244800342,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "e92f7qcnvxx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705982409013,
  "history_end_time" : 1706244799975,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "tryw5ht0dir",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705896759200,
  "history_end_time" : 1706244799012,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "sj6dzkdall1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705896009801,
  "history_end_time" : 1706244798531,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "vzm48zt098z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705849064045,
  "history_end_time" : 1706244798019,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "tkq412hc8j8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705809608953,
  "history_end_time" : 1705849649602,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "9yte14ddgbx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705793527099,
  "history_end_time" : 1705849647061,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "18udaeaeitc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705790835091,
  "history_end_time" : 1705790835091,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "42nm7li4x17",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705770628118,
  "history_end_time" : 1705849642248,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "5h9ypk5lc7a",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705762760873,
  "history_end_time" : 1705849640892,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "kzhvkw7qy1r",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705723209525,
  "history_end_time" : 1705789738449,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "rw1lk3iq1b2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705636808837,
  "history_end_time" : 1705770636228,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "idu0b1s5ftc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705550409285,
  "history_end_time" : 1705770635547,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "q7p20fe1uxr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705464008877,
  "history_end_time" : 1705770635057,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "jn8l5z8ij4x",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705422422512,
  "history_end_time" : 1705422422512,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "o913e8vmg11",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705377609401,
  "history_end_time" : 1705770633050,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "hn3mm3loh4i",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705291209072,
  "history_end_time" : 1705770632170,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "bdwzxlctwrn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705278850586,
  "history_end_time" : 1705278850586,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "mnra2or2rix",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705270952784,
  "history_end_time" : 1705270952784,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9uxilfxpszq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705204809518,
  "history_end_time" : 1705789662005,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "dy2i68naelz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705169057223,
  "history_end_time" : 1705169057223,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "uirdiahnxg2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705118409666,
  "history_end_time" : 1705789660773,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "r6324mg0ng7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705072448125,
  "history_end_time" : 1705072448125,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "hrfai4ir974",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705032009460,
  "history_end_time" : 1705789659545,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "1eqeknhorbo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704979918357,
  "history_end_time" : 1704979918357,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "yze1h4i71xy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704945609667,
  "history_end_time" : 1705789658812,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "nnmx2p6v97o",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704918977735,
  "history_end_time" : 1704918977735,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "n7mcon5skxt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704908919773,
  "history_end_time" : 1704908919773,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xxtap16fqw6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704859207626,
  "history_end_time" : 1705789668259,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "3lupoa4nduk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704775840754,
  "history_end_time" : 1704775840754,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "78iz6mxmn4n",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704772806848,
  "history_end_time" : 1705789667289,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "dzpvrtpddag",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704726161271,
  "history_end_time" : 1704727049031,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "5lv12s2144c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704686408418,
  "history_end_time" : 1705789666647,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "lqkgqnumn7c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704644803723,
  "history_end_time" : 1704644803723,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "frojp4kasah",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704600008214,
  "history_end_time" : 1705789665920,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "x7374s1v7cm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704566156628,
  "history_end_time" : 1704566156628,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "yk0jupak783",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704565587385,
  "history_end_time" : 1704565587385,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "slue1kpp3cm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704564424162,
  "history_end_time" : 1704564424162,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "t1goigwdnfy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704562992181,
  "history_end_time" : 1704562992181,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "7bc09uziw6b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704561889800,
  "history_end_time" : 1704561889800,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6238wzaw6o0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704561861173,
  "history_end_time" : 1704561887040,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "loi3yebdo3k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704555479210,
  "history_end_time" : 1704555479210,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ct682u60jam",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704555028201,
  "history_end_time" : 1704555028201,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "nllgq0s8xcc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704553241772,
  "history_end_time" : 1704553241772,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "y5h5iq7emjk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704552254630,
  "history_end_time" : 1704552254630,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "289xto9xn19",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704513607408,
  "history_end_time" : 1705789671128,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "r4pktnt9ecr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704427207560,
  "history_end_time" : 1705789671870,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "en3076bd2cw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704340807587,
  "history_end_time" : 1705789673113,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "np984p68jtd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704330109314,
  "history_end_time" : 1704330109314,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "aeeo6pra1qh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704329364855,
  "history_end_time" : 1704329364855,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "kbw5lban09p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704254407554,
  "history_end_time" : 1705789675679,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "rm9lhkn5mzs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704208947961,
  "history_end_time" : 1704208947961,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "80hatqy7tn3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704207352022,
  "history_end_time" : 1704207352022,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "55t0r7i5i22",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704205859376,
  "history_end_time" : 1704205859376,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "06ot6t973ru",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704168007433,
  "history_end_time" : 1705789676826,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "aoc60ua44z7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704081607536,
  "history_end_time" : 1705789677619,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "1gpz2ym1k66",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703995208426,
  "history_end_time" : 1705789678814,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "jfb1owt77zo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703962871408,
  "history_end_time" : 1703962871408,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "x2b251kao1b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703960265447,
  "history_end_time" : 1703960265447,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "up17v1uv5gd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703959737848,
  "history_end_time" : 1703959737848,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "gy6xazsx9fh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703958611592,
  "history_end_time" : 1703958611592,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ynrsladzn7a",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703955838229,
  "history_end_time" : 1703955838229,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "crf1x7revef",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703954150375,
  "history_end_time" : 1703954150375,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "y06qvi5iv17",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703915768075,
  "history_end_time" : 1703915768075,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8z093yk62nb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703915283490,
  "history_end_time" : 1703915283490,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "f8onhmvgbgw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703914476645,
  "history_end_time" : 1703914476645,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "4fod3rhyecm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703912302178,
  "history_end_time" : 1703912302178,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xwjm36vi6x0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703908807169,
  "history_end_time" : 1705789681233,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "yf3ydz0ynev",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703906215383,
  "history_end_time" : 1703906215383,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "sbyqxtxds5x",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703900919147,
  "history_end_time" : 1703900919147,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "2onyyapk0ub",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703899837770,
  "history_end_time" : 1703899837770,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "q5tvikztc40",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703897422954,
  "history_end_time" : 1703897422954,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "7r5tt17pdba",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703896125586,
  "history_end_time" : 1703896125586,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "n7uzrajxjct",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703890275994,
  "history_end_time" : 1703890275994,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tjq98fufnsg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703886800812,
  "history_end_time" : 1703886800812,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "cwuuctt7cj0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703885997769,
  "history_end_time" : 1703885997769,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "horkxpraonr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703880194719,
  "history_end_time" : 1703880194719,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "m5uowgvfxyu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703872753037,
  "history_end_time" : 1703872753037,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "a5ywh0pu31l",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703869828239,
  "history_end_time" : 1703869828239,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3iu29mfen64",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703868616934,
  "history_end_time" : 1703868616934,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "79ugaa74u40",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703867114050,
  "history_end_time" : 1703867114050,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wklv0lft03s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703864885449,
  "history_end_time" : 1703864885449,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "jgqdoshuj1e",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703862637367,
  "history_end_time" : 1703862637367,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "dd0f7abdbnl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703827227344,
  "history_end_time" : 1703827227344,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9ebjsdzfo64",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703822411485,
  "history_end_time" : 1703822411485,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "34xpx49h5q0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703786924637,
  "history_end_time" : 1703789718828,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "j71lrl215lk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703786053491,
  "history_end_time" : 1703786917617,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ov1f7izs8jq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703778395397,
  "history_end_time" : 1703778395397,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6f9vioto37m",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703739034493,
  "history_end_time" : 1703739034493,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9kk4t2zhrdf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703738754541,
  "history_end_time" : 1703792459278,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "25d3gevd8be",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703736166911,
  "history_end_time" : 1703737316888,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "vbce4menzrn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703694763575,
  "history_end_time" : 1703694763575,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "dwoi54jo5re",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703659541204,
  "history_end_time" : 1703659541204,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3bkmff50ona",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703658144702,
  "history_end_time" : 1703658144702,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ch774akz1bv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703650855776,
  "history_end_time" : 1703650855776,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ne8q7gjgnqs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703646751538,
  "history_end_time" : 1703650812446,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "twp359ky72s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703642120898,
  "history_end_time" : 1703646749628,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "4fblfjlvdzu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703641988944,
  "history_end_time" : 1703642074635,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "j00snf16pih",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703629665510,
  "history_end_time" : 1703629665510,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vg7oh4t2qvz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703626687988,
  "history_end_time" : 1703627783059,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "wn3yzgfs14n",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703625782101,
  "history_end_time" : 1703625782101,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6381gezthcy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703624783983,
  "history_end_time" : 1703624783983,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "YC6XAbFnfsdS",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir, homedir\nimport pandas as pd\n\nworking_dir = work_dir\nfinal_output_name = \"final_merged_data_3yrs_all_active_stations_v1.csv\"\nchunk_size = '10MB'  # You can adjust this chunk size based on your hardware and data size\n\n\ndef merge_all_data_together():\n    amsr_file = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_amsr_dask.csv'\n    snotel_file = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_swe_restored_dask_all_vars.csv'\n    gridmet_file = f'{working_dir}/training_all_active_snotel_station_list_elevation.csv_gridmet.csv'\n    terrain_file = f'{working_dir}/training_all_active_snotel_station_list_elevation.csv_terrain_4km_grid_shift.csv'\n    fsca_file = f'{homedir}/fsca/fsca_final_training_all.csv'\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    \n    if os.path.exists(final_final_output_file):\n      print(f\"The file '{final_final_output_file}' exists. Skipping\")\n      return final_final_output_file\n      \n    \n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(amsr_file, blocksize=chunk_size)\n    print(\"amsr.columns = \", amsr.columns)\n    snotel = dd.read_csv(snotel_file, blocksize=chunk_size)\n    print(\"snotel.columns = \", snotel.columns)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(gridmet_file, blocksize=chunk_size)\n    gridmet = gridmet.drop(columns=[\"Unnamed: 0\"])\n    print(\"gridmet.columns = \", gridmet.columns)\n    terrain = dd.read_csv(terrain_file, blocksize=chunk_size)\n    terrain = terrain.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    terrain = terrain[[\"stationTriplet\", \"elevation\", \"lat\", \"lon\", 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness']]\n    print(\"terrain.columns = \", terrain.columns)\n    snowcover = dd.read_csv(fsca_file, blocksize=chunk_size)\n    snowcover = snowcover.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    print(\"snowcover.columns = \", snowcover.columns)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n    print(\"all the dataframes are partitioned\")\n\n    # Merge DataFrames based on specified columns\n    print(\"start to merge amsr and snotel\")\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snotel.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge gridmet\")\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_gridmet.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge terrain\")\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge snowcover\")\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f'Merge completed. {output_file}')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/{final_output_name}')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/{final_output_name}', single_file=True, index=False)\n    print('Data cleaning completed.')\n    return final_final_output_file\n\n  \ndef sort_training_data(input_training_csv, sorted_training_csv):\n    # Read DataFrame from CSV\n    df = pd.read_csv(input_training_csv)\n\n    # Sort DataFrame by three columns: date, lat, and Lon\n    sorted_df = df.sort_values(by=['date', 'lat', 'lon'])\n\n    # Save the sorted DataFrame to a new CSV file\n    sorted_df.to_csv(sorted_training_csv, index=False)\n\n    print(f\"The DataFrame has been sorted and saved to '{sorted_training_csv}'.\")\n  \nif __name__ == \"__main__\":\n    merge_all_data_together()\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    sort_training_data(final_final_output_file, f'{work_dir}/{final_output_name}_sorted.csv')",
  "history_output" : "today date = 2023-12-26\ntest start date:  2023-01-20\ntest end date:  2023-10-11\n/home/chetana\n2022275\nThe file '/home/chetana/gridmet_test_run/final_merged_data_3yrs_all_active_stations_v1.csv' exists. Skipping\nThe DataFrame has been sorted and saved to '/home/chetana/gridmet_test_run/final_merged_data_3yrs_all_active_stations_v1.csv_sorted.csv'.\n",
  "history_begin_time" : 1703614879302,
  "history_end_time" : 1703615003440,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "f6lB0jaxj08u",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir, homedir\nimport pandas as pd\n\nworking_dir = work_dir\nfinal_output_name = \"final_merged_data_3yrs_all_active_stations_v1.csv\"\nchunk_size = '10MB'  # You can adjust this chunk size based on your hardware and data size\n\n\ndef merge_all_data_together():\n    amsr_file = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_amsr_dask.csv'\n    snotel_file = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_swe_restored_dask_all_vars.csv'\n    gridmet_file = f'{working_dir}/training_all_active_snotel_station_list_elevation.csv_gridmet.csv'\n    terrain_file = f'{working_dir}/training_all_active_snotel_station_list_elevation.csv_terrain_4km_grid_shift.csv'\n    fsca_file = f'{homedir}/fsca/fsca_final_training_all.csv'\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    \n    if os.path.exists(final_final_output_file):\n      print(f\"The file '{final_final_output_file}' exists. Skipping\")\n      return final_final_output_file\n      \n    \n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(amsr_file, blocksize=chunk_size)\n    print(\"amsr.columns = \", amsr.columns)\n    snotel = dd.read_csv(snotel_file, blocksize=chunk_size)\n    print(\"snotel.columns = \", snotel.columns)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(gridmet_file, blocksize=chunk_size)\n    gridmet = gridmet.drop(columns=[\"Unnamed: 0\"])\n    print(\"gridmet.columns = \", gridmet.columns)\n    terrain = dd.read_csv(terrain_file, blocksize=chunk_size)\n    terrain = terrain.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    terrain = terrain[[\"stationTriplet\", \"elevation\", \"lat\", \"lon\", 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness']]\n    print(\"terrain.columns = \", terrain.columns)\n    snowcover = dd.read_csv(fsca_file, blocksize=chunk_size)\n    snowcover = snowcover.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    print(\"snowcover.columns = \", snowcover.columns)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n    print(\"all the dataframes are partitioned\")\n\n    # Merge DataFrames based on specified columns\n    print(\"start to merge amsr and snotel\")\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snotel.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge gridmet\")\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_gridmet.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge terrain\")\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge snowcover\")\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f'Merge completed. {output_file}')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/{final_output_name}')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/{final_output_name}', single_file=True, index=False)\n    print('Data cleaning completed.')\n    return final_final_output_file\n\n  \ndef sort_training_data(input_training_csv, sorted_training_csv):\n    # Read DataFrame from CSV\n    df = pd.read_csv(input_training_csv)\n\n    # Sort DataFrame by three columns: date, lat, and Lon\n    sorted_df = df.sort_values(by=['date', 'lat', 'Lon'])\n\n    # Save the sorted DataFrame to a new CSV file\n    sorted_df.to_csv(sorted_training_csv, index=False)\n\n    print(f\"The DataFrame has been sorted and saved to '{sorted_training_csv}'.\")\n  \nif __name__ == \"__main__\":\n    merge_all_data_together()\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    sort_training_data(final_final_output_file, f'{work_dir}/{final_output_name}_sorted.csv')",
  "history_output" : "today date = 2023-12-26\ntest start date:  2023-01-20\ntest end date:  2023-10-11\n/home/chetana\n2022275\nThe file '/home/chetana/gridmet_test_run/final_merged_data_3yrs_all_active_stations_v1.csv' exists. Skipping\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/f6lB0jaxj08u/merge_custom_traning_range.py\", line 129, in <module>\n    sort_training_data(final_final_output_file, f'{work_dir}/{final_output_name}_sorted.csv')\n  File \"/home/chetana/gw-workspace/f6lB0jaxj08u/merge_custom_traning_range.py\", line 119, in sort_training_data\n    sorted_df = df.sort_values(by=['date', 'lat', 'Lon'])\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\", line 6894, in sort_values\n    keys = [self._get_label_or_level_values(x, axis=axis) for x in by]\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\", line 6894, in <listcomp>\n    keys = [self._get_label_or_level_values(x, axis=axis) for x in by]\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\", line 1850, in _get_label_or_level_values\n    raise KeyError(key)\nKeyError: 'Lon'\n",
  "history_begin_time" : 1703614830754,
  "history_end_time" : 1703614849546,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "rZf1YVlz4hm5",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir, homedir\n\nworking_dir = work_dir\nfinal_output_name = \"final_merged_data_3yrs_all_active_stations_v1.csv\"\nchunk_size = '10MB'  # You can adjust this chunk size based on your hardware and data size\n\n\ndef merge_all_data_together():\n    amsr_file = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_amsr_dask.csv'\n    snotel_file = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_swe_restored_dask_all_vars.csv'\n    gridmet_file = f'{working_dir}/training_all_active_snotel_station_list_elevation.csv_gridmet.csv'\n    terrain_file = f'{working_dir}/training_all_active_snotel_station_list_elevation.csv_terrain_4km_grid_shift.csv'\n    fsca_file = f'{homedir}/fsca/fsca_final_training_all.csv'\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    \n    if os.path.exists(final_final_output_file):\n      print(f\"The file '{final_final_output_file}' exists. Skipping\")\n      return final_final_output_file\n      \n    \n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(amsr_file, blocksize=chunk_size)\n    print(\"amsr.columns = \", amsr.columns)\n    snotel = dd.read_csv(snotel_file, blocksize=chunk_size)\n    print(\"snotel.columns = \", snotel.columns)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(gridmet_file, blocksize=chunk_size)\n    gridmet = gridmet.drop(columns=[\"Unnamed: 0\"])\n    print(\"gridmet.columns = \", gridmet.columns)\n    terrain = dd.read_csv(terrain_file, blocksize=chunk_size)\n    terrain = terrain.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    terrain = terrain[[\"stationTriplet\", \"elevation\", \"lat\", \"lon\", 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness']]\n    print(\"terrain.columns = \", terrain.columns)\n    snowcover = dd.read_csv(fsca_file, blocksize=chunk_size)\n    snowcover = snowcover.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    print(\"snowcover.columns = \", snowcover.columns)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n    print(\"all the dataframes are partitioned\")\n\n    # Merge DataFrames based on specified columns\n    print(\"start to merge amsr and snotel\")\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snotel.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge gridmet\")\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_gridmet.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge terrain\")\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge snowcover\")\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f'Merge completed. {output_file}')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/{final_output_name}')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/{final_output_name}', single_file=True, index=False)\n    print('Data cleaning completed.')\n    return final_final_output_file\n\n  \ndef sort_training_data(input_training_csv, sorted_training_csv):\n    # Read DataFrame from CSV\n    df = pd.read_csv(input_training_csv)\n\n    # Sort DataFrame by three columns: date, lat, and Lon\n    sorted_df = df.sort_values(by=['date', 'lat', 'Lon'])\n\n    # Save the sorted DataFrame to a new CSV file\n    sorted_df.to_csv(sorted_training_csv, index=False)\n\n    print(f\"The DataFrame has been sorted and saved to '{sorted_training_csv}'.\")\n  \nif __name__ == \"__main__\":\n    merge_all_data_together()\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    sort_training_data(final_final_output_file, f'{work_dir}/{final_output_name}_sorted.csv')",
  "history_output" : "today date = 2023-12-26\ntest start date:  2023-01-20\ntest end date:  2023-10-11\n/home/chetana\n2022275\nThe file '/home/chetana/gridmet_test_run/final_merged_data_3yrs_all_active_stations_v1.csv' exists. Skipping\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/rZf1YVlz4hm5/merge_custom_traning_range.py\", line 128, in <module>\n    sort_training_data(final_final_output_file, f'{work_dir}/{final_output_name}_sorted.csv')\n  File \"/home/chetana/gw-workspace/rZf1YVlz4hm5/merge_custom_traning_range.py\", line 115, in sort_training_data\n    df = pd.read_csv(input_training_csv)\nNameError: name 'pd' is not defined\n",
  "history_begin_time" : 1703614682505,
  "history_end_time" : 1703614686081,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "oo0E4hZ7HU9w",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir, homedir\n\nworking_dir = work_dir\nfinal_output_name = \"final_merged_data_3yrs_all_active_stations_v1.csv\"\nchunk_size = '10MB'  # You can adjust this chunk size based on your hardware and data size\n\n\ndef main():\n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_amsr_dask.csv', blocksize=chunk_size)\n    print(\"amsr.columns = \", amsr.columns)\n    snotel = dd.read_csv(f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_swe_restored_dask_all_vars.csv', blocksize=chunk_size)\n    print(\"snotel.columns = \", snotel.columns)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(f'{working_dir}/training_all_active_snotel_station_list_elevation.csv_gridmet.csv', blocksize=chunk_size)\n    gridmet = gridmet.drop(columns=[\"Unnamed: 0\"])\n    print(\"gridmet.columns = \", gridmet.columns)\n    terrain = dd.read_csv(f'{working_dir}/training_all_active_snotel_station_list_elevation.csv_terrain_4km_grid_shift.csv', blocksize=chunk_size)\n    terrain = terrain.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    terrain = terrain[[\"stationTriplet\", \"elevation\", \"lat\", \"lon\", 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness']]\n    print(\"terrain.columns = \", terrain.columns)\n    snowcover = dd.read_csv(f'{homedir}/fsca/fsca_final_training_all.csv', blocksize=chunk_size)\n    snowcover = snowcover.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    print(\"snowcover.columns = \", snowcover.columns)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n    print(\"all the dataframes are partitioned\")\n\n    # Merge DataFrames based on specified columns\n    print(\"start to merge amsr and snotel\")\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snotel.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge gridmet\")\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_gridmet.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge terrain\")\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge snowcover\")\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f'Merge completed. {output_file}')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/{final_output_name}')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/{final_output_name}', single_file=True, index=False)\n    print('Data cleaning completed.')\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "today date = 2023-12-26\ntest start date:  2023-01-20\ntest end date:  2023-10-11\n/home/chetana\n2022275\namsr.columns =  Index(['date', 'lat', 'lon', 'AMSR_SWE'], dtype='object')\nsnotel.columns =  Index(['station_name', 'date', 'lat', 'lon', 'swe_value', 'change_in_swe_inch',\n       'snow_depth', 'air_temperature_observed_f'],\n      dtype='object')\ngridmet.columns =  Index(['day', 'lat', 'lon', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed'],\n      dtype='object')\nterrain.columns =  Index(['stationTriplet', 'elevation', 'lat', 'lon', 'Elevation', 'Slope',\n       'Aspect', 'Curvature', 'Northness', 'Eastness'],\n      dtype='object')\nsnowcover.columns =  Index(['date', 'lat', 'lon', 'fsca'], dtype='object')\nall the dataframes are partitioned\nstart to merge amsr and snotel\nintermediate file saved to /home/chetana/gridmet_test_run/final_merged_data_3yrs_all_active_stations_v1.csv_snotel.csv\nstart to merge gridmet\nintermediate file saved to /home/chetana/gridmet_test_run/final_merged_data_3yrs_all_active_stations_v1.csv_gridmet.csv\nstart to merge terrain\nintermediate file saved to /home/chetana/gridmet_test_run/final_merged_data_3yrs_all_active_stations_v1.csv_terrain.csv\nstart to merge snowcover\nintermediate file saved to /home/chetana/gridmet_test_run/final_merged_data_3yrs_all_active_stations_v1.csv_snow_cover.csv\nMerge completed. /home/chetana/gridmet_test_run/final_merged_data_3yrs_all_active_stations_v1.csv\nData cleaning completed.\n",
  "history_begin_time" : 1703613310239,
  "history_end_time" : 1703614725228,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "u0qppgiNhrXW",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir, homedir\n\nworking_dir = work_dir\nfinal_output_name = \"final_merged_data_3yrs_all_active_stations_v1.csv\"\nchunk_size = '32MB'  # You can adjust this chunk size based on your hardware and data size\n\ndef main():\n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_amsr_dask.csv', blocksize=chunk_size)\n    print(\"amsr.columns = \", amsr.columns)\n    snotel = dd.read_csv(f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_swe_restored_dask_all_vars.csv', blocksize=chunk_size)\n    print(\"snotel.columns = \", snotel.columns)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(f'{working_dir}/training_all_active_snotel_station_list_elevation.csv_gridmet.csv', blocksize=chunk_size)\n    gridmet = gridmet.drop(columns=[\"Unnamed: 0\"])\n    print(\"gridmet.columns = \", gridmet.columns)\n    terrain = dd.read_csv(f'{working_dir}/training_all_active_snotel_station_list_elevation.csv_terrain_4km_grid_shift.csv', blocksize=chunk_size)\n    terrain = terrain.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    terrain = terrain[[\"stationTriplet\", \"elevation\", \"lat\", \"lon\", 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness']]\n    print(\"terrain.columns = \", terrain.columns)\n    snowcover = dd.read_csv(f'{homedir}/fsca/fsca_final_training_all.csv', blocksize=chunk_size)\n    snowcover = snowcover.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    print(\"snowcover.columns = \", snowcover.columns)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n    print(\"all the dataframes are partitioned\")\n\n    # Merge DataFrames based on specified columns\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snotel.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    \n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_gridmet.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    \n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    \n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n\n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print('Merge completed.')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/{final_output_name}')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/{final_output_name}', single_file=True, index=False)\n    print('Data cleaning completed.')\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "today date = 2023-12-26\ntest start date:  2023-01-20\ntest end date:  2023-10-11\n/home/chetana\n2022275\namsr.columns =  Index(['date', 'lat', 'lon', 'AMSR_SWE'], dtype='object')\nsnotel.columns =  Index(['station_name', 'date', 'lat', 'lon', 'swe_value', 'change_in_swe_inch',\n       'snow_depth', 'air_temperature_observed_f'],\n      dtype='object')\ngridmet.columns =  Index(['day', 'lat', 'lon', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed'],\n      dtype='object')\nterrain.columns =  Index(['stationTriplet', 'elevation', 'lat', 'lon', 'Elevation', 'Slope',\n       'Aspect', 'Curvature', 'Northness', 'Eastness'],\n      dtype='object')\nsnowcover.columns =  Index(['date', 'lat', 'lon', 'fsca'], dtype='object')\nall the dataframes are partitioned\nMerge completed.\nData cleaning completed.\n",
  "history_begin_time" : 1703612496090,
  "history_end_time" : 1703613844010,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "PqSZFJ6oocqC",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir, homedir\n\nworking_dir = work_dir\nfinal_output_name = \"final_merged_data_3yrs_all_active_stations_v1.csv\"\nchunk_size = '32MB'  # You can adjust this chunk size based on your hardware and data size\n\ndef main():\n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_amsr_dask.csv', blocksize=chunk_size)\n    print(\"amsr.columns = \", amsr.columns)\n    snotel = dd.read_csv(f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_swe_restored_dask_all_vars.csv', blocksize=chunk_size)\n    print(\"snotel.columns = \", snotel.columns)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(f'{working_dir}/training_all_active_snotel_station_list_elevation.csv_gridmet.csv', blocksize=chunk_size)\n    print(\"gridmet.columns = \", gridmet.columns)\n    terrain = dd.read_csv(f'{working_dir}/training_all_active_snotel_station_list_elevation.csv_terrain_4km_grid_shift.csv', blocksize=chunk_size)\n    print(\"terrain.columns = \", terrain.columns)\n    snowcover = dd.read_csv(f'{homedir}/fsca/fsca_final_training_all.csv', blocksize=chunk_size)\n    print(\"snowcover.columns = \", snowcover.columns)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n    print(\"all the dataframes are partitioned\")\n\n    # Merge DataFrames based on specified columns\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snotel.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    \n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_gridmet.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    \n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    \n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n\n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print('Merge completed.')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/{final_output_name}')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/{final_output_name}', single_file=True, index=False)\n    print('Data cleaning completed.')\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "today date = 2023-12-26\ntest start date:  2023-01-20\ntest end date:  2023-10-11\n/home/chetana\n2022275\namsr.columns =  Index(['date', 'lat', 'lon', 'AMSR_SWE'], dtype='object')\nsnotel.columns =  Index(['station_name', 'date', 'lat', 'lon', 'swe_value', 'change_in_swe_inch',\n       'snow_depth', 'air_temperature_observed_f'],\n      dtype='object')\ngridmet.columns =  Index(['Unnamed: 0', 'day', 'lat', 'lon', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed'],\n      dtype='object')\nterrain.columns =  Index(['stationTriplet', 'stationId', 'stateCode', 'networkCode', 'name',\n       'dcoCode', 'countyName', 'huc', 'elevation', 'latitude', 'longitude',\n       'dataTimeZone', 'pedonCode', 'shefId', 'beginDate', 'endDate',\n       'Latitude', 'Longitude', 'x', 'y', 'Elevation', 'Slope', 'Aspect',\n       'Curvature', 'Northness', 'Eastness'],\n      dtype='object')\nsnowcover.columns =  Index(['date', 'latitude', 'longitude', 'fsca'], dtype='object')\nall the dataframes are partitioned\n",
  "history_begin_time" : 1703612033164,
  "history_end_time" : 1703612085001,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "ksnjcb0ccz9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702875592841,
  "history_end_time" : 1702875592841,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "lksg3jvvj1d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702871264379,
  "history_end_time" : 1702871264379,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wvygxz1u5li",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702867996396,
  "history_end_time" : 1702867996396,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "eei96z8op2s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702866593394,
  "history_end_time" : 1702866593394,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "dr5dqt66aaw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702866137629,
  "history_end_time" : 1702866137629,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "l5vy58rapzo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702657305630,
  "history_end_time" : 1702657305630,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "b25gcyikblm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702633223049,
  "history_end_time" : 1702633223049,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "b88935ri4ft",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702633156927,
  "history_end_time" : 1702633163903,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "u5rd6rawidg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702274520887,
  "history_end_time" : 1702274520887,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "l5ulhwq89do",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702257109204,
  "history_end_time" : 1702257109204,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "icg8xf4mnfq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702253506525,
  "history_end_time" : 1702253506525,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "iwpv8lvxlys",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702047800939,
  "history_end_time" : 1702047800939,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ngqi3lx86mp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702046671869,
  "history_end_time" : 1702047789488,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "6htjojo920o",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701838624056,
  "history_end_time" : 1701838624056,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "62a2zxf1c9u",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701272631481,
  "history_end_time" : 1701272875115,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "9sdjzb1g7p7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701272152705,
  "history_end_time" : 1701272363359,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "yttpq0fswha",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701269761343,
  "history_end_time" : 1701269761343,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "r6nck7fd7t2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701245472022,
  "history_end_time" : 1701245472022,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "mvb156frcb9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701234300621,
  "history_end_time" : 1701234300621,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3kzroiwkb7l",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701232375282,
  "history_end_time" : 1701234158046,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "l3561p5dedf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701231048671,
  "history_end_time" : 1701231048671,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6o1zk3af34o",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933468,
  "history_end_time" : 1701230952352,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "9kugcrrlmr5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230796336,
  "history_end_time" : 1701230932259,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "c21egwm4unn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230384890,
  "history_end_time" : 1701230384890,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "v1pcgenic0c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701229983512,
  "history_end_time" : 1701229983512,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "78jk45cxkkq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701228899375,
  "history_end_time" : 1701228899375,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9nba3nxwzbd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701228374808,
  "history_end_time" : 1701228374808,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6kuli8zvis2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701228236346,
  "history_end_time" : 1701228236346,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "y3y6ngo766a",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701228118511,
  "history_end_time" : 1701228118511,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "jk24k07myoh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701228056533,
  "history_end_time" : 1701228056533,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "b0x6tohno7u",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701227912529,
  "history_end_time" : 1701227912529,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "FODTNKfyRv8C",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir, homedir\n\nworking_dir = work_dir\nfinal_output_name = \"final_merged_data_3yrs_cleaned_v4.csv\"\nchunk_size = '32MB'  # You can adjust this chunk size based on your hardware and data size\n\ndef main():\n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\n    snotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(f'{working_dir}/training_ready_gridmet_3_yrs.csv', blocksize=chunk_size)\n    terrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n    snowcover = dd.read_csv(f'{homedir}/fsca/fsca_final_training_all.csv', blocksize=chunk_size)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n    print(\"all the dataframes are partitioned\")\n\n    # Merge DataFrames based on specified columns\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snotel.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    \n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_gridmet.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    \n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    \n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n\n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print('Merge completed.')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/{final_output_name}')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/{final_output_name}', single_file=True, index=False)\n    print('Data cleaning completed.')\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "today date = 2023-11-28\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\nall the dataframes are partitioned\nMerge completed.\n/home/chetana/anaconda3/lib/python3.9/site-packages/dask/dataframe/io/csv.py:195: DtypeWarning: Columns (6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = reader(bio, **kwargs)\n/home/chetana/anaconda3/lib/python3.9/site-packages/dask/dataframe/io/csv.py:195: DtypeWarning: Columns (6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = reader(bio, **kwargs)\nData cleaning completed.\n",
  "history_begin_time" : 1701184445165,
  "history_end_time" : 1701184673160,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "GWw9jxxLCq3e",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir, homedir\n\nworking_dir = work_dir\nfinal_output_name = \"final_merged_data_3yrs_cleaned_v4.csv\"\nchunk_size = '32MB'  # You can adjust this chunk size based on your hardware and data size\n\ndef main():\n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\n    snotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(f'{working_dir}/training_ready_gridmet_3_yrs.csv', blocksize=chunk_size)\n    terrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n    snowcover = dd.read_csv(f'{homedir}/fsca/fsca_final_training_all.csv', blocksize=chunk_size)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n    print(\"all the dataframes are partitioned\")\n\n    # Merge DataFrames based on specified columns\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snotel.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_gridmet.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n\n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print('Merge completed.')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/{final_output_name}')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/{final_output_name}', single_file=True, index=False)\n    print('Data cleaning completed.')\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "today date = 2023-11-28\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\nall the dataframes are partitioned\n",
  "history_begin_time" : 1701180455970,
  "history_end_time" : 1701184369641,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "KcD56mvVpwKw",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir, homedir\n\nworking_dir = work_dir\nfinal_output_name = \"final_merged_data_3yrs_cleaned_v4.csv\"\nchunk_size = '32MB'  # You can adjust this chunk size based on your hardware and data size\n\ndef main():\n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\n    snotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(f'{working_dir}/training_ready_gridmet_3_yrs.csv', blocksize=chunk_size)\n    terrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n    snowcover = dd.read_csv(f'{homedir}/fsca/fsca_final_training_all.csv', blocksize=chunk_size)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n\n    # Merge DataFrames based on specified columns\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n\n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print('Merge completed.')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/{final_output_name}')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/{final_output_name}', single_file=True, index=False)\n    print('Data cleaning completed.')\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "today date = 2023-11-28\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\n",
  "history_begin_time" : 1701151718499,
  "history_end_time" : 1701152576569,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "pa0tf1n65ic",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701013937413,
  "history_end_time" : 1701015920041,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "1bnu31g674j",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700974688727,
  "history_end_time" : 1700974688727,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6r9ssmt52h7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700885116836,
  "history_end_time" : 1700885116836,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "fbqAVkKUvylD",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nhome_dir = '/home/chetana'\nfinal_output_name = \"final_merged_data_3yrs_cleaned_v4.csv\"\nchunk_size = '32MB'  # You can adjust this chunk size based on your hardware and data size\n\ndef main():\n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\n    snotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(f'{working_dir}/training_ready_gridmet_3_yrs.csv', blocksize=chunk_size)\n    terrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n    snowcover = dd.read_csv(f'{home_dir}/fsca/fsca_final.csv', blocksize=chunk_size)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n\n    # Merge DataFrames based on specified columns\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n\n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print('Merge completed.')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/{final_output_name}')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/{final_output_name}', single_file=True, index=False)\n    print('Data cleaning completed.')\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "today date = 2023-11-22\ntest start date:  2022-12-15\ntest end date:  2023-10-07\n/home/chetana\n",
  "history_begin_time" : 1700637965724,
  "history_end_time" : 1700638336302,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "NhXnZh92Eile",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nhome_dir = '/home/chetana'\nchunk_size = '32MB'  # You can adjust this chunk size based on your hardware and data size\n\ndef main():\n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\n    snotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(f'{working_dir}/training_ready_gridmet_3_yrs.csv', blocksize=chunk_size)\n    terrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n    snowcover = dd.read_csv(f'{home_dir}/fsca/fsca_final.csv', blocksize=chunk_size)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n\n    # Merge DataFrames based on specified columns\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n\n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, 'final_merged_data_3_yrs_v3.csv')\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print('Merge completed.')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/final_merged_data_3_yrs_v3.csv')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv', single_file=True, index=False)\n    print('Data cleaning completed.')\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "today date = 2023-11-22\ntest start date:  2022-12-15\ntest end date:  2023-10-07\n/home/chetana\n",
  "history_begin_time" : 1700629094746,
  "history_end_time" : 1700629571008,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "oBqejZaTt5Ak",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nhome_dir = '/home/chetana'\nchunk_size = '32MB'  # You can adjust this chunk size based on your hardware and data size\n\ndef main():\n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\n    snotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(f'{working_dir}/training_ready_gridmet_3_yrs.csv', blocksize=chunk_size)\n    terrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n    snowcover = dd.read_csv(f'{home_dir}/fsca/fsca_final.csv', blocksize=chunk_size)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n\n    # Merge DataFrames based on specified columns\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n\n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, 'final_merged_data_3_yrs_v3.csv')\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print('Merge completed.')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/final_merged_data_3_yrs_v3.csv')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv', single_file=True, index=False)\n    print('Data cleaning completed.')\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "today date = 2023-11-22\ntest start date:  2022-12-15\ntest end date:  2023-10-07\n/home/chetana\n",
  "history_begin_time" : 1700618382213,
  "history_end_time" : 1700619591945,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "emSJutfHjV62",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nhome_dir = '/home/chetana'\nchunk_size = '32MB'  # You can adjust this chunk size based on your hardware and data size\n\ndef main():\n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\n    snotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(f'{working_dir}/training_ready_gridmet_3_yrs.csv', blocksize=chunk_size)\n    terrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n    snowcover = dd.read_csv(f'{home_dir}/fsca/fsca_final.csv', blocksize=chunk_size)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = terrian.repartition(partition_size=chunk_size)\n\n    # Merge DataFrames based on specified columns\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n\n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, 'final_merged_data_3_yrs_v3.csv')\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print('Merge completed.')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/final_merged_data_3_yrs_v3.csv')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv', single_file=True, index=False)\n    print('Data cleaning completed.')\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "today date = 2023-11-22\ntest start date:  2022-12-15\ntest end date:  2023-10-07\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/emSJutfHjV62/merge_custom_traning_range.py\", line 60, in <module>\n    main()\n  File \"/home/chetana/gw-workspace/emSJutfHjV62/merge_custom_traning_range.py\", line 40, in main\n    snow_cover = terrian.repartition(partition_size=chunk_size)\nNameError: name 'terrian' is not defined\n",
  "history_begin_time" : 1700618336386,
  "history_end_time" : 1700618342301,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "8ctVVw1TsoAu",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nhome_dir = '/home/chetana'\nchunk_size = '32MB'  # You can adjust this chunk size based on your hardware and data size\n\ndef main():\n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\n    snotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet_3_yrs.csv', blocksize=chunk_size)\n    terrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n    snowcover = dd.read_csv(f'{home_dir}/fsca/fsca_final.csv', blocksize=chunk_size)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = terrian.repartition(partition_size=chunk_size)\n\n    # Merge DataFrames based on specified columns\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n\n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, 'final_merged_data_3_yrs_v3.csv')\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print('Merge completed.')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/final_merged_data_3_yrs_v3.csv')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv', single_file=True, index=False)\n    print('Data cleaning completed.')\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "today date = 2023-11-22\ntest start date:  2022-12-15\ntest end date:  2023-10-07\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/dask/backends.py\", line 136, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 761, in read\n    return read_pandas(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 561, in read_pandas\n    b_out = read_bytes(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/dask/bytes/core.py\", line 111, in read_bytes\n    size = fs.info(path)[\"size\"]\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/fsspec/implementations/local.py\", line 83, in info\n    out = os.stat(path, follow_symlinks=False)\nFileNotFoundError: [Errno 2] No such file or directory: '/home/chetana/gridmet_test_run/gridmet_climatology/training_ready_gridmet_3_yrs.csv'\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/8ctVVw1TsoAu/merge_custom_traning_range.py\", line 59, in <module>\n    main()\n  File \"/home/chetana/gw-workspace/8ctVVw1TsoAu/merge_custom_traning_range.py\", line 29, in main\n    gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet_3_yrs.csv', blocksize=chunk_size)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/dask/backends.py\", line 138, in wrapper\n    raise type(e)(\nFileNotFoundError: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: [Errno 2] No such file or directory: '/home/chetana/gridmet_test_run/gridmet_climatology/training_ready_gridmet_3_yrs.csv'\n",
  "history_begin_time" : 1700618201117,
  "history_end_time" : 1700618202709,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "KcBUyayk64OQ",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nhome_dir = '/home/chetana'\nchunk_size = '32MB'  # You can adjust this chunk size based on your hardware and data size\n\ndef main():\n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\n    snotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    terrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n    snowcover = dd.read_csv(f'{home_dir}/fsca/fsca_final.csv', blocksize=chunk_size)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = terrian.repartition(partition_size=chunk_size)\n\n    # Merge DataFrames based on specified columns\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n\n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, 'final_merged_data_3_yrs_v3.csv')\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print('Merge completed.')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/final_merged_data_3_yrs_v3.csv')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv', single_file=True, index=False)\n    print('Data cleaning completed.')\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "today date = 2023-11-22\ntest start date:  2022-12-15\ntest end date:  2023-10-07\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/dask/backends.py\", line 136, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 761, in read\n    return read_pandas(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 561, in read_pandas\n    b_out = read_bytes(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/dask/bytes/core.py\", line 111, in read_bytes\n    size = fs.info(path)[\"size\"]\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/fsspec/implementations/local.py\", line 83, in info\n    out = os.stat(path, follow_symlinks=False)\nFileNotFoundError: [Errno 2] No such file or directory: '/home/chetana/gridmet_test_run/gridmet_climatology/training_ready_gridmet.csv'\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/KcBUyayk64OQ/merge_custom_traning_range.py\", line 59, in <module>\n    main()\n  File \"/home/chetana/gw-workspace/KcBUyayk64OQ/merge_custom_traning_range.py\", line 29, in main\n    gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/dask/backends.py\", line 138, in wrapper\n    raise type(e)(\nFileNotFoundError: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: [Errno 2] No such file or directory: '/home/chetana/gridmet_test_run/gridmet_climatology/training_ready_gridmet.csv'\n",
  "history_begin_time" : 1700617045687,
  "history_end_time" : 1700617053747,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "65jz12hokk2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700471590200,
  "history_end_time" : 1700471590200,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "1v1cg9yx0i0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700468936440,
  "history_end_time" : 1700468936440,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "b5o4ds1ji3y",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700461923019,
  "history_end_time" : 1700462913690,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "2clnpxj0pe1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500144,
  "history_end_time" : 1700448500144,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ms3i1fw6zq1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700447319851,
  "history_end_time" : 1700447319851,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ivxc6f3oj6c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700230067250,
  "history_end_time" : 1700230067250,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "lhwefpq9qz1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700229012374,
  "history_end_time" : 1700229012374,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "zy1gdkz8mkf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700210213817,
  "history_end_time" : 1700210213817,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ssy4j5i0crw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700209780169,
  "history_end_time" : 1700209780169,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8pvfarz515u",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700209729254,
  "history_end_time" : 1700209729254,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "agu5z3qjyqt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700203478655,
  "history_end_time" : 1700204245687,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "6gvftpxxgaz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700201828271,
  "history_end_time" : 1700201828271,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xkufewvp1sj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700200332862,
  "history_end_time" : 1700200332862,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0irm0u579bf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700145667875,
  "history_end_time" : 1700145667875,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "2pc21f99szf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700143295313,
  "history_end_time" : 1700143295313,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "igw3whx1319",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700141615818,
  "history_end_time" : 1700141615818,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "bshy3yfovy7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700134126845,
  "history_end_time" : 1700134126845,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "38853n1w5sw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700133783713,
  "history_end_time" : 1700133783713,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "syrm1wmwm4e",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699992839766,
  "history_end_time" : 1699992839766,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9kbr19oe31w",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699982145458,
  "history_end_time" : 1699982145458,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wztghmuwk9d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699941614808,
  "history_end_time" : 1699941614808,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ctn98cdr360",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699939440568,
  "history_end_time" : 1699939440568,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "s5qpvdefxmc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699937910471,
  "history_end_time" : 1699937910471,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "sfptl7pig86",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699805634665,
  "history_end_time" : 1699806085200,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "pomcks3n3bl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699684154070,
  "history_end_time" : 1705789690241,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "u1r2fwibqvw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699681071364,
  "history_end_time" : 1699681071364,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "pzjkh57bqzg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698762678703,
  "history_end_time" : 1698762678703,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rtpyerpnmy5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698762637984,
  "history_end_time" : 1698762637984,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "4k48xd2xj6e",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698276496964,
  "history_end_time" : 1698276496964,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6vfiabgali2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698252277366,
  "history_end_time" : 1698252277366,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "jah3parcn8j",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698251392470,
  "history_end_time" : 1698251392470,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "gui7ou7xwxs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698228210971,
  "history_end_time" : 1698228210971,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rhr63u6s2yb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698227897143,
  "history_end_time" : 1698227897143,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "j18ko3vw28k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698163737294,
  "history_end_time" : 1698163737294,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "o34imph8ghj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698163445830,
  "history_end_time" : 1698163445830,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9qamp1t5nqk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698163121543,
  "history_end_time" : 1698163121543,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vwsbguih7n7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698160809396,
  "history_end_time" : 1698160809396,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ralpez8araa",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698157805187,
  "history_end_time" : 1698157805187,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "q7rma52fvta",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698152099750,
  "history_end_time" : 1698152099750,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "zuyrgigsi3d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698095495773,
  "history_end_time" : 1698095495773,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "cg58byzeonl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698075453621,
  "history_end_time" : 1698075453621,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ogcw4hr48v5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1697349530013,
  "history_end_time" : 1697349530013,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "si4sxxgcz9s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1697348852379,
  "history_end_time" : 1697348852379,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0nr32xvstea",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1697189923578,
  "history_end_time" : 1697189923578,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tvsx1spr316",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1697188523317,
  "history_end_time" : 1697188523317,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wk59w2jt768",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1697187892287,
  "history_end_time" : 1697187892287,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "1yic2gu10o6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1697187368004,
  "history_end_time" : 1697187368004,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vnpwq1kfjk8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696863953337,
  "history_end_time" : 1696863953337,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "i1p0958o1l5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696862402979,
  "history_end_time" : 1696862402979,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "dh6prk1dop5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696832263693,
  "history_end_time" : 1696832263693,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "v0qbpv2alts",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696831867391,
  "history_end_time" : 1696831867391,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vn24gd4iyms",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696830174415,
  "history_end_time" : 1696830174415,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "d9uecspy6po",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696787541977,
  "history_end_time" : 1696787541977,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0a2b24kbc6o",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696786838259,
  "history_end_time" : 1696786838259,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "zn71eca3uma",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696771780949,
  "history_end_time" : 1696771780949,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "v9u1e7bvk3i",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696602943960,
  "history_end_time" : 1696602943960,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "1peha6o0q3s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696432484353,
  "history_end_time" : 1696432484353,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "5s4ixfhud8d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696432299775,
  "history_end_time" : 1696432482237,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ow293to5dhq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695827991148,
  "history_end_time" : 1695827991148,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vdk4ez78miq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695827889185,
  "history_end_time" : 1695827965229,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "wnu6nkkwgqn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695827855653,
  "history_end_time" : 1695827867013,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "2wzwttw2j2s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695696616126,
  "history_end_time" : 1695696616126,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "s1lq9cro9pi",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695694257336,
  "history_end_time" : 1695694257336,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "txot8wj4sis",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695693585755,
  "history_end_time" : 1695693585755,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xlaobebezdd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695693149412,
  "history_end_time" : 1695693149412,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "crc8wlk55tb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695580915854,
  "history_end_time" : 1695580915854,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "k2oqqeklm9m",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695576291671,
  "history_end_time" : 1695576291671,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ozv6qh0p32d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695575931035,
  "history_end_time" : 1695575931035,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "e7cn6yx1eeu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695535769219,
  "history_end_time" : 1695535769219,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "oe9iu2nxrza",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695535478715,
  "history_end_time" : 1695535478715,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "03ra1dbsh5i",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695535214031,
  "history_end_time" : 1695535214031,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ist8l8u6now",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695534943596,
  "history_end_time" : 1695534943596,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "7uk8vqucmhb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695534671835,
  "history_end_time" : 1695534671835,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "2afzcdhvp7z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695533024174,
  "history_end_time" : 1695533024174,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6gt7ytt0rw5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695529187875,
  "history_end_time" : 1695529187875,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "k4ow5sd3xhv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695528505204,
  "history_end_time" : 1695528505204,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3893n1vyzw0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695515862562,
  "history_end_time" : 1695515862562,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "udqbtyfeb79",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695506423865,
  "history_end_time" : 1695506423865,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "hncl0n1toa5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695418741350,
  "history_end_time" : 1695418741350,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "agxhr5kr2ew",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695417619686,
  "history_end_time" : 1695417619686,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "438vo0i339j",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695417171289,
  "history_end_time" : 1695417171289,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tmba37c1v0q",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695417052739,
  "history_end_time" : 1695417052739,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "39ec0firehu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695416916063,
  "history_end_time" : 1695416916063,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ds0s4owq8tn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695106488989,
  "history_end_time" : 1695106488989,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rbt5put7g4r",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695106316226,
  "history_end_time" : 1695106316226,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "k7x9r715o8i",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695054045036,
  "history_end_time" : 1695054045036,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "skxs2px5pit",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695054019804,
  "history_end_time" : 1695054033337,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "3503j1430a1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695053979923,
  "history_end_time" : 1695054019279,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "u126p6qkb7m",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695053793463,
  "history_end_time" : 1695053793463,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "z0zyr00gv1p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695053733480,
  "history_end_time" : 1695053733480,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "fjools0yx1t",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694971144855,
  "history_end_time" : 1694972839692,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "lw5yzef2drz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970707953,
  "history_end_time" : 1694970707953,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "srraf32mp6c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594774,
  "history_end_time" : 1694970594774,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "u85hw9thpor",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970131663,
  "history_end_time" : 1694970131663,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "sqllvzwud4b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694969350146,
  "history_end_time" : 1694969350146,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "bkahs3bpwc8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694905307741,
  "history_end_time" : 1694905307741,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "pkhi09xyjvk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887153,
  "history_end_time" : 1694897887153,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vnYKEk5kMttS",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '32MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\ngridmet = gridmet.rename(columns={'day': 'date'})\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\nprint('merge completed.')\n\n\ndf = dd.read_csv(f'{work_dir}/final_merged_data_3_yrs.csv')\ndf = df.drop_duplicates(keep='first')\ndf.to_csv(f'{work_dir}/final_merged_data_3yrs_cleaned.csv', single_file=True, index=False)",
  "history_output" : "today date = 2023-09-11\n/home/chetana\nmerge completed.\n",
  "history_begin_time" : 1694445066010,
  "history_end_time" : 1694448714178,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ISKrXMBwXjS6",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '32MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\ngridmet = gridmet.rename(columns={'day': 'date'})\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\nprint('merge completed.')\n\n\ndf = dd.read_csv('final_merged_data_3_yrs.csv')\ndf = df.drop_duplicates(keep='first')\ndf.to_csv(f'{working_dir}/final_merged_data_3yrs_cleaned.csv')",
  "history_output" : "today date = 2023-09-11\n/home/chetana\nmerge completed.\nTraceback (most recent call last):\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 136, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 761, in read\n    return read_pandas(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 561, in read_pandas\n    b_out = read_bytes(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/bytes/core.py\", line 111, in read_bytes\n    size = fs.info(path)[\"size\"]\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/fsspec/implementations/local.py\", line 87, in info\n    out = os.stat(path, follow_symlinks=False)\nFileNotFoundError: [Errno 2] No such file or directory: '/home/chetana/gw-workspace/ISKrXMBwXjS6/final_merged_data_3_yrs.csv'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/ISKrXMBwXjS6/merge_custom_traning_range.py\", line 32, in <module>\n    df = dd.read_csv('final_merged_data_3_yrs.csv')\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 138, in wrapper\n    raise type(e)(\nFileNotFoundError: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: [Errno 2] No such file or directory: '/home/chetana/gw-workspace/ISKrXMBwXjS6/final_merged_data_3_yrs.csv'\n",
  "history_begin_time" : 1694421100044,
  "history_end_time" : 1694424730068,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "p49g9uAwsrSx",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '32MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\ngridmet = gridmet.rename(columns={'day': 'date'})\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\nprint('merge completed.')\n",
  "history_output" : "today date = 2023-09-10\n/home/chetana\nmerge completed.\n",
  "history_begin_time" : 1694322596784,
  "history_end_time" : 1694327296809,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "LkmwrAi0zDh7",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '64MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\ngridmet = gridmet.rename(columns={'day': 'date'})\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\nprint('merge completed.')\n",
  "history_output" : "today date = 2023-09-10\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/LkmwrAi0zDh7/merge_custom_traning_range.py\", line 28, in <module>\n    merged_df.to_csv(output_file, single_file=True, index=False)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 994, in to_csv\n    return list(dask.compute(*values, **compute_kwargs))\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/threaded.py\", line 89, in get\n    results = get_async(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 511, in get_async\n    raise_exception(exc, tb)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 319, in reraise\n    raise exc\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 224, in execute_task\n    result = _execute_task(task, data)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 289, in merge_chunk\n    out = lhs.merge(rhs, *args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/concat.py\", line 212, in concatenate_managers\n    values = values.copy()\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 3.53 GiB for an array with shape (6, 78902593) and data type float64\n",
  "history_begin_time" : 1694321441151,
  "history_end_time" : 1694322427753,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "2DxS4gZcEhEK",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '64MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\ngridmet = gridmet.rename(columns={'day': 'date'})\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'day'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\nprint('merge completed.')\n",
  "history_output" : "today date = 2023-09-10\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/2DxS4gZcEhEK/merge_custom_traning_range.py\", line 23, in <module>\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'day'], how='outer')\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 726, in merge\n    return hash_join(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 400, in hash_join\n    meta = _lhs_meta.merge(_rhs_meta, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/frame.py\", line 10093, in merge\n    return merge(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 110, in merge\n    op = _MergeOperation(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 703, in __init__\n    ) = self._get_merge_keys()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 1162, in _get_merge_keys\n    right_keys.append(right._get_label_or_level_values(rk))\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 1850, in _get_label_or_level_values\n    raise KeyError(key)\nKeyError: 'day'\n",
  "history_begin_time" : 1694321418010,
  "history_end_time" : 1694321426134,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "0HYiKf2yAKHJ",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '64MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\nprint('merge completed.')\n",
  "history_output" : "today date = 2023-09-10\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/0HYiKf2yAKHJ/merge_custom_traning_range.py\", line 22, in <module>\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 726, in merge\n    return hash_join(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 400, in hash_join\n    meta = _lhs_meta.merge(_rhs_meta, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/frame.py\", line 10093, in merge\n    return merge(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 110, in merge\n    op = _MergeOperation(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 703, in __init__\n    ) = self._get_merge_keys()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 1162, in _get_merge_keys\n    right_keys.append(right._get_label_or_level_values(rk))\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 1850, in _get_label_or_level_values\n    raise KeyError(key)\nKeyError: 'date'\n",
  "history_begin_time" : 1694321143879,
  "history_end_time" : 1694321158497,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "CkdvHh5Sq2YZ",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '64MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\nprint('merge completed.')\n",
  "history_output" : "today date = 2023-09-09\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 136, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 761, in read\n    return read_pandas(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 561, in read_pandas\n    b_out = read_bytes(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/bytes/core.py\", line 111, in read_bytes\n    size = fs.info(path)[\"size\"]\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/fsspec/implementations/local.py\", line 87, in info\n    out = os.stat(path, follow_symlinks=False)\nFileNotFoundError: [Errno 2] No such file or directory: '/home/chetana/gridmet_test_run/gridmet_climatology/testing_ready_gridmet.csv'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/CkdvHh5Sq2YZ/merge_custom_traning_range.py\", line 11, in <module>\n    gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 138, in wrapper\n    raise type(e)(\nFileNotFoundError: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: [Errno 2] No such file or directory: '/home/chetana/gridmet_test_run/gridmet_climatology/testing_ready_gridmet.csv'\n",
  "history_begin_time" : 1694296537295,
  "history_end_time" : 1694296539799,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "utrQ6i3XRugd",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '64MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\nprint('merge completed.')\n",
  "history_output" : "today date = 2023-09-09\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 136, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 761, in read\n    return read_pandas(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 561, in read_pandas\n    b_out = read_bytes(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/bytes/core.py\", line 111, in read_bytes\n    size = fs.info(path)[\"size\"]\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/fsspec/implementations/local.py\", line 87, in info\n    out = os.stat(path, follow_symlinks=False)\nFileNotFoundError: [Errno 2] No such file or directory: '/home/chetana/gridmet_test_run/gridmet_climatology/testing_ready_gridmet.csv'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/utrQ6i3XRugd/merge_custom_traning_range.py\", line 11, in <module>\n    gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 138, in wrapper\n    raise type(e)(\nFileNotFoundError: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: [Errno 2] No such file or directory: '/home/chetana/gridmet_test_run/gridmet_climatology/testing_ready_gridmet.csv'\n",
  "history_begin_time" : 1694296461122,
  "history_end_time" : 1694296463617,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "YudcKdFOZRFm",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '64MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\nprint('merge completed.')\n",
  "history_output" : "today date = 2023-09-09\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 136, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 761, in read\n    return read_pandas(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 561, in read_pandas\n    b_out = read_bytes(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/bytes/core.py\", line 111, in read_bytes\n    size = fs.info(path)[\"size\"]\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/fsspec/implementations/local.py\", line 87, in info\n    out = os.stat(path, follow_symlinks=False)\nFileNotFoundError: [Errno 2] No such file or directory: '/home/chetana/gridmet_test_run/gridmet_climatology/testing_ready_gridmet.csv'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/YudcKdFOZRFm/merge_custom_traning_range.py\", line 11, in <module>\n    gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 138, in wrapper\n    raise type(e)(\nFileNotFoundError: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: [Errno 2] No such file or directory: '/home/chetana/gridmet_test_run/gridmet_climatology/testing_ready_gridmet.csv'\n",
  "history_begin_time" : 1694296438258,
  "history_end_time" : 1694296441348,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "BgTtprmmDSMl",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '64MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\nprint('merge completed.')\n",
  "history_output" : null,
  "history_begin_time" : 1694296431475,
  "history_end_time" : null,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : null
},{
  "history_id" : "prD3BEUypldj",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '64MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\n",
  "history_begin_time" : 1692192092321,
  "history_end_time" : 1692195042963,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "yEUYg2Nb9Fk5",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '64MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\n#amsr = amsr.repartition(partition_size=chunk_size)\n#snotel = snotel.repartition(partition_size=chunk_size)\n#gridmet = gridmet.repartition(partition_size=chunk_size)\n#terrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False, compute=False)\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\n",
  "history_begin_time" : 1692192040184,
  "history_end_time" : 1692192043025,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "z5SFly4L0uNd",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '64MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False, compute=False, chunksize=chunk_size)\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\n",
  "history_begin_time" : 1692191994169,
  "history_end_time" : 1692192009120,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "YTF8A77bPqgj",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '64MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the final merged DataFrame to a CSV file\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\n\n# Compute and persist the merged DataFrame\nmerged_df = merged_df.persist()\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\nsh: line 1:  7881 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python merge_custom_traning_range.py\n",
  "history_begin_time" : 1692191556216,
  "history_end_time" : 1692191717071,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "5VRYBoVkmR4e",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '64MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size, compression='gzip')\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size, compression='gzip')\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size, compression='gzip')\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size, compression='gzip')\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the final merged DataFrame to a CSV file\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\n\n# Compute and persist the merged DataFrame\nmerged_df = merged_df.persist()\n\n# Perform your computations on the persisted merged DataFrame\n# ...\n\n# Optionally, you can also call merged_df.compute() to trigger the computation and wait for it to finish.\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\n/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py:544: UserWarning: Warning gzip compression does not support breaking apart files\nPlease ensure that each individual file can fit in memory and\nuse the keyword ``blocksize=None to remove this message``\nSetting ``blocksize=None``\n  warn(\nTraceback (most recent call last):\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 136, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 761, in read\n    return read_pandas(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 561, in read_pandas\n    b_out = read_bytes(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/bytes/core.py\", line 173, in read_bytes\n    sample_buff = f.read(sample)\n  File \"/home/chetana/anaconda3/lib/python3.9/gzip.py\", line 300, in read\n    return self._buffer.read(size)\n  File \"/home/chetana/anaconda3/lib/python3.9/_compression.py\", line 68, in readinto\n    data = self.read(len(byte_view))\n  File \"/home/chetana/anaconda3/lib/python3.9/gzip.py\", line 487, in read\n    if not self._read_gzip_header():\n  File \"/home/chetana/anaconda3/lib/python3.9/gzip.py\", line 435, in _read_gzip_header\n    raise BadGzipFile('Not a gzipped file (%r)' % magic)\ngzip.BadGzipFile: Not a gzipped file (b'da')\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/5VRYBoVkmR4e/merge_custom_traning_range.py\", line 9, in <module>\n    amsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size, compression='gzip')\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 138, in wrapper\n    raise type(e)(\ngzip.BadGzipFile: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: Not a gzipped file (b'da')\n",
  "history_begin_time" : 1692191461468,
  "history_end_time" : 1692191471476,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "dLs0mseIX2HS",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\n# Define a custom chunk size based on available memory\nchunk_size = '32 MiB'\n\n# Create a temporary directory for Dask to spill data\ntemp_dir = os.path.join(working_dir, 'dask_temp')\nos.makedirs(temp_dir, exist_ok=True)\n\n# Read the CSV files with a smaller chunk size\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size, usecols=['lat', 'lon', 'date', 'SWE', 'Flag'])  # Select necessary columns\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size, usecols=['lat', 'lon', 'date', 'swe_value'])\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Merge DataFrames based on specified columns and save intermediate results\nmerged_df = amsr.merge(snotel, on=['lat', 'lon', 'date'], how='inner')\nmerged_df.to_csv(os.path.join(temp_dir, 'merged_amsr_snotel.csv'), index=False, single_file=True)\n\nmerged_df = merged_df.merge(gridmet, on=['lat', 'lon', 'date'], how='inner')\nmerged_df.to_csv(os.path.join(temp_dir, 'merged_gridmet.csv'), index=False, single_file=True)\n\n# Drop unnecessary columns\n#merged_df = merged_df.drop(['unnecessary_col1', 'unnecessary_col2'], axis=1)\n\nmerged_df = merged_df.merge(terrain, on=['lat', 'lon'], how='inner')\nmerged_df.to_csv(os.path.join(temp_dir, 'merged_final_3_yrs.csv'), index=False, single_file=True)\n\n# Drop duplicates based on all columns\nmerged_df = merged_df.drop_duplicates()\n\n# Save the final merged DataFrame to a CSV file\noutput_csv = f'{working_dir}/merged_data_3_yrs.csv'\nmerged_df.to_csv(output_csv, index=False, single_file=True)\n\nprint(f\"Merged data (without duplicates) saved to {output_csv}\")\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/dLs0mseIX2HS/merge_custom_traning_range.py\", line 31, in <module>\n    merged_df.to_csv(os.path.join(temp_dir, 'merged_final_3_yrs.csv'), index=False, single_file=True)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 994, in to_csv\n    return list(dask.compute(*values, **compute_kwargs))\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/threaded.py\", line 89, in get\n    results = get_async(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 511, in get_async\n    raise_exception(exc, tb)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 319, in reraise\n    raise exc\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 224, in execute_task\n    result = _execute_task(task, data)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 289, in merge_chunk\n    out = lhs.merge(rhs, *args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/concat.py\", line 212, in concatenate_managers\n    values = values.copy()\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 5.83 GiB for an array with shape (6, 130429334) and data type float64\n",
  "history_begin_time" : 1692190428106,
  "history_end_time" : 1692190778027,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "98zU5ujLxotA",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\n# Define a custom chunk size based on available memory\nchunk_size = '32 MiB'\n\n# Create a temporary directory for Dask to spill data\ntemp_dir = os.path.join(working_dir, 'dask_temp')\nos.makedirs(temp_dir, exist_ok=True)\n\n# Read the CSV files with a smaller chunk size\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Merge DataFrames based on specified columns and save intermediate results\nmerged_df = amsr.merge(snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df.to_csv(os.path.join(temp_dir, 'merged_amsr_snotel.csv'), index=False, single_file=True)\n\nmerged_df = merged_df.merge(gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df.to_csv(os.path.join(temp_dir, 'merged_gridmet.csv'), index=False, single_file=True)\n\nmerged_df = merged_df.merge(terrain, on=['lat', 'lon'], how='outer')\nmerged_df.to_csv(os.path.join(temp_dir, 'merged_final.csv'), index=False, single_file=True)\n\n# Drop duplicates based on all columns\nmerged_df = merged_df.drop_duplicates()\n\n# Save the final merged DataFrame to a CSV file\noutput_csv = f'{working_dir}/merged_data_3_yrs.csv'\nmerged_df.to_csv(output_csv, index=False, single_file=True)\n\n\nprint(f\"Merged data (without duplicates) saved to {output_csv}\")",
  "history_output" : "today date = 2023-08-16\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/98zU5ujLxotA/merge_custom_traning_range.py\", line 28, in <module>\n    merged_df.to_csv(os.path.join(temp_dir, 'merged_final.csv'), index=False, single_file=True)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 994, in to_csv\n    return list(dask.compute(*values, **compute_kwargs))\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/threaded.py\", line 89, in get\n    results = get_async(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 511, in get_async\n    raise_exception(exc, tb)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 319, in reraise\n    raise exc\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 224, in execute_task\n    result = _execute_task(task, data)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 289, in merge_chunk\n    out = lhs.merge(rhs, *args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/concat.py\", line 212, in concatenate_managers\n    values = values.copy()\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 4.75 GiB for an array with shape (6, 106348126) and data type float64\nsh: line 1:  3289 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python merge_custom_traning_range.py\n",
  "history_begin_time" : 1692188911679,
  "history_end_time" : 1692189475914,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "v3PiC7nthWQ1",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\n# Define a custom chunk size based on available memory\nchunk_size = '32 MiB'\n\n# Create a temporary directory for Dask to spill data\ntemp_dir = os.path.join(working_dir, 'dask_temp')\nos.makedirs(temp_dir, exist_ok=True)\n\n# Read the CSV files with a smaller chunk size\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Merge DataFrames based on specified columns and save intermediate results\nmerged_df = amsr.merge(snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df.to_csv(os.path.join(temp_dir, 'merged_amsr_snotel.csv'), index=False, single_file=True)\n\nmerged_df = merged_df.merge(gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df.to_csv(os.path.join(temp_dir, 'merged_gridmet.csv'), index=False, single_file=True)\n\nmerged_df = merged_df.merge(terrain, on=['lat', 'lon'], how='outer')\nmerged_df.to_csv(os.path.join(temp_dir, 'merged_final.csv'), index=False, single_file=True)\n\n# Drop duplicates based on all columns\nmerged_df = merged_df.drop_duplicates()\n\n# Save the final merged DataFrame to a CSV file\noutput_csv = f'{working_dir}/merged_data_3_yrs.csv'\nmerged_df.to_csv(output_csv, index=False, single_file=True)\n\n\nprint(f\"Merged data (without duplicates) saved to {output_csv}\")",
  "history_output" : "today date = 2023-08-16\n/home/chetana\nsh: line 1: 29909 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python merge_custom_traning_range.py\n",
  "history_begin_time" : 1692166850136,
  "history_end_time" : 1692167287628,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "XOONN2jgWhub",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\n# Define a custom chunk size based on available memory\nchunk_size = '256 MiB'\n\n# Create a temporary directory for Dask to spill data\ntemp_dir = os.path.join(working_dir, 'dask_temp')\nos.makedirs(temp_dir, exist_ok=True)\n\n# Read the CSV files with a smaller chunk size\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Define the order in which DataFrames are merged\nmerge_order = [(amsr, snotel), (gridmet, terrain)]\n\n# Iterate over pairs of DataFrames and save intermediate results\nmerged_df = None\nfor left, right in merge_order:\n    if merged_df is None:\n        merged_df = left.merge(right, on=['lat', 'lon', 'date'], how='outer')\n    else:\n        temp_merged = left.merge(right, on=['lat', 'lon', 'date'], how='outer')\n        temp_merged.to_csv(os.path.join(temp_dir, 'merged_temp.csv'), index=False, single_file=True)\n        \n        # Drop duplicates based on all columns\n        temp_merged = temp_merged.drop_duplicates()\n        temp_merged.to_csv(os.path.join(temp_dir, 'merged_temp_deduped.csv'), index=False, single_file=True)\n        \n        # Continue merging with existing merged_df\n        merged_df = merged_df.merge(temp_merged, on=['lat', 'lon', 'date'], how='outer')\n        merged_df = merged_df.drop_duplicates()\n        \n        # Remove intermediate files if needed\n        os.remove(os.path.join(temp_dir, 'merged_temp.csv'))\n        os.remove(os.path.join(temp_dir, 'merged_temp_deduped.csv'))\n\n# Save the final merged DataFrame to a CSV file\noutput_csv = f'{working_dir}/merged_data_3_yrs.csv'\nmerged_df.to_csv(output_csv, index=False, single_file=True)\n\nprint(f\"Merged data (without duplicates) saved to {output_csv}\")\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/XOONN2jgWhub/merge_custom_traning_range.py\", line 29, in <module>\n    temp_merged = left.merge(right, on=['lat', 'lon', 'date'], how='outer')\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/core.py\", line 5691, in merge\n    return merge(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 726, in merge\n    return hash_join(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 400, in hash_join\n    meta = _lhs_meta.merge(_rhs_meta, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/frame.py\", line 10093, in merge\n    return merge(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 110, in merge\n    op = _MergeOperation(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 703, in __init__\n    ) = self._get_merge_keys()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 1162, in _get_merge_keys\n    right_keys.append(right._get_label_or_level_values(rk))\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 1850, in _get_label_or_level_values\n    raise KeyError(key)\nKeyError: 'date'\n",
  "history_begin_time" : 1692166789688,
  "history_end_time" : 1692166801037,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "UcYEMnaQHqTr",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\n# Define a custom chunk size based on available memory\nchunk_size = '256 MiB'\n\n# Create a temporary directory for Dask to spill data\ntemp_dir = os.path.join(working_dir, 'dask_temp')\nos.makedirs(temp_dir, exist_ok=True)\n\n# Read the CSV files with a smaller chunk size\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Merge DataFrames based on specified columns and save intermediate results\nmerged_df = amsr.merge(snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df.to_csv(os.path.join(temp_dir, 'merged_amsr_snotel.csv'), index=False, single_file=True)\n\nmerged_df = merged_df.merge(gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df.to_csv(os.path.join(temp_dir, 'merged_gridmet.csv'), index=False, single_file=True)\n\nmerged_df = merged_df.merge(terrain, on=['lat', 'lon'], how='outer')\nmerged_df.to_csv(os.path.join(temp_dir, 'merged_final.csv'), index=False, single_file=True)\n\n# Drop duplicates based on all columns\nmerged_df = merged_df.drop_duplicates()\n\n# Save the final merged DataFrame to a CSV file\noutput_csv = f'{working_dir}/merged_data_3_yrs.csv'\nmerged_df.to_csv(output_csv, index=False, single_file=True)\n\n\nprint(f\"Merged data (without duplicates) saved to {output_csv}\")\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/UcYEMnaQHqTr/merge_custom_traning_range.py\", line 28, in <module>\n    merged_df.to_csv(os.path.join(temp_dir, 'merged_final.csv'), index=False, single_file=True)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 994, in to_csv\n    return list(dask.compute(*values, **compute_kwargs))\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/threaded.py\", line 89, in get\n    results = get_async(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 511, in get_async\n    raise_exception(exc, tb)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 319, in reraise\n    raise exc\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 224, in execute_task\n    result = _execute_task(task, data)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 289, in merge_chunk\n    out = lhs.merge(rhs, *args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/concat.py\", line 212, in concatenate_managers\n    values = values.copy()\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 13.4 GiB for an array with shape (7, 257808468) and data type float64\n",
  "history_begin_time" : 1692165824781,
  "history_end_time" : 1692166243884,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ORzb67mrRIVZ",
  "history_input" : "import dask.dataframe as dd\nimport dask.config\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\n# Define a custom chunk size based on available memory\nchunk_size = '256 MiB'\n\n# Define memory limit (adjust as needed)\nmemory_limit = '40GB'  # Set an appropriate value based on available RAM\n\n# Set the memory limit in Dask's configuration\ndask.config.set({'io': {'pandas': {'memory_limit': memory_limit}}})\n\n# Create a temporary directory for Dask to spill data\ntemp_dir = os.path.join(working_dir, 'dask_temp')\nos.makedirs(temp_dir, exist_ok=True)\n\n# Read the CSV files with a smaller chunk size\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = amsr.merge(snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = merged_df.merge(gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = merged_df.merge(terrain, on=['lat', 'lon'], how='outer')\n\n# Drop duplicates based on all columns\nmerged_df = merged_df.drop_duplicates()\n\n# Save the merged DataFrame to a CSV file\noutput_csv = f'{working_dir}/merged_data_3_yrs.csv'\nmerged_df.to_csv(output_csv, index=False, single_file=True)\n\n# Compute and wait for the computation to complete\nmerged_df.compute()\n\nprint(f\"Merged data (without duplicates) saved to {output_csv}\")\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/ORzb67mrRIVZ/merge_custom_traning_range.py\", line 37, in <module>\n    merged_df.to_csv(output_csv, index=False, single_file=True)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 994, in to_csv\n    return list(dask.compute(*values, **compute_kwargs))\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/threaded.py\", line 89, in get\n    results = get_async(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 511, in get_async\n    raise_exception(exc, tb)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 319, in reraise\n    raise exc\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 224, in execute_task\n    result = _execute_task(task, data)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 289, in merge_chunk\n    out = lhs.merge(rhs, *args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/concat.py\", line 212, in concatenate_managers\n    values = values.copy()\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 13.4 GiB for an array with shape (7, 257808468) and data type float64\n",
  "history_begin_time" : 1692165487824,
  "history_end_time" : 1692165737157,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "jxQ8EbyxJPQO",
  "history_input" : "import dask.dataframe as dd\nimport dask.config\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\n# Define a custom chunk size based on available memory\nchunk_size = '256 MiB'\n\n# Define memory limit (adjust as needed)\nmemory_limit = '40GB'  # Set an appropriate value based on available RAM\n\n# Set the memory limit in Dask's configuration\ndask.config.set({'io': {'pandas': {'memory_limit': memory_limit}}})\n\n# Create a temporary directory for Dask to spill data\ntemp_dir = os.path.join(working_dir, 'dask_temp')\nos.makedirs(temp_dir, exist_ok=True)\n\n# Read the CSV files with a smaller chunk size\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = amsr.merge(snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = merged_df.merge(gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = merged_df.merge(terrain, on=['lat', 'lon'], how='outer')\n\n# Drop duplicates based on all columns\nmerged_df = merged_df.drop_duplicates()\n\n# Save the merged DataFrame to a CSV file\noutput_csv = f'{working_dir}/merged_data.csv'\nmerged_df.to_csv(output_csv, index=False)\n\n# Compute and wait for the computation to complete\nmerged_df.compute()\n\nprint(f\"Merged data (without duplicates) saved to {output_csv}\")\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/jxQ8EbyxJPQO/merge_custom_traning_range.py\", line 37, in <module>\n    merged_df.to_csv(output_csv, index=False)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/core.py\", line 1856, in to_csv\n    return to_csv(self, filename, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 952, in to_csv\n    files = open_files(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/fsspec/core.py\", line 293, in open_files\n    [fs.makedirs(parent, exist_ok=True) for parent in parents]\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/fsspec/core.py\", line 293, in <listcomp>\n    [fs.makedirs(parent, exist_ok=True) for parent in parents]\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/fsspec/implementations/local.py\", line 54, in makedirs\n    os.makedirs(path, exist_ok=exist_ok)\n  File \"/home/chetana/anaconda3/lib/python3.9/os.py\", line 225, in makedirs\n    mkdir(name, mode)\nFileExistsError: [Errno 17] File exists: '/home/chetana/gridmet_test_run/merged_data.csv'\n",
  "history_begin_time" : 1692165416567,
  "history_end_time" : 1692165420705,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "vXN3L8DGMjFA",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\n# Define a custom chunk size based on available memory\nchunk_size = '256 MiB'\n\n# Define memory limit (adjust as needed)\nmemory_limit = '40GB'  # Set an appropriate value based on available RAM\n\n# Create a temporary directory for Dask to spill data\ntemp_dir = os.path.join(working_dir, 'dask_temp')\nos.makedirs(temp_dir, exist_ok=True)\n\n# Read the CSV files with memory limit and temporary directory\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size, memory_limit=memory_limit, temporary_directory=temp_dir)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size, memory_limit=memory_limit, temporary_directory=temp_dir)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size, memory_limit=memory_limit, temporary_directory=temp_dir)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size, memory_limit=memory_limit, temporary_directory=temp_dir)\n\n# Merge DataFrames based on specified columns\nmerged_df = amsr.merge(snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = merged_df.merge(gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = merged_df.merge(terrain, on=['lat', 'lon'], how='outer')\n\n# Drop duplicates based on all columns\nmerged_df = merged_df.drop_duplicates()\n\n# Save the merged DataFrame to a CSV file\noutput_csv = f'{working_dir}/merged_data.csv'\nmerged_df.to_csv(output_csv, index=False)\n\n# Compute and wait for the computation to complete\n#merged_df.compute()\n\nprint(f\"Merged data (without duplicates) saved to {output_csv}\")\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 136, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 761, in read\n    return read_pandas(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 630, in read_pandas\n    head = reader(BytesIO(b_sample), nrows=sample_rows, **head_kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 211, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\nTypeError: read_csv() got an unexpected keyword argument 'memory_limit'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/vXN3L8DGMjFA/merge_custom_traning_range.py\", line 18, in <module>\n    amsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size, memory_limit=memory_limit, temporary_directory=temp_dir)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 138, in wrapper\n    raise type(e)(\nTypeError: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: read_csv() got an unexpected keyword argument 'memory_limit'\n",
  "history_begin_time" : 1692165336940,
  "history_end_time" : 1692165340408,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "p2TRG13I7tgn",
  "history_input" : "import dask.dataframe as dd\nimport os\n\nworking_dir = '.'\n\n# Define a custom chunk size based on available memory\nchunk_size = '256 MiB'\n\n# Define memory limit (adjust as needed)\nmemory_limit = '40GB'  # Set an appropriate value based on available RAM\n\n# Create a temporary directory for Dask to spill data\ntemp_dir = os.path.join(working_dir, 'dask_temp')\nos.makedirs(temp_dir, exist_ok=True)\n\n# Read the CSV files with memory limit and temporary directory\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size, memory_limit=memory_limit, temporary_directory=temp_dir)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size, memory_limit=memory_limit, temporary_directory=temp_dir)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size, memory_limit=memory_limit, temporary_directory=temp_dir)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size, memory_limit=memory_limit, temporary_directory=temp_dir)\n\n# Merge DataFrames based on specified columns\nmerged_df = amsr.merge(snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = merged_df.merge(gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = merged_df.merge(terrain, on=['lat', 'lon'], how='outer')\n\n# Drop duplicates based on all columns\nmerged_df = merged_df.drop_duplicates()\n\n# Save the merged DataFrame to a CSV file\noutput_csv = f'{working_dir}/merged_data.csv'\nmerged_df.to_csv(output_csv, index=False)\n\n# Compute and wait for the computation to complete\n#merged_df.compute()\n\nprint(f\"Merged data (without duplicates) saved to {output_csv}\")\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 136, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 761, in read\n    return read_pandas(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 561, in read_pandas\n    b_out = read_bytes(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/bytes/core.py\", line 111, in read_bytes\n    size = fs.info(path)[\"size\"]\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/fsspec/implementations/local.py\", line 87, in info\n    out = os.stat(path, follow_symlinks=False)\nFileNotFoundError: [Errno 2] No such file or directory: '/home/chetana/gw-workspace/p2TRG13I7tgn/training_ready_amsr_3_yrs.csv'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/p2TRG13I7tgn/merge_custom_traning_range.py\", line 17, in <module>\n    amsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size, memory_limit=memory_limit, temporary_directory=temp_dir)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 138, in wrapper\n    raise type(e)(\nFileNotFoundError: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: [Errno 2] No such file or directory: '/home/chetana/gw-workspace/p2TRG13I7tgn/training_ready_amsr_3_yrs.csv'\n",
  "history_begin_time" : 1692165292926,
  "history_end_time" : 1692165302501,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "tmc072vE02pu",
  "history_input" : "import dask.dataframe as dd\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\n# Read the CSV files\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv')\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv')\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv')\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv')\n\n# Merge DataFrames based on specified columns\nmerged_df = snotel.merge(amsr, on=['lat', 'lon', 'date'], how='outer')\nprint('completed snotel <-> amsr merge')\nmerged_df = merged_df.merge(gridmet, on=['lat', 'lon', 'date'], how='outer')\nprint('completed snotel_amsr <-> gridmet merge')\nmerged_df = merged_df.merge(terrain, on=['lat', 'lon'], how='outer')\nprint('compelted snotel_amsr_grimet <-> terrian merge')\n\n# Drop duplicates based on all columns\nmerged_df = merged_df.drop_duplicates()\n\n# Save the merged DataFrame to a CSV file\noutput_csv = f'{working_dir}/merged_data_3_yrs.csv'\nmerged_df.to_csv(output_csv, index=False)\n\n# Compute and wait for the computation to complete (if needed)\nmerged_df.compute()\n\nprint(f\"Merged data (without duplicates) saved to {output_csv}\")\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\ncompleted snotel <-> amsr merge\ncompleted snotel_amsr <-> gridmet merge\ncompelted snotel_amsr_grimet <-> terrian merge\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/tmc072vE02pu/merge_custom_traning_range.py\", line 25, in <module>\n    merged_df.to_csv(output_csv, index=False)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 994, in to_csv\n    return list(dask.compute(*values, **compute_kwargs))\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/threaded.py\", line 89, in get\n    results = get_async(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 511, in get_async\n    raise_exception(exc, tb)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 319, in reraise\n    raise exc\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 224, in execute_task\n    result = _execute_task(task, data)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 289, in merge_chunk\n    out = lhs.merge(rhs, *args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/concat.py\", line 212, in concatenate_managers\n    values = values.copy()\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 11.5 GiB for an array with shape (6, 257808468) and data type float64\n",
  "history_begin_time" : 1692164750384,
  "history_end_time" : 1692165144917,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Qjb9777yKfq4",
  "history_input" : "import dask.dataframe as dd\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\n# Read the CSV files\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv')\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv')\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv')\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv')\n\n# Merge DataFrames based on specified columns\nmerged_df = snotel.merge(amsr, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = merged_df.merge(gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = merged_df.merge(terrain, on=['lat', 'lon'], how='outer')\n\n# Drop duplicates based on all columns\nmerged_df = merged_df.drop_duplicates()\n\n# Save the merged DataFrame to a CSV file\noutput_csv = f'{working_dir}/merged_data_3_yrs.csv'\nmerged_df.to_csv(output_csv, index=False)\n\n# Compute and wait for the computation to complete (if needed)\nmerged_df.compute()\n\nprint(f\"Merged data (without duplicates) saved to {output_csv}\")\n",
  "history_output" : null,
  "history_begin_time" : 1692164667924,
  "history_end_time" : null,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : null
},{
  "history_id" : "pR1xj0UHuGsc",
  "history_input" : "import dask.dataframe as dd\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\n# Read the CSV files\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv')\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv')\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv')\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv')\n\n# Merge DataFrames based on specified columns\nmerged_df = snotel.merge(amsr, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = merged_df.merge(gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = merged_df.merge(terrain, on=['lat', 'lon'], how='outer')\n\n# Drop duplicates based on all columns\nmerged_df = merged_df.drop_duplicates()\n\n# Save the merged DataFrame to a CSV file\noutput_csv = f'{working_dir}/merged_data_3_yrs.csv'\nmerged_df.to_csv(output_csv, index=False)\n\n# Compute and wait for the computation to complete (if needed)\nmerged_df.compute()\n\nprint(f\"Merged data (without duplicates) saved to {output_csv}\")\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\n",
  "history_begin_time" : 1692164620925,
  "history_end_time" : 1692164667961,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "t8cbvf2kbq0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531335834,
  "history_end_time" : 1691531335834,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "sxwmabed15w",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531292844,
  "history_end_time" : 1691531292844,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "885xpzpe1da",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531254771,
  "history_end_time" : 1691531284904,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "2ekosjovj9k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531163961,
  "history_end_time" : 1691531163961,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "4791ge4k1tq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531121014,
  "history_end_time" : 1691531121014,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "8n6w2cjsj7r",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531061019,
  "history_end_time" : 1691531061019,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "kroadp4thod",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530848386,
  "history_end_time" : 1691530848386,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "q087n8t57q4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717766,
  "history_end_time" : 1691530721109,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "vb9ks58sr66",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530690280,
  "history_end_time" : 1691530716753,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "77dcsormpmx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530621141,
  "history_end_time" : 1691530622443,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "li45n4zcf64",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530617330,
  "history_end_time" : 1691530617330,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "etfp4vl7p5r",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530599904,
  "history_end_time" : 1691530614288,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "QkMR0GC9egfS",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask instead of Pandas\n#snotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv')\n#gridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv')\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv')\n#terrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv')\n\n# Perform the merges using Dask DataFrames\n#snotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\n#merged_data = dd.merge(snotel_gridmet, terrain, on=['lat', 'lon'], how='inner')\n#merged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Compute the result and convert to Pandas DataFrame if needed\n#merged_data.to_csv('/home/chetana/gridmet_test_run/training_snotel_gridmet_3_yrs.csv', index=False, single_file=True)\n\nsnotel_gridmet_merged = dd.read_csv('/home/chetana/gridmet_test_run/training_snotel_gridmet_3_yrs.csv', usecols=['date', 'SWE', 'lat', 'lon', 'etr', 'pr',\n       'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'elevation', 'slope',\n       'curvature', 'aspect', 'eastness', 'northness'])\n\nmerged_data = dd.merge(snotel_gridmet_merged, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = merged_data.drop_duplicates()\nmerged_data.to_csv('/home/chetana/gridmet_test_run/training_ready_3_yr_data.csv', single_file=True, index=False)\n\n# Now you have the merged data as a Pandas DataFrame\n",
  "history_output" : "",
  "history_begin_time" : 1691384668402,
  "history_end_time" : 1691384954294,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "MsrbHDXjpeUV",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask instead of Pandas\n#snotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv')\n#gridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv')\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv')\n#terrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv')\n\n# Perform the merges using Dask DataFrames\n#snotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\n#merged_data = dd.merge(snotel_gridmet, terrain, on=['lat', 'lon'], how='inner')\n#merged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Compute the result and convert to Pandas DataFrame if needed\n#merged_data.to_csv('/home/chetana/gridmet_test_run/training_snotel_gridmet_3_yrs.csv', index=False, single_file=True)\n\nsnotel_gridmet_merged = dd.read_csv('/home/chetana/gridmet_test_run/training_snotel_gridmet_3_yrs.csv', usecols=['date', 'SWE', 'lat', 'lon', 'etr', 'pr',\n       'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'elevation', 'slope',\n       'curvature', 'aspect', 'eastness', 'northness'])\n\nmerged_data = dd.merge(snotel_gridmet_merged, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = merged_data.drop_duplicates()\nmerged_data.to_csv('/home/chetana/gridmet_test_run/training_ready_2_yr_data.csv', single_file=True, index=False)\n\n# Now you have the merged data as a Pandas DataFrame\n",
  "history_output" : "",
  "history_begin_time" : 1691383717686,
  "history_end_time" : 1691384053108,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "gQpWBeTGfVYl",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask instead of Pandas\n#snotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv')\n#gridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv')\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv')\n#terrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv')\n\n# Perform the merges using Dask DataFrames\n#snotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\n#merged_data = dd.merge(snotel_gridmet, terrain, on=['lat', 'lon'], how='inner')\n#merged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Compute the result and convert to Pandas DataFrame if needed\n#merged_data.to_csv('/home/chetana/gridmet_test_run/training_snotel_gridmet_3_yrs.csv', index=False, single_file=True)\n\nsnotel_gridmet_merged = dd.read_csv('/home/chetana/gridmet_test_run/training_snotel_gridmet_3_yrs.csv', usecols=['date', 'SWE', 'lat', 'lon', 'etr', 'pr',\n       'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'elevation', 'slope',\n       'curvature', 'aspect', 'eastness', 'northness'])\n\nmerged_data = dd.merge(snotel_gridmet_merged, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data.to_csv('/home/chetana/gridmet_test_run/training_final_3_yr_training_ready.csv', single_file=True, index=False)\n\n# Now you have the merged data as a Pandas DataFrame\n",
  "history_output" : "",
  "history_begin_time" : 1691368736436,
  "history_end_time" : 1691372133466,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Ht5XX8CxhstS",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask instead of Pandas\n#snotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv')\n#gridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv')\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv')\n#terrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv')\n\n# Perform the merges using Dask DataFrames\n#snotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\n#merged_data = dd.merge(snotel_gridmet, terrain, on=['lat', 'lon'], how='inner')\n#merged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Compute the result and convert to Pandas DataFrame if needed\n#merged_data.to_csv('/home/chetana/gridmet_test_run/training_snotel_gridmet_3_yrs.csv', index=False, single_file=True)\n\nsnotel_gridmet_merged = dd.read_csv('/home/chetana/gridmet_test_run/training_snotel_gridmet_3_yrs.csv', usecols=['date', 'SWE', 'lat', 'lon', 'etr', 'pr',\n       'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'elevation', 'slope',\n       'curvature', 'aspect', 'eastness', 'northness'])\n\nmerged_data = dd.merge(snotel_gridmet_merged, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data.to_csv('/home/chetana/gridmet_test_run/training_final_3_yr_training_ready.csv', single_file=True, index=False)\n\n# Now you have the merged data as a Pandas DataFrame\n",
  "history_output" : null,
  "history_begin_time" : 1691368727994,
  "history_end_time" : null,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : null
},{
  "history_id" : "Fm233HyAlNE4",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask instead of Pandas\n#snotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv')\n#gridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv')\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv')\n#terrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv')\n\n# Perform the merges using Dask DataFrames\n#snotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\n#merged_data = dd.merge(snotel_gridmet, terrain, on=['lat', 'lon'], how='inner')\n#merged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Compute the result and convert to Pandas DataFrame if needed\n#merged_data.to_csv('/home/chetana/gridmet_test_run/training_snotel_gridmet_3_yrs.csv', index=False, single_file=True)\n\nsnotel_gridmet_merged = dd.read_csv('/home/chetana/gridmet_test_run/training_snotel_gridmet_3_yrs.csv')\n\nmerged_data = dd.merge(snotel_gridmet_merged, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data.to_csv('/home/chetana/gridmet_test_run/training_final_3_yr_training_ready.csv', single_file=True, index=False)\n\n# Now you have the merged data as a Pandas DataFrame\n",
  "history_output" : "",
  "history_begin_time" : 1691365701482,
  "history_end_time" : 1691368728002,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "nyPtpPq6J27N",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask instead of Pandas\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv')\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv')\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv')\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv')\n\n# Perform the merges using Dask DataFrames\nsnotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(snotel_gridmet, terrain, on=['lat', 'lon'], how='inner')\n#merged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False, single_file=True)\n\n# Now you have the merged data as a Pandas DataFrame\n",
  "history_output" : "",
  "history_begin_time" : 1691363789027,
  "history_end_time" : 1691364194713,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "nEmUEAJFm7OZ",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask with a smaller blocksize and only necessary columns\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv')\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', usecols=['date', 'lat', 'lon', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs'])\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv')\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv')\n\n# Perform the merges using Dask DataFrames with all merge keys specified at once\nmerged_data = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner', suffixes=('_snotel', '_gridmet'))\nmerged_data = dd.merge(merged_data, amsr, on=['date', 'lat', 'lon'], how='inner', suffixes=('', '_amsr'))\nmerged_data = dd.merge(merged_data, terrain, on=['lat', 'lon'], how='inner', suffixes=('', '_terrain'))\n\n# Write intermediate result to disk in CSV format\nmerged_data.to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False, single_file=True)\n\n# The above line will execute lazily and data will spill to disk if RAM is insufficient\n# We don't need to use `persist` since the spill-to-disk mechanism will handle data storage\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/nEmUEAJFm7OZ/merge_custom_traning_range.py\", line 15, in <module>\n    merged_data.to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False, single_file=True)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 994, in to_csv\n    return list(dask.compute(*values, **compute_kwargs))\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/threaded.py\", line 89, in get\n    results = get_async(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 511, in get_async\n    raise_exception(exc, tb)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 319, in reraise\n    raise exc\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 224, in execute_task\n    result = _execute_task(task, data)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 289, in merge_chunk\n    out = lhs.merge(rhs, *args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/concat.py\", line 212, in concatenate_managers\n    values = values.copy()\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 5.44 GiB for an array with shape (6, 121609632) and data type float64\nsh: line 1: 13057 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python merge_custom_traning_range.py\n",
  "history_begin_time" : 1691363631859,
  "history_end_time" : 1691363768940,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "8BQ7efQSX58O",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask with a smaller blocksize and only necessary columns\nblocksize = \"16MB\"\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv')\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', usecols=['date', 'lat', 'lon', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs'])\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv')\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv')\n\n# Perform the merges using Dask DataFrames with all merge keys specified at once\nmerged_data = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(merged_data, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(merged_data, terrain, on=['lat', 'lon'], how='inner')\n\n# Persist intermediate result to disk if enough disk space is available\nmerged_data = merged_data.persist()\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/8BQ7efQSX58O/merge_custom_traning_range.py\", line 16, in <module>\n    merged_data = merged_data.persist()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/threaded.py\", line 89, in get\n    results = get_async(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 511, in get_async\n    raise_exception(exc, tb)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 319, in reraise\n    raise exc\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 224, in execute_task\n    result = _execute_task(task, data)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 289, in merge_chunk\n    out = lhs.merge(rhs, *args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/concat.py\", line 212, in concatenate_managers\n    values = values.copy()\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 5.44 GiB for an array with shape (6, 121756443) and data type float64\nsh: line 1: 12603 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python merge_custom_traning_range.py\n",
  "history_begin_time" : 1691363355002,
  "history_end_time" : 1691363507755,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "njbpUsRMRccq",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask with a smaller blocksize and only necessary columns\nblocksize = \"16MB\"\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=blocksize)\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs'])\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', blocksize=blocksize)\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv', blocksize=blocksize)\n\n# Perform the merges using Dask DataFrames with all merge keys specified at once\nmerged_data = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(merged_data, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(merged_data, terrain, on=['lat', 'lon'], how='inner')\n\n# Persist intermediate result to disk if enough disk space is available\nmerged_data = merged_data.persist()\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/njbpUsRMRccq/merge_custom_traning_range.py\", line 19, in <module>\n    merged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/dispatch.py\", line 68, in concat\n    return func(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/backends.py\", line 667, in concat_pandas\n    out = pd.concat(dfs3, join=join, sort=False)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"<__array_function__ internals>\", line 180, in concatenate\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 10.9 GiB for an array with shape (6, 243366075) and data type float64\n",
  "history_begin_time" : 1691362366332,
  "history_end_time" : 1691362534449,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "5Lk0smDI14C3",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask with a smaller blocksize and only necessary columns\nblocksize = \"16MB\"\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=blocksize)\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', blocksize=blocksize)\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', blocksize=blocksize)\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv', blocksize=blocksize)\n\n# Perform the merges using Dask DataFrames with all merge keys specified at once\nmerged_data = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(merged_data, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(merged_data, terrain, on=['lat', 'lon'], how='inner')\n\n# Persist intermediate result to disk if enough disk space is available\nmerged_data = merged_data.persist()\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False)\n",
  "history_output" : "sh: line 1:  9298 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python merge_custom_traning_range.py\n",
  "history_begin_time" : 1691361850300,
  "history_end_time" : 1691361958513,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "DqyyOBpyf5mO",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask with a smaller blocksize and only necessary columns\nblocksize = \"16MB\"\nusecols = ['date', 'lat', 'lon', 'other_needed_columns']\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=blocksize, usecols=usecols)\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', blocksize=blocksize, usecols=usecols)\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', blocksize=blocksize, usecols=usecols)\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv', blocksize=blocksize, usecols=['lat', 'lon', 'other_needed_columns'])\n\n# Perform the merges using Dask DataFrames with all merge keys specified at once\nmerged_data = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(merged_data, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(merged_data, terrain, on=['lat', 'lon'], how='inner')\n\n# Convert columns to categorical data type if appropriate\nmerged_data['categorical_column'] = merged_data['categorical_column'].astype('category')\n\n# Persist intermediate result to disk if enough disk space is available\nmerged_data = merged_data.persist()\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 136, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 761, in read\n    return read_pandas(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 630, in read_pandas\n    head = reader(BytesIO(b_sample), nrows=sample_rows, **head_kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 211, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 950, in read_csv\n    return _read(filepath_or_buffer, kwds)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 605, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1442, in __init__\n    self._engine = self._make_engine(f, self.engine)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1753, in _make_engine\n    return mapping[engine](f, **self.options)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 135, in __init__\n    self._validate_usecols_names(usecols, self.orig_names)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\", line 917, in _validate_usecols_names\n    raise ValueError(\nValueError: Usecols do not match columns, columns expected but not found: ['other_needed_columns']\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/DqyyOBpyf5mO/merge_custom_traning_range.py\", line 6, in <module>\n    snotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=blocksize, usecols=usecols)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 138, in wrapper\n    raise type(e)(\nValueError: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: Usecols do not match columns, columns expected but not found: ['other_needed_columns']\n",
  "history_begin_time" : 1691361755680,
  "history_end_time" : 1691361763742,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "t1UwS2Ci5Ki6",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask with a smaller blocksize\nblocksize = \"32MB\"\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=blocksize)\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', blocksize=blocksize)\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', blocksize=blocksize)\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv', blocksize=blocksize)\n\n# Perform the merges using Dask DataFrames\nsnotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nsnotel_gridmet_amsr = dd.merge(snotel_gridmet, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Persist intermediate result to disk if enough disk space is available\nmerged_data = merged_data.persist()\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/t1UwS2Ci5Ki6/merge_custom_traning_range.py\", line 16, in <module>\n    merged_data = merged_data.persist()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/threaded.py\", line 89, in get\n    results = get_async(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 511, in get_async\n    raise_exception(exc, tb)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 319, in reraise\n    raise exc\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 224, in execute_task\n    result = _execute_task(task, data)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 289, in merge_chunk\n    out = lhs.merge(rhs, *args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/array_algos/take.py\", line 158, in _take_nd_ndarray\n    out = np.empty(out_shape, dtype=dtype)\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 2.73 GiB for an array with shape (6, 60966423) and data type float64\n",
  "history_begin_time" : 1691361471841,
  "history_end_time" : 1691361560749,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "T3BRtlIQvn4G",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask with a smaller blocksize\nblocksize = \"32MB\"\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=blocksize)\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', blocksize=blocksize)\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', blocksize=blocksize)\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv', blocksize=blocksize)\n\n# Perform the merges using Dask DataFrames\nsnotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nsnotel_gridmet_amsr = dd.merge(snotel_gridmet, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Persist intermediate result to disk if enough disk space is available\nmerged_data = merged_data.persist()\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False)\n",
  "history_output" : null,
  "history_begin_time" : 1691361180061,
  "history_end_time" : null,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : null
},{
  "history_id" : "kclAXhqALlUm",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask with a smaller blocksize\nblocksize = \"32MB\"\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon'])\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', blocksize=blocksize)\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', blocksize=blocksize)\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv', blocksize=blocksize)\n\n# Perform the merges using Dask DataFrames\nsnotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nsnotel_gridmet_amsr = dd.merge(snotel_gridmet, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Persist intermediate result to disk if enough disk space is available\nmerged_data = merged_data.persist()\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False)\n",
  "history_output" : "",
  "history_begin_time" : 1691361149208,
  "history_end_time" : 1691361180069,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "6ND27QqJwXjh",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask with a smaller blocksize\nblocksize = \"64MB\"\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon'])\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', blocksize=blocksize)\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', blocksize=blocksize)\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv', blocksize=blocksize)\n\n# Perform the merges using Dask DataFrames\nsnotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nsnotel_gridmet_amsr = dd.merge(snotel_gridmet, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Persist intermediate result to disk if enough disk space is available\nmerged_data = merged_data.persist()\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/6ND27QqJwXjh/merge_custom_traning_range.py\", line 16, in <module>\n    merged_data = merged_data.persist()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/threaded.py\", line 89, in get\n    results = get_async(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 511, in get_async\n    raise_exception(exc, tb)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 319, in reraise\n    raise exc\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 224, in execute_task\n    result = _execute_task(task, data)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 289, in merge_chunk\n    out = lhs.merge(rhs, *args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/concat.py\", line 212, in concatenate_managers\n    values = values.copy()\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 1.81 GiB for an array with shape (2, 121756443) and data type int64\nsh: line 1:  6882 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python merge_custom_traning_range.py\n",
  "history_begin_time" : 1691360928524,
  "history_end_time" : 1691361081045,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "veeDuOJivucf",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask with a smaller blocksize\nblocksize = \"32MB\"\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon'])\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon'])\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon'])\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv', blocksize=blocksize, usecols=['lat', 'lon'])\n\n# Perform the merges using Dask DataFrames\nsnotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nsnotel_gridmet_amsr = dd.merge(snotel_gridmet, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Persist intermediate result to disk if enough disk space is available\nmerged_data = merged_data.persist()\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False)\n",
  "history_output" : "",
  "history_begin_time" : 1691358253989,
  "history_end_time" : 1691358870989,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "LVYMCzXna8kY",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask with a smaller blocksize\nblocksize = \"32MB\"\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon'])\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon'])\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon'])\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv', blocksize=blocksize, usecols=['lat', 'lon'])\n\n# Perform the merges using Dask DataFrames\nsnotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nsnotel_gridmet_amsr = dd.merge(snotel_gridmet, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Persist intermediate result to disk if enough disk space is available\nmerged_data = merged_data.persist()\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False, single_file=True)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/LVYMCzXna8kY/merge_custom_traning_range.py\", line 19, in <module>\n    merged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False, single_file=True)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 211, in wrapper\n    return func(*args, **kwargs)\nTypeError: to_csv() got an unexpected keyword argument 'single_file'\n",
  "history_begin_time" : 1691358150598,
  "history_end_time" : 1691358182988,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "foLq4AgG1y0v",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask with a smaller blocksize\nblocksize = \"32MB\"\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon', 'other_needed_columns'])\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon', 'other_needed_columns'])\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon', 'other_needed_columns'])\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv', blocksize=blocksize, usecols=['lat', 'lon', 'other_needed_columns'])\n\n# Perform the merges using Dask DataFrames\nsnotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nsnotel_gridmet_amsr = dd.merge(snotel_gridmet, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Persist intermediate result to disk if enough disk space is available\nmerged_data = merged_data.persist()\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False, single_file=True)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 136, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 761, in read\n    return read_pandas(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 630, in read_pandas\n    head = reader(BytesIO(b_sample), nrows=sample_rows, **head_kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 211, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 950, in read_csv\n    return _read(filepath_or_buffer, kwds)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 605, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1442, in __init__\n    self._engine = self._make_engine(f, self.engine)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1753, in _make_engine\n    return mapping[engine](f, **self.options)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 135, in __init__\n    self._validate_usecols_names(usecols, self.orig_names)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\", line 917, in _validate_usecols_names\n    raise ValueError(\nValueError: Usecols do not match columns, columns expected but not found: ['other_needed_columns']\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/foLq4AgG1y0v/merge_custom_traning_range.py\", line 5, in <module>\n    snotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon', 'other_needed_columns'])\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 138, in wrapper\n    raise type(e)(\nValueError: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: Usecols do not match columns, columns expected but not found: ['other_needed_columns']\n",
  "history_begin_time" : 1691358090119,
  "history_end_time" : 1691358099746,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "oD7sBowsZQqg",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask instead of Pandas\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=\"64MB\")\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', blocksize=\"64MB\")\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', blocksize=\"64MB\")\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv', blocksize=\"64MB\")\n\n# Perform the merges using Dask DataFrames\nsnotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nsnotel_gridmet_amsr = dd.merge(snotel_gridmet, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False, single_file=True)\n\n# Now you have the merged data as a Pandas DataFrame\n",
  "history_output" : "sh: line 1:  2881 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python merge_custom_traning_range.py\n",
  "history_begin_time" : 1691357792430,
  "history_end_time" : 1691357933521,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "9QAjnQlXjRMK",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask instead of Pandas\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=\"64MB\")\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', blocksize=\"64MB\")\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', blocksize=\"64MB\")\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv', blocksize=\"64MB\")\n\n# Perform the merges using Dask DataFrames\nsnotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nsnotel_gridmet_amsr = dd.merge(snotel_gridmet, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False, single_file=True)\n\n# Now you have the merged data as a Pandas DataFrame\n",
  "history_output" : "sh: line 1:  1305 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python merge_custom_traning_range.py\n",
  "history_begin_time" : 1691355130245,
  "history_end_time" : 1691355272757,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "tDoXLKtqLseH",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask instead of Pandas\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv')\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv')\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv')\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv')\n\n# Perform the merges using Dask DataFrames\nsnotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nsnotel_gridmet_amsr = dd.merge(snotel_gridmet, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False, single_file=True)\n\n# Now you have the merged data as a Pandas DataFrame\n",
  "history_output" : "sh: line 1:   871 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python merge_custom_traning_range.py\n",
  "history_begin_time" : 1691354746353,
  "history_end_time" : 1691355073155,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},]
