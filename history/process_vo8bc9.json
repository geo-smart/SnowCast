[{
  "history_id" : "lqkgqnumn7c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704644803723,
  "history_end_time" : 1704644803723,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "frojp4kasah",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704600008214,
  "history_end_time" : 1704600008214,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "x7374s1v7cm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704566156628,
  "history_end_time" : 1704566156628,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "yk0jupak783",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704565587385,
  "history_end_time" : 1704565587385,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "slue1kpp3cm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704564424162,
  "history_end_time" : 1704564424162,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "t1goigwdnfy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704562992181,
  "history_end_time" : 1704562992181,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "7bc09uziw6b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704561889800,
  "history_end_time" : 1704561889800,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6238wzaw6o0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704561861173,
  "history_end_time" : 1704561887040,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "loi3yebdo3k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704555479210,
  "history_end_time" : 1704555479210,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ct682u60jam",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704555028201,
  "history_end_time" : 1704555028201,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "nllgq0s8xcc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704553241772,
  "history_end_time" : 1704553241772,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "y5h5iq7emjk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704552254630,
  "history_end_time" : 1704552254630,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "289xto9xn19",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704513607408,
  "history_end_time" : 1704513607408,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "r4pktnt9ecr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704427207560,
  "history_end_time" : 1704427207560,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "en3076bd2cw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704340807587,
  "history_end_time" : 1704340807587,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "np984p68jtd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704330109314,
  "history_end_time" : 1704330109314,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "aeeo6pra1qh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704329364855,
  "history_end_time" : 1704329364855,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "kbw5lban09p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704254407554,
  "history_end_time" : 1704254407554,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rm9lhkn5mzs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704208947961,
  "history_end_time" : 1704208947961,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "80hatqy7tn3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704207352022,
  "history_end_time" : 1704207352022,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "55t0r7i5i22",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704205859376,
  "history_end_time" : 1704205859376,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "06ot6t973ru",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704168007433,
  "history_end_time" : 1704168007433,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "aoc60ua44z7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704081607536,
  "history_end_time" : 1704081607536,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "1gpz2ym1k66",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703995208426,
  "history_end_time" : 1703995208426,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "jfb1owt77zo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703962871408,
  "history_end_time" : 1703962871408,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "x2b251kao1b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703960265447,
  "history_end_time" : 1703960265447,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "up17v1uv5gd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703959737848,
  "history_end_time" : 1703959737848,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "gy6xazsx9fh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703958611592,
  "history_end_time" : 1703958611592,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ynrsladzn7a",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703955838229,
  "history_end_time" : 1703955838229,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "crf1x7revef",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703954150375,
  "history_end_time" : 1703954150375,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "y06qvi5iv17",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703915768075,
  "history_end_time" : 1703915768075,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8z093yk62nb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703915283490,
  "history_end_time" : 1703915283490,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "f8onhmvgbgw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703914476645,
  "history_end_time" : 1703914476645,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "4fod3rhyecm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703912302178,
  "history_end_time" : 1703912302178,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xwjm36vi6x0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703908807169,
  "history_end_time" : 1703908807169,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "yf3ydz0ynev",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703906215383,
  "history_end_time" : 1703906215383,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "sbyqxtxds5x",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703900919147,
  "history_end_time" : 1703900919147,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "2onyyapk0ub",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703899837770,
  "history_end_time" : 1703899837770,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "q5tvikztc40",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703897422954,
  "history_end_time" : 1703897422954,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "7r5tt17pdba",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703896125586,
  "history_end_time" : 1703896125586,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "n7uzrajxjct",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703890275994,
  "history_end_time" : 1703890275994,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tjq98fufnsg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703886800812,
  "history_end_time" : 1703886800812,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "cwuuctt7cj0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703885997769,
  "history_end_time" : 1703885997769,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "horkxpraonr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703880194719,
  "history_end_time" : 1703880194719,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "m5uowgvfxyu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703872753037,
  "history_end_time" : 1703872753037,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "a5ywh0pu31l",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703869828239,
  "history_end_time" : 1703869828239,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3iu29mfen64",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703868616934,
  "history_end_time" : 1703868616934,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "79ugaa74u40",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703867114050,
  "history_end_time" : 1703867114050,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wklv0lft03s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703864885449,
  "history_end_time" : 1703864885449,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "jgqdoshuj1e",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703862637367,
  "history_end_time" : 1703862637367,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "dd0f7abdbnl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703827227344,
  "history_end_time" : 1703827227344,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9ebjsdzfo64",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703822411485,
  "history_end_time" : 1703822411485,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "34xpx49h5q0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703786924637,
  "history_end_time" : 1703789718828,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "j71lrl215lk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703786053491,
  "history_end_time" : 1703786917617,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ov1f7izs8jq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703778395397,
  "history_end_time" : 1703778395397,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6f9vioto37m",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703739034493,
  "history_end_time" : 1703739034493,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9kk4t2zhrdf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703738754541,
  "history_end_time" : 1703792459278,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "25d3gevd8be",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703736166911,
  "history_end_time" : 1703737316888,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "vbce4menzrn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703694763575,
  "history_end_time" : 1703694763575,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "dwoi54jo5re",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703659541204,
  "history_end_time" : 1703659541204,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3bkmff50ona",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703658144702,
  "history_end_time" : 1703658144702,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ch774akz1bv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703650855776,
  "history_end_time" : 1703650855776,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ne8q7gjgnqs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703646751538,
  "history_end_time" : 1703650812446,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "twp359ky72s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703642120898,
  "history_end_time" : 1703646749628,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "4fblfjlvdzu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703641988944,
  "history_end_time" : 1703642074635,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "j00snf16pih",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703629665510,
  "history_end_time" : 1703629665510,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vg7oh4t2qvz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703626687988,
  "history_end_time" : 1703627783059,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "wn3yzgfs14n",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703625782101,
  "history_end_time" : 1703625782101,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6381gezthcy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703624783983,
  "history_end_time" : 1703624783983,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "YC6XAbFnfsdS",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir, homedir\nimport pandas as pd\n\nworking_dir = work_dir\nfinal_output_name = \"final_merged_data_3yrs_all_active_stations_v1.csv\"\nchunk_size = '10MB'  # You can adjust this chunk size based on your hardware and data size\n\n\ndef merge_all_data_together():\n    amsr_file = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_amsr_dask.csv'\n    snotel_file = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_swe_restored_dask_all_vars.csv'\n    gridmet_file = f'{working_dir}/training_all_active_snotel_station_list_elevation.csv_gridmet.csv'\n    terrain_file = f'{working_dir}/training_all_active_snotel_station_list_elevation.csv_terrain_4km_grid_shift.csv'\n    fsca_file = f'{homedir}/fsca/fsca_final_training_all.csv'\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    \n    if os.path.exists(final_final_output_file):\n      print(f\"The file '{final_final_output_file}' exists. Skipping\")\n      return final_final_output_file\n      \n    \n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(amsr_file, blocksize=chunk_size)\n    print(\"amsr.columns = \", amsr.columns)\n    snotel = dd.read_csv(snotel_file, blocksize=chunk_size)\n    print(\"snotel.columns = \", snotel.columns)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(gridmet_file, blocksize=chunk_size)\n    gridmet = gridmet.drop(columns=[\"Unnamed: 0\"])\n    print(\"gridmet.columns = \", gridmet.columns)\n    terrain = dd.read_csv(terrain_file, blocksize=chunk_size)\n    terrain = terrain.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    terrain = terrain[[\"stationTriplet\", \"elevation\", \"lat\", \"lon\", 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness']]\n    print(\"terrain.columns = \", terrain.columns)\n    snowcover = dd.read_csv(fsca_file, blocksize=chunk_size)\n    snowcover = snowcover.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    print(\"snowcover.columns = \", snowcover.columns)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n    print(\"all the dataframes are partitioned\")\n\n    # Merge DataFrames based on specified columns\n    print(\"start to merge amsr and snotel\")\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snotel.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge gridmet\")\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_gridmet.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge terrain\")\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge snowcover\")\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f'Merge completed. {output_file}')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/{final_output_name}')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/{final_output_name}', single_file=True, index=False)\n    print('Data cleaning completed.')\n    return final_final_output_file\n\n  \ndef sort_training_data(input_training_csv, sorted_training_csv):\n    # Read DataFrame from CSV\n    df = pd.read_csv(input_training_csv)\n\n    # Sort DataFrame by three columns: date, lat, and Lon\n    sorted_df = df.sort_values(by=['date', 'lat', 'lon'])\n\n    # Save the sorted DataFrame to a new CSV file\n    sorted_df.to_csv(sorted_training_csv, index=False)\n\n    print(f\"The DataFrame has been sorted and saved to '{sorted_training_csv}'.\")\n  \nif __name__ == \"__main__\":\n    merge_all_data_together()\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    sort_training_data(final_final_output_file, f'{work_dir}/{final_output_name}_sorted.csv')",
  "history_output" : "today date = 2023-12-26\ntest start date:  2023-01-20\ntest end date:  2023-10-11\n/home/chetana\n2022275\nThe file '/home/chetana/gridmet_test_run/final_merged_data_3yrs_all_active_stations_v1.csv' exists. Skipping\nThe DataFrame has been sorted and saved to '/home/chetana/gridmet_test_run/final_merged_data_3yrs_all_active_stations_v1.csv_sorted.csv'.\n",
  "history_begin_time" : 1703614879302,
  "history_end_time" : 1703615003440,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "f6lB0jaxj08u",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir, homedir\nimport pandas as pd\n\nworking_dir = work_dir\nfinal_output_name = \"final_merged_data_3yrs_all_active_stations_v1.csv\"\nchunk_size = '10MB'  # You can adjust this chunk size based on your hardware and data size\n\n\ndef merge_all_data_together():\n    amsr_file = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_amsr_dask.csv'\n    snotel_file = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_swe_restored_dask_all_vars.csv'\n    gridmet_file = f'{working_dir}/training_all_active_snotel_station_list_elevation.csv_gridmet.csv'\n    terrain_file = f'{working_dir}/training_all_active_snotel_station_list_elevation.csv_terrain_4km_grid_shift.csv'\n    fsca_file = f'{homedir}/fsca/fsca_final_training_all.csv'\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    \n    if os.path.exists(final_final_output_file):\n      print(f\"The file '{final_final_output_file}' exists. Skipping\")\n      return final_final_output_file\n      \n    \n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(amsr_file, blocksize=chunk_size)\n    print(\"amsr.columns = \", amsr.columns)\n    snotel = dd.read_csv(snotel_file, blocksize=chunk_size)\n    print(\"snotel.columns = \", snotel.columns)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(gridmet_file, blocksize=chunk_size)\n    gridmet = gridmet.drop(columns=[\"Unnamed: 0\"])\n    print(\"gridmet.columns = \", gridmet.columns)\n    terrain = dd.read_csv(terrain_file, blocksize=chunk_size)\n    terrain = terrain.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    terrain = terrain[[\"stationTriplet\", \"elevation\", \"lat\", \"lon\", 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness']]\n    print(\"terrain.columns = \", terrain.columns)\n    snowcover = dd.read_csv(fsca_file, blocksize=chunk_size)\n    snowcover = snowcover.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    print(\"snowcover.columns = \", snowcover.columns)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n    print(\"all the dataframes are partitioned\")\n\n    # Merge DataFrames based on specified columns\n    print(\"start to merge amsr and snotel\")\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snotel.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge gridmet\")\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_gridmet.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge terrain\")\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge snowcover\")\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f'Merge completed. {output_file}')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/{final_output_name}')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/{final_output_name}', single_file=True, index=False)\n    print('Data cleaning completed.')\n    return final_final_output_file\n\n  \ndef sort_training_data(input_training_csv, sorted_training_csv):\n    # Read DataFrame from CSV\n    df = pd.read_csv(input_training_csv)\n\n    # Sort DataFrame by three columns: date, lat, and Lon\n    sorted_df = df.sort_values(by=['date', 'lat', 'Lon'])\n\n    # Save the sorted DataFrame to a new CSV file\n    sorted_df.to_csv(sorted_training_csv, index=False)\n\n    print(f\"The DataFrame has been sorted and saved to '{sorted_training_csv}'.\")\n  \nif __name__ == \"__main__\":\n    merge_all_data_together()\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    sort_training_data(final_final_output_file, f'{work_dir}/{final_output_name}_sorted.csv')",
  "history_output" : "today date = 2023-12-26\ntest start date:  2023-01-20\ntest end date:  2023-10-11\n/home/chetana\n2022275\nThe file '/home/chetana/gridmet_test_run/final_merged_data_3yrs_all_active_stations_v1.csv' exists. Skipping\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/f6lB0jaxj08u/merge_custom_traning_range.py\", line 129, in <module>\n    sort_training_data(final_final_output_file, f'{work_dir}/{final_output_name}_sorted.csv')\n  File \"/home/chetana/gw-workspace/f6lB0jaxj08u/merge_custom_traning_range.py\", line 119, in sort_training_data\n    sorted_df = df.sort_values(by=['date', 'lat', 'Lon'])\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\", line 6894, in sort_values\n    keys = [self._get_label_or_level_values(x, axis=axis) for x in by]\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\", line 6894, in <listcomp>\n    keys = [self._get_label_or_level_values(x, axis=axis) for x in by]\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\", line 1850, in _get_label_or_level_values\n    raise KeyError(key)\nKeyError: 'Lon'\n",
  "history_begin_time" : 1703614830754,
  "history_end_time" : 1703614849546,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "rZf1YVlz4hm5",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir, homedir\n\nworking_dir = work_dir\nfinal_output_name = \"final_merged_data_3yrs_all_active_stations_v1.csv\"\nchunk_size = '10MB'  # You can adjust this chunk size based on your hardware and data size\n\n\ndef merge_all_data_together():\n    amsr_file = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_amsr_dask.csv'\n    snotel_file = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_swe_restored_dask_all_vars.csv'\n    gridmet_file = f'{working_dir}/training_all_active_snotel_station_list_elevation.csv_gridmet.csv'\n    terrain_file = f'{working_dir}/training_all_active_snotel_station_list_elevation.csv_terrain_4km_grid_shift.csv'\n    fsca_file = f'{homedir}/fsca/fsca_final_training_all.csv'\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    \n    if os.path.exists(final_final_output_file):\n      print(f\"The file '{final_final_output_file}' exists. Skipping\")\n      return final_final_output_file\n      \n    \n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(amsr_file, blocksize=chunk_size)\n    print(\"amsr.columns = \", amsr.columns)\n    snotel = dd.read_csv(snotel_file, blocksize=chunk_size)\n    print(\"snotel.columns = \", snotel.columns)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(gridmet_file, blocksize=chunk_size)\n    gridmet = gridmet.drop(columns=[\"Unnamed: 0\"])\n    print(\"gridmet.columns = \", gridmet.columns)\n    terrain = dd.read_csv(terrain_file, blocksize=chunk_size)\n    terrain = terrain.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    terrain = terrain[[\"stationTriplet\", \"elevation\", \"lat\", \"lon\", 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness']]\n    print(\"terrain.columns = \", terrain.columns)\n    snowcover = dd.read_csv(fsca_file, blocksize=chunk_size)\n    snowcover = snowcover.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    print(\"snowcover.columns = \", snowcover.columns)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n    print(\"all the dataframes are partitioned\")\n\n    # Merge DataFrames based on specified columns\n    print(\"start to merge amsr and snotel\")\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snotel.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge gridmet\")\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_gridmet.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge terrain\")\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge snowcover\")\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f'Merge completed. {output_file}')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/{final_output_name}')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/{final_output_name}', single_file=True, index=False)\n    print('Data cleaning completed.')\n    return final_final_output_file\n\n  \ndef sort_training_data(input_training_csv, sorted_training_csv):\n    # Read DataFrame from CSV\n    df = pd.read_csv(input_training_csv)\n\n    # Sort DataFrame by three columns: date, lat, and Lon\n    sorted_df = df.sort_values(by=['date', 'lat', 'Lon'])\n\n    # Save the sorted DataFrame to a new CSV file\n    sorted_df.to_csv(sorted_training_csv, index=False)\n\n    print(f\"The DataFrame has been sorted and saved to '{sorted_training_csv}'.\")\n  \nif __name__ == \"__main__\":\n    merge_all_data_together()\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    sort_training_data(final_final_output_file, f'{work_dir}/{final_output_name}_sorted.csv')",
  "history_output" : "today date = 2023-12-26\ntest start date:  2023-01-20\ntest end date:  2023-10-11\n/home/chetana\n2022275\nThe file '/home/chetana/gridmet_test_run/final_merged_data_3yrs_all_active_stations_v1.csv' exists. Skipping\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/rZf1YVlz4hm5/merge_custom_traning_range.py\", line 128, in <module>\n    sort_training_data(final_final_output_file, f'{work_dir}/{final_output_name}_sorted.csv')\n  File \"/home/chetana/gw-workspace/rZf1YVlz4hm5/merge_custom_traning_range.py\", line 115, in sort_training_data\n    df = pd.read_csv(input_training_csv)\nNameError: name 'pd' is not defined\n",
  "history_begin_time" : 1703614682505,
  "history_end_time" : 1703614686081,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "oo0E4hZ7HU9w",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir, homedir\n\nworking_dir = work_dir\nfinal_output_name = \"final_merged_data_3yrs_all_active_stations_v1.csv\"\nchunk_size = '10MB'  # You can adjust this chunk size based on your hardware and data size\n\n\ndef main():\n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_amsr_dask.csv', blocksize=chunk_size)\n    print(\"amsr.columns = \", amsr.columns)\n    snotel = dd.read_csv(f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_swe_restored_dask_all_vars.csv', blocksize=chunk_size)\n    print(\"snotel.columns = \", snotel.columns)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(f'{working_dir}/training_all_active_snotel_station_list_elevation.csv_gridmet.csv', blocksize=chunk_size)\n    gridmet = gridmet.drop(columns=[\"Unnamed: 0\"])\n    print(\"gridmet.columns = \", gridmet.columns)\n    terrain = dd.read_csv(f'{working_dir}/training_all_active_snotel_station_list_elevation.csv_terrain_4km_grid_shift.csv', blocksize=chunk_size)\n    terrain = terrain.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    terrain = terrain[[\"stationTriplet\", \"elevation\", \"lat\", \"lon\", 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness']]\n    print(\"terrain.columns = \", terrain.columns)\n    snowcover = dd.read_csv(f'{homedir}/fsca/fsca_final_training_all.csv', blocksize=chunk_size)\n    snowcover = snowcover.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    print(\"snowcover.columns = \", snowcover.columns)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n    print(\"all the dataframes are partitioned\")\n\n    # Merge DataFrames based on specified columns\n    print(\"start to merge amsr and snotel\")\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snotel.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge gridmet\")\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_gridmet.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge terrain\")\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge snowcover\")\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f'Merge completed. {output_file}')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/{final_output_name}')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/{final_output_name}', single_file=True, index=False)\n    print('Data cleaning completed.')\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "today date = 2023-12-26\ntest start date:  2023-01-20\ntest end date:  2023-10-11\n/home/chetana\n2022275\namsr.columns =  Index(['date', 'lat', 'lon', 'AMSR_SWE'], dtype='object')\nsnotel.columns =  Index(['station_name', 'date', 'lat', 'lon', 'swe_value', 'change_in_swe_inch',\n       'snow_depth', 'air_temperature_observed_f'],\n      dtype='object')\ngridmet.columns =  Index(['day', 'lat', 'lon', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed'],\n      dtype='object')\nterrain.columns =  Index(['stationTriplet', 'elevation', 'lat', 'lon', 'Elevation', 'Slope',\n       'Aspect', 'Curvature', 'Northness', 'Eastness'],\n      dtype='object')\nsnowcover.columns =  Index(['date', 'lat', 'lon', 'fsca'], dtype='object')\nall the dataframes are partitioned\nstart to merge amsr and snotel\nintermediate file saved to /home/chetana/gridmet_test_run/final_merged_data_3yrs_all_active_stations_v1.csv_snotel.csv\nstart to merge gridmet\nintermediate file saved to /home/chetana/gridmet_test_run/final_merged_data_3yrs_all_active_stations_v1.csv_gridmet.csv\nstart to merge terrain\nintermediate file saved to /home/chetana/gridmet_test_run/final_merged_data_3yrs_all_active_stations_v1.csv_terrain.csv\nstart to merge snowcover\nintermediate file saved to /home/chetana/gridmet_test_run/final_merged_data_3yrs_all_active_stations_v1.csv_snow_cover.csv\nMerge completed. /home/chetana/gridmet_test_run/final_merged_data_3yrs_all_active_stations_v1.csv\nData cleaning completed.\n",
  "history_begin_time" : 1703613310239,
  "history_end_time" : 1703614725228,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "u0qppgiNhrXW",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir, homedir\n\nworking_dir = work_dir\nfinal_output_name = \"final_merged_data_3yrs_all_active_stations_v1.csv\"\nchunk_size = '32MB'  # You can adjust this chunk size based on your hardware and data size\n\ndef main():\n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_amsr_dask.csv', blocksize=chunk_size)\n    print(\"amsr.columns = \", amsr.columns)\n    snotel = dd.read_csv(f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_swe_restored_dask_all_vars.csv', blocksize=chunk_size)\n    print(\"snotel.columns = \", snotel.columns)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(f'{working_dir}/training_all_active_snotel_station_list_elevation.csv_gridmet.csv', blocksize=chunk_size)\n    gridmet = gridmet.drop(columns=[\"Unnamed: 0\"])\n    print(\"gridmet.columns = \", gridmet.columns)\n    terrain = dd.read_csv(f'{working_dir}/training_all_active_snotel_station_list_elevation.csv_terrain_4km_grid_shift.csv', blocksize=chunk_size)\n    terrain = terrain.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    terrain = terrain[[\"stationTriplet\", \"elevation\", \"lat\", \"lon\", 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness']]\n    print(\"terrain.columns = \", terrain.columns)\n    snowcover = dd.read_csv(f'{homedir}/fsca/fsca_final_training_all.csv', blocksize=chunk_size)\n    snowcover = snowcover.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    print(\"snowcover.columns = \", snowcover.columns)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n    print(\"all the dataframes are partitioned\")\n\n    # Merge DataFrames based on specified columns\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snotel.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    \n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_gridmet.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    \n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    \n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n\n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print('Merge completed.')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/{final_output_name}')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/{final_output_name}', single_file=True, index=False)\n    print('Data cleaning completed.')\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "today date = 2023-12-26\ntest start date:  2023-01-20\ntest end date:  2023-10-11\n/home/chetana\n2022275\namsr.columns =  Index(['date', 'lat', 'lon', 'AMSR_SWE'], dtype='object')\nsnotel.columns =  Index(['station_name', 'date', 'lat', 'lon', 'swe_value', 'change_in_swe_inch',\n       'snow_depth', 'air_temperature_observed_f'],\n      dtype='object')\ngridmet.columns =  Index(['day', 'lat', 'lon', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed'],\n      dtype='object')\nterrain.columns =  Index(['stationTriplet', 'elevation', 'lat', 'lon', 'Elevation', 'Slope',\n       'Aspect', 'Curvature', 'Northness', 'Eastness'],\n      dtype='object')\nsnowcover.columns =  Index(['date', 'lat', 'lon', 'fsca'], dtype='object')\nall the dataframes are partitioned\nMerge completed.\nData cleaning completed.\n",
  "history_begin_time" : 1703612496090,
  "history_end_time" : 1703613844010,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "PqSZFJ6oocqC",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir, homedir\n\nworking_dir = work_dir\nfinal_output_name = \"final_merged_data_3yrs_all_active_stations_v1.csv\"\nchunk_size = '32MB'  # You can adjust this chunk size based on your hardware and data size\n\ndef main():\n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_amsr_dask.csv', blocksize=chunk_size)\n    print(\"amsr.columns = \", amsr.columns)\n    snotel = dd.read_csv(f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_swe_restored_dask_all_vars.csv', blocksize=chunk_size)\n    print(\"snotel.columns = \", snotel.columns)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(f'{working_dir}/training_all_active_snotel_station_list_elevation.csv_gridmet.csv', blocksize=chunk_size)\n    print(\"gridmet.columns = \", gridmet.columns)\n    terrain = dd.read_csv(f'{working_dir}/training_all_active_snotel_station_list_elevation.csv_terrain_4km_grid_shift.csv', blocksize=chunk_size)\n    print(\"terrain.columns = \", terrain.columns)\n    snowcover = dd.read_csv(f'{homedir}/fsca/fsca_final_training_all.csv', blocksize=chunk_size)\n    print(\"snowcover.columns = \", snowcover.columns)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n    print(\"all the dataframes are partitioned\")\n\n    # Merge DataFrames based on specified columns\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snotel.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    \n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_gridmet.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    \n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    \n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n\n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print('Merge completed.')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/{final_output_name}')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/{final_output_name}', single_file=True, index=False)\n    print('Data cleaning completed.')\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "today date = 2023-12-26\ntest start date:  2023-01-20\ntest end date:  2023-10-11\n/home/chetana\n2022275\namsr.columns =  Index(['date', 'lat', 'lon', 'AMSR_SWE'], dtype='object')\nsnotel.columns =  Index(['station_name', 'date', 'lat', 'lon', 'swe_value', 'change_in_swe_inch',\n       'snow_depth', 'air_temperature_observed_f'],\n      dtype='object')\ngridmet.columns =  Index(['Unnamed: 0', 'day', 'lat', 'lon', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed'],\n      dtype='object')\nterrain.columns =  Index(['stationTriplet', 'stationId', 'stateCode', 'networkCode', 'name',\n       'dcoCode', 'countyName', 'huc', 'elevation', 'latitude', 'longitude',\n       'dataTimeZone', 'pedonCode', 'shefId', 'beginDate', 'endDate',\n       'Latitude', 'Longitude', 'x', 'y', 'Elevation', 'Slope', 'Aspect',\n       'Curvature', 'Northness', 'Eastness'],\n      dtype='object')\nsnowcover.columns =  Index(['date', 'latitude', 'longitude', 'fsca'], dtype='object')\nall the dataframes are partitioned\n",
  "history_begin_time" : 1703612033164,
  "history_end_time" : 1703612085001,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "ksnjcb0ccz9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702875592841,
  "history_end_time" : 1702875592841,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "lksg3jvvj1d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702871264379,
  "history_end_time" : 1702871264379,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wvygxz1u5li",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702867996396,
  "history_end_time" : 1702867996396,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "eei96z8op2s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702866593394,
  "history_end_time" : 1702866593394,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "dr5dqt66aaw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702866137629,
  "history_end_time" : 1702866137629,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "l5vy58rapzo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702657305630,
  "history_end_time" : 1702657305630,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "b25gcyikblm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702633223049,
  "history_end_time" : 1702633223049,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "b88935ri4ft",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702633156927,
  "history_end_time" : 1702633163903,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "u5rd6rawidg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702274520887,
  "history_end_time" : 1702274520887,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "l5ulhwq89do",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702257109204,
  "history_end_time" : 1702257109204,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "icg8xf4mnfq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702253506525,
  "history_end_time" : 1702253506525,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "iwpv8lvxlys",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702047800939,
  "history_end_time" : 1702047800939,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ngqi3lx86mp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702046671869,
  "history_end_time" : 1702047789488,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "6htjojo920o",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701838624056,
  "history_end_time" : 1701838624056,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "62a2zxf1c9u",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701272631481,
  "history_end_time" : 1701272875115,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "9sdjzb1g7p7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701272152705,
  "history_end_time" : 1701272363359,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "yttpq0fswha",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701269761343,
  "history_end_time" : 1701269761343,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "r6nck7fd7t2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701245472022,
  "history_end_time" : 1701245472022,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "mvb156frcb9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701234300621,
  "history_end_time" : 1701234300621,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3kzroiwkb7l",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701232375282,
  "history_end_time" : 1701234158046,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "l3561p5dedf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701231048671,
  "history_end_time" : 1701231048671,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6o1zk3af34o",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933468,
  "history_end_time" : 1701230952352,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "9kugcrrlmr5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230796336,
  "history_end_time" : 1701230932259,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "c21egwm4unn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230384890,
  "history_end_time" : 1701230384890,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "v1pcgenic0c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701229983512,
  "history_end_time" : 1701229983512,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "78jk45cxkkq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701228899375,
  "history_end_time" : 1701228899375,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9nba3nxwzbd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701228374808,
  "history_end_time" : 1701228374808,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6kuli8zvis2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701228236346,
  "history_end_time" : 1701228236346,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "y3y6ngo766a",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701228118511,
  "history_end_time" : 1701228118511,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "jk24k07myoh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701228056533,
  "history_end_time" : 1701228056533,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "b0x6tohno7u",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701227912529,
  "history_end_time" : 1701227912529,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "FODTNKfyRv8C",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir, homedir\n\nworking_dir = work_dir\nfinal_output_name = \"final_merged_data_3yrs_cleaned_v4.csv\"\nchunk_size = '32MB'  # You can adjust this chunk size based on your hardware and data size\n\ndef main():\n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\n    snotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(f'{working_dir}/training_ready_gridmet_3_yrs.csv', blocksize=chunk_size)\n    terrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n    snowcover = dd.read_csv(f'{homedir}/fsca/fsca_final_training_all.csv', blocksize=chunk_size)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n    print(\"all the dataframes are partitioned\")\n\n    # Merge DataFrames based on specified columns\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snotel.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    \n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_gridmet.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    \n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    \n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n\n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print('Merge completed.')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/{final_output_name}')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/{final_output_name}', single_file=True, index=False)\n    print('Data cleaning completed.')\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "today date = 2023-11-28\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\nall the dataframes are partitioned\nMerge completed.\n/home/chetana/anaconda3/lib/python3.9/site-packages/dask/dataframe/io/csv.py:195: DtypeWarning: Columns (6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = reader(bio, **kwargs)\n/home/chetana/anaconda3/lib/python3.9/site-packages/dask/dataframe/io/csv.py:195: DtypeWarning: Columns (6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = reader(bio, **kwargs)\nData cleaning completed.\n",
  "history_begin_time" : 1701184445165,
  "history_end_time" : 1701184673160,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "GWw9jxxLCq3e",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir, homedir\n\nworking_dir = work_dir\nfinal_output_name = \"final_merged_data_3yrs_cleaned_v4.csv\"\nchunk_size = '32MB'  # You can adjust this chunk size based on your hardware and data size\n\ndef main():\n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\n    snotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(f'{working_dir}/training_ready_gridmet_3_yrs.csv', blocksize=chunk_size)\n    terrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n    snowcover = dd.read_csv(f'{homedir}/fsca/fsca_final_training_all.csv', blocksize=chunk_size)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n    print(\"all the dataframes are partitioned\")\n\n    # Merge DataFrames based on specified columns\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snotel.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_gridmet.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n\n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print('Merge completed.')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/{final_output_name}')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/{final_output_name}', single_file=True, index=False)\n    print('Data cleaning completed.')\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "today date = 2023-11-28\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\nall the dataframes are partitioned\n",
  "history_begin_time" : 1701180455970,
  "history_end_time" : 1701184369641,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "KcD56mvVpwKw",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir, homedir\n\nworking_dir = work_dir\nfinal_output_name = \"final_merged_data_3yrs_cleaned_v4.csv\"\nchunk_size = '32MB'  # You can adjust this chunk size based on your hardware and data size\n\ndef main():\n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\n    snotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(f'{working_dir}/training_ready_gridmet_3_yrs.csv', blocksize=chunk_size)\n    terrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n    snowcover = dd.read_csv(f'{homedir}/fsca/fsca_final_training_all.csv', blocksize=chunk_size)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n\n    # Merge DataFrames based on specified columns\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n\n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print('Merge completed.')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/{final_output_name}')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/{final_output_name}', single_file=True, index=False)\n    print('Data cleaning completed.')\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "today date = 2023-11-28\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\n2022275\n",
  "history_begin_time" : 1701151718499,
  "history_end_time" : 1701152576569,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "pa0tf1n65ic",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701013937413,
  "history_end_time" : 1701015920041,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "1bnu31g674j",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700974688727,
  "history_end_time" : 1700974688727,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6r9ssmt52h7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700885116836,
  "history_end_time" : 1700885116836,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "fbqAVkKUvylD",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nhome_dir = '/home/chetana'\nfinal_output_name = \"final_merged_data_3yrs_cleaned_v4.csv\"\nchunk_size = '32MB'  # You can adjust this chunk size based on your hardware and data size\n\ndef main():\n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\n    snotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(f'{working_dir}/training_ready_gridmet_3_yrs.csv', blocksize=chunk_size)\n    terrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n    snowcover = dd.read_csv(f'{home_dir}/fsca/fsca_final.csv', blocksize=chunk_size)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n\n    # Merge DataFrames based on specified columns\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n\n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print('Merge completed.')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/{final_output_name}')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/{final_output_name}', single_file=True, index=False)\n    print('Data cleaning completed.')\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "today date = 2023-11-22\ntest start date:  2022-12-15\ntest end date:  2023-10-07\n/home/chetana\n",
  "history_begin_time" : 1700637965724,
  "history_end_time" : 1700638336302,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "NhXnZh92Eile",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nhome_dir = '/home/chetana'\nchunk_size = '32MB'  # You can adjust this chunk size based on your hardware and data size\n\ndef main():\n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\n    snotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(f'{working_dir}/training_ready_gridmet_3_yrs.csv', blocksize=chunk_size)\n    terrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n    snowcover = dd.read_csv(f'{home_dir}/fsca/fsca_final.csv', blocksize=chunk_size)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n\n    # Merge DataFrames based on specified columns\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n\n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, 'final_merged_data_3_yrs_v3.csv')\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print('Merge completed.')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/final_merged_data_3_yrs_v3.csv')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv', single_file=True, index=False)\n    print('Data cleaning completed.')\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "today date = 2023-11-22\ntest start date:  2022-12-15\ntest end date:  2023-10-07\n/home/chetana\n",
  "history_begin_time" : 1700629094746,
  "history_end_time" : 1700629571008,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "oBqejZaTt5Ak",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nhome_dir = '/home/chetana'\nchunk_size = '32MB'  # You can adjust this chunk size based on your hardware and data size\n\ndef main():\n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\n    snotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(f'{working_dir}/training_ready_gridmet_3_yrs.csv', blocksize=chunk_size)\n    terrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n    snowcover = dd.read_csv(f'{home_dir}/fsca/fsca_final.csv', blocksize=chunk_size)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n\n    # Merge DataFrames based on specified columns\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n\n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, 'final_merged_data_3_yrs_v3.csv')\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print('Merge completed.')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/final_merged_data_3_yrs_v3.csv')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv', single_file=True, index=False)\n    print('Data cleaning completed.')\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "today date = 2023-11-22\ntest start date:  2022-12-15\ntest end date:  2023-10-07\n/home/chetana\n",
  "history_begin_time" : 1700618382213,
  "history_end_time" : 1700619591945,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "emSJutfHjV62",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nhome_dir = '/home/chetana'\nchunk_size = '32MB'  # You can adjust this chunk size based on your hardware and data size\n\ndef main():\n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\n    snotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(f'{working_dir}/training_ready_gridmet_3_yrs.csv', blocksize=chunk_size)\n    terrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n    snowcover = dd.read_csv(f'{home_dir}/fsca/fsca_final.csv', blocksize=chunk_size)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = terrian.repartition(partition_size=chunk_size)\n\n    # Merge DataFrames based on specified columns\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n\n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, 'final_merged_data_3_yrs_v3.csv')\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print('Merge completed.')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/final_merged_data_3_yrs_v3.csv')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv', single_file=True, index=False)\n    print('Data cleaning completed.')\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "today date = 2023-11-22\ntest start date:  2022-12-15\ntest end date:  2023-10-07\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/emSJutfHjV62/merge_custom_traning_range.py\", line 60, in <module>\n    main()\n  File \"/home/chetana/gw-workspace/emSJutfHjV62/merge_custom_traning_range.py\", line 40, in main\n    snow_cover = terrian.repartition(partition_size=chunk_size)\nNameError: name 'terrian' is not defined\n",
  "history_begin_time" : 1700618336386,
  "history_end_time" : 1700618342301,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "8ctVVw1TsoAu",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nhome_dir = '/home/chetana'\nchunk_size = '32MB'  # You can adjust this chunk size based on your hardware and data size\n\ndef main():\n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\n    snotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet_3_yrs.csv', blocksize=chunk_size)\n    terrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n    snowcover = dd.read_csv(f'{home_dir}/fsca/fsca_final.csv', blocksize=chunk_size)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = terrian.repartition(partition_size=chunk_size)\n\n    # Merge DataFrames based on specified columns\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n\n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, 'final_merged_data_3_yrs_v3.csv')\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print('Merge completed.')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/final_merged_data_3_yrs_v3.csv')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv', single_file=True, index=False)\n    print('Data cleaning completed.')\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "today date = 2023-11-22\ntest start date:  2022-12-15\ntest end date:  2023-10-07\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/dask/backends.py\", line 136, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 761, in read\n    return read_pandas(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 561, in read_pandas\n    b_out = read_bytes(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/dask/bytes/core.py\", line 111, in read_bytes\n    size = fs.info(path)[\"size\"]\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/fsspec/implementations/local.py\", line 83, in info\n    out = os.stat(path, follow_symlinks=False)\nFileNotFoundError: [Errno 2] No such file or directory: '/home/chetana/gridmet_test_run/gridmet_climatology/training_ready_gridmet_3_yrs.csv'\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/8ctVVw1TsoAu/merge_custom_traning_range.py\", line 59, in <module>\n    main()\n  File \"/home/chetana/gw-workspace/8ctVVw1TsoAu/merge_custom_traning_range.py\", line 29, in main\n    gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet_3_yrs.csv', blocksize=chunk_size)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/dask/backends.py\", line 138, in wrapper\n    raise type(e)(\nFileNotFoundError: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: [Errno 2] No such file or directory: '/home/chetana/gridmet_test_run/gridmet_climatology/training_ready_gridmet_3_yrs.csv'\n",
  "history_begin_time" : 1700618201117,
  "history_end_time" : 1700618202709,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "KcBUyayk64OQ",
  "history_input" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nhome_dir = '/home/chetana'\nchunk_size = '32MB'  # You can adjust this chunk size based on your hardware and data size\n\ndef main():\n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\n    snotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    terrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n    snowcover = dd.read_csv(f'{home_dir}/fsca/fsca_final.csv', blocksize=chunk_size)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = terrian.repartition(partition_size=chunk_size)\n\n    # Merge DataFrames based on specified columns\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n\n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, 'final_merged_data_3_yrs_v3.csv')\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print('Merge completed.')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/final_merged_data_3_yrs_v3.csv')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv', single_file=True, index=False)\n    print('Data cleaning completed.')\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "today date = 2023-11-22\ntest start date:  2022-12-15\ntest end date:  2023-10-07\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/dask/backends.py\", line 136, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 761, in read\n    return read_pandas(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 561, in read_pandas\n    b_out = read_bytes(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/dask/bytes/core.py\", line 111, in read_bytes\n    size = fs.info(path)[\"size\"]\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/fsspec/implementations/local.py\", line 83, in info\n    out = os.stat(path, follow_symlinks=False)\nFileNotFoundError: [Errno 2] No such file or directory: '/home/chetana/gridmet_test_run/gridmet_climatology/training_ready_gridmet.csv'\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/KcBUyayk64OQ/merge_custom_traning_range.py\", line 59, in <module>\n    main()\n  File \"/home/chetana/gw-workspace/KcBUyayk64OQ/merge_custom_traning_range.py\", line 29, in main\n    gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/dask/backends.py\", line 138, in wrapper\n    raise type(e)(\nFileNotFoundError: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: [Errno 2] No such file or directory: '/home/chetana/gridmet_test_run/gridmet_climatology/training_ready_gridmet.csv'\n",
  "history_begin_time" : 1700617045687,
  "history_end_time" : 1700617053747,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "65jz12hokk2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700471590200,
  "history_end_time" : 1700471590200,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "1v1cg9yx0i0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700468936440,
  "history_end_time" : 1700468936440,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "b5o4ds1ji3y",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700461923019,
  "history_end_time" : 1700462913690,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "2clnpxj0pe1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500144,
  "history_end_time" : 1700448500144,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ms3i1fw6zq1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700447319851,
  "history_end_time" : 1700447319851,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ivxc6f3oj6c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700230067250,
  "history_end_time" : 1700230067250,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "lhwefpq9qz1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700229012374,
  "history_end_time" : 1700229012374,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "zy1gdkz8mkf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700210213817,
  "history_end_time" : 1700210213817,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ssy4j5i0crw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700209780169,
  "history_end_time" : 1700209780169,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8pvfarz515u",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700209729254,
  "history_end_time" : 1700209729254,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "agu5z3qjyqt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700203478655,
  "history_end_time" : 1700204245687,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "6gvftpxxgaz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700201828271,
  "history_end_time" : 1700201828271,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xkufewvp1sj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700200332862,
  "history_end_time" : 1700200332862,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0irm0u579bf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700145667875,
  "history_end_time" : 1700145667875,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "2pc21f99szf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700143295313,
  "history_end_time" : 1700143295313,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "igw3whx1319",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700141615818,
  "history_end_time" : 1700141615818,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "bshy3yfovy7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700134126845,
  "history_end_time" : 1700134126845,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "38853n1w5sw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700133783713,
  "history_end_time" : 1700133783713,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "syrm1wmwm4e",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699992839766,
  "history_end_time" : 1699992839766,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9kbr19oe31w",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699982145458,
  "history_end_time" : 1699982145458,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wztghmuwk9d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699941614808,
  "history_end_time" : 1699941614808,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ctn98cdr360",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699939440568,
  "history_end_time" : 1699939440568,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "s5qpvdefxmc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699937910471,
  "history_end_time" : 1699937910471,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "sfptl7pig86",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699805634665,
  "history_end_time" : 1699806085200,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "pomcks3n3bl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699684154070,
  "history_end_time" : 1699684154070,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "u1r2fwibqvw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699681071364,
  "history_end_time" : 1699681071364,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "pzjkh57bqzg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698762678703,
  "history_end_time" : 1698762678703,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rtpyerpnmy5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698762637984,
  "history_end_time" : 1698762637984,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "4k48xd2xj6e",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698276496964,
  "history_end_time" : 1698276496964,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6vfiabgali2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698252277366,
  "history_end_time" : 1698252277366,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "jah3parcn8j",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698251392470,
  "history_end_time" : 1698251392470,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "gui7ou7xwxs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698228210971,
  "history_end_time" : 1698228210971,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rhr63u6s2yb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698227897143,
  "history_end_time" : 1698227897143,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "j18ko3vw28k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698163737294,
  "history_end_time" : 1698163737294,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "o34imph8ghj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698163445830,
  "history_end_time" : 1698163445830,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9qamp1t5nqk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698163121543,
  "history_end_time" : 1698163121543,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vwsbguih7n7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698160809396,
  "history_end_time" : 1698160809396,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ralpez8araa",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698157805187,
  "history_end_time" : 1698157805187,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "q7rma52fvta",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698152099750,
  "history_end_time" : 1698152099750,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "zuyrgigsi3d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698095495773,
  "history_end_time" : 1698095495773,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "cg58byzeonl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698075453621,
  "history_end_time" : 1698075453621,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ogcw4hr48v5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1697349530013,
  "history_end_time" : 1697349530013,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "si4sxxgcz9s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1697348852379,
  "history_end_time" : 1697348852379,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0nr32xvstea",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1697189923578,
  "history_end_time" : 1697189923578,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tvsx1spr316",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1697188523317,
  "history_end_time" : 1697188523317,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wk59w2jt768",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1697187892287,
  "history_end_time" : 1697187892287,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "1yic2gu10o6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1697187368004,
  "history_end_time" : 1697187368004,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vnpwq1kfjk8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696863953337,
  "history_end_time" : 1696863953337,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "i1p0958o1l5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696862402979,
  "history_end_time" : 1696862402979,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "dh6prk1dop5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696832263693,
  "history_end_time" : 1696832263693,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "v0qbpv2alts",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696831867391,
  "history_end_time" : 1696831867391,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vn24gd4iyms",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696830174415,
  "history_end_time" : 1696830174415,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "d9uecspy6po",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696787541977,
  "history_end_time" : 1696787541977,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0a2b24kbc6o",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696786838259,
  "history_end_time" : 1696786838259,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "zn71eca3uma",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696771780949,
  "history_end_time" : 1696771780949,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "v9u1e7bvk3i",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696602943960,
  "history_end_time" : 1696602943960,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "1peha6o0q3s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696432484353,
  "history_end_time" : 1696432484353,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "5s4ixfhud8d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696432299775,
  "history_end_time" : 1696432482237,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ow293to5dhq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695827991148,
  "history_end_time" : 1695827991148,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vdk4ez78miq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695827889185,
  "history_end_time" : 1695827965229,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "wnu6nkkwgqn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695827855653,
  "history_end_time" : 1695827867013,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "2wzwttw2j2s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695696616126,
  "history_end_time" : 1695696616126,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "s1lq9cro9pi",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695694257336,
  "history_end_time" : 1695694257336,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "txot8wj4sis",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695693585755,
  "history_end_time" : 1695693585755,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xlaobebezdd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695693149412,
  "history_end_time" : 1695693149412,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "crc8wlk55tb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695580915854,
  "history_end_time" : 1695580915854,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "k2oqqeklm9m",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695576291671,
  "history_end_time" : 1695576291671,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ozv6qh0p32d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695575931035,
  "history_end_time" : 1695575931035,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "e7cn6yx1eeu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695535769219,
  "history_end_time" : 1695535769219,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "oe9iu2nxrza",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695535478715,
  "history_end_time" : 1695535478715,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "03ra1dbsh5i",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695535214031,
  "history_end_time" : 1695535214031,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ist8l8u6now",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695534943596,
  "history_end_time" : 1695534943596,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "7uk8vqucmhb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695534671835,
  "history_end_time" : 1695534671835,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "2afzcdhvp7z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695533024174,
  "history_end_time" : 1695533024174,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6gt7ytt0rw5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695529187875,
  "history_end_time" : 1695529187875,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "k4ow5sd3xhv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695528505204,
  "history_end_time" : 1695528505204,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3893n1vyzw0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695515862562,
  "history_end_time" : 1695515862562,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "udqbtyfeb79",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695506423865,
  "history_end_time" : 1695506423865,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "hncl0n1toa5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695418741350,
  "history_end_time" : 1695418741350,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "agxhr5kr2ew",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695417619686,
  "history_end_time" : 1695417619686,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "438vo0i339j",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695417171289,
  "history_end_time" : 1695417171289,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tmba37c1v0q",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695417052739,
  "history_end_time" : 1695417052739,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "39ec0firehu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695416916063,
  "history_end_time" : 1695416916063,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ds0s4owq8tn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695106488989,
  "history_end_time" : 1695106488989,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rbt5put7g4r",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695106316226,
  "history_end_time" : 1695106316226,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "k7x9r715o8i",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695054045036,
  "history_end_time" : 1695054045036,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "skxs2px5pit",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695054019804,
  "history_end_time" : 1695054033337,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "3503j1430a1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695053979923,
  "history_end_time" : 1695054019279,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "u126p6qkb7m",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695053793463,
  "history_end_time" : 1695053793463,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "z0zyr00gv1p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695053733480,
  "history_end_time" : 1695053733480,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "fjools0yx1t",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694971144855,
  "history_end_time" : 1694972839692,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "lw5yzef2drz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970707953,
  "history_end_time" : 1694970707953,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "srraf32mp6c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594774,
  "history_end_time" : 1694970594774,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "u85hw9thpor",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970131663,
  "history_end_time" : 1694970131663,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "sqllvzwud4b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694969350146,
  "history_end_time" : 1694969350146,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "bkahs3bpwc8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694905307741,
  "history_end_time" : 1694905307741,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "pkhi09xyjvk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887153,
  "history_end_time" : 1694897887153,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vnYKEk5kMttS",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '32MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\ngridmet = gridmet.rename(columns={'day': 'date'})\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\nprint('merge completed.')\n\n\ndf = dd.read_csv(f'{work_dir}/final_merged_data_3_yrs.csv')\ndf = df.drop_duplicates(keep='first')\ndf.to_csv(f'{work_dir}/final_merged_data_3yrs_cleaned.csv', single_file=True, index=False)",
  "history_output" : "today date = 2023-09-11\n/home/chetana\nmerge completed.\n",
  "history_begin_time" : 1694445066010,
  "history_end_time" : 1694448714178,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ISKrXMBwXjS6",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '32MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\ngridmet = gridmet.rename(columns={'day': 'date'})\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\nprint('merge completed.')\n\n\ndf = dd.read_csv('final_merged_data_3_yrs.csv')\ndf = df.drop_duplicates(keep='first')\ndf.to_csv(f'{working_dir}/final_merged_data_3yrs_cleaned.csv')",
  "history_output" : "today date = 2023-09-11\n/home/chetana\nmerge completed.\nTraceback (most recent call last):\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 136, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 761, in read\n    return read_pandas(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 561, in read_pandas\n    b_out = read_bytes(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/bytes/core.py\", line 111, in read_bytes\n    size = fs.info(path)[\"size\"]\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/fsspec/implementations/local.py\", line 87, in info\n    out = os.stat(path, follow_symlinks=False)\nFileNotFoundError: [Errno 2] No such file or directory: '/home/chetana/gw-workspace/ISKrXMBwXjS6/final_merged_data_3_yrs.csv'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/ISKrXMBwXjS6/merge_custom_traning_range.py\", line 32, in <module>\n    df = dd.read_csv('final_merged_data_3_yrs.csv')\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 138, in wrapper\n    raise type(e)(\nFileNotFoundError: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: [Errno 2] No such file or directory: '/home/chetana/gw-workspace/ISKrXMBwXjS6/final_merged_data_3_yrs.csv'\n",
  "history_begin_time" : 1694421100044,
  "history_end_time" : 1694424730068,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "p49g9uAwsrSx",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '32MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\ngridmet = gridmet.rename(columns={'day': 'date'})\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\nprint('merge completed.')\n",
  "history_output" : "today date = 2023-09-10\n/home/chetana\nmerge completed.\n",
  "history_begin_time" : 1694322596784,
  "history_end_time" : 1694327296809,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "LkmwrAi0zDh7",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '64MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\ngridmet = gridmet.rename(columns={'day': 'date'})\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\nprint('merge completed.')\n",
  "history_output" : "today date = 2023-09-10\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/LkmwrAi0zDh7/merge_custom_traning_range.py\", line 28, in <module>\n    merged_df.to_csv(output_file, single_file=True, index=False)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 994, in to_csv\n    return list(dask.compute(*values, **compute_kwargs))\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/threaded.py\", line 89, in get\n    results = get_async(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 511, in get_async\n    raise_exception(exc, tb)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 319, in reraise\n    raise exc\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 224, in execute_task\n    result = _execute_task(task, data)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 289, in merge_chunk\n    out = lhs.merge(rhs, *args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/concat.py\", line 212, in concatenate_managers\n    values = values.copy()\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 3.53 GiB for an array with shape (6, 78902593) and data type float64\n",
  "history_begin_time" : 1694321441151,
  "history_end_time" : 1694322427753,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "2DxS4gZcEhEK",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '64MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\ngridmet = gridmet.rename(columns={'day': 'date'})\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'day'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\nprint('merge completed.')\n",
  "history_output" : "today date = 2023-09-10\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/2DxS4gZcEhEK/merge_custom_traning_range.py\", line 23, in <module>\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'day'], how='outer')\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 726, in merge\n    return hash_join(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 400, in hash_join\n    meta = _lhs_meta.merge(_rhs_meta, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/frame.py\", line 10093, in merge\n    return merge(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 110, in merge\n    op = _MergeOperation(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 703, in __init__\n    ) = self._get_merge_keys()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 1162, in _get_merge_keys\n    right_keys.append(right._get_label_or_level_values(rk))\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 1850, in _get_label_or_level_values\n    raise KeyError(key)\nKeyError: 'day'\n",
  "history_begin_time" : 1694321418010,
  "history_end_time" : 1694321426134,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "0HYiKf2yAKHJ",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '64MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\nprint('merge completed.')\n",
  "history_output" : "today date = 2023-09-10\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/0HYiKf2yAKHJ/merge_custom_traning_range.py\", line 22, in <module>\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 726, in merge\n    return hash_join(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 400, in hash_join\n    meta = _lhs_meta.merge(_rhs_meta, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/frame.py\", line 10093, in merge\n    return merge(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 110, in merge\n    op = _MergeOperation(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 703, in __init__\n    ) = self._get_merge_keys()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 1162, in _get_merge_keys\n    right_keys.append(right._get_label_or_level_values(rk))\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 1850, in _get_label_or_level_values\n    raise KeyError(key)\nKeyError: 'date'\n",
  "history_begin_time" : 1694321143879,
  "history_end_time" : 1694321158497,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "CkdvHh5Sq2YZ",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '64MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\nprint('merge completed.')\n",
  "history_output" : "today date = 2023-09-09\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 136, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 761, in read\n    return read_pandas(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 561, in read_pandas\n    b_out = read_bytes(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/bytes/core.py\", line 111, in read_bytes\n    size = fs.info(path)[\"size\"]\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/fsspec/implementations/local.py\", line 87, in info\n    out = os.stat(path, follow_symlinks=False)\nFileNotFoundError: [Errno 2] No such file or directory: '/home/chetana/gridmet_test_run/gridmet_climatology/testing_ready_gridmet.csv'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/CkdvHh5Sq2YZ/merge_custom_traning_range.py\", line 11, in <module>\n    gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 138, in wrapper\n    raise type(e)(\nFileNotFoundError: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: [Errno 2] No such file or directory: '/home/chetana/gridmet_test_run/gridmet_climatology/testing_ready_gridmet.csv'\n",
  "history_begin_time" : 1694296537295,
  "history_end_time" : 1694296539799,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "utrQ6i3XRugd",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '64MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\nprint('merge completed.')\n",
  "history_output" : "today date = 2023-09-09\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 136, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 761, in read\n    return read_pandas(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 561, in read_pandas\n    b_out = read_bytes(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/bytes/core.py\", line 111, in read_bytes\n    size = fs.info(path)[\"size\"]\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/fsspec/implementations/local.py\", line 87, in info\n    out = os.stat(path, follow_symlinks=False)\nFileNotFoundError: [Errno 2] No such file or directory: '/home/chetana/gridmet_test_run/gridmet_climatology/testing_ready_gridmet.csv'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/utrQ6i3XRugd/merge_custom_traning_range.py\", line 11, in <module>\n    gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 138, in wrapper\n    raise type(e)(\nFileNotFoundError: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: [Errno 2] No such file or directory: '/home/chetana/gridmet_test_run/gridmet_climatology/testing_ready_gridmet.csv'\n",
  "history_begin_time" : 1694296461122,
  "history_end_time" : 1694296463617,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "YudcKdFOZRFm",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '64MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\nprint('merge completed.')\n",
  "history_output" : "today date = 2023-09-09\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 136, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 761, in read\n    return read_pandas(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 561, in read_pandas\n    b_out = read_bytes(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/bytes/core.py\", line 111, in read_bytes\n    size = fs.info(path)[\"size\"]\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/fsspec/implementations/local.py\", line 87, in info\n    out = os.stat(path, follow_symlinks=False)\nFileNotFoundError: [Errno 2] No such file or directory: '/home/chetana/gridmet_test_run/gridmet_climatology/testing_ready_gridmet.csv'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/YudcKdFOZRFm/merge_custom_traning_range.py\", line 11, in <module>\n    gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 138, in wrapper\n    raise type(e)(\nFileNotFoundError: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: [Errno 2] No such file or directory: '/home/chetana/gridmet_test_run/gridmet_climatology/testing_ready_gridmet.csv'\n",
  "history_begin_time" : 1694296438258,
  "history_end_time" : 1694296441348,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "BgTtprmmDSMl",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '64MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\nprint('merge completed.')\n",
  "history_output" : null,
  "history_begin_time" : 1694296431475,
  "history_end_time" : null,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : null
},{
  "history_id" : "prD3BEUypldj",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '64MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\n",
  "history_begin_time" : 1692192092321,
  "history_end_time" : 1692195042963,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "yEUYg2Nb9Fk5",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '64MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\n#amsr = amsr.repartition(partition_size=chunk_size)\n#snotel = snotel.repartition(partition_size=chunk_size)\n#gridmet = gridmet.repartition(partition_size=chunk_size)\n#terrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False, compute=False)\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\n",
  "history_begin_time" : 1692192040184,
  "history_end_time" : 1692192043025,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "z5SFly4L0uNd",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '64MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False, compute=False, chunksize=chunk_size)\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\n",
  "history_begin_time" : 1692191994169,
  "history_end_time" : 1692192009120,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "YTF8A77bPqgj",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '64MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the final merged DataFrame to a CSV file\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\n\n# Compute and persist the merged DataFrame\nmerged_df = merged_df.persist()\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\nsh: line 1:  7881 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python merge_custom_traning_range.py\n",
  "history_begin_time" : 1692191556216,
  "history_end_time" : 1692191717071,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "5VRYBoVkmR4e",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '64MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size, compression='gzip')\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size, compression='gzip')\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size, compression='gzip')\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size, compression='gzip')\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the final merged DataFrame to a CSV file\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\n\n# Compute and persist the merged DataFrame\nmerged_df = merged_df.persist()\n\n# Perform your computations on the persisted merged DataFrame\n# ...\n\n# Optionally, you can also call merged_df.compute() to trigger the computation and wait for it to finish.\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\n/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py:544: UserWarning: Warning gzip compression does not support breaking apart files\nPlease ensure that each individual file can fit in memory and\nuse the keyword ``blocksize=None to remove this message``\nSetting ``blocksize=None``\n  warn(\nTraceback (most recent call last):\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 136, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 761, in read\n    return read_pandas(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 561, in read_pandas\n    b_out = read_bytes(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/bytes/core.py\", line 173, in read_bytes\n    sample_buff = f.read(sample)\n  File \"/home/chetana/anaconda3/lib/python3.9/gzip.py\", line 300, in read\n    return self._buffer.read(size)\n  File \"/home/chetana/anaconda3/lib/python3.9/_compression.py\", line 68, in readinto\n    data = self.read(len(byte_view))\n  File \"/home/chetana/anaconda3/lib/python3.9/gzip.py\", line 487, in read\n    if not self._read_gzip_header():\n  File \"/home/chetana/anaconda3/lib/python3.9/gzip.py\", line 435, in _read_gzip_header\n    raise BadGzipFile('Not a gzipped file (%r)' % magic)\ngzip.BadGzipFile: Not a gzipped file (b'da')\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/5VRYBoVkmR4e/merge_custom_traning_range.py\", line 9, in <module>\n    amsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size, compression='gzip')\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 138, in wrapper\n    raise type(e)(\ngzip.BadGzipFile: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: Not a gzipped file (b'da')\n",
  "history_begin_time" : 1692191461468,
  "history_end_time" : 1692191471476,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "dLs0mseIX2HS",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\n# Define a custom chunk size based on available memory\nchunk_size = '32 MiB'\n\n# Create a temporary directory for Dask to spill data\ntemp_dir = os.path.join(working_dir, 'dask_temp')\nos.makedirs(temp_dir, exist_ok=True)\n\n# Read the CSV files with a smaller chunk size\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size, usecols=['lat', 'lon', 'date', 'SWE', 'Flag'])  # Select necessary columns\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size, usecols=['lat', 'lon', 'date', 'swe_value'])\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Merge DataFrames based on specified columns and save intermediate results\nmerged_df = amsr.merge(snotel, on=['lat', 'lon', 'date'], how='inner')\nmerged_df.to_csv(os.path.join(temp_dir, 'merged_amsr_snotel.csv'), index=False, single_file=True)\n\nmerged_df = merged_df.merge(gridmet, on=['lat', 'lon', 'date'], how='inner')\nmerged_df.to_csv(os.path.join(temp_dir, 'merged_gridmet.csv'), index=False, single_file=True)\n\n# Drop unnecessary columns\n#merged_df = merged_df.drop(['unnecessary_col1', 'unnecessary_col2'], axis=1)\n\nmerged_df = merged_df.merge(terrain, on=['lat', 'lon'], how='inner')\nmerged_df.to_csv(os.path.join(temp_dir, 'merged_final_3_yrs.csv'), index=False, single_file=True)\n\n# Drop duplicates based on all columns\nmerged_df = merged_df.drop_duplicates()\n\n# Save the final merged DataFrame to a CSV file\noutput_csv = f'{working_dir}/merged_data_3_yrs.csv'\nmerged_df.to_csv(output_csv, index=False, single_file=True)\n\nprint(f\"Merged data (without duplicates) saved to {output_csv}\")\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/dLs0mseIX2HS/merge_custom_traning_range.py\", line 31, in <module>\n    merged_df.to_csv(os.path.join(temp_dir, 'merged_final_3_yrs.csv'), index=False, single_file=True)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 994, in to_csv\n    return list(dask.compute(*values, **compute_kwargs))\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/threaded.py\", line 89, in get\n    results = get_async(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 511, in get_async\n    raise_exception(exc, tb)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 319, in reraise\n    raise exc\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 224, in execute_task\n    result = _execute_task(task, data)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 289, in merge_chunk\n    out = lhs.merge(rhs, *args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/concat.py\", line 212, in concatenate_managers\n    values = values.copy()\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 5.83 GiB for an array with shape (6, 130429334) and data type float64\n",
  "history_begin_time" : 1692190428106,
  "history_end_time" : 1692190778027,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "98zU5ujLxotA",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\n# Define a custom chunk size based on available memory\nchunk_size = '32 MiB'\n\n# Create a temporary directory for Dask to spill data\ntemp_dir = os.path.join(working_dir, 'dask_temp')\nos.makedirs(temp_dir, exist_ok=True)\n\n# Read the CSV files with a smaller chunk size\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Merge DataFrames based on specified columns and save intermediate results\nmerged_df = amsr.merge(snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df.to_csv(os.path.join(temp_dir, 'merged_amsr_snotel.csv'), index=False, single_file=True)\n\nmerged_df = merged_df.merge(gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df.to_csv(os.path.join(temp_dir, 'merged_gridmet.csv'), index=False, single_file=True)\n\nmerged_df = merged_df.merge(terrain, on=['lat', 'lon'], how='outer')\nmerged_df.to_csv(os.path.join(temp_dir, 'merged_final.csv'), index=False, single_file=True)\n\n# Drop duplicates based on all columns\nmerged_df = merged_df.drop_duplicates()\n\n# Save the final merged DataFrame to a CSV file\noutput_csv = f'{working_dir}/merged_data_3_yrs.csv'\nmerged_df.to_csv(output_csv, index=False, single_file=True)\n\n\nprint(f\"Merged data (without duplicates) saved to {output_csv}\")",
  "history_output" : "today date = 2023-08-16\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/98zU5ujLxotA/merge_custom_traning_range.py\", line 28, in <module>\n    merged_df.to_csv(os.path.join(temp_dir, 'merged_final.csv'), index=False, single_file=True)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 994, in to_csv\n    return list(dask.compute(*values, **compute_kwargs))\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/threaded.py\", line 89, in get\n    results = get_async(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 511, in get_async\n    raise_exception(exc, tb)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 319, in reraise\n    raise exc\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 224, in execute_task\n    result = _execute_task(task, data)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 289, in merge_chunk\n    out = lhs.merge(rhs, *args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/concat.py\", line 212, in concatenate_managers\n    values = values.copy()\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 4.75 GiB for an array with shape (6, 106348126) and data type float64\nsh: line 1:  3289 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python merge_custom_traning_range.py\n",
  "history_begin_time" : 1692188911679,
  "history_end_time" : 1692189475914,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "v3PiC7nthWQ1",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\n# Define a custom chunk size based on available memory\nchunk_size = '32 MiB'\n\n# Create a temporary directory for Dask to spill data\ntemp_dir = os.path.join(working_dir, 'dask_temp')\nos.makedirs(temp_dir, exist_ok=True)\n\n# Read the CSV files with a smaller chunk size\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Merge DataFrames based on specified columns and save intermediate results\nmerged_df = amsr.merge(snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df.to_csv(os.path.join(temp_dir, 'merged_amsr_snotel.csv'), index=False, single_file=True)\n\nmerged_df = merged_df.merge(gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df.to_csv(os.path.join(temp_dir, 'merged_gridmet.csv'), index=False, single_file=True)\n\nmerged_df = merged_df.merge(terrain, on=['lat', 'lon'], how='outer')\nmerged_df.to_csv(os.path.join(temp_dir, 'merged_final.csv'), index=False, single_file=True)\n\n# Drop duplicates based on all columns\nmerged_df = merged_df.drop_duplicates()\n\n# Save the final merged DataFrame to a CSV file\noutput_csv = f'{working_dir}/merged_data_3_yrs.csv'\nmerged_df.to_csv(output_csv, index=False, single_file=True)\n\n\nprint(f\"Merged data (without duplicates) saved to {output_csv}\")",
  "history_output" : "today date = 2023-08-16\n/home/chetana\nsh: line 1: 29909 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python merge_custom_traning_range.py\n",
  "history_begin_time" : 1692166850136,
  "history_end_time" : 1692167287628,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "XOONN2jgWhub",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\n# Define a custom chunk size based on available memory\nchunk_size = '256 MiB'\n\n# Create a temporary directory for Dask to spill data\ntemp_dir = os.path.join(working_dir, 'dask_temp')\nos.makedirs(temp_dir, exist_ok=True)\n\n# Read the CSV files with a smaller chunk size\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Define the order in which DataFrames are merged\nmerge_order = [(amsr, snotel), (gridmet, terrain)]\n\n# Iterate over pairs of DataFrames and save intermediate results\nmerged_df = None\nfor left, right in merge_order:\n    if merged_df is None:\n        merged_df = left.merge(right, on=['lat', 'lon', 'date'], how='outer')\n    else:\n        temp_merged = left.merge(right, on=['lat', 'lon', 'date'], how='outer')\n        temp_merged.to_csv(os.path.join(temp_dir, 'merged_temp.csv'), index=False, single_file=True)\n        \n        # Drop duplicates based on all columns\n        temp_merged = temp_merged.drop_duplicates()\n        temp_merged.to_csv(os.path.join(temp_dir, 'merged_temp_deduped.csv'), index=False, single_file=True)\n        \n        # Continue merging with existing merged_df\n        merged_df = merged_df.merge(temp_merged, on=['lat', 'lon', 'date'], how='outer')\n        merged_df = merged_df.drop_duplicates()\n        \n        # Remove intermediate files if needed\n        os.remove(os.path.join(temp_dir, 'merged_temp.csv'))\n        os.remove(os.path.join(temp_dir, 'merged_temp_deduped.csv'))\n\n# Save the final merged DataFrame to a CSV file\noutput_csv = f'{working_dir}/merged_data_3_yrs.csv'\nmerged_df.to_csv(output_csv, index=False, single_file=True)\n\nprint(f\"Merged data (without duplicates) saved to {output_csv}\")\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/XOONN2jgWhub/merge_custom_traning_range.py\", line 29, in <module>\n    temp_merged = left.merge(right, on=['lat', 'lon', 'date'], how='outer')\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/core.py\", line 5691, in merge\n    return merge(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 726, in merge\n    return hash_join(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 400, in hash_join\n    meta = _lhs_meta.merge(_rhs_meta, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/frame.py\", line 10093, in merge\n    return merge(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 110, in merge\n    op = _MergeOperation(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 703, in __init__\n    ) = self._get_merge_keys()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 1162, in _get_merge_keys\n    right_keys.append(right._get_label_or_level_values(rk))\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 1850, in _get_label_or_level_values\n    raise KeyError(key)\nKeyError: 'date'\n",
  "history_begin_time" : 1692166789688,
  "history_end_time" : 1692166801037,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "UcYEMnaQHqTr",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\n# Define a custom chunk size based on available memory\nchunk_size = '256 MiB'\n\n# Create a temporary directory for Dask to spill data\ntemp_dir = os.path.join(working_dir, 'dask_temp')\nos.makedirs(temp_dir, exist_ok=True)\n\n# Read the CSV files with a smaller chunk size\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Merge DataFrames based on specified columns and save intermediate results\nmerged_df = amsr.merge(snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df.to_csv(os.path.join(temp_dir, 'merged_amsr_snotel.csv'), index=False, single_file=True)\n\nmerged_df = merged_df.merge(gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df.to_csv(os.path.join(temp_dir, 'merged_gridmet.csv'), index=False, single_file=True)\n\nmerged_df = merged_df.merge(terrain, on=['lat', 'lon'], how='outer')\nmerged_df.to_csv(os.path.join(temp_dir, 'merged_final.csv'), index=False, single_file=True)\n\n# Drop duplicates based on all columns\nmerged_df = merged_df.drop_duplicates()\n\n# Save the final merged DataFrame to a CSV file\noutput_csv = f'{working_dir}/merged_data_3_yrs.csv'\nmerged_df.to_csv(output_csv, index=False, single_file=True)\n\n\nprint(f\"Merged data (without duplicates) saved to {output_csv}\")\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/UcYEMnaQHqTr/merge_custom_traning_range.py\", line 28, in <module>\n    merged_df.to_csv(os.path.join(temp_dir, 'merged_final.csv'), index=False, single_file=True)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 994, in to_csv\n    return list(dask.compute(*values, **compute_kwargs))\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/threaded.py\", line 89, in get\n    results = get_async(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 511, in get_async\n    raise_exception(exc, tb)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 319, in reraise\n    raise exc\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 224, in execute_task\n    result = _execute_task(task, data)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 289, in merge_chunk\n    out = lhs.merge(rhs, *args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/concat.py\", line 212, in concatenate_managers\n    values = values.copy()\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 13.4 GiB for an array with shape (7, 257808468) and data type float64\n",
  "history_begin_time" : 1692165824781,
  "history_end_time" : 1692166243884,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ORzb67mrRIVZ",
  "history_input" : "import dask.dataframe as dd\nimport dask.config\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\n# Define a custom chunk size based on available memory\nchunk_size = '256 MiB'\n\n# Define memory limit (adjust as needed)\nmemory_limit = '40GB'  # Set an appropriate value based on available RAM\n\n# Set the memory limit in Dask's configuration\ndask.config.set({'io': {'pandas': {'memory_limit': memory_limit}}})\n\n# Create a temporary directory for Dask to spill data\ntemp_dir = os.path.join(working_dir, 'dask_temp')\nos.makedirs(temp_dir, exist_ok=True)\n\n# Read the CSV files with a smaller chunk size\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = amsr.merge(snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = merged_df.merge(gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = merged_df.merge(terrain, on=['lat', 'lon'], how='outer')\n\n# Drop duplicates based on all columns\nmerged_df = merged_df.drop_duplicates()\n\n# Save the merged DataFrame to a CSV file\noutput_csv = f'{working_dir}/merged_data_3_yrs.csv'\nmerged_df.to_csv(output_csv, index=False, single_file=True)\n\n# Compute and wait for the computation to complete\nmerged_df.compute()\n\nprint(f\"Merged data (without duplicates) saved to {output_csv}\")\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/ORzb67mrRIVZ/merge_custom_traning_range.py\", line 37, in <module>\n    merged_df.to_csv(output_csv, index=False, single_file=True)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 994, in to_csv\n    return list(dask.compute(*values, **compute_kwargs))\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/threaded.py\", line 89, in get\n    results = get_async(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 511, in get_async\n    raise_exception(exc, tb)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 319, in reraise\n    raise exc\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 224, in execute_task\n    result = _execute_task(task, data)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 289, in merge_chunk\n    out = lhs.merge(rhs, *args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/concat.py\", line 212, in concatenate_managers\n    values = values.copy()\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 13.4 GiB for an array with shape (7, 257808468) and data type float64\n",
  "history_begin_time" : 1692165487824,
  "history_end_time" : 1692165737157,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "jxQ8EbyxJPQO",
  "history_input" : "import dask.dataframe as dd\nimport dask.config\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\n# Define a custom chunk size based on available memory\nchunk_size = '256 MiB'\n\n# Define memory limit (adjust as needed)\nmemory_limit = '40GB'  # Set an appropriate value based on available RAM\n\n# Set the memory limit in Dask's configuration\ndask.config.set({'io': {'pandas': {'memory_limit': memory_limit}}})\n\n# Create a temporary directory for Dask to spill data\ntemp_dir = os.path.join(working_dir, 'dask_temp')\nos.makedirs(temp_dir, exist_ok=True)\n\n# Read the CSV files with a smaller chunk size\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = amsr.merge(snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = merged_df.merge(gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = merged_df.merge(terrain, on=['lat', 'lon'], how='outer')\n\n# Drop duplicates based on all columns\nmerged_df = merged_df.drop_duplicates()\n\n# Save the merged DataFrame to a CSV file\noutput_csv = f'{working_dir}/merged_data.csv'\nmerged_df.to_csv(output_csv, index=False)\n\n# Compute and wait for the computation to complete\nmerged_df.compute()\n\nprint(f\"Merged data (without duplicates) saved to {output_csv}\")\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/jxQ8EbyxJPQO/merge_custom_traning_range.py\", line 37, in <module>\n    merged_df.to_csv(output_csv, index=False)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/core.py\", line 1856, in to_csv\n    return to_csv(self, filename, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 952, in to_csv\n    files = open_files(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/fsspec/core.py\", line 293, in open_files\n    [fs.makedirs(parent, exist_ok=True) for parent in parents]\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/fsspec/core.py\", line 293, in <listcomp>\n    [fs.makedirs(parent, exist_ok=True) for parent in parents]\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/fsspec/implementations/local.py\", line 54, in makedirs\n    os.makedirs(path, exist_ok=exist_ok)\n  File \"/home/chetana/anaconda3/lib/python3.9/os.py\", line 225, in makedirs\n    mkdir(name, mode)\nFileExistsError: [Errno 17] File exists: '/home/chetana/gridmet_test_run/merged_data.csv'\n",
  "history_begin_time" : 1692165416567,
  "history_end_time" : 1692165420705,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "vXN3L8DGMjFA",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\n# Define a custom chunk size based on available memory\nchunk_size = '256 MiB'\n\n# Define memory limit (adjust as needed)\nmemory_limit = '40GB'  # Set an appropriate value based on available RAM\n\n# Create a temporary directory for Dask to spill data\ntemp_dir = os.path.join(working_dir, 'dask_temp')\nos.makedirs(temp_dir, exist_ok=True)\n\n# Read the CSV files with memory limit and temporary directory\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size, memory_limit=memory_limit, temporary_directory=temp_dir)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size, memory_limit=memory_limit, temporary_directory=temp_dir)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size, memory_limit=memory_limit, temporary_directory=temp_dir)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size, memory_limit=memory_limit, temporary_directory=temp_dir)\n\n# Merge DataFrames based on specified columns\nmerged_df = amsr.merge(snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = merged_df.merge(gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = merged_df.merge(terrain, on=['lat', 'lon'], how='outer')\n\n# Drop duplicates based on all columns\nmerged_df = merged_df.drop_duplicates()\n\n# Save the merged DataFrame to a CSV file\noutput_csv = f'{working_dir}/merged_data.csv'\nmerged_df.to_csv(output_csv, index=False)\n\n# Compute and wait for the computation to complete\n#merged_df.compute()\n\nprint(f\"Merged data (without duplicates) saved to {output_csv}\")\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 136, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 761, in read\n    return read_pandas(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 630, in read_pandas\n    head = reader(BytesIO(b_sample), nrows=sample_rows, **head_kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 211, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\nTypeError: read_csv() got an unexpected keyword argument 'memory_limit'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/vXN3L8DGMjFA/merge_custom_traning_range.py\", line 18, in <module>\n    amsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size, memory_limit=memory_limit, temporary_directory=temp_dir)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 138, in wrapper\n    raise type(e)(\nTypeError: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: read_csv() got an unexpected keyword argument 'memory_limit'\n",
  "history_begin_time" : 1692165336940,
  "history_end_time" : 1692165340408,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "p2TRG13I7tgn",
  "history_input" : "import dask.dataframe as dd\nimport os\n\nworking_dir = '.'\n\n# Define a custom chunk size based on available memory\nchunk_size = '256 MiB'\n\n# Define memory limit (adjust as needed)\nmemory_limit = '40GB'  # Set an appropriate value based on available RAM\n\n# Create a temporary directory for Dask to spill data\ntemp_dir = os.path.join(working_dir, 'dask_temp')\nos.makedirs(temp_dir, exist_ok=True)\n\n# Read the CSV files with memory limit and temporary directory\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size, memory_limit=memory_limit, temporary_directory=temp_dir)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size, memory_limit=memory_limit, temporary_directory=temp_dir)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size, memory_limit=memory_limit, temporary_directory=temp_dir)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size, memory_limit=memory_limit, temporary_directory=temp_dir)\n\n# Merge DataFrames based on specified columns\nmerged_df = amsr.merge(snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = merged_df.merge(gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = merged_df.merge(terrain, on=['lat', 'lon'], how='outer')\n\n# Drop duplicates based on all columns\nmerged_df = merged_df.drop_duplicates()\n\n# Save the merged DataFrame to a CSV file\noutput_csv = f'{working_dir}/merged_data.csv'\nmerged_df.to_csv(output_csv, index=False)\n\n# Compute and wait for the computation to complete\n#merged_df.compute()\n\nprint(f\"Merged data (without duplicates) saved to {output_csv}\")\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 136, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 761, in read\n    return read_pandas(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 561, in read_pandas\n    b_out = read_bytes(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/bytes/core.py\", line 111, in read_bytes\n    size = fs.info(path)[\"size\"]\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/fsspec/implementations/local.py\", line 87, in info\n    out = os.stat(path, follow_symlinks=False)\nFileNotFoundError: [Errno 2] No such file or directory: '/home/chetana/gw-workspace/p2TRG13I7tgn/training_ready_amsr_3_yrs.csv'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/p2TRG13I7tgn/merge_custom_traning_range.py\", line 17, in <module>\n    amsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size, memory_limit=memory_limit, temporary_directory=temp_dir)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 138, in wrapper\n    raise type(e)(\nFileNotFoundError: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: [Errno 2] No such file or directory: '/home/chetana/gw-workspace/p2TRG13I7tgn/training_ready_amsr_3_yrs.csv'\n",
  "history_begin_time" : 1692165292926,
  "history_end_time" : 1692165302501,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "tmc072vE02pu",
  "history_input" : "import dask.dataframe as dd\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\n# Read the CSV files\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv')\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv')\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv')\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv')\n\n# Merge DataFrames based on specified columns\nmerged_df = snotel.merge(amsr, on=['lat', 'lon', 'date'], how='outer')\nprint('completed snotel <-> amsr merge')\nmerged_df = merged_df.merge(gridmet, on=['lat', 'lon', 'date'], how='outer')\nprint('completed snotel_amsr <-> gridmet merge')\nmerged_df = merged_df.merge(terrain, on=['lat', 'lon'], how='outer')\nprint('compelted snotel_amsr_grimet <-> terrian merge')\n\n# Drop duplicates based on all columns\nmerged_df = merged_df.drop_duplicates()\n\n# Save the merged DataFrame to a CSV file\noutput_csv = f'{working_dir}/merged_data_3_yrs.csv'\nmerged_df.to_csv(output_csv, index=False)\n\n# Compute and wait for the computation to complete (if needed)\nmerged_df.compute()\n\nprint(f\"Merged data (without duplicates) saved to {output_csv}\")\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\ncompleted snotel <-> amsr merge\ncompleted snotel_amsr <-> gridmet merge\ncompelted snotel_amsr_grimet <-> terrian merge\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/tmc072vE02pu/merge_custom_traning_range.py\", line 25, in <module>\n    merged_df.to_csv(output_csv, index=False)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 994, in to_csv\n    return list(dask.compute(*values, **compute_kwargs))\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/threaded.py\", line 89, in get\n    results = get_async(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 511, in get_async\n    raise_exception(exc, tb)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 319, in reraise\n    raise exc\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 224, in execute_task\n    result = _execute_task(task, data)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 289, in merge_chunk\n    out = lhs.merge(rhs, *args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/concat.py\", line 212, in concatenate_managers\n    values = values.copy()\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 11.5 GiB for an array with shape (6, 257808468) and data type float64\n",
  "history_begin_time" : 1692164750384,
  "history_end_time" : 1692165144917,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Qjb9777yKfq4",
  "history_input" : "import dask.dataframe as dd\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\n# Read the CSV files\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv')\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv')\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv')\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv')\n\n# Merge DataFrames based on specified columns\nmerged_df = snotel.merge(amsr, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = merged_df.merge(gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = merged_df.merge(terrain, on=['lat', 'lon'], how='outer')\n\n# Drop duplicates based on all columns\nmerged_df = merged_df.drop_duplicates()\n\n# Save the merged DataFrame to a CSV file\noutput_csv = f'{working_dir}/merged_data_3_yrs.csv'\nmerged_df.to_csv(output_csv, index=False)\n\n# Compute and wait for the computation to complete (if needed)\nmerged_df.compute()\n\nprint(f\"Merged data (without duplicates) saved to {output_csv}\")\n",
  "history_output" : null,
  "history_begin_time" : 1692164667924,
  "history_end_time" : null,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : null
},{
  "history_id" : "pR1xj0UHuGsc",
  "history_input" : "import dask.dataframe as dd\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\n# Read the CSV files\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv')\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv')\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv')\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv')\n\n# Merge DataFrames based on specified columns\nmerged_df = snotel.merge(amsr, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = merged_df.merge(gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = merged_df.merge(terrain, on=['lat', 'lon'], how='outer')\n\n# Drop duplicates based on all columns\nmerged_df = merged_df.drop_duplicates()\n\n# Save the merged DataFrame to a CSV file\noutput_csv = f'{working_dir}/merged_data_3_yrs.csv'\nmerged_df.to_csv(output_csv, index=False)\n\n# Compute and wait for the computation to complete (if needed)\nmerged_df.compute()\n\nprint(f\"Merged data (without duplicates) saved to {output_csv}\")\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\n",
  "history_begin_time" : 1692164620925,
  "history_end_time" : 1692164667961,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "t8cbvf2kbq0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531335834,
  "history_end_time" : 1691531335834,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "sxwmabed15w",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531292844,
  "history_end_time" : 1691531292844,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "885xpzpe1da",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531254771,
  "history_end_time" : 1691531284904,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "2ekosjovj9k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531163961,
  "history_end_time" : 1691531163961,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "4791ge4k1tq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531121014,
  "history_end_time" : 1691531121014,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "8n6w2cjsj7r",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531061019,
  "history_end_time" : 1691531061019,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "kroadp4thod",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530848386,
  "history_end_time" : 1691530848386,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "q087n8t57q4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717766,
  "history_end_time" : 1691530721109,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "vb9ks58sr66",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530690280,
  "history_end_time" : 1691530716753,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "77dcsormpmx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530621141,
  "history_end_time" : 1691530622443,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "li45n4zcf64",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530617330,
  "history_end_time" : 1691530617330,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "etfp4vl7p5r",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530599904,
  "history_end_time" : 1691530614288,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "QkMR0GC9egfS",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask instead of Pandas\n#snotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv')\n#gridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv')\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv')\n#terrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv')\n\n# Perform the merges using Dask DataFrames\n#snotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\n#merged_data = dd.merge(snotel_gridmet, terrain, on=['lat', 'lon'], how='inner')\n#merged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Compute the result and convert to Pandas DataFrame if needed\n#merged_data.to_csv('/home/chetana/gridmet_test_run/training_snotel_gridmet_3_yrs.csv', index=False, single_file=True)\n\nsnotel_gridmet_merged = dd.read_csv('/home/chetana/gridmet_test_run/training_snotel_gridmet_3_yrs.csv', usecols=['date', 'SWE', 'lat', 'lon', 'etr', 'pr',\n       'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'elevation', 'slope',\n       'curvature', 'aspect', 'eastness', 'northness'])\n\nmerged_data = dd.merge(snotel_gridmet_merged, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = merged_data.drop_duplicates()\nmerged_data.to_csv('/home/chetana/gridmet_test_run/training_ready_3_yr_data.csv', single_file=True, index=False)\n\n# Now you have the merged data as a Pandas DataFrame\n",
  "history_output" : "",
  "history_begin_time" : 1691384668402,
  "history_end_time" : 1691384954294,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "MsrbHDXjpeUV",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask instead of Pandas\n#snotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv')\n#gridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv')\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv')\n#terrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv')\n\n# Perform the merges using Dask DataFrames\n#snotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\n#merged_data = dd.merge(snotel_gridmet, terrain, on=['lat', 'lon'], how='inner')\n#merged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Compute the result and convert to Pandas DataFrame if needed\n#merged_data.to_csv('/home/chetana/gridmet_test_run/training_snotel_gridmet_3_yrs.csv', index=False, single_file=True)\n\nsnotel_gridmet_merged = dd.read_csv('/home/chetana/gridmet_test_run/training_snotel_gridmet_3_yrs.csv', usecols=['date', 'SWE', 'lat', 'lon', 'etr', 'pr',\n       'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'elevation', 'slope',\n       'curvature', 'aspect', 'eastness', 'northness'])\n\nmerged_data = dd.merge(snotel_gridmet_merged, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = merged_data.drop_duplicates()\nmerged_data.to_csv('/home/chetana/gridmet_test_run/training_ready_2_yr_data.csv', single_file=True, index=False)\n\n# Now you have the merged data as a Pandas DataFrame\n",
  "history_output" : "",
  "history_begin_time" : 1691383717686,
  "history_end_time" : 1691384053108,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "gQpWBeTGfVYl",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask instead of Pandas\n#snotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv')\n#gridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv')\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv')\n#terrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv')\n\n# Perform the merges using Dask DataFrames\n#snotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\n#merged_data = dd.merge(snotel_gridmet, terrain, on=['lat', 'lon'], how='inner')\n#merged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Compute the result and convert to Pandas DataFrame if needed\n#merged_data.to_csv('/home/chetana/gridmet_test_run/training_snotel_gridmet_3_yrs.csv', index=False, single_file=True)\n\nsnotel_gridmet_merged = dd.read_csv('/home/chetana/gridmet_test_run/training_snotel_gridmet_3_yrs.csv', usecols=['date', 'SWE', 'lat', 'lon', 'etr', 'pr',\n       'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'elevation', 'slope',\n       'curvature', 'aspect', 'eastness', 'northness'])\n\nmerged_data = dd.merge(snotel_gridmet_merged, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data.to_csv('/home/chetana/gridmet_test_run/training_final_3_yr_training_ready.csv', single_file=True, index=False)\n\n# Now you have the merged data as a Pandas DataFrame\n",
  "history_output" : "",
  "history_begin_time" : 1691368736436,
  "history_end_time" : 1691372133466,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Ht5XX8CxhstS",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask instead of Pandas\n#snotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv')\n#gridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv')\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv')\n#terrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv')\n\n# Perform the merges using Dask DataFrames\n#snotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\n#merged_data = dd.merge(snotel_gridmet, terrain, on=['lat', 'lon'], how='inner')\n#merged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Compute the result and convert to Pandas DataFrame if needed\n#merged_data.to_csv('/home/chetana/gridmet_test_run/training_snotel_gridmet_3_yrs.csv', index=False, single_file=True)\n\nsnotel_gridmet_merged = dd.read_csv('/home/chetana/gridmet_test_run/training_snotel_gridmet_3_yrs.csv', usecols=['date', 'SWE', 'lat', 'lon', 'etr', 'pr',\n       'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'elevation', 'slope',\n       'curvature', 'aspect', 'eastness', 'northness'])\n\nmerged_data = dd.merge(snotel_gridmet_merged, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data.to_csv('/home/chetana/gridmet_test_run/training_final_3_yr_training_ready.csv', single_file=True, index=False)\n\n# Now you have the merged data as a Pandas DataFrame\n",
  "history_output" : null,
  "history_begin_time" : 1691368727994,
  "history_end_time" : null,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : null
},{
  "history_id" : "Fm233HyAlNE4",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask instead of Pandas\n#snotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv')\n#gridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv')\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv')\n#terrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv')\n\n# Perform the merges using Dask DataFrames\n#snotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\n#merged_data = dd.merge(snotel_gridmet, terrain, on=['lat', 'lon'], how='inner')\n#merged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Compute the result and convert to Pandas DataFrame if needed\n#merged_data.to_csv('/home/chetana/gridmet_test_run/training_snotel_gridmet_3_yrs.csv', index=False, single_file=True)\n\nsnotel_gridmet_merged = dd.read_csv('/home/chetana/gridmet_test_run/training_snotel_gridmet_3_yrs.csv')\n\nmerged_data = dd.merge(snotel_gridmet_merged, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data.to_csv('/home/chetana/gridmet_test_run/training_final_3_yr_training_ready.csv', single_file=True, index=False)\n\n# Now you have the merged data as a Pandas DataFrame\n",
  "history_output" : "",
  "history_begin_time" : 1691365701482,
  "history_end_time" : 1691368728002,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "nyPtpPq6J27N",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask instead of Pandas\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv')\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv')\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv')\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv')\n\n# Perform the merges using Dask DataFrames\nsnotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(snotel_gridmet, terrain, on=['lat', 'lon'], how='inner')\n#merged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False, single_file=True)\n\n# Now you have the merged data as a Pandas DataFrame\n",
  "history_output" : "",
  "history_begin_time" : 1691363789027,
  "history_end_time" : 1691364194713,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "nEmUEAJFm7OZ",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask with a smaller blocksize and only necessary columns\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv')\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', usecols=['date', 'lat', 'lon', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs'])\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv')\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv')\n\n# Perform the merges using Dask DataFrames with all merge keys specified at once\nmerged_data = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner', suffixes=('_snotel', '_gridmet'))\nmerged_data = dd.merge(merged_data, amsr, on=['date', 'lat', 'lon'], how='inner', suffixes=('', '_amsr'))\nmerged_data = dd.merge(merged_data, terrain, on=['lat', 'lon'], how='inner', suffixes=('', '_terrain'))\n\n# Write intermediate result to disk in CSV format\nmerged_data.to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False, single_file=True)\n\n# The above line will execute lazily and data will spill to disk if RAM is insufficient\n# We don't need to use `persist` since the spill-to-disk mechanism will handle data storage\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/nEmUEAJFm7OZ/merge_custom_traning_range.py\", line 15, in <module>\n    merged_data.to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False, single_file=True)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 994, in to_csv\n    return list(dask.compute(*values, **compute_kwargs))\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/threaded.py\", line 89, in get\n    results = get_async(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 511, in get_async\n    raise_exception(exc, tb)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 319, in reraise\n    raise exc\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 224, in execute_task\n    result = _execute_task(task, data)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 289, in merge_chunk\n    out = lhs.merge(rhs, *args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/concat.py\", line 212, in concatenate_managers\n    values = values.copy()\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 5.44 GiB for an array with shape (6, 121609632) and data type float64\nsh: line 1: 13057 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python merge_custom_traning_range.py\n",
  "history_begin_time" : 1691363631859,
  "history_end_time" : 1691363768940,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "8BQ7efQSX58O",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask with a smaller blocksize and only necessary columns\nblocksize = \"16MB\"\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv')\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', usecols=['date', 'lat', 'lon', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs'])\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv')\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv')\n\n# Perform the merges using Dask DataFrames with all merge keys specified at once\nmerged_data = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(merged_data, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(merged_data, terrain, on=['lat', 'lon'], how='inner')\n\n# Persist intermediate result to disk if enough disk space is available\nmerged_data = merged_data.persist()\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/8BQ7efQSX58O/merge_custom_traning_range.py\", line 16, in <module>\n    merged_data = merged_data.persist()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/threaded.py\", line 89, in get\n    results = get_async(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 511, in get_async\n    raise_exception(exc, tb)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 319, in reraise\n    raise exc\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 224, in execute_task\n    result = _execute_task(task, data)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 289, in merge_chunk\n    out = lhs.merge(rhs, *args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/concat.py\", line 212, in concatenate_managers\n    values = values.copy()\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 5.44 GiB for an array with shape (6, 121756443) and data type float64\nsh: line 1: 12603 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python merge_custom_traning_range.py\n",
  "history_begin_time" : 1691363355002,
  "history_end_time" : 1691363507755,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "njbpUsRMRccq",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask with a smaller blocksize and only necessary columns\nblocksize = \"16MB\"\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=blocksize)\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs'])\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', blocksize=blocksize)\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv', blocksize=blocksize)\n\n# Perform the merges using Dask DataFrames with all merge keys specified at once\nmerged_data = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(merged_data, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(merged_data, terrain, on=['lat', 'lon'], how='inner')\n\n# Persist intermediate result to disk if enough disk space is available\nmerged_data = merged_data.persist()\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/njbpUsRMRccq/merge_custom_traning_range.py\", line 19, in <module>\n    merged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/dispatch.py\", line 68, in concat\n    return func(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/backends.py\", line 667, in concat_pandas\n    out = pd.concat(dfs3, join=join, sort=False)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"<__array_function__ internals>\", line 180, in concatenate\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 10.9 GiB for an array with shape (6, 243366075) and data type float64\n",
  "history_begin_time" : 1691362366332,
  "history_end_time" : 1691362534449,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "5Lk0smDI14C3",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask with a smaller blocksize and only necessary columns\nblocksize = \"16MB\"\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=blocksize)\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', blocksize=blocksize)\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', blocksize=blocksize)\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv', blocksize=blocksize)\n\n# Perform the merges using Dask DataFrames with all merge keys specified at once\nmerged_data = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(merged_data, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(merged_data, terrain, on=['lat', 'lon'], how='inner')\n\n# Persist intermediate result to disk if enough disk space is available\nmerged_data = merged_data.persist()\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False)\n",
  "history_output" : "sh: line 1:  9298 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python merge_custom_traning_range.py\n",
  "history_begin_time" : 1691361850300,
  "history_end_time" : 1691361958513,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "DqyyOBpyf5mO",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask with a smaller blocksize and only necessary columns\nblocksize = \"16MB\"\nusecols = ['date', 'lat', 'lon', 'other_needed_columns']\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=blocksize, usecols=usecols)\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', blocksize=blocksize, usecols=usecols)\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', blocksize=blocksize, usecols=usecols)\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv', blocksize=blocksize, usecols=['lat', 'lon', 'other_needed_columns'])\n\n# Perform the merges using Dask DataFrames with all merge keys specified at once\nmerged_data = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(merged_data, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(merged_data, terrain, on=['lat', 'lon'], how='inner')\n\n# Convert columns to categorical data type if appropriate\nmerged_data['categorical_column'] = merged_data['categorical_column'].astype('category')\n\n# Persist intermediate result to disk if enough disk space is available\nmerged_data = merged_data.persist()\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 136, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 761, in read\n    return read_pandas(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 630, in read_pandas\n    head = reader(BytesIO(b_sample), nrows=sample_rows, **head_kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 211, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 950, in read_csv\n    return _read(filepath_or_buffer, kwds)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 605, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1442, in __init__\n    self._engine = self._make_engine(f, self.engine)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1753, in _make_engine\n    return mapping[engine](f, **self.options)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 135, in __init__\n    self._validate_usecols_names(usecols, self.orig_names)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\", line 917, in _validate_usecols_names\n    raise ValueError(\nValueError: Usecols do not match columns, columns expected but not found: ['other_needed_columns']\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/DqyyOBpyf5mO/merge_custom_traning_range.py\", line 6, in <module>\n    snotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=blocksize, usecols=usecols)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 138, in wrapper\n    raise type(e)(\nValueError: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: Usecols do not match columns, columns expected but not found: ['other_needed_columns']\n",
  "history_begin_time" : 1691361755680,
  "history_end_time" : 1691361763742,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "t1UwS2Ci5Ki6",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask with a smaller blocksize\nblocksize = \"32MB\"\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=blocksize)\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', blocksize=blocksize)\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', blocksize=blocksize)\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv', blocksize=blocksize)\n\n# Perform the merges using Dask DataFrames\nsnotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nsnotel_gridmet_amsr = dd.merge(snotel_gridmet, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Persist intermediate result to disk if enough disk space is available\nmerged_data = merged_data.persist()\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/t1UwS2Ci5Ki6/merge_custom_traning_range.py\", line 16, in <module>\n    merged_data = merged_data.persist()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/threaded.py\", line 89, in get\n    results = get_async(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 511, in get_async\n    raise_exception(exc, tb)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 319, in reraise\n    raise exc\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 224, in execute_task\n    result = _execute_task(task, data)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 289, in merge_chunk\n    out = lhs.merge(rhs, *args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/array_algos/take.py\", line 158, in _take_nd_ndarray\n    out = np.empty(out_shape, dtype=dtype)\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 2.73 GiB for an array with shape (6, 60966423) and data type float64\n",
  "history_begin_time" : 1691361471841,
  "history_end_time" : 1691361560749,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "T3BRtlIQvn4G",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask with a smaller blocksize\nblocksize = \"32MB\"\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=blocksize)\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', blocksize=blocksize)\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', blocksize=blocksize)\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv', blocksize=blocksize)\n\n# Perform the merges using Dask DataFrames\nsnotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nsnotel_gridmet_amsr = dd.merge(snotel_gridmet, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Persist intermediate result to disk if enough disk space is available\nmerged_data = merged_data.persist()\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False)\n",
  "history_output" : null,
  "history_begin_time" : 1691361180061,
  "history_end_time" : null,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : null
},{
  "history_id" : "kclAXhqALlUm",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask with a smaller blocksize\nblocksize = \"32MB\"\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon'])\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', blocksize=blocksize)\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', blocksize=blocksize)\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv', blocksize=blocksize)\n\n# Perform the merges using Dask DataFrames\nsnotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nsnotel_gridmet_amsr = dd.merge(snotel_gridmet, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Persist intermediate result to disk if enough disk space is available\nmerged_data = merged_data.persist()\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False)\n",
  "history_output" : "",
  "history_begin_time" : 1691361149208,
  "history_end_time" : 1691361180069,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "6ND27QqJwXjh",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask with a smaller blocksize\nblocksize = \"64MB\"\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon'])\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', blocksize=blocksize)\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', blocksize=blocksize)\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv', blocksize=blocksize)\n\n# Perform the merges using Dask DataFrames\nsnotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nsnotel_gridmet_amsr = dd.merge(snotel_gridmet, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Persist intermediate result to disk if enough disk space is available\nmerged_data = merged_data.persist()\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/6ND27QqJwXjh/merge_custom_traning_range.py\", line 16, in <module>\n    merged_data = merged_data.persist()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/threaded.py\", line 89, in get\n    results = get_async(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 511, in get_async\n    raise_exception(exc, tb)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 319, in reraise\n    raise exc\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 224, in execute_task\n    result = _execute_task(task, data)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 289, in merge_chunk\n    out = lhs.merge(rhs, *args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/concat.py\", line 212, in concatenate_managers\n    values = values.copy()\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 1.81 GiB for an array with shape (2, 121756443) and data type int64\nsh: line 1:  6882 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python merge_custom_traning_range.py\n",
  "history_begin_time" : 1691360928524,
  "history_end_time" : 1691361081045,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "veeDuOJivucf",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask with a smaller blocksize\nblocksize = \"32MB\"\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon'])\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon'])\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon'])\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv', blocksize=blocksize, usecols=['lat', 'lon'])\n\n# Perform the merges using Dask DataFrames\nsnotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nsnotel_gridmet_amsr = dd.merge(snotel_gridmet, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Persist intermediate result to disk if enough disk space is available\nmerged_data = merged_data.persist()\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False)\n",
  "history_output" : "",
  "history_begin_time" : 1691358253989,
  "history_end_time" : 1691358870989,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "LVYMCzXna8kY",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask with a smaller blocksize\nblocksize = \"32MB\"\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon'])\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon'])\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon'])\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv', blocksize=blocksize, usecols=['lat', 'lon'])\n\n# Perform the merges using Dask DataFrames\nsnotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nsnotel_gridmet_amsr = dd.merge(snotel_gridmet, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Persist intermediate result to disk if enough disk space is available\nmerged_data = merged_data.persist()\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False, single_file=True)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/LVYMCzXna8kY/merge_custom_traning_range.py\", line 19, in <module>\n    merged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False, single_file=True)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 211, in wrapper\n    return func(*args, **kwargs)\nTypeError: to_csv() got an unexpected keyword argument 'single_file'\n",
  "history_begin_time" : 1691358150598,
  "history_end_time" : 1691358182988,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "foLq4AgG1y0v",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask with a smaller blocksize\nblocksize = \"32MB\"\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon', 'other_needed_columns'])\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon', 'other_needed_columns'])\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon', 'other_needed_columns'])\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv', blocksize=blocksize, usecols=['lat', 'lon', 'other_needed_columns'])\n\n# Perform the merges using Dask DataFrames\nsnotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nsnotel_gridmet_amsr = dd.merge(snotel_gridmet, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Persist intermediate result to disk if enough disk space is available\nmerged_data = merged_data.persist()\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False, single_file=True)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 136, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 761, in read\n    return read_pandas(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 630, in read_pandas\n    head = reader(BytesIO(b_sample), nrows=sample_rows, **head_kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 211, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 950, in read_csv\n    return _read(filepath_or_buffer, kwds)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 605, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1442, in __init__\n    self._engine = self._make_engine(f, self.engine)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1753, in _make_engine\n    return mapping[engine](f, **self.options)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 135, in __init__\n    self._validate_usecols_names(usecols, self.orig_names)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\", line 917, in _validate_usecols_names\n    raise ValueError(\nValueError: Usecols do not match columns, columns expected but not found: ['other_needed_columns']\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/foLq4AgG1y0v/merge_custom_traning_range.py\", line 5, in <module>\n    snotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon', 'other_needed_columns'])\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 138, in wrapper\n    raise type(e)(\nValueError: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: Usecols do not match columns, columns expected but not found: ['other_needed_columns']\n",
  "history_begin_time" : 1691358090119,
  "history_end_time" : 1691358099746,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "oD7sBowsZQqg",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask instead of Pandas\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=\"64MB\")\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', blocksize=\"64MB\")\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', blocksize=\"64MB\")\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv', blocksize=\"64MB\")\n\n# Perform the merges using Dask DataFrames\nsnotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nsnotel_gridmet_amsr = dd.merge(snotel_gridmet, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False, single_file=True)\n\n# Now you have the merged data as a Pandas DataFrame\n",
  "history_output" : "sh: line 1:  2881 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python merge_custom_traning_range.py\n",
  "history_begin_time" : 1691357792430,
  "history_end_time" : 1691357933521,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "9QAjnQlXjRMK",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask instead of Pandas\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=\"64MB\")\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', blocksize=\"64MB\")\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', blocksize=\"64MB\")\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv', blocksize=\"64MB\")\n\n# Perform the merges using Dask DataFrames\nsnotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nsnotel_gridmet_amsr = dd.merge(snotel_gridmet, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False, single_file=True)\n\n# Now you have the merged data as a Pandas DataFrame\n",
  "history_output" : "sh: line 1:  1305 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python merge_custom_traning_range.py\n",
  "history_begin_time" : 1691355130245,
  "history_end_time" : 1691355272757,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "tDoXLKtqLseH",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask instead of Pandas\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv')\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv')\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv')\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv')\n\n# Perform the merges using Dask DataFrames\nsnotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nsnotel_gridmet_amsr = dd.merge(snotel_gridmet, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False, single_file=True)\n\n# Now you have the merged data as a Pandas DataFrame\n",
  "history_output" : "sh: line 1:   871 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python merge_custom_traning_range.py\n",
  "history_begin_time" : 1691354746353,
  "history_end_time" : 1691355073155,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},]
