[{
  "history_id" : "vnpwq1kfjk8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696863953337,
  "history_end_time" : 1696863953337,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "i1p0958o1l5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696862402979,
  "history_end_time" : 1696862402979,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "dh6prk1dop5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696832263693,
  "history_end_time" : 1696832263693,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "v0qbpv2alts",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696831867391,
  "history_end_time" : 1696831867391,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vn24gd4iyms",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696830174415,
  "history_end_time" : 1696830174415,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "d9uecspy6po",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696787541977,
  "history_end_time" : 1696787541977,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0a2b24kbc6o",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696786838259,
  "history_end_time" : 1696786838259,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "zn71eca3uma",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696771780949,
  "history_end_time" : 1696771780949,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "v9u1e7bvk3i",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696602943960,
  "history_end_time" : 1696602943960,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "1peha6o0q3s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696432484353,
  "history_end_time" : 1696432484353,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "5s4ixfhud8d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696432299775,
  "history_end_time" : 1696432482237,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ow293to5dhq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695827991148,
  "history_end_time" : 1695827991148,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vdk4ez78miq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695827889185,
  "history_end_time" : 1695827965229,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "wnu6nkkwgqn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695827855653,
  "history_end_time" : 1695827867013,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "2wzwttw2j2s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695696616126,
  "history_end_time" : 1695696616126,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "s1lq9cro9pi",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695694257336,
  "history_end_time" : 1695694257336,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "txot8wj4sis",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695693585755,
  "history_end_time" : 1695693585755,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xlaobebezdd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695693149412,
  "history_end_time" : 1695693149412,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "crc8wlk55tb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695580915854,
  "history_end_time" : 1695580915854,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "k2oqqeklm9m",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695576291671,
  "history_end_time" : 1695576291671,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ozv6qh0p32d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695575931035,
  "history_end_time" : 1695575931035,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "e7cn6yx1eeu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695535769219,
  "history_end_time" : 1695535769219,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "oe9iu2nxrza",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695535478715,
  "history_end_time" : 1695535478715,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "03ra1dbsh5i",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695535214031,
  "history_end_time" : 1695535214031,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ist8l8u6now",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695534943596,
  "history_end_time" : 1695534943596,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "7uk8vqucmhb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695534671835,
  "history_end_time" : 1695534671835,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "2afzcdhvp7z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695533024174,
  "history_end_time" : 1695533024174,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6gt7ytt0rw5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695529187875,
  "history_end_time" : 1695529187875,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "k4ow5sd3xhv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695528505204,
  "history_end_time" : 1695528505204,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3893n1vyzw0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695515862562,
  "history_end_time" : 1695515862562,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "udqbtyfeb79",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695506423865,
  "history_end_time" : 1695506423865,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "hncl0n1toa5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695418741350,
  "history_end_time" : 1695418741350,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "agxhr5kr2ew",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695417619686,
  "history_end_time" : 1695417619686,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "438vo0i339j",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695417171289,
  "history_end_time" : 1695417171289,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tmba37c1v0q",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695417052739,
  "history_end_time" : 1695417052739,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "39ec0firehu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695416916063,
  "history_end_time" : 1695416916063,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ds0s4owq8tn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695106488989,
  "history_end_time" : 1695106488989,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rbt5put7g4r",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695106316226,
  "history_end_time" : 1695106316226,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "k7x9r715o8i",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695054045036,
  "history_end_time" : 1695054045036,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "skxs2px5pit",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695054019804,
  "history_end_time" : 1695054033337,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "3503j1430a1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695053979923,
  "history_end_time" : 1695054019279,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "u126p6qkb7m",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695053793463,
  "history_end_time" : 1695053793463,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "z0zyr00gv1p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695053733480,
  "history_end_time" : 1695053733480,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "fjools0yx1t",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694971144855,
  "history_end_time" : 1694972839692,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "lw5yzef2drz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970707953,
  "history_end_time" : 1694970707953,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "srraf32mp6c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594774,
  "history_end_time" : 1694970594774,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "u85hw9thpor",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970131663,
  "history_end_time" : 1694970131663,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "sqllvzwud4b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694969350146,
  "history_end_time" : 1694969350146,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "bkahs3bpwc8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694905307741,
  "history_end_time" : 1694905307741,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "pkhi09xyjvk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887153,
  "history_end_time" : 1694897887153,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vnYKEk5kMttS",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '32MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\ngridmet = gridmet.rename(columns={'day': 'date'})\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\nprint('merge completed.')\n\n\ndf = dd.read_csv(f'{work_dir}/final_merged_data_3_yrs.csv')\ndf = df.drop_duplicates(keep='first')\ndf.to_csv(f'{work_dir}/final_merged_data_3yrs_cleaned.csv', single_file=True, index=False)",
  "history_output" : "today date = 2023-09-11\n/home/chetana\nmerge completed.\n",
  "history_begin_time" : 1694445066010,
  "history_end_time" : 1694448714178,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ISKrXMBwXjS6",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '32MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\ngridmet = gridmet.rename(columns={'day': 'date'})\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\nprint('merge completed.')\n\n\ndf = dd.read_csv('final_merged_data_3_yrs.csv')\ndf = df.drop_duplicates(keep='first')\ndf.to_csv(f'{working_dir}/final_merged_data_3yrs_cleaned.csv')",
  "history_output" : "today date = 2023-09-11\n/home/chetana\nmerge completed.\nTraceback (most recent call last):\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 136, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 761, in read\n    return read_pandas(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 561, in read_pandas\n    b_out = read_bytes(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/bytes/core.py\", line 111, in read_bytes\n    size = fs.info(path)[\"size\"]\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/fsspec/implementations/local.py\", line 87, in info\n    out = os.stat(path, follow_symlinks=False)\nFileNotFoundError: [Errno 2] No such file or directory: '/home/chetana/gw-workspace/ISKrXMBwXjS6/final_merged_data_3_yrs.csv'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/ISKrXMBwXjS6/merge_custom_traning_range.py\", line 32, in <module>\n    df = dd.read_csv('final_merged_data_3_yrs.csv')\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 138, in wrapper\n    raise type(e)(\nFileNotFoundError: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: [Errno 2] No such file or directory: '/home/chetana/gw-workspace/ISKrXMBwXjS6/final_merged_data_3_yrs.csv'\n",
  "history_begin_time" : 1694421100044,
  "history_end_time" : 1694424730068,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "p49g9uAwsrSx",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '32MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\ngridmet = gridmet.rename(columns={'day': 'date'})\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\nprint('merge completed.')\n",
  "history_output" : "today date = 2023-09-10\n/home/chetana\nmerge completed.\n",
  "history_begin_time" : 1694322596784,
  "history_end_time" : 1694327296809,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "LkmwrAi0zDh7",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '64MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\ngridmet = gridmet.rename(columns={'day': 'date'})\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\nprint('merge completed.')\n",
  "history_output" : "today date = 2023-09-10\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/LkmwrAi0zDh7/merge_custom_traning_range.py\", line 28, in <module>\n    merged_df.to_csv(output_file, single_file=True, index=False)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 994, in to_csv\n    return list(dask.compute(*values, **compute_kwargs))\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/threaded.py\", line 89, in get\n    results = get_async(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 511, in get_async\n    raise_exception(exc, tb)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 319, in reraise\n    raise exc\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 224, in execute_task\n    result = _execute_task(task, data)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 289, in merge_chunk\n    out = lhs.merge(rhs, *args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/concat.py\", line 212, in concatenate_managers\n    values = values.copy()\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 3.53 GiB for an array with shape (6, 78902593) and data type float64\n",
  "history_begin_time" : 1694321441151,
  "history_end_time" : 1694322427753,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "2DxS4gZcEhEK",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '64MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\ngridmet = gridmet.rename(columns={'day': 'date'})\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'day'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\nprint('merge completed.')\n",
  "history_output" : "today date = 2023-09-10\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/2DxS4gZcEhEK/merge_custom_traning_range.py\", line 23, in <module>\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'day'], how='outer')\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 726, in merge\n    return hash_join(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 400, in hash_join\n    meta = _lhs_meta.merge(_rhs_meta, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/frame.py\", line 10093, in merge\n    return merge(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 110, in merge\n    op = _MergeOperation(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 703, in __init__\n    ) = self._get_merge_keys()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 1162, in _get_merge_keys\n    right_keys.append(right._get_label_or_level_values(rk))\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 1850, in _get_label_or_level_values\n    raise KeyError(key)\nKeyError: 'day'\n",
  "history_begin_time" : 1694321418010,
  "history_end_time" : 1694321426134,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "0HYiKf2yAKHJ",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '64MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\nprint('merge completed.')\n",
  "history_output" : "today date = 2023-09-10\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/0HYiKf2yAKHJ/merge_custom_traning_range.py\", line 22, in <module>\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 726, in merge\n    return hash_join(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 400, in hash_join\n    meta = _lhs_meta.merge(_rhs_meta, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/frame.py\", line 10093, in merge\n    return merge(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 110, in merge\n    op = _MergeOperation(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 703, in __init__\n    ) = self._get_merge_keys()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 1162, in _get_merge_keys\n    right_keys.append(right._get_label_or_level_values(rk))\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 1850, in _get_label_or_level_values\n    raise KeyError(key)\nKeyError: 'date'\n",
  "history_begin_time" : 1694321143879,
  "history_end_time" : 1694321158497,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "CkdvHh5Sq2YZ",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '64MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\nprint('merge completed.')\n",
  "history_output" : "today date = 2023-09-09\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 136, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 761, in read\n    return read_pandas(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 561, in read_pandas\n    b_out = read_bytes(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/bytes/core.py\", line 111, in read_bytes\n    size = fs.info(path)[\"size\"]\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/fsspec/implementations/local.py\", line 87, in info\n    out = os.stat(path, follow_symlinks=False)\nFileNotFoundError: [Errno 2] No such file or directory: '/home/chetana/gridmet_test_run/gridmet_climatology/testing_ready_gridmet.csv'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/CkdvHh5Sq2YZ/merge_custom_traning_range.py\", line 11, in <module>\n    gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 138, in wrapper\n    raise type(e)(\nFileNotFoundError: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: [Errno 2] No such file or directory: '/home/chetana/gridmet_test_run/gridmet_climatology/testing_ready_gridmet.csv'\n",
  "history_begin_time" : 1694296537295,
  "history_end_time" : 1694296539799,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "utrQ6i3XRugd",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '64MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\nprint('merge completed.')\n",
  "history_output" : "today date = 2023-09-09\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 136, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 761, in read\n    return read_pandas(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 561, in read_pandas\n    b_out = read_bytes(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/bytes/core.py\", line 111, in read_bytes\n    size = fs.info(path)[\"size\"]\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/fsspec/implementations/local.py\", line 87, in info\n    out = os.stat(path, follow_symlinks=False)\nFileNotFoundError: [Errno 2] No such file or directory: '/home/chetana/gridmet_test_run/gridmet_climatology/testing_ready_gridmet.csv'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/utrQ6i3XRugd/merge_custom_traning_range.py\", line 11, in <module>\n    gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 138, in wrapper\n    raise type(e)(\nFileNotFoundError: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: [Errno 2] No such file or directory: '/home/chetana/gridmet_test_run/gridmet_climatology/testing_ready_gridmet.csv'\n",
  "history_begin_time" : 1694296461122,
  "history_end_time" : 1694296463617,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "YudcKdFOZRFm",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '64MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\nprint('merge completed.')\n",
  "history_output" : "today date = 2023-09-09\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 136, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 761, in read\n    return read_pandas(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 561, in read_pandas\n    b_out = read_bytes(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/bytes/core.py\", line 111, in read_bytes\n    size = fs.info(path)[\"size\"]\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/fsspec/implementations/local.py\", line 87, in info\n    out = os.stat(path, follow_symlinks=False)\nFileNotFoundError: [Errno 2] No such file or directory: '/home/chetana/gridmet_test_run/gridmet_climatology/testing_ready_gridmet.csv'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/YudcKdFOZRFm/merge_custom_traning_range.py\", line 11, in <module>\n    gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 138, in wrapper\n    raise type(e)(\nFileNotFoundError: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: [Errno 2] No such file or directory: '/home/chetana/gridmet_test_run/gridmet_climatology/testing_ready_gridmet.csv'\n",
  "history_begin_time" : 1694296438258,
  "history_end_time" : 1694296441348,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "BgTtprmmDSMl",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '64MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\nprint('merge completed.')\n",
  "history_output" : null,
  "history_begin_time" : 1694296431475,
  "history_end_time" : null,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : null
},{
  "history_id" : "prD3BEUypldj",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '64MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\n",
  "history_begin_time" : 1692192092321,
  "history_end_time" : 1692195042963,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "yEUYg2Nb9Fk5",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '64MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\n#amsr = amsr.repartition(partition_size=chunk_size)\n#snotel = snotel.repartition(partition_size=chunk_size)\n#gridmet = gridmet.repartition(partition_size=chunk_size)\n#terrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False, compute=False)\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\n",
  "history_begin_time" : 1692192040184,
  "history_end_time" : 1692192043025,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "z5SFly4L0uNd",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '64MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the merged DataFrame to a CSV file in chunks\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False, compute=False, chunksize=chunk_size)\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\n",
  "history_begin_time" : 1692191994169,
  "history_end_time" : 1692192009120,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "YTF8A77bPqgj",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '64MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the final merged DataFrame to a CSV file\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\n\n# Compute and persist the merged DataFrame\nmerged_df = merged_df.persist()\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\nsh: line 1:  7881 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python merge_custom_traning_range.py\n",
  "history_begin_time" : 1692191556216,
  "history_end_time" : 1692191717071,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "5VRYBoVkmR4e",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '64MB'  # You can adjust this chunk size based on your hardware and data size\n\n# Read the CSV files with a smaller chunk size and compression\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size, compression='gzip')\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size, compression='gzip')\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size, compression='gzip')\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size, compression='gzip')\n\n# Repartition DataFrames for optimized processing\namsr = amsr.repartition(partition_size=chunk_size)\nsnotel = snotel.repartition(partition_size=chunk_size)\ngridmet = gridmet.repartition(partition_size=chunk_size)\nterrain = terrain.repartition(partition_size=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n# Save the final merged DataFrame to a CSV file\noutput_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\nmerged_df.to_csv(output_file, single_file=True, index=False)\n\n# Compute and persist the merged DataFrame\nmerged_df = merged_df.persist()\n\n# Perform your computations on the persisted merged DataFrame\n# ...\n\n# Optionally, you can also call merged_df.compute() to trigger the computation and wait for it to finish.\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\n/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py:544: UserWarning: Warning gzip compression does not support breaking apart files\nPlease ensure that each individual file can fit in memory and\nuse the keyword ``blocksize=None to remove this message``\nSetting ``blocksize=None``\n  warn(\nTraceback (most recent call last):\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 136, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 761, in read\n    return read_pandas(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 561, in read_pandas\n    b_out = read_bytes(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/bytes/core.py\", line 173, in read_bytes\n    sample_buff = f.read(sample)\n  File \"/home/chetana/anaconda3/lib/python3.9/gzip.py\", line 300, in read\n    return self._buffer.read(size)\n  File \"/home/chetana/anaconda3/lib/python3.9/_compression.py\", line 68, in readinto\n    data = self.read(len(byte_view))\n  File \"/home/chetana/anaconda3/lib/python3.9/gzip.py\", line 487, in read\n    if not self._read_gzip_header():\n  File \"/home/chetana/anaconda3/lib/python3.9/gzip.py\", line 435, in _read_gzip_header\n    raise BadGzipFile('Not a gzipped file (%r)' % magic)\ngzip.BadGzipFile: Not a gzipped file (b'da')\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/5VRYBoVkmR4e/merge_custom_traning_range.py\", line 9, in <module>\n    amsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size, compression='gzip')\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 138, in wrapper\n    raise type(e)(\ngzip.BadGzipFile: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: Not a gzipped file (b'da')\n",
  "history_begin_time" : 1692191461468,
  "history_end_time" : 1692191471476,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "dLs0mseIX2HS",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\n# Define a custom chunk size based on available memory\nchunk_size = '32 MiB'\n\n# Create a temporary directory for Dask to spill data\ntemp_dir = os.path.join(working_dir, 'dask_temp')\nos.makedirs(temp_dir, exist_ok=True)\n\n# Read the CSV files with a smaller chunk size\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size, usecols=['lat', 'lon', 'date', 'SWE', 'Flag'])  # Select necessary columns\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size, usecols=['lat', 'lon', 'date', 'swe_value'])\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Merge DataFrames based on specified columns and save intermediate results\nmerged_df = amsr.merge(snotel, on=['lat', 'lon', 'date'], how='inner')\nmerged_df.to_csv(os.path.join(temp_dir, 'merged_amsr_snotel.csv'), index=False, single_file=True)\n\nmerged_df = merged_df.merge(gridmet, on=['lat', 'lon', 'date'], how='inner')\nmerged_df.to_csv(os.path.join(temp_dir, 'merged_gridmet.csv'), index=False, single_file=True)\n\n# Drop unnecessary columns\n#merged_df = merged_df.drop(['unnecessary_col1', 'unnecessary_col2'], axis=1)\n\nmerged_df = merged_df.merge(terrain, on=['lat', 'lon'], how='inner')\nmerged_df.to_csv(os.path.join(temp_dir, 'merged_final_3_yrs.csv'), index=False, single_file=True)\n\n# Drop duplicates based on all columns\nmerged_df = merged_df.drop_duplicates()\n\n# Save the final merged DataFrame to a CSV file\noutput_csv = f'{working_dir}/merged_data_3_yrs.csv'\nmerged_df.to_csv(output_csv, index=False, single_file=True)\n\nprint(f\"Merged data (without duplicates) saved to {output_csv}\")\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/dLs0mseIX2HS/merge_custom_traning_range.py\", line 31, in <module>\n    merged_df.to_csv(os.path.join(temp_dir, 'merged_final_3_yrs.csv'), index=False, single_file=True)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 994, in to_csv\n    return list(dask.compute(*values, **compute_kwargs))\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/threaded.py\", line 89, in get\n    results = get_async(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 511, in get_async\n    raise_exception(exc, tb)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 319, in reraise\n    raise exc\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 224, in execute_task\n    result = _execute_task(task, data)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 289, in merge_chunk\n    out = lhs.merge(rhs, *args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/concat.py\", line 212, in concatenate_managers\n    values = values.copy()\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 5.83 GiB for an array with shape (6, 130429334) and data type float64\n",
  "history_begin_time" : 1692190428106,
  "history_end_time" : 1692190778027,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "98zU5ujLxotA",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\n# Define a custom chunk size based on available memory\nchunk_size = '32 MiB'\n\n# Create a temporary directory for Dask to spill data\ntemp_dir = os.path.join(working_dir, 'dask_temp')\nos.makedirs(temp_dir, exist_ok=True)\n\n# Read the CSV files with a smaller chunk size\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Merge DataFrames based on specified columns and save intermediate results\nmerged_df = amsr.merge(snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df.to_csv(os.path.join(temp_dir, 'merged_amsr_snotel.csv'), index=False, single_file=True)\n\nmerged_df = merged_df.merge(gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df.to_csv(os.path.join(temp_dir, 'merged_gridmet.csv'), index=False, single_file=True)\n\nmerged_df = merged_df.merge(terrain, on=['lat', 'lon'], how='outer')\nmerged_df.to_csv(os.path.join(temp_dir, 'merged_final.csv'), index=False, single_file=True)\n\n# Drop duplicates based on all columns\nmerged_df = merged_df.drop_duplicates()\n\n# Save the final merged DataFrame to a CSV file\noutput_csv = f'{working_dir}/merged_data_3_yrs.csv'\nmerged_df.to_csv(output_csv, index=False, single_file=True)\n\n\nprint(f\"Merged data (without duplicates) saved to {output_csv}\")",
  "history_output" : "today date = 2023-08-16\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/98zU5ujLxotA/merge_custom_traning_range.py\", line 28, in <module>\n    merged_df.to_csv(os.path.join(temp_dir, 'merged_final.csv'), index=False, single_file=True)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 994, in to_csv\n    return list(dask.compute(*values, **compute_kwargs))\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/threaded.py\", line 89, in get\n    results = get_async(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 511, in get_async\n    raise_exception(exc, tb)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 319, in reraise\n    raise exc\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 224, in execute_task\n    result = _execute_task(task, data)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 289, in merge_chunk\n    out = lhs.merge(rhs, *args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/concat.py\", line 212, in concatenate_managers\n    values = values.copy()\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 4.75 GiB for an array with shape (6, 106348126) and data type float64\nsh: line 1:  3289 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python merge_custom_traning_range.py\n",
  "history_begin_time" : 1692188911679,
  "history_end_time" : 1692189475914,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "v3PiC7nthWQ1",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\n# Define a custom chunk size based on available memory\nchunk_size = '32 MiB'\n\n# Create a temporary directory for Dask to spill data\ntemp_dir = os.path.join(working_dir, 'dask_temp')\nos.makedirs(temp_dir, exist_ok=True)\n\n# Read the CSV files with a smaller chunk size\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Merge DataFrames based on specified columns and save intermediate results\nmerged_df = amsr.merge(snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df.to_csv(os.path.join(temp_dir, 'merged_amsr_snotel.csv'), index=False, single_file=True)\n\nmerged_df = merged_df.merge(gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df.to_csv(os.path.join(temp_dir, 'merged_gridmet.csv'), index=False, single_file=True)\n\nmerged_df = merged_df.merge(terrain, on=['lat', 'lon'], how='outer')\nmerged_df.to_csv(os.path.join(temp_dir, 'merged_final.csv'), index=False, single_file=True)\n\n# Drop duplicates based on all columns\nmerged_df = merged_df.drop_duplicates()\n\n# Save the final merged DataFrame to a CSV file\noutput_csv = f'{working_dir}/merged_data_3_yrs.csv'\nmerged_df.to_csv(output_csv, index=False, single_file=True)\n\n\nprint(f\"Merged data (without duplicates) saved to {output_csv}\")",
  "history_output" : "today date = 2023-08-16\n/home/chetana\nsh: line 1: 29909 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python merge_custom_traning_range.py\n",
  "history_begin_time" : 1692166850136,
  "history_end_time" : 1692167287628,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "XOONN2jgWhub",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\n# Define a custom chunk size based on available memory\nchunk_size = '256 MiB'\n\n# Create a temporary directory for Dask to spill data\ntemp_dir = os.path.join(working_dir, 'dask_temp')\nos.makedirs(temp_dir, exist_ok=True)\n\n# Read the CSV files with a smaller chunk size\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Define the order in which DataFrames are merged\nmerge_order = [(amsr, snotel), (gridmet, terrain)]\n\n# Iterate over pairs of DataFrames and save intermediate results\nmerged_df = None\nfor left, right in merge_order:\n    if merged_df is None:\n        merged_df = left.merge(right, on=['lat', 'lon', 'date'], how='outer')\n    else:\n        temp_merged = left.merge(right, on=['lat', 'lon', 'date'], how='outer')\n        temp_merged.to_csv(os.path.join(temp_dir, 'merged_temp.csv'), index=False, single_file=True)\n        \n        # Drop duplicates based on all columns\n        temp_merged = temp_merged.drop_duplicates()\n        temp_merged.to_csv(os.path.join(temp_dir, 'merged_temp_deduped.csv'), index=False, single_file=True)\n        \n        # Continue merging with existing merged_df\n        merged_df = merged_df.merge(temp_merged, on=['lat', 'lon', 'date'], how='outer')\n        merged_df = merged_df.drop_duplicates()\n        \n        # Remove intermediate files if needed\n        os.remove(os.path.join(temp_dir, 'merged_temp.csv'))\n        os.remove(os.path.join(temp_dir, 'merged_temp_deduped.csv'))\n\n# Save the final merged DataFrame to a CSV file\noutput_csv = f'{working_dir}/merged_data_3_yrs.csv'\nmerged_df.to_csv(output_csv, index=False, single_file=True)\n\nprint(f\"Merged data (without duplicates) saved to {output_csv}\")\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/XOONN2jgWhub/merge_custom_traning_range.py\", line 29, in <module>\n    temp_merged = left.merge(right, on=['lat', 'lon', 'date'], how='outer')\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/core.py\", line 5691, in merge\n    return merge(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 726, in merge\n    return hash_join(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 400, in hash_join\n    meta = _lhs_meta.merge(_rhs_meta, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/frame.py\", line 10093, in merge\n    return merge(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 110, in merge\n    op = _MergeOperation(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 703, in __init__\n    ) = self._get_merge_keys()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 1162, in _get_merge_keys\n    right_keys.append(right._get_label_or_level_values(rk))\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 1850, in _get_label_or_level_values\n    raise KeyError(key)\nKeyError: 'date'\n",
  "history_begin_time" : 1692166789688,
  "history_end_time" : 1692166801037,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "UcYEMnaQHqTr",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\n# Define a custom chunk size based on available memory\nchunk_size = '256 MiB'\n\n# Create a temporary directory for Dask to spill data\ntemp_dir = os.path.join(working_dir, 'dask_temp')\nos.makedirs(temp_dir, exist_ok=True)\n\n# Read the CSV files with a smaller chunk size\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Merge DataFrames based on specified columns and save intermediate results\nmerged_df = amsr.merge(snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df.to_csv(os.path.join(temp_dir, 'merged_amsr_snotel.csv'), index=False, single_file=True)\n\nmerged_df = merged_df.merge(gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df.to_csv(os.path.join(temp_dir, 'merged_gridmet.csv'), index=False, single_file=True)\n\nmerged_df = merged_df.merge(terrain, on=['lat', 'lon'], how='outer')\nmerged_df.to_csv(os.path.join(temp_dir, 'merged_final.csv'), index=False, single_file=True)\n\n# Drop duplicates based on all columns\nmerged_df = merged_df.drop_duplicates()\n\n# Save the final merged DataFrame to a CSV file\noutput_csv = f'{working_dir}/merged_data_3_yrs.csv'\nmerged_df.to_csv(output_csv, index=False, single_file=True)\n\n\nprint(f\"Merged data (without duplicates) saved to {output_csv}\")\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/UcYEMnaQHqTr/merge_custom_traning_range.py\", line 28, in <module>\n    merged_df.to_csv(os.path.join(temp_dir, 'merged_final.csv'), index=False, single_file=True)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 994, in to_csv\n    return list(dask.compute(*values, **compute_kwargs))\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/threaded.py\", line 89, in get\n    results = get_async(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 511, in get_async\n    raise_exception(exc, tb)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 319, in reraise\n    raise exc\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 224, in execute_task\n    result = _execute_task(task, data)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 289, in merge_chunk\n    out = lhs.merge(rhs, *args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/concat.py\", line 212, in concatenate_managers\n    values = values.copy()\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 13.4 GiB for an array with shape (7, 257808468) and data type float64\n",
  "history_begin_time" : 1692165824781,
  "history_end_time" : 1692166243884,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ORzb67mrRIVZ",
  "history_input" : "import dask.dataframe as dd\nimport dask.config\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\n# Define a custom chunk size based on available memory\nchunk_size = '256 MiB'\n\n# Define memory limit (adjust as needed)\nmemory_limit = '40GB'  # Set an appropriate value based on available RAM\n\n# Set the memory limit in Dask's configuration\ndask.config.set({'io': {'pandas': {'memory_limit': memory_limit}}})\n\n# Create a temporary directory for Dask to spill data\ntemp_dir = os.path.join(working_dir, 'dask_temp')\nos.makedirs(temp_dir, exist_ok=True)\n\n# Read the CSV files with a smaller chunk size\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = amsr.merge(snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = merged_df.merge(gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = merged_df.merge(terrain, on=['lat', 'lon'], how='outer')\n\n# Drop duplicates based on all columns\nmerged_df = merged_df.drop_duplicates()\n\n# Save the merged DataFrame to a CSV file\noutput_csv = f'{working_dir}/merged_data_3_yrs.csv'\nmerged_df.to_csv(output_csv, index=False, single_file=True)\n\n# Compute and wait for the computation to complete\nmerged_df.compute()\n\nprint(f\"Merged data (without duplicates) saved to {output_csv}\")\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/ORzb67mrRIVZ/merge_custom_traning_range.py\", line 37, in <module>\n    merged_df.to_csv(output_csv, index=False, single_file=True)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 994, in to_csv\n    return list(dask.compute(*values, **compute_kwargs))\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/threaded.py\", line 89, in get\n    results = get_async(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 511, in get_async\n    raise_exception(exc, tb)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 319, in reraise\n    raise exc\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 224, in execute_task\n    result = _execute_task(task, data)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 289, in merge_chunk\n    out = lhs.merge(rhs, *args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/concat.py\", line 212, in concatenate_managers\n    values = values.copy()\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 13.4 GiB for an array with shape (7, 257808468) and data type float64\n",
  "history_begin_time" : 1692165487824,
  "history_end_time" : 1692165737157,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "jxQ8EbyxJPQO",
  "history_input" : "import dask.dataframe as dd\nimport dask.config\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\n# Define a custom chunk size based on available memory\nchunk_size = '256 MiB'\n\n# Define memory limit (adjust as needed)\nmemory_limit = '40GB'  # Set an appropriate value based on available RAM\n\n# Set the memory limit in Dask's configuration\ndask.config.set({'io': {'pandas': {'memory_limit': memory_limit}}})\n\n# Create a temporary directory for Dask to spill data\ntemp_dir = os.path.join(working_dir, 'dask_temp')\nos.makedirs(temp_dir, exist_ok=True)\n\n# Read the CSV files with a smaller chunk size\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n# Merge DataFrames based on specified columns\nmerged_df = amsr.merge(snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = merged_df.merge(gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = merged_df.merge(terrain, on=['lat', 'lon'], how='outer')\n\n# Drop duplicates based on all columns\nmerged_df = merged_df.drop_duplicates()\n\n# Save the merged DataFrame to a CSV file\noutput_csv = f'{working_dir}/merged_data.csv'\nmerged_df.to_csv(output_csv, index=False)\n\n# Compute and wait for the computation to complete\nmerged_df.compute()\n\nprint(f\"Merged data (without duplicates) saved to {output_csv}\")\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/jxQ8EbyxJPQO/merge_custom_traning_range.py\", line 37, in <module>\n    merged_df.to_csv(output_csv, index=False)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/core.py\", line 1856, in to_csv\n    return to_csv(self, filename, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 952, in to_csv\n    files = open_files(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/fsspec/core.py\", line 293, in open_files\n    [fs.makedirs(parent, exist_ok=True) for parent in parents]\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/fsspec/core.py\", line 293, in <listcomp>\n    [fs.makedirs(parent, exist_ok=True) for parent in parents]\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/fsspec/implementations/local.py\", line 54, in makedirs\n    os.makedirs(path, exist_ok=exist_ok)\n  File \"/home/chetana/anaconda3/lib/python3.9/os.py\", line 225, in makedirs\n    mkdir(name, mode)\nFileExistsError: [Errno 17] File exists: '/home/chetana/gridmet_test_run/merged_data.csv'\n",
  "history_begin_time" : 1692165416567,
  "history_end_time" : 1692165420705,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "vXN3L8DGMjFA",
  "history_input" : "import dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\n# Define a custom chunk size based on available memory\nchunk_size = '256 MiB'\n\n# Define memory limit (adjust as needed)\nmemory_limit = '40GB'  # Set an appropriate value based on available RAM\n\n# Create a temporary directory for Dask to spill data\ntemp_dir = os.path.join(working_dir, 'dask_temp')\nos.makedirs(temp_dir, exist_ok=True)\n\n# Read the CSV files with memory limit and temporary directory\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size, memory_limit=memory_limit, temporary_directory=temp_dir)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size, memory_limit=memory_limit, temporary_directory=temp_dir)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size, memory_limit=memory_limit, temporary_directory=temp_dir)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size, memory_limit=memory_limit, temporary_directory=temp_dir)\n\n# Merge DataFrames based on specified columns\nmerged_df = amsr.merge(snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = merged_df.merge(gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = merged_df.merge(terrain, on=['lat', 'lon'], how='outer')\n\n# Drop duplicates based on all columns\nmerged_df = merged_df.drop_duplicates()\n\n# Save the merged DataFrame to a CSV file\noutput_csv = f'{working_dir}/merged_data.csv'\nmerged_df.to_csv(output_csv, index=False)\n\n# Compute and wait for the computation to complete\n#merged_df.compute()\n\nprint(f\"Merged data (without duplicates) saved to {output_csv}\")\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 136, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 761, in read\n    return read_pandas(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 630, in read_pandas\n    head = reader(BytesIO(b_sample), nrows=sample_rows, **head_kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 211, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\nTypeError: read_csv() got an unexpected keyword argument 'memory_limit'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/vXN3L8DGMjFA/merge_custom_traning_range.py\", line 18, in <module>\n    amsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size, memory_limit=memory_limit, temporary_directory=temp_dir)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 138, in wrapper\n    raise type(e)(\nTypeError: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: read_csv() got an unexpected keyword argument 'memory_limit'\n",
  "history_begin_time" : 1692165336940,
  "history_end_time" : 1692165340408,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "p2TRG13I7tgn",
  "history_input" : "import dask.dataframe as dd\nimport os\n\nworking_dir = '.'\n\n# Define a custom chunk size based on available memory\nchunk_size = '256 MiB'\n\n# Define memory limit (adjust as needed)\nmemory_limit = '40GB'  # Set an appropriate value based on available RAM\n\n# Create a temporary directory for Dask to spill data\ntemp_dir = os.path.join(working_dir, 'dask_temp')\nos.makedirs(temp_dir, exist_ok=True)\n\n# Read the CSV files with memory limit and temporary directory\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size, memory_limit=memory_limit, temporary_directory=temp_dir)\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size, memory_limit=memory_limit, temporary_directory=temp_dir)\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv', blocksize=chunk_size, memory_limit=memory_limit, temporary_directory=temp_dir)\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size, memory_limit=memory_limit, temporary_directory=temp_dir)\n\n# Merge DataFrames based on specified columns\nmerged_df = amsr.merge(snotel, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = merged_df.merge(gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = merged_df.merge(terrain, on=['lat', 'lon'], how='outer')\n\n# Drop duplicates based on all columns\nmerged_df = merged_df.drop_duplicates()\n\n# Save the merged DataFrame to a CSV file\noutput_csv = f'{working_dir}/merged_data.csv'\nmerged_df.to_csv(output_csv, index=False)\n\n# Compute and wait for the computation to complete\n#merged_df.compute()\n\nprint(f\"Merged data (without duplicates) saved to {output_csv}\")\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 136, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 761, in read\n    return read_pandas(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 561, in read_pandas\n    b_out = read_bytes(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/bytes/core.py\", line 111, in read_bytes\n    size = fs.info(path)[\"size\"]\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/fsspec/implementations/local.py\", line 87, in info\n    out = os.stat(path, follow_symlinks=False)\nFileNotFoundError: [Errno 2] No such file or directory: '/home/chetana/gw-workspace/p2TRG13I7tgn/training_ready_amsr_3_yrs.csv'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/p2TRG13I7tgn/merge_custom_traning_range.py\", line 17, in <module>\n    amsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size, memory_limit=memory_limit, temporary_directory=temp_dir)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 138, in wrapper\n    raise type(e)(\nFileNotFoundError: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: [Errno 2] No such file or directory: '/home/chetana/gw-workspace/p2TRG13I7tgn/training_ready_amsr_3_yrs.csv'\n",
  "history_begin_time" : 1692165292926,
  "history_end_time" : 1692165302501,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "tmc072vE02pu",
  "history_input" : "import dask.dataframe as dd\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\n# Read the CSV files\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv')\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv')\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv')\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv')\n\n# Merge DataFrames based on specified columns\nmerged_df = snotel.merge(amsr, on=['lat', 'lon', 'date'], how='outer')\nprint('completed snotel <-> amsr merge')\nmerged_df = merged_df.merge(gridmet, on=['lat', 'lon', 'date'], how='outer')\nprint('completed snotel_amsr <-> gridmet merge')\nmerged_df = merged_df.merge(terrain, on=['lat', 'lon'], how='outer')\nprint('compelted snotel_amsr_grimet <-> terrian merge')\n\n# Drop duplicates based on all columns\nmerged_df = merged_df.drop_duplicates()\n\n# Save the merged DataFrame to a CSV file\noutput_csv = f'{working_dir}/merged_data_3_yrs.csv'\nmerged_df.to_csv(output_csv, index=False)\n\n# Compute and wait for the computation to complete (if needed)\nmerged_df.compute()\n\nprint(f\"Merged data (without duplicates) saved to {output_csv}\")\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\ncompleted snotel <-> amsr merge\ncompleted snotel_amsr <-> gridmet merge\ncompelted snotel_amsr_grimet <-> terrian merge\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/tmc072vE02pu/merge_custom_traning_range.py\", line 25, in <module>\n    merged_df.to_csv(output_csv, index=False)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 994, in to_csv\n    return list(dask.compute(*values, **compute_kwargs))\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/threaded.py\", line 89, in get\n    results = get_async(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 511, in get_async\n    raise_exception(exc, tb)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 319, in reraise\n    raise exc\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 224, in execute_task\n    result = _execute_task(task, data)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 289, in merge_chunk\n    out = lhs.merge(rhs, *args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/concat.py\", line 212, in concatenate_managers\n    values = values.copy()\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 11.5 GiB for an array with shape (6, 257808468) and data type float64\n",
  "history_begin_time" : 1692164750384,
  "history_end_time" : 1692165144917,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Qjb9777yKfq4",
  "history_input" : "import dask.dataframe as dd\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\n# Read the CSV files\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv')\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv')\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv')\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv')\n\n# Merge DataFrames based on specified columns\nmerged_df = snotel.merge(amsr, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = merged_df.merge(gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = merged_df.merge(terrain, on=['lat', 'lon'], how='outer')\n\n# Drop duplicates based on all columns\nmerged_df = merged_df.drop_duplicates()\n\n# Save the merged DataFrame to a CSV file\noutput_csv = f'{working_dir}/merged_data_3_yrs.csv'\nmerged_df.to_csv(output_csv, index=False)\n\n# Compute and wait for the computation to complete (if needed)\nmerged_df.compute()\n\nprint(f\"Merged data (without duplicates) saved to {output_csv}\")\n",
  "history_output" : null,
  "history_begin_time" : 1692164667924,
  "history_end_time" : null,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : null
},{
  "history_id" : "pR1xj0UHuGsc",
  "history_input" : "import dask.dataframe as dd\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\n# Read the CSV files\namsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv')\nsnotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv')\ngridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/testing_ready_gridmet.csv')\nterrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv')\n\n# Merge DataFrames based on specified columns\nmerged_df = snotel.merge(amsr, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = merged_df.merge(gridmet, on=['lat', 'lon', 'date'], how='outer')\nmerged_df = merged_df.merge(terrain, on=['lat', 'lon'], how='outer')\n\n# Drop duplicates based on all columns\nmerged_df = merged_df.drop_duplicates()\n\n# Save the merged DataFrame to a CSV file\noutput_csv = f'{working_dir}/merged_data_3_yrs.csv'\nmerged_df.to_csv(output_csv, index=False)\n\n# Compute and wait for the computation to complete (if needed)\nmerged_df.compute()\n\nprint(f\"Merged data (without duplicates) saved to {output_csv}\")\n",
  "history_output" : "today date = 2023-08-16\n/home/chetana\n",
  "history_begin_time" : 1692164620925,
  "history_end_time" : 1692164667961,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "t8cbvf2kbq0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531335834,
  "history_end_time" : 1691531335834,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "sxwmabed15w",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531292844,
  "history_end_time" : 1691531292844,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "885xpzpe1da",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531254771,
  "history_end_time" : 1691531284904,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "2ekosjovj9k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531163961,
  "history_end_time" : 1691531163961,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "4791ge4k1tq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531121014,
  "history_end_time" : 1691531121014,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "8n6w2cjsj7r",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531061019,
  "history_end_time" : 1691531061019,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "kroadp4thod",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530848386,
  "history_end_time" : 1691530848386,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "q087n8t57q4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717766,
  "history_end_time" : 1691530721109,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "vb9ks58sr66",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530690280,
  "history_end_time" : 1691530716753,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "77dcsormpmx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530621141,
  "history_end_time" : 1691530622443,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "li45n4zcf64",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530617330,
  "history_end_time" : 1691530617330,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "etfp4vl7p5r",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530599904,
  "history_end_time" : 1691530614288,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "QkMR0GC9egfS",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask instead of Pandas\n#snotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv')\n#gridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv')\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv')\n#terrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv')\n\n# Perform the merges using Dask DataFrames\n#snotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\n#merged_data = dd.merge(snotel_gridmet, terrain, on=['lat', 'lon'], how='inner')\n#merged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Compute the result and convert to Pandas DataFrame if needed\n#merged_data.to_csv('/home/chetana/gridmet_test_run/training_snotel_gridmet_3_yrs.csv', index=False, single_file=True)\n\nsnotel_gridmet_merged = dd.read_csv('/home/chetana/gridmet_test_run/training_snotel_gridmet_3_yrs.csv', usecols=['date', 'SWE', 'lat', 'lon', 'etr', 'pr',\n       'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'elevation', 'slope',\n       'curvature', 'aspect', 'eastness', 'northness'])\n\nmerged_data = dd.merge(snotel_gridmet_merged, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = merged_data.drop_duplicates()\nmerged_data.to_csv('/home/chetana/gridmet_test_run/training_ready_3_yr_data.csv', single_file=True, index=False)\n\n# Now you have the merged data as a Pandas DataFrame\n",
  "history_output" : "",
  "history_begin_time" : 1691384668402,
  "history_end_time" : 1691384954294,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "MsrbHDXjpeUV",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask instead of Pandas\n#snotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv')\n#gridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv')\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv')\n#terrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv')\n\n# Perform the merges using Dask DataFrames\n#snotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\n#merged_data = dd.merge(snotel_gridmet, terrain, on=['lat', 'lon'], how='inner')\n#merged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Compute the result and convert to Pandas DataFrame if needed\n#merged_data.to_csv('/home/chetana/gridmet_test_run/training_snotel_gridmet_3_yrs.csv', index=False, single_file=True)\n\nsnotel_gridmet_merged = dd.read_csv('/home/chetana/gridmet_test_run/training_snotel_gridmet_3_yrs.csv', usecols=['date', 'SWE', 'lat', 'lon', 'etr', 'pr',\n       'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'elevation', 'slope',\n       'curvature', 'aspect', 'eastness', 'northness'])\n\nmerged_data = dd.merge(snotel_gridmet_merged, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = merged_data.drop_duplicates()\nmerged_data.to_csv('/home/chetana/gridmet_test_run/training_ready_2_yr_data.csv', single_file=True, index=False)\n\n# Now you have the merged data as a Pandas DataFrame\n",
  "history_output" : "",
  "history_begin_time" : 1691383717686,
  "history_end_time" : 1691384053108,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "gQpWBeTGfVYl",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask instead of Pandas\n#snotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv')\n#gridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv')\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv')\n#terrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv')\n\n# Perform the merges using Dask DataFrames\n#snotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\n#merged_data = dd.merge(snotel_gridmet, terrain, on=['lat', 'lon'], how='inner')\n#merged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Compute the result and convert to Pandas DataFrame if needed\n#merged_data.to_csv('/home/chetana/gridmet_test_run/training_snotel_gridmet_3_yrs.csv', index=False, single_file=True)\n\nsnotel_gridmet_merged = dd.read_csv('/home/chetana/gridmet_test_run/training_snotel_gridmet_3_yrs.csv', usecols=['date', 'SWE', 'lat', 'lon', 'etr', 'pr',\n       'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'elevation', 'slope',\n       'curvature', 'aspect', 'eastness', 'northness'])\n\nmerged_data = dd.merge(snotel_gridmet_merged, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data.to_csv('/home/chetana/gridmet_test_run/training_final_3_yr_training_ready.csv', single_file=True, index=False)\n\n# Now you have the merged data as a Pandas DataFrame\n",
  "history_output" : "",
  "history_begin_time" : 1691368736436,
  "history_end_time" : 1691372133466,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Ht5XX8CxhstS",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask instead of Pandas\n#snotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv')\n#gridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv')\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv')\n#terrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv')\n\n# Perform the merges using Dask DataFrames\n#snotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\n#merged_data = dd.merge(snotel_gridmet, terrain, on=['lat', 'lon'], how='inner')\n#merged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Compute the result and convert to Pandas DataFrame if needed\n#merged_data.to_csv('/home/chetana/gridmet_test_run/training_snotel_gridmet_3_yrs.csv', index=False, single_file=True)\n\nsnotel_gridmet_merged = dd.read_csv('/home/chetana/gridmet_test_run/training_snotel_gridmet_3_yrs.csv', usecols=['date', 'SWE', 'lat', 'lon', 'etr', 'pr',\n       'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'elevation', 'slope',\n       'curvature', 'aspect', 'eastness', 'northness'])\n\nmerged_data = dd.merge(snotel_gridmet_merged, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data.to_csv('/home/chetana/gridmet_test_run/training_final_3_yr_training_ready.csv', single_file=True, index=False)\n\n# Now you have the merged data as a Pandas DataFrame\n",
  "history_output" : null,
  "history_begin_time" : 1691368727994,
  "history_end_time" : null,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : null
},{
  "history_id" : "Fm233HyAlNE4",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask instead of Pandas\n#snotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv')\n#gridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv')\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv')\n#terrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv')\n\n# Perform the merges using Dask DataFrames\n#snotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\n#merged_data = dd.merge(snotel_gridmet, terrain, on=['lat', 'lon'], how='inner')\n#merged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Compute the result and convert to Pandas DataFrame if needed\n#merged_data.to_csv('/home/chetana/gridmet_test_run/training_snotel_gridmet_3_yrs.csv', index=False, single_file=True)\n\nsnotel_gridmet_merged = dd.read_csv('/home/chetana/gridmet_test_run/training_snotel_gridmet_3_yrs.csv')\n\nmerged_data = dd.merge(snotel_gridmet_merged, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data.to_csv('/home/chetana/gridmet_test_run/training_final_3_yr_training_ready.csv', single_file=True, index=False)\n\n# Now you have the merged data as a Pandas DataFrame\n",
  "history_output" : "",
  "history_begin_time" : 1691365701482,
  "history_end_time" : 1691368728002,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "nyPtpPq6J27N",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask instead of Pandas\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv')\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv')\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv')\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv')\n\n# Perform the merges using Dask DataFrames\nsnotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(snotel_gridmet, terrain, on=['lat', 'lon'], how='inner')\n#merged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False, single_file=True)\n\n# Now you have the merged data as a Pandas DataFrame\n",
  "history_output" : "",
  "history_begin_time" : 1691363789027,
  "history_end_time" : 1691364194713,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "nEmUEAJFm7OZ",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask with a smaller blocksize and only necessary columns\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv')\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', usecols=['date', 'lat', 'lon', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs'])\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv')\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv')\n\n# Perform the merges using Dask DataFrames with all merge keys specified at once\nmerged_data = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner', suffixes=('_snotel', '_gridmet'))\nmerged_data = dd.merge(merged_data, amsr, on=['date', 'lat', 'lon'], how='inner', suffixes=('', '_amsr'))\nmerged_data = dd.merge(merged_data, terrain, on=['lat', 'lon'], how='inner', suffixes=('', '_terrain'))\n\n# Write intermediate result to disk in CSV format\nmerged_data.to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False, single_file=True)\n\n# The above line will execute lazily and data will spill to disk if RAM is insufficient\n# We don't need to use `persist` since the spill-to-disk mechanism will handle data storage\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/nEmUEAJFm7OZ/merge_custom_traning_range.py\", line 15, in <module>\n    merged_data.to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False, single_file=True)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 994, in to_csv\n    return list(dask.compute(*values, **compute_kwargs))\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/threaded.py\", line 89, in get\n    results = get_async(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 511, in get_async\n    raise_exception(exc, tb)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 319, in reraise\n    raise exc\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 224, in execute_task\n    result = _execute_task(task, data)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 289, in merge_chunk\n    out = lhs.merge(rhs, *args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/concat.py\", line 212, in concatenate_managers\n    values = values.copy()\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 5.44 GiB for an array with shape (6, 121609632) and data type float64\nsh: line 1: 13057 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python merge_custom_traning_range.py\n",
  "history_begin_time" : 1691363631859,
  "history_end_time" : 1691363768940,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "8BQ7efQSX58O",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask with a smaller blocksize and only necessary columns\nblocksize = \"16MB\"\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv')\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', usecols=['date', 'lat', 'lon', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs'])\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv')\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv')\n\n# Perform the merges using Dask DataFrames with all merge keys specified at once\nmerged_data = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(merged_data, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(merged_data, terrain, on=['lat', 'lon'], how='inner')\n\n# Persist intermediate result to disk if enough disk space is available\nmerged_data = merged_data.persist()\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/8BQ7efQSX58O/merge_custom_traning_range.py\", line 16, in <module>\n    merged_data = merged_data.persist()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/threaded.py\", line 89, in get\n    results = get_async(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 511, in get_async\n    raise_exception(exc, tb)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 319, in reraise\n    raise exc\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 224, in execute_task\n    result = _execute_task(task, data)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 289, in merge_chunk\n    out = lhs.merge(rhs, *args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/concat.py\", line 212, in concatenate_managers\n    values = values.copy()\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 5.44 GiB for an array with shape (6, 121756443) and data type float64\nsh: line 1: 12603 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python merge_custom_traning_range.py\n",
  "history_begin_time" : 1691363355002,
  "history_end_time" : 1691363507755,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "njbpUsRMRccq",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask with a smaller blocksize and only necessary columns\nblocksize = \"16MB\"\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=blocksize)\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs'])\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', blocksize=blocksize)\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv', blocksize=blocksize)\n\n# Perform the merges using Dask DataFrames with all merge keys specified at once\nmerged_data = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(merged_data, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(merged_data, terrain, on=['lat', 'lon'], how='inner')\n\n# Persist intermediate result to disk if enough disk space is available\nmerged_data = merged_data.persist()\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/njbpUsRMRccq/merge_custom_traning_range.py\", line 19, in <module>\n    merged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/dispatch.py\", line 68, in concat\n    return func(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/backends.py\", line 667, in concat_pandas\n    out = pd.concat(dfs3, join=join, sort=False)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"<__array_function__ internals>\", line 180, in concatenate\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 10.9 GiB for an array with shape (6, 243366075) and data type float64\n",
  "history_begin_time" : 1691362366332,
  "history_end_time" : 1691362534449,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "5Lk0smDI14C3",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask with a smaller blocksize and only necessary columns\nblocksize = \"16MB\"\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=blocksize)\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', blocksize=blocksize)\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', blocksize=blocksize)\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv', blocksize=blocksize)\n\n# Perform the merges using Dask DataFrames with all merge keys specified at once\nmerged_data = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(merged_data, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(merged_data, terrain, on=['lat', 'lon'], how='inner')\n\n# Persist intermediate result to disk if enough disk space is available\nmerged_data = merged_data.persist()\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False)\n",
  "history_output" : "sh: line 1:  9298 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python merge_custom_traning_range.py\n",
  "history_begin_time" : 1691361850300,
  "history_end_time" : 1691361958513,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "DqyyOBpyf5mO",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask with a smaller blocksize and only necessary columns\nblocksize = \"16MB\"\nusecols = ['date', 'lat', 'lon', 'other_needed_columns']\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=blocksize, usecols=usecols)\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', blocksize=blocksize, usecols=usecols)\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', blocksize=blocksize, usecols=usecols)\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv', blocksize=blocksize, usecols=['lat', 'lon', 'other_needed_columns'])\n\n# Perform the merges using Dask DataFrames with all merge keys specified at once\nmerged_data = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(merged_data, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(merged_data, terrain, on=['lat', 'lon'], how='inner')\n\n# Convert columns to categorical data type if appropriate\nmerged_data['categorical_column'] = merged_data['categorical_column'].astype('category')\n\n# Persist intermediate result to disk if enough disk space is available\nmerged_data = merged_data.persist()\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 136, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 761, in read\n    return read_pandas(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 630, in read_pandas\n    head = reader(BytesIO(b_sample), nrows=sample_rows, **head_kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 211, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 950, in read_csv\n    return _read(filepath_or_buffer, kwds)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 605, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1442, in __init__\n    self._engine = self._make_engine(f, self.engine)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1753, in _make_engine\n    return mapping[engine](f, **self.options)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 135, in __init__\n    self._validate_usecols_names(usecols, self.orig_names)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\", line 917, in _validate_usecols_names\n    raise ValueError(\nValueError: Usecols do not match columns, columns expected but not found: ['other_needed_columns']\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/DqyyOBpyf5mO/merge_custom_traning_range.py\", line 6, in <module>\n    snotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=blocksize, usecols=usecols)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 138, in wrapper\n    raise type(e)(\nValueError: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: Usecols do not match columns, columns expected but not found: ['other_needed_columns']\n",
  "history_begin_time" : 1691361755680,
  "history_end_time" : 1691361763742,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "t1UwS2Ci5Ki6",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask with a smaller blocksize\nblocksize = \"32MB\"\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=blocksize)\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', blocksize=blocksize)\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', blocksize=blocksize)\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv', blocksize=blocksize)\n\n# Perform the merges using Dask DataFrames\nsnotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nsnotel_gridmet_amsr = dd.merge(snotel_gridmet, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Persist intermediate result to disk if enough disk space is available\nmerged_data = merged_data.persist()\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/t1UwS2Ci5Ki6/merge_custom_traning_range.py\", line 16, in <module>\n    merged_data = merged_data.persist()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/threaded.py\", line 89, in get\n    results = get_async(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 511, in get_async\n    raise_exception(exc, tb)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 319, in reraise\n    raise exc\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 224, in execute_task\n    result = _execute_task(task, data)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 289, in merge_chunk\n    out = lhs.merge(rhs, *args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/array_algos/take.py\", line 158, in _take_nd_ndarray\n    out = np.empty(out_shape, dtype=dtype)\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 2.73 GiB for an array with shape (6, 60966423) and data type float64\n",
  "history_begin_time" : 1691361471841,
  "history_end_time" : 1691361560749,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "T3BRtlIQvn4G",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask with a smaller blocksize\nblocksize = \"32MB\"\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=blocksize)\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', blocksize=blocksize)\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', blocksize=blocksize)\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv', blocksize=blocksize)\n\n# Perform the merges using Dask DataFrames\nsnotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nsnotel_gridmet_amsr = dd.merge(snotel_gridmet, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Persist intermediate result to disk if enough disk space is available\nmerged_data = merged_data.persist()\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False)\n",
  "history_output" : null,
  "history_begin_time" : 1691361180061,
  "history_end_time" : null,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : null
},{
  "history_id" : "kclAXhqALlUm",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask with a smaller blocksize\nblocksize = \"32MB\"\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon'])\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', blocksize=blocksize)\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', blocksize=blocksize)\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv', blocksize=blocksize)\n\n# Perform the merges using Dask DataFrames\nsnotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nsnotel_gridmet_amsr = dd.merge(snotel_gridmet, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Persist intermediate result to disk if enough disk space is available\nmerged_data = merged_data.persist()\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False)\n",
  "history_output" : "",
  "history_begin_time" : 1691361149208,
  "history_end_time" : 1691361180069,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "6ND27QqJwXjh",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask with a smaller blocksize\nblocksize = \"64MB\"\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon'])\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', blocksize=blocksize)\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', blocksize=blocksize)\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv', blocksize=blocksize)\n\n# Perform the merges using Dask DataFrames\nsnotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nsnotel_gridmet_amsr = dd.merge(snotel_gridmet, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Persist intermediate result to disk if enough disk space is available\nmerged_data = merged_data.persist()\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/6ND27QqJwXjh/merge_custom_traning_range.py\", line 16, in <module>\n    merged_data = merged_data.persist()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/threaded.py\", line 89, in get\n    results = get_async(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 511, in get_async\n    raise_exception(exc, tb)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 319, in reraise\n    raise exc\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 224, in execute_task\n    result = _execute_task(task, data)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/multi.py\", line 289, in merge_chunk\n    out = lhs.merge(rhs, *args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/concat.py\", line 212, in concatenate_managers\n    values = values.copy()\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 1.81 GiB for an array with shape (2, 121756443) and data type int64\nsh: line 1:  6882 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python merge_custom_traning_range.py\n",
  "history_begin_time" : 1691360928524,
  "history_end_time" : 1691361081045,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "veeDuOJivucf",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask with a smaller blocksize\nblocksize = \"32MB\"\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon'])\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon'])\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon'])\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv', blocksize=blocksize, usecols=['lat', 'lon'])\n\n# Perform the merges using Dask DataFrames\nsnotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nsnotel_gridmet_amsr = dd.merge(snotel_gridmet, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Persist intermediate result to disk if enough disk space is available\nmerged_data = merged_data.persist()\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False)\n",
  "history_output" : "",
  "history_begin_time" : 1691358253989,
  "history_end_time" : 1691358870989,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "LVYMCzXna8kY",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask with a smaller blocksize\nblocksize = \"32MB\"\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon'])\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon'])\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon'])\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv', blocksize=blocksize, usecols=['lat', 'lon'])\n\n# Perform the merges using Dask DataFrames\nsnotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nsnotel_gridmet_amsr = dd.merge(snotel_gridmet, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Persist intermediate result to disk if enough disk space is available\nmerged_data = merged_data.persist()\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False, single_file=True)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/LVYMCzXna8kY/merge_custom_traning_range.py\", line 19, in <module>\n    merged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False, single_file=True)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 211, in wrapper\n    return func(*args, **kwargs)\nTypeError: to_csv() got an unexpected keyword argument 'single_file'\n",
  "history_begin_time" : 1691358150598,
  "history_end_time" : 1691358182988,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "foLq4AgG1y0v",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask with a smaller blocksize\nblocksize = \"32MB\"\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon', 'other_needed_columns'])\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon', 'other_needed_columns'])\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon', 'other_needed_columns'])\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv', blocksize=blocksize, usecols=['lat', 'lon', 'other_needed_columns'])\n\n# Perform the merges using Dask DataFrames\nsnotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nsnotel_gridmet_amsr = dd.merge(snotel_gridmet, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Persist intermediate result to disk if enough disk space is available\nmerged_data = merged_data.persist()\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.compute().to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False, single_file=True)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 136, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 761, in read\n    return read_pandas(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 630, in read_pandas\n    head = reader(BytesIO(b_sample), nrows=sample_rows, **head_kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 211, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 950, in read_csv\n    return _read(filepath_or_buffer, kwds)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 605, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1442, in __init__\n    self._engine = self._make_engine(f, self.engine)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1753, in _make_engine\n    return mapping[engine](f, **self.options)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 135, in __init__\n    self._validate_usecols_names(usecols, self.orig_names)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\", line 917, in _validate_usecols_names\n    raise ValueError(\nValueError: Usecols do not match columns, columns expected but not found: ['other_needed_columns']\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/foLq4AgG1y0v/merge_custom_traning_range.py\", line 5, in <module>\n    snotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=blocksize, usecols=['date', 'lat', 'lon', 'other_needed_columns'])\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 138, in wrapper\n    raise type(e)(\nValueError: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: Usecols do not match columns, columns expected but not found: ['other_needed_columns']\n",
  "history_begin_time" : 1691358090119,
  "history_end_time" : 1691358099746,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "oD7sBowsZQqg",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask instead of Pandas\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=\"64MB\")\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', blocksize=\"64MB\")\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', blocksize=\"64MB\")\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv', blocksize=\"64MB\")\n\n# Perform the merges using Dask DataFrames\nsnotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nsnotel_gridmet_amsr = dd.merge(snotel_gridmet, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False, single_file=True)\n\n# Now you have the merged data as a Pandas DataFrame\n",
  "history_output" : "sh: line 1:  2881 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python merge_custom_traning_range.py\n",
  "history_begin_time" : 1691357792430,
  "history_end_time" : 1691357933521,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "9QAjnQlXjRMK",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask instead of Pandas\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', blocksize=\"64MB\")\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', blocksize=\"64MB\")\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', blocksize=\"64MB\")\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv', blocksize=\"64MB\")\n\n# Perform the merges using Dask DataFrames\nsnotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nsnotel_gridmet_amsr = dd.merge(snotel_gridmet, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False, single_file=True)\n\n# Now you have the merged data as a Pandas DataFrame\n",
  "history_output" : "sh: line 1:  1305 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python merge_custom_traning_range.py\n",
  "history_begin_time" : 1691355130245,
  "history_end_time" : 1691355272757,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "tDoXLKtqLseH",
  "history_input" : "import dask.dataframe as dd\n\n# Load the data using Dask instead of Pandas\nsnotel = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv')\ngridmet = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv')\namsr = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv')\nterrain = dd.read_csv('/home/chetana/gridmet_test_run/training_ready_terrain.csv')\n\n# Perform the merges using Dask DataFrames\nsnotel_gridmet = dd.merge(snotel, gridmet, on=['date', 'lat', 'lon'], how='inner')\nsnotel_gridmet_amsr = dd.merge(snotel_gridmet, amsr, on=['date', 'lat', 'lon'], how='inner')\nmerged_data = dd.merge(snotel_gridmet_amsr, terrain, on=['lat', 'lon'], how='inner')\n\n# Compute the result and convert to Pandas DataFrame if needed\nmerged_data.to_csv('/home/chetana/gridmet_test_run/training_ready_merged_3_year.csv', index=False, single_file=True)\n\n# Now you have the merged data as a Pandas DataFrame\n",
  "history_output" : "sh: line 1:   871 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python merge_custom_traning_range.py\n",
  "history_begin_time" : 1691354746353,
  "history_end_time" : 1691355073155,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : null,
  "indicator" : "Done"
},]
