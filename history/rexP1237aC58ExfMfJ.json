[{
  "history_id" : "qydi4lcfkqn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500107,
  "history_end_time" : 1700448500107,
  "history_notes" : null,
  "history_process" : "78vedq",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "yad660kgtg0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500109,
  "history_end_time" : 1700448500109,
  "history_notes" : null,
  "history_process" : "mxpyqt",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8an4ifefntv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500111,
  "history_end_time" : 1700448500111,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6rug9908n08",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500113,
  "history_end_time" : 1700448500113,
  "history_notes" : null,
  "history_process" : "u7xh2p",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "yrkyxhhy3i4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500114,
  "history_end_time" : 1700448500114,
  "history_notes" : null,
  "history_process" : "e8k4wq",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "bp33xfnv3l0",
  "history_input" : "import joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import homedir, work_dir, month_to_season, test_start_date\nimport os\nimport random\nimport string\nimport shutil\n\ndef generate_random_string(length):\n    # Define the characters that can be used in the random string\n    characters = string.ascii_letters + string.digits  # You can customize this to include other characters if needed\n\n    # Generate a random string of the specified length\n    random_string = ''.join(random.choice(characters) for _ in range(length))\n\n    return random_string\n  \n\ndef load_model(model_path):\n    \"\"\"\n    Load a machine learning model from a file.\n\n    Args:\n        model_path (str): Path to the saved model file.\n\n    Returns:\n        model: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file.\n\n    Args:\n        file_path (str): Path to the CSV file containing the data.\n\n    Returns:\n        pd.DataFrame: A pandas DataFrame containing the loaded data.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data for model prediction.\n\n    Args:\n        data (pd.DataFrame): Input data in the form of a pandas DataFrame.\n\n    Returns:\n        pd.DataFrame: Preprocessed data ready for prediction.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #print(\"check date format: \", data.head())\n    #data['date'] = data['date'].dt.strftime('%j').astype(int)\n    #data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    \n    #data = data.apply(pd.to_numeric, errors='coerce')\n\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness',\n                         'cumulative_AMSR_SWE': 'cumulative_SWE',\n                         'cumulative_AMSR_Flag': 'cumulative_Flag',\n                         'cumulative_tmmn':'cumulative_air_temperature_tmmn',\n                         'cumulative_etr': 'cumulative_potential_evapotranspiration',\n                         'cumulative_vpd': 'cumulative_mean_vapor_pressure_deficit',\n                         'cumulative_rmax': 'cumulative_relative_humidity_rmax', \n                         'cumulative_rmin': 'cumulative_relative_humidity_rmin',\n                         'cumulative_pr': 'cumulative_precipitation_amount',\n                         'cumulative_tmmx': 'cumulative_air_temperature_tmmx',\n                         'cumulative_vs': 'cumulative_wind_speed',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n#                          'relative_humidity_rmin': '',\n#                          'cumulative_rmin',\n#                          'mean_vapor_pressure_deficit', \n#                          'cumulative_vpd', \n#                          'wind_speed',\n#                          'cumulative_vs', \n#                          'relative_humidity_rmax', 'cumulative_rmax',\n\n# 'precipitation_amount', 'cumulative_pr', 'air_temperature_tmmx',\n\n# 'cumulative_tmmx', 'potential_evapotranspiration', 'cumulative_etr',\n\n# 'air_temperature_tmmn', 'cumulative_tmmn', 'x', 'y', 'elevation',\n\n# 'slope', 'aspect', 'curvature', 'northness', 'eastness', 'AMSR_SWE',\n\n# 'cumulative_AMSR_SWE', 'AMSR_Flag', 'cumulative_AMSR_Flag',\n                        }, inplace=True)\n\n    print(data.head())\n    print(data.columns)\n    \n    # filter out three days for final visualization to accelerate the process\n    #dates_to_match = ['2018-03-15', '2018-04-15', '2018-05-15']\n    #mask = data['date'].dt.strftime('%Y-%m-%d').isin(dates_to_match)\n    # Filter the DataFrame based on the mask\n    #data = data[mask]\n    \n    desired_order = [\n  'SWE', \n                    'air_temperature_tmmn', \n                    #'cumulative_SWE', \n  'air_temperature_tmmx',\n                    #'cumulative_potential_evapotranspiration', \n  #'cumulative_mean_vapor_pressure_deficit',\n   #                 'cumulative_relative_humidity_rmax', \n    #                'cumulative_relative_humidity_rmin', \n     #               'cumulative_precipitation_amount',\n#                     'cumulative_Flag', \n                    'elevation',\n                    \n                    'potential_evapotranspiration',\n  'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount', 'wind_speed','slope','aspect','northness',\n                     'lat', 'lon',\n      \n# 'air_temperature_tmmx', 'wind_speed',  'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', \n# 'cumulative_Flag', 'cumulative_mean_vapor_pressure_deficit', 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed',\n                    ]\n#     ['lat', 'lon', 'elevation', 'slope', 'curvature', 'aspect', 'eastness',\n\n# 'northness', 'water_year', 'cumulative_SWE', 'cumulative_Flag',\n\n# 'cumulative_air_temperature_tmmn',\n\n# 'cumulative_potential_evapotranspiration',\n\n# 'cumulative_mean_vapor_pressure_deficit',\n\n# 'cumulative_relative_humidity_rmax',\n\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    print(\"reorganized columns: \", data.columns)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Predict snow water equivalent (SWE) using a machine learning model.\n\n    Args:\n        model: The machine learning model for prediction.\n        data (pd.DataFrame): Input data for prediction.\n\n    Returns:\n        pd.DataFrame: Dataframe with predicted SWE values.\n    \"\"\"\n    data = data.fillna(-999)\n    input_data = data\n    input_data = data.drop([\"lat\", \"lon\"], axis=1)\n    #input_data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin',], axis=1)\n    predictions = model.predict(input_data)\n    data['predicted_swe'] = predictions\n    return data\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge predicted SWE data with the original data.\n\n    Args:\n        original_data (pd.DataFrame): Original input data.\n        predicted_data (pd.DataFrame): Dataframe with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged dataframe.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    if \"date\" not in predicted_data:\n    \tpredicted_data[\"date\"] = test_start_date\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    print(\"original_data.columns: \", original_data.columns)\n    print(\"new_data_extracted.columns: \", new_data_extracted.columns)\n    print(\"new prediction statistics: \", new_data_extracted[\"predicted_swe\"].describe())\n    #merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['date', 'lat', 'lon'], how='left')\n    return merged_df\n\ndef predict():\n    \"\"\"\n    Main function for predicting snow water equivalent (SWE).\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'{homedir}/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"Using model: {model_path}\")\n  \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    #output_path = f'{work_dir}/test_data_predicted_three_days_only.csv'\n    latest_output_path = f'{work_dir}/test_data_predicted_latest.csv'\n    output_path = f'{work_dir}/test_data_predicted_{generate_random_string(5)}.csv'\n  \n    if os.path.exists(output_path):\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    #print(\"new_data shape: \", new_data.head())\n\n    preprocessed_data = preprocess_data(new_data)\n    if len(new_data) < len(preprocessed_data):\n      raise ValueError(\"Why the preprocessed data increased?\")\n    #print('Data preprocessing completed.', preprocessed_data.head())\n    #print(f'Model used: {model_path}')\n    predicted_data = predict_swe(model, preprocessed_data)\n    print(\"how many predicted? \", len(predicted_data))\n    \n    if \"date\" not in preprocessed_data:\n    \tpreprocessed_data[\"date\"] = test_start_date\n    predicted_data = merge_data(preprocessed_data, predicted_data)\n    \n    \n    #print('Data prediction completed.')\n  \n    #print(predicted_data['date'])\n    predicted_data.to_csv(output_path, index=False)\n    print(\"Prediction successfully done \", output_path)\n    \n    shutil.copy(output_path, latest_output_path)\n    print(f\"Copied to {latest_output_path}\")\n\n#     if len(predicted_data) == height * width:\n#         print(f\"The image width, height match with the number of rows in the CSV. {len(predicted_data)} rows\")\n#     else:\n#         raise Exception(\"The total number of rows does not match\")\n\npredict()\n",
  "history_output" : "today date = 2023-11-20\ntest start date:  2022-10-15\ntest end date:  2023-10-07\n/home/chetana\nUsing model: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\n    lat      lon  air_temperature_tmmx  ...  cumulative_Flag       date  water_year\n0  49.0 -125.000                   NaN  ...             3629 2022-10-15        2023\n1  49.0 -124.964                   NaN  ...             3629 2022-10-15        2023\n2  49.0 -124.928                   NaN  ...             3629 2022-10-15        2023\n3  49.0 -124.892                   NaN  ...             3629 2022-10-15        2023\n4  49.0 -124.856                   NaN  ...             3629 2022-10-15        2023\n[5 rows x 32 columns]\nIndex(['lat', 'lon', 'air_temperature_tmmx', 'cumulative_air_temperature_tmmx',\n       'wind_speed', 'cumulative_wind_speed', 'air_temperature_tmmn',\n       'cumulative_air_temperature_tmmn', 'relative_humidity_rmin',\n       'cumulative_relative_humidity_rmin', 'relative_humidity_rmax',\n       'cumulative_relative_humidity_rmax', 'mean_vapor_pressure_deficit',\n       'cumulative_mean_vapor_pressure_deficit', 'precipitation_amount',\n       'cumulative_precipitation_amount', 'potential_evapotranspiration',\n       'cumulative_potential_evapotranspiration', 'x', 'y', 'elevation',\n       'slope', 'aspect', 'curvature', 'northness', 'eastness', 'SWE',\n       'cumulative_SWE', 'Flag', 'cumulative_Flag', 'date', 'water_year'],\n      dtype='object')\nreorganized columns:  Index(['SWE', 'air_temperature_tmmn', 'air_temperature_tmmx', 'elevation',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'wind_speed', 'slope', 'aspect', 'northness',\n       'lat', 'lon'],\n      dtype='object')\n/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but ExtraTreesRegressor was fitted without feature names\n  warnings.warn(\nhow many predicted?  462204\noriginal_data.columns:  Index(['SWE', 'air_temperature_tmmn', 'air_temperature_tmmx', 'elevation',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'wind_speed', 'slope', 'aspect', 'northness',\n       'lat', 'lon', 'date'],\n      dtype='object')\nnew_data_extracted.columns:  Index(['date', 'lat', 'lon', 'predicted_swe'], dtype='object')\nnew prediction statistics:  count    462204.000000\nmean          2.250049\nstd           1.914669\nmin           0.000000\n25%           0.160000\n50%           1.996000\n75%           4.359000\nmax           7.551000\nName: predicted_swe, dtype: float64\n",
  "history_begin_time" : 1700448559285,
  "history_end_time" : 1700448575892,
  "history_notes" : null,
  "history_process" : "h1qp9v",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "0ah1wb7oxao",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500118,
  "history_end_time" : 1700448500118,
  "history_notes" : null,
  "history_process" : "urd0nk",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "eows0065n8p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500120,
  "history_end_time" : 1700448500120,
  "history_notes" : null,
  "history_process" : "525l8q",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "w00s0oqamd8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500121,
  "history_end_time" : 1700448500121,
  "history_notes" : null,
  "history_process" : "7temiv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "sd8bxdhryun",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500122,
  "history_end_time" : 1700448500122,
  "history_notes" : null,
  "history_process" : "rmxece",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "j00fmqmeiux",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500122,
  "history_end_time" : 1700448500122,
  "history_notes" : null,
  "history_process" : "illwc1",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "waoiq24iakd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500123,
  "history_end_time" : 1700448500123,
  "history_notes" : null,
  "history_process" : "sjs5by",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "z2zdegp7vra",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500124,
  "history_end_time" : 1700448500124,
  "history_notes" : null,
  "history_process" : "y7nb46",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6bsj9b9scyj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500125,
  "history_end_time" : 1700448500125,
  "history_notes" : null,
  "history_process" : "a8p3n7",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9aag1kwiw78",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500125,
  "history_end_time" : 1700448500125,
  "history_notes" : null,
  "history_process" : "smsdr0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "09lhvu2opbw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500126,
  "history_end_time" : 1700448500126,
  "history_notes" : null,
  "history_process" : "4i0sop",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "s2b6lequ2lj",
  "history_input" : "import os\nimport pandas as pd\nfrom datetime import datetime\nfrom snowcast_utils import work_dir, test_start_date\n\ndef get_water_year(date):\n    if date.month >= 10:  # If the month is October or later\n        return date.year + 1  # Water year starts in the following calendar year\n    else:\n        return date.year\n\ndef merge_all_gridmet_amsr_csv_into_one(gridmet_csv_folder, dem_all_csv, testing_all_csv):\n    \"\"\"\n    Merge all GridMET and AMSR CSV files into one combined CSV file.\n\n    Args:\n        gridmet_csv_folder (str): The folder containing GridMET CSV files.\n        dem_all_csv (str): Path to the DEM (Digital Elevation Model) CSV file.\n        testing_all_csv (str): Path to save the merged CSV file.\n\n    Returns:\n        None\n    \"\"\"\n    # List of file paths for the CSV files\n    csv_files = []\n    selected_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n    for file in os.listdir(gridmet_csv_folder):\n        if file.endswith('_cumulative.csv') and test_start_date in file:\n            csv_files.append(os.path.join(gridmet_csv_folder, file))\n\n    # Initialize an empty list to store all dataframes\n    dfs = []\n\n    # Read each CSV file into separate dataframes\n    for file in csv_files:\n        print(f\"reading {file}\")\n        df = pd.read_csv(file, encoding='utf-8', index_col=False)\n        dfs.append(df)\n\n    dem_df = pd.read_csv(f\"{work_dir}/dem_all.csv\", encoding='utf-8', index_col=False)\n    dfs.append(dem_df)\n\n    date = test_start_date\n    \n    date = date.replace(\"-\", \".\")\n    amsr_df = pd.read_csv(f'{work_dir}/testing_ready_amsr_{date}.csv_cumulative.csv', index_col=False)\n    amsr_df.rename(columns={'gridmet_lat': 'Latitude', 'gridmet_lon': 'Longitude'}, inplace=True)\n    dfs.append(amsr_df)\n\n    # Merge the dataframes based on the latitude and longitude columns\n    merged_df = dfs[0]  # Start with the first dataframe\n    for i in range(1, len(dfs)):\n        print(dfs[i].shape)\n        merged_df = pd.merge(merged_df, dfs[i], on=['Latitude', 'Longitude'])\n        \n    # add water year\n    merged_df[\"water_year\"] = get_water_year(selected_date)\n\n    # Save the merged dataframe to a new CSV file\n    merged_df.to_csv(testing_all_csv, index=False)\n    print(f\"All input CSV files are merged to {testing_all_csv}\")\n    print(merged_df.columns)\n    print(merged_df.describe(include='all'))\n    print(merged_df[\"cumulative_pr\"].describe())\n\nif __name__ == \"__main__\":\n    # Replace with the actual path to your folder\n    gridmet_csv_folder = f\"{work_dir}/gridmet_climatology/\"\n    merge_all_gridmet_amsr_csv_into_one(f\"{work_dir}/testing_output/\",\n                                        f\"{work_dir}/dem_all.csv\",\n                                        f\"{work_dir}/testing_all_ready.csv\")\n",
  "history_output" : "today date = 2023-11-20\ntest start date:  2022-10-15\ntest end date:  2023-10-07\n/home/chetana\nreading /home/chetana/gridmet_test_run/testing_output/2022_tmmx_2022-10-15.csv_cumulative.csv\nreading /home/chetana/gridmet_test_run/testing_output/2022_vs_2022-10-15.csv_cumulative.csv\nreading /home/chetana/gridmet_test_run/testing_output/2022_tmmn_2022-10-15.csv_cumulative.csv\nreading /home/chetana/gridmet_test_run/testing_output/2022_rmin_2022-10-15.csv_cumulative.csv\nreading /home/chetana/gridmet_test_run/testing_output/2022_rmax_2022-10-15.csv_cumulative.csv\nreading /home/chetana/gridmet_test_run/testing_output/2022_vpd_2022-10-15.csv_cumulative.csv\nreading /home/chetana/gridmet_test_run/testing_output/2022_pr_2022-10-15.csv_cumulative.csv\nreading /home/chetana/gridmet_test_run/testing_output/2022_etr_2022-10-15.csv_cumulative.csv\n(462204, 4)\n(462204, 4)\n(462204, 4)\n(462204, 4)\n(462204, 4)\n(462204, 4)\n(462204, 4)\n(462204, 10)\n(462204, 7)\nAll input CSV files are merged to /home/chetana/gridmet_test_run/testing_all_ready.csv\nIndex(['Latitude', 'Longitude', 'tmmx', 'cumulative_tmmx', 'vs',\n       'cumulative_vs', 'tmmn', 'cumulative_tmmn', 'rmin', 'cumulative_rmin',\n       'rmax', 'cumulative_rmax', 'vpd', 'cumulative_vpd', 'pr',\n       'cumulative_pr', 'etr', 'cumulative_etr', 'x', 'y', 'Elevation',\n       'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness', 'AMSR_SWE',\n       'cumulative_AMSR_SWE', 'AMSR_Flag', 'cumulative_AMSR_Flag', 'date',\n       'water_year'],\n      dtype='object')\n             Latitude     Longitude  ...        date  water_year\ncount   462204.000000  462204.00000  ...      462204    462204.0\nunique            NaN           NaN  ...           1         NaN\ntop               NaN           NaN  ...  2022-10-15         NaN\nfreq              NaN           NaN  ...      462204         NaN\nmean        37.030000    -112.52600  ...         NaN      2023.0\nstd          6.921275       7.21226  ...         NaN         0.0\nmin         25.060000    -125.00000  ...         NaN      2023.0\n25%         31.036000    -118.77200  ...         NaN      2023.0\n50%         37.030000    -112.52600  ...         NaN      2023.0\n75%         43.024000    -106.28000  ...         NaN      2023.0\nmax         49.000000    -100.05200  ...         NaN      2023.0\n[11 rows x 32 columns]\ncount    462204.000000\nmean          9.365204\nstd          17.196108\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%          12.500000\nmax         218.000000\nName: cumulative_pr, dtype: float64\n",
  "history_begin_time" : 1700448540882,
  "history_end_time" : 1700448557098,
  "history_notes" : null,
  "history_process" : "b63prf",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "4fz4vokjs4z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500127,
  "history_end_time" : 1700448500127,
  "history_notes" : null,
  "history_process" : "zh38b6",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "w6sgvh6jf50",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500128,
  "history_end_time" : 1700448500128,
  "history_notes" : null,
  "history_process" : "wdh394",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tnmy17rjuti",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500129,
  "history_end_time" : 1700448500129,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "yuj1z99qvys",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500130,
  "history_end_time" : 1700448500130,
  "history_notes" : null,
  "history_process" : "ilbqzg",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "i82svmjycoz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500131,
  "history_end_time" : 1700448500131,
  "history_notes" : null,
  "history_process" : "do86ae",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3b4szsmiwts",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500131,
  "history_end_time" : 1700448500131,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rjeghrpkjip",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500132,
  "history_end_time" : 1700448500132,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "qjm1qkfiqdr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500133,
  "history_end_time" : 1700448500133,
  "history_notes" : null,
  "history_process" : "br9etb",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "00cj5a2bmyd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500134,
  "history_end_time" : 1700448500134,
  "history_notes" : null,
  "history_process" : "c2xkhz",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "n4jtako430y",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500134,
  "history_end_time" : 1700448500134,
  "history_notes" : null,
  "history_process" : "doinnd",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "qh46dpyp1rm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500135,
  "history_end_time" : 1700448500135,
  "history_notes" : null,
  "history_process" : "b7a4fu",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "myo40yniiwr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500136,
  "history_end_time" : 1700448500136,
  "history_notes" : null,
  "history_process" : "gnpbdq",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "l98td6lg68g",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500136,
  "history_end_time" : 1700448500136,
  "history_notes" : null,
  "history_process" : "oon4sb",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "qswyuywzset",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500137,
  "history_end_time" : 1700448500137,
  "history_notes" : null,
  "history_process" : "fa7e4u",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "obgtbbvhw29",
  "history_input" : "\"\"\"\nScript for downloading specific variables of GridMET climatology data.\n\nThis script downloads specific meteorological variables from the GridMET climatology dataset\nfor a specified year. It uses the netCDF4 library for handling NetCDF files, urllib for downloading files,\nand pandas for data manipulation. The script also removes existing files in the target folder before downloading.\n\n\nUsage:\n    Run this script to download specific meteorological variables for a specified year from the GridMET dataset.\n\n\"\"\"\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport netCDF4 as nc\nimport urllib.request\nfrom datetime import datetime, timedelta, date\nfrom snowcast_utils import test_start_date, work_dir\nimport matplotlib.pyplot as plt\n\n# Define the folder to store downloaded files\ngridmet_folder_name = f'{work_dir}/gridmet_climatology'\n\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\n\n\ngridmet_var_mapping = {\n  \"etr\": \"potential_evapotranspiration\",\n  \"pr\":\"precipitation_amount\",\n  \"rmax\":\"relative_humidity\",\n  \"rmin\":\"relative_humidity\",\n  \"tmmn\":\"air_temperature\",\n  \"tmmx\":\"air_temperature\",\n  \"vpd\":\"mean_vapor_pressure_deficit\",\n  \"vs\":\"wind_speed\",\n}\n# Define the custom colormap with specified colors and ranges\ncolors = [\n    (0.8627, 0.8627, 0.8627),  # #DCDCDC - 0 - 1\n    (0.8627, 1.0000, 1.0000),  # #DCFFFF - 1 - 2\n    (0.6000, 1.0000, 1.0000),  # #99FFFF - 2 - 4\n    (0.5569, 0.8235, 1.0000),  # #8ED2FF - 4 - 6\n    (0.4509, 0.6196, 0.8745),  # #739EDF - 6 - 8\n    (0.4157, 0.4706, 1.0000),  # #6A78FF - 8 - 10\n    (0.4235, 0.2784, 1.0000),  # #6C47FF - 10 - 12\n    (0.5529, 0.0980, 1.0000),  # #8D19FF - 12 - 14\n    (0.7333, 0.0000, 0.9176),  # #BB00EA - 14 - 16\n    (0.8392, 0.0000, 0.7490),  # #D600BF - 16 - 18\n    (0.7569, 0.0039, 0.4549),  # #C10074 - 18 - 20\n    (0.6784, 0.0000, 0.1961),  # #AD0032 - 20 - 30\n    (0.5020, 0.0000, 0.0000)   # #800000 - > 30\n]\n\n# Define your value ranges for color mapping\n#value_ranges = [1, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 30]\n#value_ranges = [0.1, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.5, 1.8, 2, 2.5, 3]\n\ndef create_color_maps_with_value_range(df_col, value_ranges=None):\n  if value_ranges == None:\n    max_value = df_col.max()\n    min_value = df_col.min()\n    if min_value < 0:\n      min_value = 0\n    step_size = (max_value - min_value) / 12\n\n    # Create 10 periods\n    new_value_ranges = [min_value + i * step_size for i in range(12)]\n  # Define your custom function to map data values to colors\n  def map_value_to_color(value):\n    # Iterate through the value ranges to find the appropriate color index\n    for i, range_max in enumerate(new_value_ranges):\n      if value <= range_max:\n        return colors[i]\n\n      # If the value is greater than the largest range, return the last color\n      return colors[-1]\n\n    # Map predicted_swe values to colors using the custom function\n  color_mapping = [map_value_to_color(value) for value in df_col.values]\n  return color_mapping, new_value_ranges\n\ndef get_current_year():\n    \"\"\"\n    Get the current year.\n\n    Returns:\n        int: The current year.\n    \"\"\"\n    now = datetime.now()\n    current_year = now.year\n    return current_year\n\ndef remove_files_in_folder(folder_path):\n    \"\"\"\n    Remove all files in a specified folder.\n\n    Parameters:\n        folder_path (str): Path to the folder to remove files from.\n    \"\"\"\n    # Get a list of files in the folder\n    files = os.listdir(folder_path)\n\n    # Loop through the files and remove them\n    for file in files:\n        file_path = os.path.join(folder_path, file)\n        if os.path.isfile(file_path):\n            os.remove(file_path)\n            print(f\"Deleted file: {file_path}\")\n\ndef download_file(url, target_file_path, variable):\n    \"\"\"\n    Download a file from a URL and save it to a specified location.\n\n    Parameters:\n        url (str): URL of the file to download.\n        target_file_path (str): Path where the downloaded file should be saved.\n        variable (str): Name of the meteorological variable being downloaded.\n    \"\"\"\n    try:\n        with urllib.request.urlopen(url) as response:\n            print(f\"Downloading {url}\")\n            file_content = response.read()\n        save_path = target_file_path\n        with open(save_path, 'wb') as file:\n            file.write(file_content)\n        print(f\"File downloaded successfully and saved as: {save_path}\")\n    except Exception as e:\n        print(f\"An error occurred while downloading the file: {str(e)}\")\n\ndef download_gridmet_of_specific_variables(year_list):\n    \"\"\"\n    Download specific meteorological variables from the GridMET climatology dataset.\n    \"\"\"\n    # Make a directory to store the downloaded files\n    \n\n    base_metadata_url = \"http://www.northwestknowledge.net/metdata/data/\"\n    variables_list = ['tmmn', 'tmmx', 'pr', 'vpd', 'etr', 'rmax', 'rmin', 'vs']\n\n    for var in variables_list:\n        for y in year_list:\n            download_link = base_metadata_url + var + '_' + '%s' % y + '.nc'\n            target_file_path = os.path.join(gridmet_folder_name, var + '_' + '%s' % y + '.nc')\n            if not os.path.exists(target_file_path):\n                download_file(download_link, target_file_path, var)\n            else:\n                print(f\"File {target_file_path} exists\")\n\n\ndef get_current_year():\n    now = datetime.now()\n    current_year = now.year\n    return current_year\n\n\ndef get_file_name_from_path(file_path):\n    # Get the file name from the file path\n    file_name = os.path.basename(file_path)\n    return file_name\n\ndef get_var_from_file_name(file_name):\n    # Assuming the file name format is \"tmmm_year.csv\"\n    var_name = str(file_name.split('_')[0])\n    return var_name\n\ndef get_coordinates_of_template_tif():\n  \t# Load the CSV file and extract coordinates\n    coordinates = []\n    df = pd.read_csv(dem_csv)\n    for index, row in df.iterrows():\n        # Process each row here\n        lon, lat = float(row[\"Latitude\"]), float(row[\"Longitude\"])\n        coordinates.append((lon, lat))\n    return coordinates\n\ndef find_nearest_index(array, value):\n    # Find the index of the element in the array that is closest to the given value\n    return (abs(array - value)).argmin()\n\ndef create_gridmet_to_dem_mapper(nc_file):\n    western_us_dem_df = pd.read_csv(western_us_coords)\n    # Check if the CSV already exists\n    target_csv_path = f'{work_dir}/gridmet_to_dem_mapper.csv'\n    if os.path.exists(target_csv_path):\n        print(f\"File {target_csv_path} already exists, skipping..\")\n        return\n    \n    # get the netcdf file and generate the csv file for every coordinate in the dem_template.csv\n    selected_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n    # Read the NetCDF file\n    with nc.Dataset(nc_file) as nc_file:\n      \n      # Get the values at each coordinate using rasterio's sample function\n      latitudes = nc_file.variables['lat'][:]\n      longitudes = nc_file.variables['lon'][:]\n      \n      def get_gridmet_var_value(row):\n        # Perform your custom calculation here\n        gridmet_lat_index = find_nearest_index(latitudes, float(row[\"Latitude\"]))\n        gridmet_lon_index = find_nearest_index(longitudes, float(row[\"Longitude\"]))\n        return latitudes[gridmet_lat_index], longitudes[gridmet_lon_index], gridmet_lat_index, gridmet_lon_index\n    \n      # Use the apply function to apply the custom function to each row\n      western_us_dem_df[['gridmet_lat', 'gridmet_lon', \n                         'gridmet_lat_idx', 'gridmet_lon_idx',]] = western_us_dem_df.apply(lambda row: pd.Series(get_gridmet_var_value(row)), axis=1)\n      western_us_dem_df.rename(columns={\"Latitude\": \"dem_lat\", \n                                        \"Longitude\": \"dem_lon\"}, inplace=True)\n      \n    # print(western_us_dem_df.head())\n    \n    # Save the new converted AMSR to CSV file\n    western_us_dem_df.to_csv(target_csv_path, index=False)\n    \n    return western_us_dem_df\n  \n  \ndef get_nc_csv_by_coords_and_variable(nc_file,\n                                      var_name,\n                                      target_date=test_start_date):\n    \n    create_gridmet_to_dem_mapper(nc_file)\n  \t\n    mapper_df = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n    \n    # get the netcdf file and generate the csv file for every coordinate in the dem_template.csv\n    selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n    # Read the NetCDF file\n    with nc.Dataset(nc_file) as nc_file:\n      # Get a list of all variables in the NetCDF file\n      variables = nc_file.variables.keys()\n      \n      # Get the values at each coordinate using rasterio's sample function\n      latitudes = nc_file.variables['lat'][:]\n      longitudes = nc_file.variables['lon'][:]\n      day = nc_file.variables['day'][:]\n      long_var_name = gridmet_var_mapping[var_name]\n      var_col = nc_file.variables[long_var_name][:]\n      #print(\"val_col.shape: \", var_col.shape)\n      \n      # Calculate the day of the year\n      day_of_year = selected_date.timetuple().tm_yday\n      day_index = day_of_year - 1\n      #print('day_index:', day_index)\n      \n      def get_gridmet_var_value(row):\n        # Perform your custom calculation here\n        lat_index = int(row[\"gridmet_lat_idx\"])\n        lon_index = int(row[\"gridmet_lon_idx\"])\n        var_value = var_col[day_index, lat_index, lon_index]\n        \n        return var_value\n    \n      # Use the apply function to apply the custom function to each row\n      # print(mapper_df.columns)\n      # print(mapper_df.head())\n      mapper_df[var_name] = mapper_df.apply(get_gridmet_var_value, axis=1)\n      \n      # print(\"mapper_df[var_name]: \", mapper_df[var_name].describe())\n      \n      # drop useless columns\n      mapper_df = mapper_df[[\"dem_lat\", \"dem_lon\", var_name]]\n      mapper_df.rename(columns={\"dem_lat\": \"Latitude\",\n                               \"dem_lon\": \"Longitude\"}, inplace=True)\n\n      \n    # print(mapper_df.head())\n    return mapper_df\n\n\ndef turn_gridmet_nc_to_csv(target_date=test_start_date):\n    \n    selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n    generated_csvs = []\n    for root, dirs, files in os.walk(gridmet_folder_name):\n        for file_name in files:\n            \n            if str(selected_date.year) in file_name and file_name.endswith(\".nc\"):\n                print(f\"Checking file: {file_name}\")\n                var_name = get_var_from_file_name(file_name)\n                # print(\"Variable name:\", var_name)\n                res_csv = f\"{work_dir}/testing_output/{str(selected_date.year)}_{var_name}_{target_date}.csv\"\n\n                if os.path.exists(res_csv):\n                    #os.remove(res_csv)\n                    print(f\"{res_csv} already exists. Skipping..\")\n                    generated_csvs.append(res_csv)\n                    continue\n\n                # Perform operations on each file here\n                netcdf_file_path = os.path.join(root, file_name)\n                print(\"Processing file:\", netcdf_file_path)\n                file_name = get_file_name_from_path(netcdf_file_path)\n\n                df = get_nc_csv_by_coords_and_variable(netcdf_file_path, \n                                                       var_name, target_date)\n                df.replace('--', pd.NA, inplace=True)\n                df.to_csv(res_csv, index=False)\n                print(\"gridmet var saved: \", res_csv)\n                generated_csvs.append(res_csv)\n    return generated_csvs   \n\ndef plot_gridmet(target_date=test_start_date):\n  selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n  var_name = \"pr\"\n  test_csv = f\"{work_dir}/testing_output/{str(selected_date.year)}_{var_name}_{target_date}.csv\"\n  gridmet_var_df = pd.read_csv(test_csv)\n  gridmet_var_df.replace('--', pd.NA, inplace=True)\n  gridmet_var_df.dropna(inplace=True)\n  gridmet_var_df['pr'] = pd.to_numeric(gridmet_var_df['pr'], errors='coerce')\n  #print(gridmet_var_df.head())\n  #print(gridmet_var_df[\"Latitude\"].describe())\n  #print(gridmet_var_df[\"Longitude\"].describe())\n  #print(gridmet_var_df[\"pr\"].describe())\n  \n  colormaplist, value_ranges = create_color_maps_with_value_range(gridmet_var_df[var_name])\n  \n  # Create a scatter plot\n  plt.scatter(gridmet_var_df[\"Longitude\"].values, \n              gridmet_var_df[\"Latitude\"].values, \n              label='Pressure', \n              color=colormaplist, \n              marker='o')\n\n  # Add labels and a legend\n  plt.xlabel('X-axis')\n  plt.ylabel('Y-axis')\n  plt.title('Scatter Plot Example')\n  plt.legend()\n  \n  res_png_path = f\"{work_dir}/testing_output/{str(selected_date.year)}_{var_name}_{target_date}.png\"\n  plt.savefig(res_png_path)\n  print(f\"test image is saved at {res_png_path}\")\n                \ndef prepare_folder_and_get_year_list(target_date=test_start_date):\n  # Check if the folder exists, if not, create it\n  if not os.path.exists(gridmet_folder_name):\n      os.makedirs(gridmet_folder_name)\n\n  selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n  year_list = [selected_date.year]\n\n  # Remove any existing files in the folder\n  if selected_date.year == datetime.now().year:\n      remove_files_in_folder(gridmet_folder_name)  # only redownload when the year is the current year\n  return year_list\n\n\ndef prepare_cumulative_history_csvs(target_date=test_start_date):\n  selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n  print(selected_date)\n  if selected_date.month < 10:\n    past_october_1 = datetime(selected_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(selected_date.year, 10, 1)\n\n  # Traverse and print every day from past October 1 to the specific date\n  current_date = past_october_1\n  \n  date_keyed_objects = {}\n  while current_date <= selected_date:\n    print(current_date.strftime('%Y-%m-%d'))\n    current_date_str = current_date.strftime('%Y-%m-%d')\n    \n    download_gridmet_of_specific_variables(prepare_folder_and_get_year_list(target_date=current_date_str))\n    generated_csvs = turn_gridmet_nc_to_csv(target_date=current_date_str)\n    \n    # read the csv into dataframe and merge to the big dataframe\n    date_keyed_objects[current_date_str] = generated_csvs\n    \n    current_date += timedelta(days=1)\n    \n  print(\"date_keyed_objects: \", date_keyed_objects)\n  target_generated_csvs = date_keyed_objects[target_date]\n  for index, single_csv in enumerate(target_generated_csvs):\n    print(f\"creating cumulative for {single_csv}\")\n    \n    if os.path.exists(f\"{single_csv}_cumulative.csv\"):\n      print(f\"{single_csv}_cumulative.csv already exists, skipping..\")\n      continue\n    \n    # Extract the file name without extension\n    file_name = os.path.splitext(os.path.basename(single_csv))[0]\n\n\t# Split the file name using underscores\n    var_name = file_name.split('_')[1]\n    print(f\"Found variable name {var_name}\")\n    current_date = past_october_1\n    new_df = pd.read_csv(single_csv)\n    print(new_df.head())\n    while current_date <= selected_date:\n      #print(current_date.strftime('%Y-%m-%d'))\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      current_generated_csv = date_keyed_objects[current_date_str][index]\n      previous_df = pd.read_csv(current_generated_csv)\n      previous_df = previous_df.rename(columns={var_name: f'{var_name}_{current_date_str}'})\n      new_df = pd.merge(new_df, previous_df, on=['Latitude', 'Longitude'])\n      current_date += timedelta(days=1)\n    \n    # add all the columns together and save to new csv\n    # Adding all columns except latitude and longitude\n    new_df = new_df.apply(pd.to_numeric, errors='coerce')\n    new_df[f'cumulative_{var_name}'] = new_df.drop(['Latitude', 'Longitude'], axis=1).sum(axis=1)\n    new_df = new_df.loc[:, ['Latitude', 'Longitude', var_name, f'cumulative_{var_name}']]\n    print(\"new_df final shape: \", new_df.head())\n    new_df.to_csv(f\"{single_csv}_cumulative.csv\", index=False)\n    print(f\"new df is saved to {single_csv}_cumulative.csv\")\n    print(new_df.describe())\n\n\nif __name__ == \"__main__\":\n  # Run the download function\n#   download_gridmet_of_specific_variables(prepare_folder_and_get_year_list())\n#   turn_gridmet_nc_to_csv()\n#   plot_gridmet()\n\n  # prepare testing data with cumulative variables\n  prepare_cumulative_history_csvs()\n\n",
  "history_output" : "today date = 2023-11-20\ntest start date:  2022-10-15\ntest end date:  2023-10-07\n/home/chetana\n2022-10-15 00:00:00\n2022-10-01\nFile /home/chetana/gridmet_test_run/gridmet_climatology/tmmn_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/tmmx_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/pr_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/vpd_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/etr_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/rmax_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/rmin_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/vs_2022.nc exists\nChecking file: rmax_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_rmax_2022-10-01.csv already exists. Skipping..\nChecking file: vpd_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_vpd_2022-10-01.csv already exists. Skipping..\nChecking file: vs_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_vs_2022-10-01.csv already exists. Skipping..\nChecking file: tmmn_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_tmmn_2022-10-01.csv already exists. Skipping..\nChecking file: pr_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_pr_2022-10-01.csv already exists. Skipping..\nChecking file: tmmx_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_tmmx_2022-10-01.csv already exists. Skipping..\nChecking file: rmin_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_rmin_2022-10-01.csv already exists. Skipping..\nChecking file: etr_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_etr_2022-10-01.csv already exists. Skipping..\n2022-10-02\nFile /home/chetana/gridmet_test_run/gridmet_climatology/tmmn_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/tmmx_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/pr_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/vpd_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/etr_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/rmax_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/rmin_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/vs_2022.nc exists\nChecking file: rmax_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_rmax_2022-10-02.csv already exists. Skipping..\nChecking file: vpd_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_vpd_2022-10-02.csv already exists. Skipping..\nChecking file: vs_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_vs_2022-10-02.csv already exists. Skipping..\nChecking file: tmmn_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_tmmn_2022-10-02.csv already exists. Skipping..\nChecking file: pr_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_pr_2022-10-02.csv already exists. Skipping..\nChecking file: tmmx_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_tmmx_2022-10-02.csv already exists. Skipping..\nChecking file: rmin_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_rmin_2022-10-02.csv already exists. Skipping..\nChecking file: etr_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_etr_2022-10-02.csv already exists. Skipping..\n2022-10-03\nFile /home/chetana/gridmet_test_run/gridmet_climatology/tmmn_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/tmmx_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/pr_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/vpd_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/etr_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/rmax_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/rmin_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/vs_2022.nc exists\nChecking file: rmax_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_rmax_2022-10-03.csv already exists. Skipping..\nChecking file: vpd_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_vpd_2022-10-03.csv already exists. Skipping..\nChecking file: vs_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_vs_2022-10-03.csv already exists. Skipping..\nChecking file: tmmn_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_tmmn_2022-10-03.csv already exists. Skipping..\nChecking file: pr_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_pr_2022-10-03.csv already exists. Skipping..\nChecking file: tmmx_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_tmmx_2022-10-03.csv already exists. Skipping..\nChecking file: rmin_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_rmin_2022-10-03.csv already exists. Skipping..\nChecking file: etr_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_etr_2022-10-03.csv already exists. Skipping..\n2022-10-04\nFile /home/chetana/gridmet_test_run/gridmet_climatology/tmmn_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/tmmx_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/pr_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/vpd_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/etr_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/rmax_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/rmin_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/vs_2022.nc exists\nChecking file: rmax_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_rmax_2022-10-04.csv already exists. Skipping..\nChecking file: vpd_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_vpd_2022-10-04.csv already exists. Skipping..\nChecking file: vs_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_vs_2022-10-04.csv already exists. Skipping..\nChecking file: tmmn_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_tmmn_2022-10-04.csv already exists. Skipping..\nChecking file: pr_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_pr_2022-10-04.csv already exists. Skipping..\nChecking file: tmmx_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_tmmx_2022-10-04.csv already exists. Skipping..\nChecking file: rmin_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_rmin_2022-10-04.csv already exists. Skipping..\nChecking file: etr_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_etr_2022-10-04.csv already exists. Skipping..\n2022-10-05\nFile /home/chetana/gridmet_test_run/gridmet_climatology/tmmn_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/tmmx_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/pr_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/vpd_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/etr_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/rmax_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/rmin_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/vs_2022.nc exists\nChecking file: rmax_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_rmax_2022-10-05.csv already exists. Skipping..\nChecking file: vpd_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_vpd_2022-10-05.csv already exists. Skipping..\nChecking file: vs_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_vs_2022-10-05.csv already exists. Skipping..\nChecking file: tmmn_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_tmmn_2022-10-05.csv already exists. Skipping..\nChecking file: pr_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_pr_2022-10-05.csv already exists. Skipping..\nChecking file: tmmx_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_tmmx_2022-10-05.csv already exists. Skipping..\nChecking file: rmin_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_rmin_2022-10-05.csv already exists. Skipping..\nChecking file: etr_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_etr_2022-10-05.csv already exists. Skipping..\n2022-10-06\nFile /home/chetana/gridmet_test_run/gridmet_climatology/tmmn_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/tmmx_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/pr_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/vpd_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/etr_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/rmax_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/rmin_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/vs_2022.nc exists\nChecking file: rmax_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_rmax_2022-10-06.csv already exists. Skipping..\nChecking file: vpd_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_vpd_2022-10-06.csv already exists. Skipping..\nChecking file: vs_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_vs_2022-10-06.csv already exists. Skipping..\nChecking file: tmmn_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_tmmn_2022-10-06.csv already exists. Skipping..\nChecking file: pr_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_pr_2022-10-06.csv already exists. Skipping..\nChecking file: tmmx_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_tmmx_2022-10-06.csv already exists. Skipping..\nChecking file: rmin_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_rmin_2022-10-06.csv already exists. Skipping..\nChecking file: etr_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_etr_2022-10-06.csv already exists. Skipping..\n2022-10-07\nFile /home/chetana/gridmet_test_run/gridmet_climatology/tmmn_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/tmmx_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/pr_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/vpd_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/etr_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/rmax_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/rmin_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/vs_2022.nc exists\nChecking file: rmax_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_rmax_2022-10-07.csv already exists. Skipping..\nChecking file: vpd_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_vpd_2022-10-07.csv already exists. Skipping..\nChecking file: vs_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_vs_2022-10-07.csv already exists. Skipping..\nChecking file: tmmn_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_tmmn_2022-10-07.csv already exists. Skipping..\nChecking file: pr_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_pr_2022-10-07.csv already exists. Skipping..\nChecking file: tmmx_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_tmmx_2022-10-07.csv already exists. Skipping..\nChecking file: rmin_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_rmin_2022-10-07.csv already exists. Skipping..\nChecking file: etr_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_etr_2022-10-07.csv already exists. Skipping..\n2022-10-08\nFile /home/chetana/gridmet_test_run/gridmet_climatology/tmmn_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/tmmx_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/pr_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/vpd_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/etr_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/rmax_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/rmin_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/vs_2022.nc exists\nChecking file: rmax_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_rmax_2022-10-08.csv already exists. Skipping..\nChecking file: vpd_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_vpd_2022-10-08.csv already exists. Skipping..\nChecking file: vs_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_vs_2022-10-08.csv already exists. Skipping..\nChecking file: tmmn_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_tmmn_2022-10-08.csv already exists. Skipping..\nChecking file: pr_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_pr_2022-10-08.csv already exists. Skipping..\nChecking file: tmmx_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_tmmx_2022-10-08.csv already exists. Skipping..\nChecking file: rmin_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_rmin_2022-10-08.csv already exists. Skipping..\nChecking file: etr_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_etr_2022-10-08.csv already exists. Skipping..\n2022-10-09\nFile /home/chetana/gridmet_test_run/gridmet_climatology/tmmn_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/tmmx_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/pr_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/vpd_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/etr_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/rmax_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/rmin_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/vs_2022.nc exists\nChecking file: rmax_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_rmax_2022-10-09.csv already exists. Skipping..\nChecking file: vpd_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_vpd_2022-10-09.csv already exists. Skipping..\nChecking file: vs_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_vs_2022-10-09.csv already exists. Skipping..\nChecking file: tmmn_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_tmmn_2022-10-09.csv already exists. Skipping..\nChecking file: pr_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_pr_2022-10-09.csv already exists. Skipping..\nChecking file: tmmx_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_tmmx_2022-10-09.csv already exists. Skipping..\nChecking file: rmin_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_rmin_2022-10-09.csv already exists. Skipping..\nChecking file: etr_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_etr_2022-10-09.csv already exists. Skipping..\n2022-10-10\nFile /home/chetana/gridmet_test_run/gridmet_climatology/tmmn_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/tmmx_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/pr_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/vpd_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/etr_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/rmax_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/rmin_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/vs_2022.nc exists\nChecking file: rmax_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_rmax_2022-10-10.csv already exists. Skipping..\nChecking file: vpd_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_vpd_2022-10-10.csv already exists. Skipping..\nChecking file: vs_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_vs_2022-10-10.csv already exists. Skipping..\nChecking file: tmmn_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_tmmn_2022-10-10.csv already exists. Skipping..\nChecking file: pr_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_pr_2022-10-10.csv already exists. Skipping..\nChecking file: tmmx_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_tmmx_2022-10-10.csv already exists. Skipping..\nChecking file: rmin_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_rmin_2022-10-10.csv already exists. Skipping..\nChecking file: etr_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_etr_2022-10-10.csv already exists. Skipping..\n2022-10-11\nFile /home/chetana/gridmet_test_run/gridmet_climatology/tmmn_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/tmmx_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/pr_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/vpd_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/etr_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/rmax_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/rmin_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/vs_2022.nc exists\nChecking file: rmax_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_rmax_2022-10-11.csv already exists. Skipping..\nChecking file: vpd_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_vpd_2022-10-11.csv already exists. Skipping..\nChecking file: vs_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_vs_2022-10-11.csv already exists. Skipping..\nChecking file: tmmn_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_tmmn_2022-10-11.csv already exists. Skipping..\nChecking file: pr_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_pr_2022-10-11.csv already exists. Skipping..\nChecking file: tmmx_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_tmmx_2022-10-11.csv already exists. Skipping..\nChecking file: rmin_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_rmin_2022-10-11.csv already exists. Skipping..\nChecking file: etr_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_etr_2022-10-11.csv already exists. Skipping..\n2022-10-12\nFile /home/chetana/gridmet_test_run/gridmet_climatology/tmmn_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/tmmx_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/pr_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/vpd_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/etr_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/rmax_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/rmin_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/vs_2022.nc exists\nChecking file: rmax_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_rmax_2022-10-12.csv already exists. Skipping..\nChecking file: vpd_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_vpd_2022-10-12.csv already exists. Skipping..\nChecking file: vs_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_vs_2022-10-12.csv already exists. Skipping..\nChecking file: tmmn_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_tmmn_2022-10-12.csv already exists. Skipping..\nChecking file: pr_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_pr_2022-10-12.csv already exists. Skipping..\nChecking file: tmmx_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_tmmx_2022-10-12.csv already exists. Skipping..\nChecking file: rmin_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_rmin_2022-10-12.csv already exists. Skipping..\nChecking file: etr_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_etr_2022-10-12.csv already exists. Skipping..\n2022-10-13\nFile /home/chetana/gridmet_test_run/gridmet_climatology/tmmn_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/tmmx_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/pr_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/vpd_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/etr_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/rmax_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/rmin_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/vs_2022.nc exists\nChecking file: rmax_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_rmax_2022-10-13.csv already exists. Skipping..\nChecking file: vpd_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_vpd_2022-10-13.csv already exists. Skipping..\nChecking file: vs_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_vs_2022-10-13.csv already exists. Skipping..\nChecking file: tmmn_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_tmmn_2022-10-13.csv already exists. Skipping..\nChecking file: pr_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_pr_2022-10-13.csv already exists. Skipping..\nChecking file: tmmx_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_tmmx_2022-10-13.csv already exists. Skipping..\nChecking file: rmin_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_rmin_2022-10-13.csv already exists. Skipping..\nChecking file: etr_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_etr_2022-10-13.csv already exists. Skipping..\n2022-10-14\nFile /home/chetana/gridmet_test_run/gridmet_climatology/tmmn_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/tmmx_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/pr_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/vpd_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/etr_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/rmax_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/rmin_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/vs_2022.nc exists\nChecking file: rmax_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_rmax_2022-10-14.csv already exists. Skipping..\nChecking file: vpd_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_vpd_2022-10-14.csv already exists. Skipping..\nChecking file: vs_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_vs_2022-10-14.csv already exists. Skipping..\nChecking file: tmmn_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_tmmn_2022-10-14.csv already exists. Skipping..\nChecking file: pr_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_pr_2022-10-14.csv already exists. Skipping..\nChecking file: tmmx_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_tmmx_2022-10-14.csv already exists. Skipping..\nChecking file: rmin_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_rmin_2022-10-14.csv already exists. Skipping..\nChecking file: etr_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_etr_2022-10-14.csv already exists. Skipping..\n2022-10-15\nFile /home/chetana/gridmet_test_run/gridmet_climatology/tmmn_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/tmmx_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/pr_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/vpd_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/etr_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/rmax_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/rmin_2022.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/vs_2022.nc exists\nChecking file: rmax_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_rmax_2022-10-15.csv already exists. Skipping..\nChecking file: vpd_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_vpd_2022-10-15.csv already exists. Skipping..\nChecking file: vs_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_vs_2022-10-15.csv already exists. Skipping..\nChecking file: tmmn_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_tmmn_2022-10-15.csv already exists. Skipping..\nChecking file: pr_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_pr_2022-10-15.csv already exists. Skipping..\nChecking file: tmmx_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_tmmx_2022-10-15.csv already exists. Skipping..\nChecking file: rmin_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_rmin_2022-10-15.csv already exists. Skipping..\nChecking file: etr_2022.nc\n/home/chetana/gridmet_test_run/testing_output/2022_etr_2022-10-15.csv already exists. Skipping..\ndate_keyed_objects:  {'2022-10-01': ['/home/chetana/gridmet_test_run/testing_output/2022_rmax_2022-10-01.csv', '/home/chetana/gridmet_test_run/testing_output/2022_vpd_2022-10-01.csv', '/home/chetana/gridmet_test_run/testing_output/2022_vs_2022-10-01.csv', '/home/chetana/gridmet_test_run/testing_output/2022_tmmn_2022-10-01.csv', '/home/chetana/gridmet_test_run/testing_output/2022_pr_2022-10-01.csv', '/home/chetana/gridmet_test_run/testing_output/2022_tmmx_2022-10-01.csv', '/home/chetana/gridmet_test_run/testing_output/2022_rmin_2022-10-01.csv', '/home/chetana/gridmet_test_run/testing_output/2022_etr_2022-10-01.csv'], '2022-10-02': ['/home/chetana/gridmet_test_run/testing_output/2022_rmax_2022-10-02.csv', '/home/chetana/gridmet_test_run/testing_output/2022_vpd_2022-10-02.csv', '/home/chetana/gridmet_test_run/testing_output/2022_vs_2022-10-02.csv', '/home/chetana/gridmet_test_run/testing_output/2022_tmmn_2022-10-02.csv', '/home/chetana/gridmet_test_run/testing_output/2022_pr_2022-10-02.csv', '/home/chetana/gridmet_test_run/testing_output/2022_tmmx_2022-10-02.csv', '/home/chetana/gridmet_test_run/testing_output/2022_rmin_2022-10-02.csv', '/home/chetana/gridmet_test_run/testing_output/2022_etr_2022-10-02.csv'], '2022-10-03': ['/home/chetana/gridmet_test_run/testing_output/2022_rmax_2022-10-03.csv', '/home/chetana/gridmet_test_run/testing_output/2022_vpd_2022-10-03.csv', '/home/chetana/gridmet_test_run/testing_output/2022_vs_2022-10-03.csv', '/home/chetana/gridmet_test_run/testing_output/2022_tmmn_2022-10-03.csv', '/home/chetana/gridmet_test_run/testing_output/2022_pr_2022-10-03.csv', '/home/chetana/gridmet_test_run/testing_output/2022_tmmx_2022-10-03.csv', '/home/chetana/gridmet_test_run/testing_output/2022_rmin_2022-10-03.csv', '/home/chetana/gridmet_test_run/testing_output/2022_etr_2022-10-03.csv'], '2022-10-04': ['/home/chetana/gridmet_test_run/testing_output/2022_rmax_2022-10-04.csv', '/home/chetana/gridmet_test_run/testing_output/2022_vpd_2022-10-04.csv', '/home/chetana/gridmet_test_run/testing_output/2022_vs_2022-10-04.csv', '/home/chetana/gridmet_test_run/testing_output/2022_tmmn_2022-10-04.csv', '/home/chetana/gridmet_test_run/testing_output/2022_pr_2022-10-04.csv', '/home/chetana/gridmet_test_run/testing_output/2022_tmmx_2022-10-04.csv', '/home/chetana/gridmet_test_run/testing_output/2022_rmin_2022-10-04.csv', '/home/chetana/gridmet_test_run/testing_output/2022_etr_2022-10-04.csv'], '2022-10-05': ['/home/chetana/gridmet_test_run/testing_output/2022_rmax_2022-10-05.csv', '/home/chetana/gridmet_test_run/testing_output/2022_vpd_2022-10-05.csv', '/home/chetana/gridmet_test_run/testing_output/2022_vs_2022-10-05.csv', '/home/chetana/gridmet_test_run/testing_output/2022_tmmn_2022-10-05.csv', '/home/chetana/gridmet_test_run/testing_output/2022_pr_2022-10-05.csv', '/home/chetana/gridmet_test_run/testing_output/2022_tmmx_2022-10-05.csv', '/home/chetana/gridmet_test_run/testing_output/2022_rmin_2022-10-05.csv', '/home/chetana/gridmet_test_run/testing_output/2022_etr_2022-10-05.csv'], '2022-10-06': ['/home/chetana/gridmet_test_run/testing_output/2022_rmax_2022-10-06.csv', '/home/chetana/gridmet_test_run/testing_output/2022_vpd_2022-10-06.csv', '/home/chetana/gridmet_test_run/testing_output/2022_vs_2022-10-06.csv', '/home/chetana/gridmet_test_run/testing_output/2022_tmmn_2022-10-06.csv', '/home/chetana/gridmet_test_run/testing_output/2022_pr_2022-10-06.csv', '/home/chetana/gridmet_test_run/testing_output/2022_tmmx_2022-10-06.csv', '/home/chetana/gridmet_test_run/testing_output/2022_rmin_2022-10-06.csv', '/home/chetana/gridmet_test_run/testing_output/2022_etr_2022-10-06.csv'], '2022-10-07': ['/home/chetana/gridmet_test_run/testing_output/2022_rmax_2022-10-07.csv', '/home/chetana/gridmet_test_run/testing_output/2022_vpd_2022-10-07.csv', '/home/chetana/gridmet_test_run/testing_output/2022_vs_2022-10-07.csv', '/home/chetana/gridmet_test_run/testing_output/2022_tmmn_2022-10-07.csv', '/home/chetana/gridmet_test_run/testing_output/2022_pr_2022-10-07.csv', '/home/chetana/gridmet_test_run/testing_output/2022_tmmx_2022-10-07.csv', '/home/chetana/gridmet_test_run/testing_output/2022_rmin_2022-10-07.csv', '/home/chetana/gridmet_test_run/testing_output/2022_etr_2022-10-07.csv'], '2022-10-08': ['/home/chetana/gridmet_test_run/testing_output/2022_rmax_2022-10-08.csv', '/home/chetana/gridmet_test_run/testing_output/2022_vpd_2022-10-08.csv', '/home/chetana/gridmet_test_run/testing_output/2022_vs_2022-10-08.csv', '/home/chetana/gridmet_test_run/testing_output/2022_tmmn_2022-10-08.csv', '/home/chetana/gridmet_test_run/testing_output/2022_pr_2022-10-08.csv', '/home/chetana/gridmet_test_run/testing_output/2022_tmmx_2022-10-08.csv', '/home/chetana/gridmet_test_run/testing_output/2022_rmin_2022-10-08.csv', '/home/chetana/gridmet_test_run/testing_output/2022_etr_2022-10-08.csv'], '2022-10-09': ['/home/chetana/gridmet_test_run/testing_output/2022_rmax_2022-10-09.csv', '/home/chetana/gridmet_test_run/testing_output/2022_vpd_2022-10-09.csv', '/home/chetana/gridmet_test_run/testing_output/2022_vs_2022-10-09.csv', '/home/chetana/gridmet_test_run/testing_output/2022_tmmn_2022-10-09.csv', '/home/chetana/gridmet_test_run/testing_output/2022_pr_2022-10-09.csv', '/home/chetana/gridmet_test_run/testing_output/2022_tmmx_2022-10-09.csv', '/home/chetana/gridmet_test_run/testing_output/2022_rmin_2022-10-09.csv', '/home/chetana/gridmet_test_run/testing_output/2022_etr_2022-10-09.csv'], '2022-10-10': ['/home/chetana/gridmet_test_run/testing_output/2022_rmax_2022-10-10.csv', '/home/chetana/gridmet_test_run/testing_output/2022_vpd_2022-10-10.csv', '/home/chetana/gridmet_test_run/testing_output/2022_vs_2022-10-10.csv', '/home/chetana/gridmet_test_run/testing_output/2022_tmmn_2022-10-10.csv', '/home/chetana/gridmet_test_run/testing_output/2022_pr_2022-10-10.csv', '/home/chetana/gridmet_test_run/testing_output/2022_tmmx_2022-10-10.csv', '/home/chetana/gridmet_test_run/testing_output/2022_rmin_2022-10-10.csv', '/home/chetana/gridmet_test_run/testing_output/2022_etr_2022-10-10.csv'], '2022-10-11': ['/home/chetana/gridmet_test_run/testing_output/2022_rmax_2022-10-11.csv', '/home/chetana/gridmet_test_run/testing_output/2022_vpd_2022-10-11.csv', '/home/chetana/gridmet_test_run/testing_output/2022_vs_2022-10-11.csv', '/home/chetana/gridmet_test_run/testing_output/2022_tmmn_2022-10-11.csv', '/home/chetana/gridmet_test_run/testing_output/2022_pr_2022-10-11.csv', '/home/chetana/gridmet_test_run/testing_output/2022_tmmx_2022-10-11.csv', '/home/chetana/gridmet_test_run/testing_output/2022_rmin_2022-10-11.csv', '/home/chetana/gridmet_test_run/testing_output/2022_etr_2022-10-11.csv'], '2022-10-12': ['/home/chetana/gridmet_test_run/testing_output/2022_rmax_2022-10-12.csv', '/home/chetana/gridmet_test_run/testing_output/2022_vpd_2022-10-12.csv', '/home/chetana/gridmet_test_run/testing_output/2022_vs_2022-10-12.csv', '/home/chetana/gridmet_test_run/testing_output/2022_tmmn_2022-10-12.csv', '/home/chetana/gridmet_test_run/testing_output/2022_pr_2022-10-12.csv', '/home/chetana/gridmet_test_run/testing_output/2022_tmmx_2022-10-12.csv', '/home/chetana/gridmet_test_run/testing_output/2022_rmin_2022-10-12.csv', '/home/chetana/gridmet_test_run/testing_output/2022_etr_2022-10-12.csv'], '2022-10-13': ['/home/chetana/gridmet_test_run/testing_output/2022_rmax_2022-10-13.csv', '/home/chetana/gridmet_test_run/testing_output/2022_vpd_2022-10-13.csv', '/home/chetana/gridmet_test_run/testing_output/2022_vs_2022-10-13.csv', '/home/chetana/gridmet_test_run/testing_output/2022_tmmn_2022-10-13.csv', '/home/chetana/gridmet_test_run/testing_output/2022_pr_2022-10-13.csv', '/home/chetana/gridmet_test_run/testing_output/2022_tmmx_2022-10-13.csv', '/home/chetana/gridmet_test_run/testing_output/2022_rmin_2022-10-13.csv', '/home/chetana/gridmet_test_run/testing_output/2022_etr_2022-10-13.csv'], '2022-10-14': ['/home/chetana/gridmet_test_run/testing_output/2022_rmax_2022-10-14.csv', '/home/chetana/gridmet_test_run/testing_output/2022_vpd_2022-10-14.csv', '/home/chetana/gridmet_test_run/testing_output/2022_vs_2022-10-14.csv', '/home/chetana/gridmet_test_run/testing_output/2022_tmmn_2022-10-14.csv', '/home/chetana/gridmet_test_run/testing_output/2022_pr_2022-10-14.csv', '/home/chetana/gridmet_test_run/testing_output/2022_tmmx_2022-10-14.csv', '/home/chetana/gridmet_test_run/testing_output/2022_rmin_2022-10-14.csv', '/home/chetana/gridmet_test_run/testing_output/2022_etr_2022-10-14.csv'], '2022-10-15': ['/home/chetana/gridmet_test_run/testing_output/2022_rmax_2022-10-15.csv', '/home/chetana/gridmet_test_run/testing_output/2022_vpd_2022-10-15.csv', '/home/chetana/gridmet_test_run/testing_output/2022_vs_2022-10-15.csv', '/home/chetana/gridmet_test_run/testing_output/2022_tmmn_2022-10-15.csv', '/home/chetana/gridmet_test_run/testing_output/2022_pr_2022-10-15.csv', '/home/chetana/gridmet_test_run/testing_output/2022_tmmx_2022-10-15.csv', '/home/chetana/gridmet_test_run/testing_output/2022_rmin_2022-10-15.csv', '/home/chetana/gridmet_test_run/testing_output/2022_etr_2022-10-15.csv']}\ncreating cumulative for /home/chetana/gridmet_test_run/testing_output/2022_rmax_2022-10-15.csv\n/home/chetana/gridmet_test_run/testing_output/2022_rmax_2022-10-15.csv_cumulative.csv already exists, skipping..\ncreating cumulative for /home/chetana/gridmet_test_run/testing_output/2022_vpd_2022-10-15.csv\n/home/chetana/gridmet_test_run/testing_output/2022_vpd_2022-10-15.csv_cumulative.csv already exists, skipping..\ncreating cumulative for /home/chetana/gridmet_test_run/testing_output/2022_vs_2022-10-15.csv\n/home/chetana/gridmet_test_run/testing_output/2022_vs_2022-10-15.csv_cumulative.csv already exists, skipping..\ncreating cumulative for /home/chetana/gridmet_test_run/testing_output/2022_tmmn_2022-10-15.csv\n/home/chetana/gridmet_test_run/testing_output/2022_tmmn_2022-10-15.csv_cumulative.csv already exists, skipping..\ncreating cumulative for /home/chetana/gridmet_test_run/testing_output/2022_pr_2022-10-15.csv\n/home/chetana/gridmet_test_run/testing_output/2022_pr_2022-10-15.csv_cumulative.csv already exists, skipping..\ncreating cumulative for /home/chetana/gridmet_test_run/testing_output/2022_tmmx_2022-10-15.csv\n/home/chetana/gridmet_test_run/testing_output/2022_tmmx_2022-10-15.csv_cumulative.csv already exists, skipping..\ncreating cumulative for /home/chetana/gridmet_test_run/testing_output/2022_rmin_2022-10-15.csv\n/home/chetana/gridmet_test_run/testing_output/2022_rmin_2022-10-15.csv_cumulative.csv already exists, skipping..\ncreating cumulative for /home/chetana/gridmet_test_run/testing_output/2022_etr_2022-10-15.csv\n/home/chetana/gridmet_test_run/testing_output/2022_etr_2022-10-15.csv_cumulative.csv already exists, skipping..\n",
  "history_begin_time" : 1700448521733,
  "history_end_time" : 1700448522701,
  "history_notes" : null,
  "history_process" : "drwmbo",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "bvrj82v1rvr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500139,
  "history_end_time" : 1700448500139,
  "history_notes" : null,
  "history_process" : "2n7b06",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "i7tepyyniur",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500139,
  "history_end_time" : 1700448500139,
  "history_notes" : null,
  "history_process" : "bwdy3s",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vf9y0kaihn5",
  "history_input" : "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import timedelta, datetime\nimport numpy as np\nimport uuid\nfrom snowcast_utils import day_index\nimport matplotlib.colors as mcolors\n\n# Import utility functions and variables from 'snowcast_utils'\nfrom snowcast_utils import homedir, work_dir, test_start_date\n\n# Define a custom colormap with specified colors and ranges\ncolors = [\n    (0.8627, 0.8627, 0.8627),  # #DCDCDC - 0 - 1\n    (0.8627, 1.0000, 1.0000),  # #DCFFFF - 1 - 2\n    (0.6000, 1.0000, 1.0000),  # #99FFFF - 2 - 4\n    (0.5569, 0.8235, 1.0000),  # #8ED2FF - 4 - 6\n    (0.4509, 0.6196, 0.8745),  # #739EDF - 6 - 8\n    (0.4157, 0.4706, 1.0000),  # #6A78FF - 8 - 10\n    (0.4235, 0.2784, 1.0000),  # #6C47FF - 10 - 12\n    (0.5529, 0.0980, 1.0000),  # #8D19FF - 12 - 14\n    (0.7333, 0.0000, 0.9176),  # #BB00EA - 14 - 16\n    (0.8392, 0.0000, 0.7490),  # #D600BF - 16 - 18\n    (0.7569, 0.0039, 0.4549),  # #C10074 - 18 - 20\n    (0.6784, 0.0000, 0.1961),  # #AD0032 - 20 - 30\n    (0.5020, 0.0000, 0.0000)   # #800000 - > 30\n]\n\ncmap_name = 'custom_snow_colormap'\ncustom_cmap = mcolors.ListedColormap(colors)\n\n# Define the lat_lon_to_map_coordinates function\ndef lat_lon_to_map_coordinates(lon, lat, m):\n    \"\"\"\n    Convert latitude and longitude coordinates to map coordinates.\n\n    Args:\n        lon (float or array-like): Longitude coordinate(s).\n        lat (float or array-like): Latitude coordinate(s).\n        m (Basemap): Basemap object representing the map projection.\n\n    Returns:\n        tuple: Tuple containing the converted map coordinates (x, y).\n    \"\"\"\n    x, y = m(lon, lat)\n    return x, y\n\n# Define value ranges for color mapping\nfixed_value_ranges = [1, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 30]\n\ndef create_color_maps_with_value_range(df_col, value_ranges=None):\n    \"\"\"\n    Create a colormap for value ranges and map data values to colors.\n\n    Args:\n        df_col (pd.Series): A Pandas Series containing data values.\n        value_ranges (list, optional): A list of value ranges for color mapping.\n            If not provided, the ranges will be determined automatically.\n\n    Returns:\n        tuple: Tuple containing the color mapping and the updated value ranges.\n    \"\"\"\n    new_value_ranges = value_ranges\n    if value_ranges is None:\n        max_value = df_col.max()\n        min_value = df_col.min()\n        if min_value < 0:\n            min_value = 0\n        step_size = (max_value - min_value) / 12\n\n        # Create 10 periods\n        new_value_ranges = [min_value + i * step_size for i in range(12)]\n    \n    print(\"new_value_ranges: \", new_value_ranges)\n  \n    # Define a custom function to map data values to colors\n    def map_value_to_color(value):\n        # Iterate through the value ranges to find the appropriate color index\n        for i, range_max in enumerate(new_value_ranges):\n            if value <= range_max:\n                return colors[i]\n\n        # If the value is greater than the largest range, return the last color\n        return colors[-1]\n\n    # Map predicted_swe values to colors using the custom function\n    color_mapping = [map_value_to_color(value) for value in df_col.values]\n    return color_mapping, new_value_ranges\n\ndef convert_csvs_to_images():\n    \"\"\"\n    Convert CSV data to images with color-coded SWE predictions.\n\n    Returns:\n        None\n    \"\"\"\n    global fixed_value_ranges\n    data = pd.read_csv(f\"{homedir}/gridmet_test_run/test_data_predicted_n97KJ.csv\")\n    print(\"statistic of predicted_swe: \", data['predicted_swe'].describe())\n    data['predicted_swe'].fillna(0, inplace=True)\n    \n    for column in data.columns:\n        column_data = data[column]\n        print(column_data.describe())\n    \n    # Create a figure with a white background\n    fig = plt.figure(facecolor='white')\n\n    lon_min, lon_max = -125, -100\n    lat_min, lat_max = 25, 49.5\n\n    m = Basemap(llcrnrlon=lon_min, llcrnrlat=lat_min, urcrnrlon=lon_max, urcrnrlat=lat_max,\n                projection='merc', resolution='i')\n\n    x, y = m(data['lon'].values, data['lat'].values)\n    print(data.columns)\n\n    color_mapping, value_ranges = create_color_maps_with_value_range(data[\"predicted_swe\"], fixed_value_ranges)\n    \n    # Plot the data using the custom colormap\n    plt.scatter(x, y, c=color_mapping, cmap=custom_cmap, s=30, edgecolors='none', alpha=0.7)\n    \n    # Draw coastlines and other map features\n    m.drawcoastlines()\n    m.drawcountries()\n    m.drawstates()\n\n    reference_date = datetime(1900, 1, 1)\n    day_value = day_index\n    \n    result_date = reference_date + timedelta(days=day_value)\n    today = result_date.strftime(\"%Y-%m-%d\")\n    timestamp_string = result_date.strftime(\"%Y-%m-%d\")\n    \n    # Add a title\n    plt.title(f'Predicted SWE in the Western US - {today}', pad=20)\n\n    # Add labels for latitude and longitude on x and y axes with smaller font size\n    plt.xlabel('Longitude', fontsize=6)\n    plt.ylabel('Latitude', fontsize=6)\n\n    # Add longitude values to the x-axis and adjust font size\n    x_ticks_labels = np.arange(lon_min, lon_max + 5, 5)\n    x_tick_labels_str = [f\"{lon:.1f}W\" if lon < 0 else f\"{lon:.1f}E\" for lon in x_ticks_labels]\n    plt.xticks(*m(x_ticks_labels, [lat_min] * len(x_ticks_labels)), fontsize=6)\n    plt.gca().set_xticklabels(x_tick_labels_str)\n\n    # Add latitude values to the y-axis and adjust font size\n    y_ticks_labels = np.arange(lat_min, lat_max + 5, 5)\n    y_tick_labels_str = [f\"{lat:.1f}N\" if lat >= 0 else f\"{abs(lat):.1f}S\" for lat in y_ticks_labels]\n    plt.yticks(*m([lon_min] * len(y_ticks_labels), y_ticks_labels), fontsize=6)\n    plt.gca().set_yticklabels(y_tick_labels_str)\n\n    # Convert map coordinates to latitude and longitude for y-axis labels\n    y_tick_positions = np.linspace(lat_min, lat_max, len(y_ticks_labels))\n    y_tick_positions_map_x, y_tick_positions_map_y = lat_lon_to_map_coordinates([lon_min] * len(y_ticks_labels), y_tick_positions, m)\n    y_tick_positions_lat, _ = m(y_tick_positions_map_x, y_tick_positions_map_y, inverse=True)\n    y_tick_positions_lat_str = [f\"{lat:.1f}N\" if lat >= 0 else f\"{abs(lat):.1f}S\" for lat in y_tick_positions_lat]\n    plt.yticks(y_tick_positions_map_y, y_tick_positions_lat_str, fontsize=6)\n\n    # Create custom legend elements using the same colormap\n    legend_elements = [Patch(color=colors[i], label=f\"{value_ranges[i]} - {value_ranges[i+1]-1}\" if i < len(value_ranges) - 1 else f\"> {value_ranges[-1]}\") for i in range(len(value_ranges))]\n\n    # Create the legend outside the map\n    legend = plt.legend(handles=legend_elements, loc='upper left', title='Legend', fontsize=8)\n    legend.set_bbox_to_anchor((1.01, 1)) \n\n    # Remove the color bar\n    #plt.colorbar().remove()\n\n    plt.text(0.98, 0.02, 'Copyright  SWE Wormhole Team',\n             horizontalalignment='right', verticalalignment='bottom',\n             transform=plt.gcf().transFigure, fontsize=6, color='black')\n\n    # Set the aspect ratio to 'equal' to keep the plot at the center\n    plt.gca().set_aspect('equal', adjustable='box')\n\n    # Adjust the bottom and top margins to create more white space between the title and the plot\n    plt.subplots_adjust(bottom=0.15, right=0.80)  # Adjust right margin to accommodate the legend\n    # Show the plot or save it to a file\n    new_plot_path = f'{homedir}/gridmet_test_run/predicted_swe-{test_start_date}.png'\n    print(f\"The new plot is saved to {new_plot_path}\")\n    plt.savefig(new_plot_path)\n    # plt.show()  # Uncomment this line if you want to display the plot directly instead of saving it to a file\n\ndef convert_csvs_to_images_simple(target_date=test_start_date, column_name = \"predicted_swe\"):\n    \"\"\"\n    Convert CSV data to simple scatter plot images for predicted SWE.\n\n    Returns:\n        None\n    \"\"\"\n    selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n    var_name = column_name\n    test_csv = f\"{homedir}/gridmet_test_run/test_data_predicted_latest.csv\"\n    #print(f\"reading {test_csv}\")\n    result_var_df = pd.read_csv(test_csv)\n    #result_var_df['date'] = pd.to_datetime(result_var_df['date'], format='%Y-%m-%d')\n    #target_date_obj = pd.to_datetime(target_date)\n    # Filter the DataFrame to get all rows for the specific date\n    #result_var_df = result_var_df[result_var_df['date'] == target_date_obj]\n    # Convert the 'date' column to datetime\n    result_var_df['date'] = pd.to_datetime(result_var_df['date'])\n\n    # Filter the DataFrame based on the target date\n    #result_var_df = result_var_df[result_var_df['date'].dt.strftime('%Y-%m-%d') == target_date]\n    print(\"only one day is seletected\")\n    #print(result_var_df.head())\n    #result_var_df.replace('--', pd.NA, inplace=True)\n    #result_var_df.dropna(inplace=True)\n    result_var_df[var_name] = pd.to_numeric(result_var_df[var_name], errors='coerce')\n    #print(result_var_df[var_name].describe())\n    \n    colormaplist, value_ranges = create_color_maps_with_value_range(result_var_df[var_name], fixed_value_ranges)\n\n    # Create a scatter plot\n    plt.scatter(result_var_df[\"lon\"].values, \n                result_var_df[\"lat\"].values, \n                label=column_name, \n                c=result_var_df[column_name], \n                cmap='viridis', \n                #s=200, \n                s=10, \n                marker='s',\n                edgecolor='none',\n               )\n\n    # Add a colorbar\n    cbar = plt.colorbar()\n    cbar.set_label(column_name)  # Label for the colorbar\n    \n    # Add labels and a legend\n    plt.xlabel('Longitude')\n    plt.ylabel('Latitude')\n    plt.title(f'{column_name} Map {target_date}')\n    plt.legend()\n\n    res_png_path = f\"{work_dir}/testing_output/{str(selected_date.year)}_{var_name}_{target_date}.png\"\n    plt.savefig(res_png_path)\n    print(f\"test image is saved at {res_png_path}\")\n    plt.close()\n\n# Uncomment the function call you want to use:\n#convert_csvs_to_images()\n\n# plot the predicted SWE first\nconvert_csvs_to_images_simple(test_start_date)\n\n# plot all the other variables\ntest_csv = f\"{homedir}/gridmet_test_run/test_data_predicted_latest.csv\"\ndf = pd.read_csv(test_csv)\n\n# Get all column names\ncolumn_names = df.columns\nfor column_name in column_names:\n    print(column_name)\n    convert_csvs_to_images_simple(test_start_date, column_name)\n\n# Define the start and end dates\n# start_date = datetime(2017, 10, 1)\n# end_date = datetime(2018, 7, 1)\n\n# # Initialize the current date to the start date\n# current_date = start_date\n\n# # Define a one-day timedelta\n# one_day = timedelta(days=1)\n\n# # Iterate through each day in the date range\n# while current_date < end_date:\n#     print(current_date.strftime('%Y-%m-%d'))\n#     convert_csvs_to_images_simple(current_date.strftime('%Y-%m-%d'))\n#     current_date += one_day\n",
  "history_output" : "today date = 2023-11-20\ntest start date:  2022-10-15\ntest end date:  2023-10-07\n/home/chetana\nonly one day is seletected\nnew_value_ranges:  [1, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 30]\n/home/chetana/gw-workspace/vf9y0kaihn5/convert_results_to_images.py:239: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n  plt.savefig(res_png_path)\ntest image is saved at /home/chetana/gridmet_test_run/testing_output/2022_predicted_swe_2022-10-15.png\nSWE\nonly one day is seletected\nnew_value_ranges:  [1, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 30]\n/home/chetana/gw-workspace/vf9y0kaihn5/convert_results_to_images.py:239: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n  plt.savefig(res_png_path)\n",
  "history_begin_time" : 1700448578417,
  "history_end_time" : 1700448598502,
  "history_notes" : null,
  "history_process" : "2wkl6e",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "minbixiom44",
  "history_input" : "import distutils.dir_util\nfrom snowcast_utils import work_dir\nimport os\nimport shutil\n\n\nprint(\"move the plots and the results into the http folder\")\n\ndef copy_if_modified(source_file, destination_file):\n    if os.path.exists(destination_file):\n        source_modified_time = os.path.getmtime(source_file)\n        dest_modified_time = os.path.getmtime(destination_file)\n        \n        # If the source file is modified after the destination file\n        if source_modified_time > dest_modified_time:\n            shutil.copy(source_file, destination_file)\n            print(f'Copied: {source_file}')\n    else:\n        shutil.copy(source_file, destination_file)\n        print(f'Copied: {source_file}')\n\nsource_folder = f\"{work_dir}/var_comparison/\"\ndestination_folder = f\"/var/www/html/swe_forecasting/plots/\"\n\n# Copy the folder with overwriting existing files/folders\ndistutils.dir_util.copy_tree(source_folder, destination_folder, update=1)\n\nprint(f\"Folder '{source_folder}' copied to '{destination_folder}' with overwriting.\")\n\n\n# copy the png from testing_output to plots\nsource_folder = f\"{work_dir}/testing_output/\"\n\n# Ensure the destination folder exists, create it if necessary\nif not os.path.exists(destination_folder):\n    os.makedirs(destination_folder)\n\n# Loop through the files in the source folder\nfor filename in os.listdir(source_folder):\n    # Check if the file is a PNG file\n    if filename.endswith('.png'):\n        # Build the source and destination file paths\n        source_file = os.path.join(source_folder, filename)\n        destination_file = os.path.join(destination_folder, filename)\n        \n        # Copy the file from the source to the destination\n        copy_if_modified(source_file, destination_file)\n        \n",
  "history_output" : "today date = 2023-11-20\ntest start date:  2022-10-15\ntest end date:  2023-10-07\n/home/chetana\nmove the plots and the results into the http folder\nFolder '/home/chetana/gridmet_test_run/var_comparison/' copied to '/var/www/html/swe_forecasting/plots/' with overwriting.\n",
  "history_begin_time" : 1700448624553,
  "history_end_time" : 1700448627218,
  "history_notes" : null,
  "history_process" : "i2fynz",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "h42ji6kmx6v",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500142,
  "history_end_time" : 1700448500142,
  "history_notes" : null,
  "history_process" : "2o6cp8",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6bx3ew09qcw",
  "history_input" : "\"\"\"\nScript for downloading AMSR snow data, converting it to DEM format, and saving as a CSV file.\n\nThis script downloads AMSR snow data, converts it to a format compatible with DEM, and saves it as a CSV file.\nIt utilizes the h5py library to read HDF5 files, pandas for data manipulation, and scipy.spatial.KDTree\nfor finding the nearest grid points. The script also checks if the target CSV file already exists to avoid redundant\ndownloads and processing.\n\nUsage:\n    Run this script to download and convert AMSR snow data for a specific date. It depends on the test_start_date from snowcast_utils to specify which date to download. You can overwrite that.\n\n\"\"\"\n\nimport os\nimport h5py\nimport subprocess\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom snowcast_utils import work_dir, test_start_date\nfrom scipy.spatial import KDTree\nimport time\nfrom datetime import datetime, timedelta, date\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\n\nlatlontree = None\n\ndef find_closest_index_numpy(target_latitude, target_longitude, lat_grid, lon_grid):\n    # Calculate the squared Euclidean distance between the target point and all grid points\n    distance_squared = (lat_grid - target_latitude)**2 + (lon_grid - target_longitude)**2\n    \n    # Find the indices of the minimum distance\n    lat_idx, lon_idx = np.unravel_index(np.argmin(distance_squared), distance_squared.shape)\n    \n    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]\n\ndef find_closest_index_tree(target_latitude, target_longitude, lat_grid, lon_grid):\n    \"\"\"\n    Find the closest grid point indices for a target latitude and longitude using KDTree.\n\n    Parameters:\n        target_latitude (float): Target latitude.\n        target_longitude (float): Target longitude.\n        lat_grid (numpy.ndarray): Array of latitude values.\n        lon_grid (numpy.ndarray): Array of longitude values.\n\n    Returns:\n        int: Latitude index.\n        int: Longitude index.\n        float: Closest latitude value.\n        float: Closest longitude value.\n    \"\"\"\n    global latlontree\n    \n    if latlontree is None:\n        # Create a KD-Tree from lat_grid and lon_grid\n        lat_grid_cleaned = np.nan_to_num(lat_grid, nan=0.0)  # Replace NaN with 0\n        lon_grid_cleaned = np.nan_to_num(lon_grid, nan=0.0)  # Replace NaN with 0\n        latlontree = KDTree(list(zip(lat_grid_cleaned.ravel(), lon_grid_cleaned.ravel())))\n      \n    # Query the KD-Tree to find the nearest point\n    distance, index = latlontree.query([target_latitude, target_longitude])\n\n    # Convert the 1D index to 2D grid indices\n    lat_idx, lon_idx = np.unravel_index(index, lat_grid.shape)\n\n    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]\n\ndef find_closest_index(target_latitude, target_longitude, lat_grid, lon_grid):\n    \"\"\"\n    Find the closest grid point indices for a target latitude and longitude.\n\n    Parameters:\n        target_latitude (float): Target latitude.\n        target_longitude (float): Target longitude.\n        lat_grid (numpy.ndarray): Array of latitude values.\n        lon_grid (numpy.ndarray): Array of longitude values.\n\n    Returns:\n        int: Latitude index.\n        int: Longitude index.\n        float: Closest latitude value.\n        float: Closest longitude value.\n    \"\"\"\n    lat_diff = np.float64(np.abs(lat_grid - target_latitude))\n    lon_diff = np.float64(np.abs(lon_grid - target_longitude))\n\n    # Find the indices corresponding to the minimum differences\n    lat_idx, lon_idx = np.unravel_index(np.argmin(lat_diff + lon_diff), lat_grid.shape)\n\n    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]\n\n  \ndef prepare_amsr_grid_mapper():\n    df = pd.DataFrame(columns=['amsr_lat', 'amsr_lon', \n                               'amsr_lat_idx', 'amsr_lon_idx',\n                               'gridmet_lat', 'gridmet_lon'])\n    date = test_start_date\n    date = date.replace(\"-\", \".\")\n    he5_date = date.replace(\".\", \"\")\n    \n    # Check if the CSV already exists\n    target_csv_path = f'{work_dir}/amsr_to_gridmet_mapper.csv'\n    if os.path.exists(target_csv_path):\n        print(f\"File {target_csv_path} already exists, skipping..\")\n        return\n    \n    target_amsr_hdf_path = f\"{work_dir}/amsr_testing/testing_amsr_{date}.he5\"\n    if os.path.exists(target_amsr_hdf_path):\n        print(f\"File {target_amsr_hdf_path} already exists, skip downloading..\")\n    else:\n        cmd = f\"curl --output {target_amsr_hdf_path} -b ~/.urs_cookies -c ~/.urs_cookies -L -n -O https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/{date}/AMSR_U2_L3_DailySnow_B02_{he5_date}.he5\"\n        print(f'Running command: {cmd}')\n        subprocess.run(cmd, shell=True)\n    \n    # Read the HDF\n    file = h5py.File(target_amsr_hdf_path, 'r')\n    hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n    lat = hem_group['lat'][:]\n    lon = hem_group['lon'][:]\n    \n    # Replace NaN values with 0\n    lat = np.nan_to_num(lat, nan=0.0)\n    lon = np.nan_to_num(lon, nan=0.0)\n    \n    # Convert the AMSR grid into our gridMET 1km grid\n    western_us_df = pd.read_csv(western_us_coords)\n    for idx, row in western_us_df.iterrows():\n        target_lat = row['Latitude']\n        target_lon = row['Longitude']\n        \n        # compare the performance and find the fastest way to search nearest point\n        closest_lat_idx, closest_lon_idx, closest_lat, closest_lon = find_closest_index(target_lat, target_lon, lat, lon)\n        df.loc[len(df.index)] = [closest_lat, \n                                 closest_lon,\n                                 closest_lat_idx,\n                                 closest_lon_idx,\n                                 target_lat, \n                                 target_lon]\n    \n    # Save the new converted AMSR to CSV file\n    df.to_csv(target_csv_path, index=False)\n  \n    print('AMSR mapper csv is created.')\n\ndef download_amsr_and_convert_grid(target_date = test_start_date):\n    \"\"\"\n    Download AMSR snow data, convert it to DEM format, and save as a CSV file.\n    \"\"\"\n    \n    prepare_amsr_grid_mapper()\n    \n    # the mapper\n    target_mapper_csv_path = f'{work_dir}/amsr_to_gridmet_mapper.csv'\n    mapper_df = pd.read_csv(target_mapper_csv_path)\n    #print(mapper_df.head())\n    \n    df = pd.DataFrame(columns=['date', 'lat', \n                               'lon', 'AMSR_SWE', \n                               'AMSR_Flag'])\n    date = target_date\n    date = date.replace(\"-\", \".\")\n    he5_date = date.replace(\".\", \"\")\n    \n    # Check if the CSV already exists\n    target_csv_path = f'{work_dir}/testing_ready_amsr_{date}.csv'\n    if os.path.exists(target_csv_path):\n        print(f\"File {target_csv_path} already exists, skipping..\")\n        return target_csv_path\n    \n    target_amsr_hdf_path = f\"{work_dir}/amsr_testing/testing_amsr_{date}.he5\"\n    if os.path.exists(target_amsr_hdf_path):\n        print(f\"File {target_amsr_hdf_path} already exists, skip downloading..\")\n    else:\n        cmd = f\"curl --output {target_amsr_hdf_path} -b ~/.urs_cookies -c ~/.urs_cookies -L -n -O https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/{date}/AMSR_U2_L3_DailySnow_B02_{he5_date}.he5\"\n        print(f'Running command: {cmd}')\n        subprocess.run(cmd, shell=True)\n    \n    # Read the HDF\n    file = h5py.File(target_amsr_hdf_path, 'r')\n    hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n    lat = hem_group['lat'][:]\n    lon = hem_group['lon'][:]\n    \n    # Replace NaN values with 0\n    lat = np.nan_to_num(lat, nan=0.0)\n    lon = np.nan_to_num(lon, nan=0.0)\n    \n    swe = hem_group['Data Fields/SWE_NorthernDaily'][:]\n    flag = hem_group['Data Fields/Flags_NorthernDaily'][:]\n    date = datetime.strptime(date, '%Y.%m.%d')\n    \n    # Convert the AMSR grid into our DEM 1km grid\n    \n    def get_swe(row):\n        # Perform your custom calculation here\n        closest_lat_idx = int(row['amsr_lat_idx'])\n        closest_lon_idx = int(row['amsr_lon_idx'])\n        closest_swe = swe[closest_lat_idx, closest_lon_idx]\n        return closest_swe\n    \n    def get_swe_flag(row):\n        # Perform your custom calculation here\n        closest_lat_idx = int(row['amsr_lat_idx'])\n        closest_lon_idx = int(row['amsr_lon_idx'])\n        closest_flag = flag[closest_lat_idx, closest_lon_idx]\n        return closest_flag\n    \n    # Use the apply function to apply the custom function to each row\n    mapper_df['AMSR_SWE'] = mapper_df.apply(get_swe, axis=1)\n    mapper_df['AMSR_Flag'] = mapper_df.apply(get_swe_flag, axis=1)\n    mapper_df['date'] = date\n    mapper_df.rename(columns={'dem_lat': 'lat'}, inplace=True)\n    mapper_df.rename(columns={'dem_lon': 'lon'}, inplace=True)\n    mapper_df = mapper_df.drop(columns=['amsr_lat',\n                                        'amsr_lon',\n                                        'amsr_lat_idx',\n                                        'amsr_lon_idx'])\n    \n    print(\"result df: \", mapper_df.head())\n    # Save the new converted AMSR to CSV file\n    print(f\"saving the new AMSR SWE to csv: {target_csv_path}\")\n    mapper_df.to_csv(target_csv_path, index=False)\n    \n    print('Completed AMSR testing data collection.')\n    return target_csv_path\n\n    \ndef get_cumulative_amsr_data(target_date = test_start_date):\n    selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n    print(selected_date)\n    if selected_date.month < 10:\n      past_october_1 = datetime(selected_date.year - 1, 10, 1)\n    else:\n      past_october_1 = datetime(selected_date.year, 10, 1)\n\n    # Traverse and print every day from past October 1 to the specific date\n    current_date = past_october_1\n\n    date_keyed_objects = {}\n    data_dict = {}\n    new_df = None\n    while current_date <= selected_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      \n      data_dict[current_date_str] = download_amsr_and_convert_grid(current_date_str)\n      \n      if new_df is None:\n        new_df = pd.read_csv(data_dict[current_date_str])\n      else:\n        current_df = pd.read_csv(data_dict[current_date_str])\n        #print(current_df.head())\n        #print(new_df.head())\n        current_df = current_df.rename(columns={\n          \"AMSR_SWE\": f'AMSR_SWE_{current_date_str}',\n          \"AMSR_Flag\": f'AMSR_Flag_{current_date_str}',\n          \"date\": f\"data_{current_date_str}\",\n        })\n        new_df = pd.merge(new_df, current_df, on=['gridmet_lat', 'gridmet_lon'])\n      \n      current_date += timedelta(days=1)\n    \n    print(new_df.describe())\n    \n    # add all the columns together and save to new csv\n    # Adding all columns except latitude and longitude\n    new_df = new_df.apply(pd.to_numeric, errors='coerce')\n    \n    #new_df = new_df.head(2000)\n    \n    swe_cols = [col for col in new_df.columns if col.startswith('AMSR_SWE')]\n    print(\"swe_cols are: \", swe_cols)\n    def interpolate_row(row):\n        values = row.values\n        if np.all(np.isnan(values)):\n          values[:] = 0  # Set all elements to zero\n        else:\n          mask = (values > 240) | np.isnan(values)\n          x = np.arange(len(values))\n          values = np.interp(x, x[~mask], values[~mask])\n\n        if np.any(values > 240) or np.any(np.isnan(values)):\n          raise ValueError(\"Single group: shouldn't have values > 240 here\")\n\n        # Replace missing values with interpolated values\n        row[:] = values\n        return row\n    \n    for swe_col in swe_cols:\n        new_df.loc[(new_df[swe_col] > 240), swe_col] = np.nan\n        new_df.loc[new_df[swe_col].isnull(), swe_col] = np.nan\n        \n    print(\"start to interpolate the values..\")\n    new_df[swe_cols].to_csv(f\"{data_dict[target_date]}_before_interpolated.csv\", index=False)\n    new_df[swe_cols] = new_df[swe_cols].apply(interpolate_row, axis=1)\n    print(\"finished interpolating the values..\")\n    print(new_df[swe_cols].head())\n    new_df[swe_cols].to_csv(f\"{data_dict[target_date]}_interpolated.csv\", index=False)\n    \n#     empty_values_exist = (new_df[swe_cols] > 240).any().any()\n#     if empty_values_exist:\n#         print(\"There are >240 values in the selected columns.\")\n#         raise ValueError(\">240 values are still in the ASMR SWE\")\n#     else:\n#         print(\"No empty or NaN values found in the selected columns.\")\n        \n#     empty_values_exist = new_df[swe_cols].isnull().any().any()\n#     if empty_values_exist:\n#         print(\"There are nan values in the selected columns.\")\n#         raise ValueError(\">240 values are still in the ASMR SWE\")\n#     else:\n#         print(\"No empty or NaN values found in the selected columns.\")\n    \n    new_df['cumulative_AMSR_SWE'] = new_df[swe_cols].sum(axis=1)\n    \n    flag_cols = [col for col in new_df.columns if col.startswith('AMSR_Flag')]\n    print(\"flag_cols are: \", flag_cols)\n    new_df['cumulative_AMSR_Flag'] = new_df[flag_cols].sum(axis=1)\n    \n    new_df.to_csv(f\"{data_dict[target_date]}_cumulative_all_columns.csv\", index=False)\n    \n    new_df = new_df.loc[:, ['gridmet_lat', 'gridmet_lon', 'AMSR_SWE', 'cumulative_AMSR_SWE', 'AMSR_Flag', 'cumulative_AMSR_Flag']]\n    new_df[\"date\"] = target_date\n    \n    print(\"new_df final shape: \", new_df.head())\n    new_df.to_csv(f\"{data_dict[target_date]}_cumulative.csv\", index=False)\n    print(f\"new df is saved to {data_dict[target_date]}_cumulative.csv\")\n    print(new_df.describe())\n    \n    \n      \n    \nif __name__ == \"__main__\":\n    # Run the download and conversion function\n    #prepare_amsr_grid_mapper()\n    \n#     download_amsr_and_convert_grid()\n    get_cumulative_amsr_data()\n",
  "history_output" : "today date = 2023-11-20\ntest start date:  2022-10-15\ntest end date:  2023-10-07\n/home/chetana\n2022-10-15 00:00:00\n2022-10-01\nFile /home/chetana/gridmet_test_run/amsr_to_gridmet_mapper.csv already exists, skipping..\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2022.10.01.csv already exists, skipping..\n2022-10-02\nFile /home/chetana/gridmet_test_run/amsr_to_gridmet_mapper.csv already exists, skipping..\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2022.10.02.csv already exists, skipping..\n2022-10-03\nFile /home/chetana/gridmet_test_run/amsr_to_gridmet_mapper.csv already exists, skipping..\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2022.10.03.csv already exists, skipping..\n2022-10-04\nFile /home/chetana/gridmet_test_run/amsr_to_gridmet_mapper.csv already exists, skipping..\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2022.10.04.csv already exists, skipping..\n2022-10-05\nFile /home/chetana/gridmet_test_run/amsr_to_gridmet_mapper.csv already exists, skipping..\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2022.10.05.csv already exists, skipping..\n2022-10-06\nFile /home/chetana/gridmet_test_run/amsr_to_gridmet_mapper.csv already exists, skipping..\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2022.10.06.csv already exists, skipping..\n2022-10-07\nFile /home/chetana/gridmet_test_run/amsr_to_gridmet_mapper.csv already exists, skipping..\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2022.10.07.csv already exists, skipping..\n2022-10-08\nFile /home/chetana/gridmet_test_run/amsr_to_gridmet_mapper.csv already exists, skipping..\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2022.10.08.csv already exists, skipping..\n2022-10-09\nFile /home/chetana/gridmet_test_run/amsr_to_gridmet_mapper.csv already exists, skipping..\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2022.10.09.csv already exists, skipping..\n2022-10-10\nFile /home/chetana/gridmet_test_run/amsr_to_gridmet_mapper.csv already exists, skipping..\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2022.10.10.csv already exists, skipping..\n2022-10-11\nFile /home/chetana/gridmet_test_run/amsr_to_gridmet_mapper.csv already exists, skipping..\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2022.10.11.csv already exists, skipping..\n2022-10-12\nFile /home/chetana/gridmet_test_run/amsr_to_gridmet_mapper.csv already exists, skipping..\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2022.10.12.csv already exists, skipping..\n2022-10-13\nFile /home/chetana/gridmet_test_run/amsr_to_gridmet_mapper.csv already exists, skipping..\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2022.10.13.csv already exists, skipping..\n2022-10-14\nFile /home/chetana/gridmet_test_run/amsr_to_gridmet_mapper.csv already exists, skipping..\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2022.10.14.csv already exists, skipping..\n2022-10-15\nFile /home/chetana/gridmet_test_run/amsr_to_gridmet_mapper.csv already exists, skipping..\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2022.10.15.csv already exists, skipping..\n         gridmet_lat   gridmet_lon  ...  AMSR_SWE_2022-10-15  AMSR_Flag_2022-10-15\ncount  462204.000000  462204.00000  ...        462204.000000         462204.000000\nmean       37.030000    -112.52600  ...           122.322381            247.340984\nstd         6.921275       7.21226  ...           126.994964              6.619506\nmin        25.060000    -125.00000  ...             0.000000            241.000000\n25%        31.036000    -118.77200  ...             0.000000            241.000000\n50%        37.030000    -112.52600  ...             0.000000            241.000000\n75%        43.024000    -106.28000  ...           254.000000            254.000000\nmax        49.000000    -100.05200  ...           255.000000            255.000000\n[8 rows x 32 columns]\n",
  "history_begin_time" : 1700448502445,
  "history_end_time" : 1700448518927,
  "history_notes" : null,
  "history_process" : "0n26v2",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "wgjg0r34ulz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500143,
  "history_end_time" : 1700448500143,
  "history_notes" : null,
  "history_process" : "rvqv35",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "2clnpxj0pe1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500144,
  "history_end_time" : 1700448500144,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "q35x6n5gzll",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500145,
  "history_end_time" : 1700448500145,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "js3gqlh2vhe",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500146,
  "history_end_time" : 1700448500146,
  "history_notes" : null,
  "history_process" : "76ewp5",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "esjhyuarif8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500146,
  "history_end_time" : 1700448500146,
  "history_notes" : null,
  "history_process" : "5wzgx5",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "szk0tmbn3ad",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500147,
  "history_end_time" : 1700448500147,
  "history_notes" : null,
  "history_process" : "d4zcq6",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "twyfvgi8inx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500148,
  "history_end_time" : 1700448500148,
  "history_notes" : null,
  "history_process" : "6x6myw",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "smp5mbqg8l3",
  "history_input" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\nfeature_names = None\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness',\n                         'cumulative_AMSR_SWE': 'cumulative_SWE',\n                         'cumulative_AMSR_Flag': 'cumulative_Flag',\n                         'cumulative_tmmn':'cumulative_air_temperature_tmmn',\n                         'cumulative_etr': 'cumulative_potential_evapotranspiration',\n                         'cumulative_vpd': 'cumulative_mean_vapor_pressure_deficit',\n                         'cumulative_rmax': 'cumulative_relative_humidity_rmax', \n                         'cumulative_rmin': 'cumulative_relative_humidity_rmin',\n                         'cumulative_pr': 'cumulative_precipitation_amount',\n                         'cumulative_tmmx': 'cumulative_air_temperature_tmmx',\n                         'cumulative_vs': 'cumulative_wind_speed',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                        }, inplace=True)\n\n    desired_order = ['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness', 'cumulative_SWE',\n'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n'cumulative_potential_evapotranspiration',\n'cumulative_mean_vapor_pressure_deficit',\n'cumulative_relative_humidity_rmax',\n'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n'cumulative_air_temperature_tmmx', 'cumulative_wind_speed']\n    \n    feature_names = desired_order\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    data = data.drop(['lat', 'lon',], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \ninterpret_prediction()\n",
  "history_output" : "today date = 2023-11-20\ntest start date:  2022-10-15\ntest end date:  2023-10-07\n/home/chetana\nusing model : /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\nnew_data shape:  (462204, 32)\nhow many rows are left? 462204\ndata.shape:  (462204, 28)\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib\n   SWE  Flag  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0  0.0   241  ...                              0.0                    0.0\n1  0.0   241  ...                              0.0                    0.0\n2  0.0   241  ...                              0.0                    0.0\n3  0.0   241  ...                              0.0                    0.0\n4  0.0   241  ...                              0.0                    0.0\n[5 rows x 26 columns]\nhow many rows are there? 462204\nX has feature names, but ExtraTreesRegressor was fitted without feature names\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/smp5mbqg8l3/interpret_model_results.py\", line 254, in <module>\n    interpret_prediction()\n  File \"/home/chetana/gw-workspace/smp5mbqg8l3/interpret_model_results.py\", line 209, in interpret_prediction\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n  File \"/home/chetana/gw-workspace/smp5mbqg8l3/interpret_model_results.py\", line 131, in predict_swe\n    predictions = model.predict(data)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 984, in predict\n    X = self._validate_X_predict(X)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 599, in _validate_X_predict\n    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 625, in _validate_data\n    self._check_n_features(X, reset=reset)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 414, in _check_n_features\n    raise ValueError(\nValueError: X has 26 features, but ExtraTreesRegressor is expecting 13 features as input.\n",
  "history_begin_time" : 1700448577731,
  "history_end_time" : 1700448598473,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "0jerxwgx5vl",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    #tr_df = tr_df.drop('date', axis=1)\n    #te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n    \n    te_df = te_df.apply(pd.to_numeric, errors='coerce')\n    print(\"te_df describe: \", te_df.describe())\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n        else:\n          print(f\"Error: {col} is not in testing csv\")\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-11-20\ntest start date:  2022-10-15\ntest end date:  2023-10-07\n/home/chetana\nGet slope statistics\ncount    766500.000000\nmean         62.437693\nstd          16.505208\nmin           4.277402\n25%          52.134558\n50%          67.681070\n75%          75.046610\nmax          83.685555\nName: slope, dtype: float64\nGet SWE statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nTraining DataFrame:           date  ...  cumulative_wind_speed\n0       44158  ...             208.000003\n1       44120  ...              72.700001\n2       44093  ...             977.200008\n3       44803  ...             811.300009\n4       43739  ...               6.800000\n...       ...  ...                    ...\n767195  43787  ...             113.000001\n767196  44026  ...             817.500007\n767197  44072  ...            1014.600011\n767198  44444  ...             918.400012\n767199  44354  ...             720.800010\n[766500 rows x 31 columns]\nTesting DataFrame:            lat      lon  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0       49.00 -125.000  ...                              0.0                    0.0\n1       49.00 -124.964  ...                              0.0                    0.0\n2       49.00 -124.928  ...                              0.0                    0.0\n3       49.00 -124.892  ...                              0.0                    0.0\n4       49.00 -124.856  ...                              0.0                    0.0\n...       ...      ...  ...                              ...                    ...\n462199  25.06 -100.196  ...                              0.0                    0.0\n462200  25.06 -100.160  ...                              0.0                    0.0\n462201  25.06 -100.124  ...                              0.0                    0.0\n462202  25.06 -100.088  ...                              0.0                    0.0\n462203  25.06 -100.052  ...                              0.0                    0.0\n[462204 rows x 28 columns]\nte_df describe:                   lat  ...  cumulative_wind_speed\ncount  462204.000000  ...          462204.000000\nmean       37.030000  ...              32.525307\nstd         6.921275  ...              25.414461\nmin        25.060000  ...               0.000000\n25%        31.036000  ...               0.000000\n50%        37.030000  ...              38.500000\n75%        43.024000  ...              50.700000\nmax        49.000000  ...             100.000000\n[8 rows x 28 columns]\nTraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'water_year', 'cumulative_SWE',\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'cumulative_SWE', 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nlength:  31\n0  -  date\nError: date is not in testing csv\n1  -  lat\n2  -  lon\n3  -  SWE\n4  -  Flag\n5  -  swe_value\nError: swe_value is not in testing csv\n6  -  air_temperature_tmmn\n7  -  potential_evapotranspiration\n8  -  mean_vapor_pressure_deficit\n9  -  relative_humidity_rmax\n10  -  relative_humidity_rmin\n11  -  precipitation_amount\n12  -  air_temperature_tmmx\n13  -  wind_speed\n14  -  elevation\n15  -  slope\n16  -  curvature\n17  -  aspect\n18  -  eastness\n19  -  northness\n20  -  water_year\nError: water_year is not in testing csv\n21  -  cumulative_SWE\n22  -  cumulative_Flag\n23  -  cumulative_air_temperature_tmmn\n24  -  cumulative_potential_evapotranspiration\n25  -  cumulative_mean_vapor_pressure_deficit\n26  -  cumulative_relative_humidity_rmax\n27  -  cumulative_relative_humidity_rmin\n28  -  cumulative_precipitation_amount\n29  -  cumulative_air_temperature_tmmx\n30  -  cumulative_wind_speed\n",
  "history_begin_time" : 1700448600599,
  "history_end_time" : 1700448623890,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "fk0d4rgmeyr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500150,
  "history_end_time" : 1700448500150,
  "history_notes" : null,
  "history_process" : "ee5ur4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "x865tyxph71",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500151,
  "history_end_time" : 1700448500151,
  "history_notes" : null,
  "history_process" : "f03i7p",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "t6so95z5m5g",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500152,
  "history_end_time" : 1700448500152,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "h3g5rfkbe7o",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500153,
  "history_end_time" : 1700448500153,
  "history_notes" : null,
  "history_process" : "j8swco",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "u14cviinizd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500154,
  "history_end_time" : 1700448500154,
  "history_notes" : null,
  "history_process" : "pnr64x",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "lauun9lh09l",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500155,
  "history_end_time" : 1700448500155,
  "history_notes" : null,
  "history_process" : "qg80lj",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "zy769t6b3ys",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500155,
  "history_end_time" : 1700448500155,
  "history_notes" : null,
  "history_process" : "ggy7gf",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "l8c0tt4fuoe",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500156,
  "history_end_time" : 1700448500156,
  "history_notes" : null,
  "history_process" : "c2qa9u",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "haj13iy4gdj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500157,
  "history_end_time" : 1700448500157,
  "history_notes" : null,
  "history_process" : "lnrsop",
  "host_id" : "100001",
  "indicator" : "Skipped"
}]
