[{
  "history_id" : "mduyrsmhg6w",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717458,
  "history_end_time" : 1691530721090,
  "history_notes" : null,
  "history_process" : "78vedq",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "63z6j4wjtqd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717459,
  "history_end_time" : 1691530721091,
  "history_notes" : null,
  "history_process" : "mxpyqt",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "r2s0z2msodu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717504,
  "history_end_time" : 1691530721091,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "mwp8danywwy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717505,
  "history_end_time" : 1691530721091,
  "history_notes" : null,
  "history_process" : "u7xh2p",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "dxd9a2732gm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717506,
  "history_end_time" : 1691530721092,
  "history_notes" : null,
  "history_process" : "e8k4wq",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "n0dujin7zo5",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1691530721092,
  "history_notes" : null,
  "history_process" : "h1qp9v",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "v463hhqwxnr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717520,
  "history_end_time" : 1691530721099,
  "history_notes" : null,
  "history_process" : "urd0nk",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "ky805vyxhpi",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717521,
  "history_end_time" : 1691530721099,
  "history_notes" : null,
  "history_process" : "525l8q",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "s6zzffbyn7e",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717522,
  "history_end_time" : 1691530721100,
  "history_notes" : null,
  "history_process" : "7temiv",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "luqy1x9z4ae",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717523,
  "history_end_time" : 1691530721100,
  "history_notes" : null,
  "history_process" : "rmxece",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "oso4nqcytf3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717533,
  "history_end_time" : 1691530721100,
  "history_notes" : null,
  "history_process" : "illwc1",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "lw5nt4z85yp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717559,
  "history_end_time" : 1691530721100,
  "history_notes" : null,
  "history_process" : "sjs5by",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "kezna2z6if2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717561,
  "history_end_time" : 1691530721101,
  "history_notes" : null,
  "history_process" : "y7nb46",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "ijewigfwywv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717562,
  "history_end_time" : 1691530721101,
  "history_notes" : null,
  "history_process" : "a8p3n7",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "lvwj4ki566g",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717580,
  "history_end_time" : 1691530721101,
  "history_notes" : null,
  "history_process" : "smsdr0",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "9dzwcrl6u5p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717582,
  "history_end_time" : 1691530721101,
  "history_notes" : null,
  "history_process" : "4i0sop",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "e06mkt9bzgw",
  "history_input" : "import os\nimport pandas as pd\nimport netCDF4 as nc\nimport csv\nfrom datetime import datetime\n\n\ngridmet_var_mapping = {\n  \"etr\": \"potential_evapotranspiration\",\n  \"pr\":\"precipitation_amount\",\n  \"rmax\":\"relative_humidity\",\n  \"rmin\":\"relative_humidity\",\n  \"tmmn\":\"air_temperature\",\n  \"tmmx\":\"air_temperature\",\n  \"vpd\":\"mean_vapor_pressure_deficit\",\n  \"vs\":\"wind_speed\",\n}\n\n\ndem_csv = \"/home/chetana/gridmet_test_run/dem_all.csv\"\n\n\ndef get_current_year():\n    now = datetime.now()\n    current_year = now.year\n    return current_year\n\n\ndef get_file_name_from_path(file_path):\n    # Get the file name from the file path\n    file_name = os.path.basename(file_path)\n    return file_name\n\ndef get_var_from_file_name(file_name):\n    # Assuming the file name format is \"tmmm_year.csv\"\n    var_name = str(file_name.split('_')[0])\n    return var_name\n\ndef get_coordinates_of_template_tif():\n  \t# Load the CSV file and extract coordinates\n    coordinates = []\n    df = pd.read_csv(dem_csv)\n    for index, row in df.iterrows():\n        # Process each row here\n        lon, lat = float(row[\"Longitude\"]), float(row[\"Latitude\"])\n        coordinates.append((lon, lat))\n    return coordinates\n\ndef find_nearest_index(array, value):\n    # Find the index of the element in the array that is closest to the given value\n    return (abs(array - value)).argmin()\n\ndef get_nc_csv_by_coords_and_variable(nc_file, coordinates, var_name):\n    coordinates = get_coordinates_of_template_tif()\n    # get the netcdf file and generate the csv file for every coordinate in the dem_template.csv\n    new_lat_data = []\n    new_lon_data = []\n    new_var_data = []\n    # Read the NetCDF file\n    with nc.Dataset(nc_file) as nc_file:\n      # Get a list of all variables in the NetCDF file\n      variables = nc_file.variables.keys()\n\n      # Print the variables and their shapes\n      for variable in variables:\n        shape = nc_file.variables[variable].shape\n        print(f\"Variable: {variable}, Shape: {shape}\")\n      \n      # Get the values at each coordinate using rasterio's sample function\n      latitudes = nc_file.variables['lat'][:]\n      longitudes = nc_file.variables['lon'][:]\n      day = nc_file.variables['day'][:]\n      long_var_name = gridmet_var_mapping[var_name]\n      print(\"long var name: \", long_var_name)\n      var_col = nc_file.variables[long_var_name][:]\n      \n      print(f\"latitudes shape: {latitudes.shape}\")\n      print(f\"longitudes shape: {longitudes.shape}\")\n      print(f\"day shape: {day.shape}\")\n      print(f\"val col shape: {var_col.shape}\")\n      \n      day_index = day[day.shape[0]-1]\n      day_index = 44998\n      print('day_index:', day_index)\n      \n      for coord in coordinates:\n        lon, lat = coord\n        new_lat_data.append(lat)\n        new_lon_data.append(lon)\n        # Access the variables in the NetCDF file\n        # Find the nearest indices for the given coordinates\n        lon_index = find_nearest_index(longitudes, lon)\n        lat_index = find_nearest_index(latitudes, lat)\n        #day_index = find_nearest_index(day, day[day.shape[0]-1])\n        #print(f\"last day: {day_index}\")\n\n        # Get the value at the specified coordinates\n        the_value = var_col[day.shape[0]-1, lat_index, lon_index]  # Assuming data_variable is a 3D variable (time, lat, lon)\n        if the_value == \"--\":\n          the_value = -9999\n        new_var_data.append(the_value)\n        #print(f\"lon - {lon} lat - {lat} lon-index {lon_index} lat-index {lat_index} day-index {day_index} value - {the_value}\")\n    # Create the DataFrame\n    data = { 'Latitude': new_lat_data, 'Longitude': new_lon_data, var_name: new_var_data}\n    df = pd.DataFrame(data)\n    return df\n\ndef turn_gridmet_nc_to_csv(folder_path, dem_all_csv, testing_all_csv):\n    coordinates = get_coordinates_of_template_tif()\n    current_year = get_current_year()\n    for root, dirs, files in os.walk(folder_path):\n        for file_name in files:\n            var_name = get_var_from_file_name(file_name)\n            print(\"Variable name:\", var_name)\n            res_csv = f\"/home/chetana/gridmet_test_run/testing_output/{str(current_year)}_{var_name}.csv\"\n            if os.path.exists(res_csv):\n                os.remove(res_csv)\n                print(f\"remove old {res_csv}\")\n            \n            if str(current_year) in file_name :\n                # Perform operations on each file here\n                netcdf_file_path = os.path.join(root, file_name)\n                print(\"Processing file:\", netcdf_file_path)\n                file_name = get_file_name_from_path(netcdf_file_path)\n                print(\"File Name:\", file_name)\n\n                df = get_nc_csv_by_coords_and_variable(netcdf_file_path, coordinates, var_name)\n                df.to_csv(res_csv)\n            \ndef merge_all_gridmet_csv_into_one(gridmet_csv_folder, dem_all_csv, testing_all_csv):\n    # List of file paths for the CSV files\n    csv_files = []\n    for file in os.listdir(gridmet_csv_folder):\n        if file.endswith('.csv'):\n            csv_files.append(os.path.join(gridmet_csv_folder, file))\n\n    # Initialize an empty list to store all dataframes\n    dfs = []\n\n    # Read each CSV file into separate dataframes\n    for file in csv_files:\n        df = pd.read_csv(file, encoding='utf-8', index_col=False)\n        dfs.append(df)\n\n    dem_df = pd.read_csv(dem_all_csv, encoding='utf-8', index_col=False)\n    dfs.append(dem_df)\n    \n    # Merge the dataframes based on the latitude and longitude columns\n    merged_df = dfs[0]  # Start with the first dataframe\n    for i in range(1, len(dfs)):\n        merged_df = pd.merge(merged_df, dfs[i], on=['Latitude', 'Longitude'])\n    amsr_df = pd.read_csv('/home/chetana/gridmet_test_run/testing_ready_amsr.csv', index_col=False)\n    amsr_df.rename(columns={'lat': 'Latitude', 'lon': 'Longitude'}, inplace=True)\n    merged_df = pd.merge(merged_df, amsr_df, on=['Latitude', 'Longitude'])\n\n    # Save the merged dataframe to a new CSV file\n    merged_df.to_csv(testing_all_csv, index=False)\n    print(f\"All input csv files are merged to {testing_all_csv}\")\n    print(merged_df.head())\n\n    \n\nif __name__ == \"__main__\":\n    # Replace with the actual path to your folder\n    gridmet_csv_folder = \"/home/chetana/gridmet_test_run/gridmet_climatology/\"\n    #turn_gridmet_nc_to_csv(gridmet_csv_folder)\n    merge_all_gridmet_csv_into_one(\"/home/chetana/gridmet_test_run/testing_output/\",\n                                  \"/home/chetana/gridmet_test_run/dem_all.csv\",\n                                  \"/home/chetana/gridmet_test_run/testing_all_ready.csv\")\n\n",
  "history_output" : "/home/chetana/gw-workspace/e06mkt9bzgw/testing_data_integration.py:151: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Unnamed: 0_x'} in the result is deprecated and will raise a MergeError in a future version.\n  merged_df = pd.merge(merged_df, dfs[i], on=['Latitude', 'Longitude'])\n/home/chetana/gw-workspace/e06mkt9bzgw/testing_data_integration.py:151: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Unnamed: 0_x'} in the result is deprecated and will raise a MergeError in a future version.\n  merged_df = pd.merge(merged_df, dfs[i], on=['Latitude', 'Longitude'])\n/home/chetana/gw-workspace/e06mkt9bzgw/testing_data_integration.py:151: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Unnamed: 0_x'} in the result is deprecated and will raise a MergeError in a future version.\n  merged_df = pd.merge(merged_df, dfs[i], on=['Latitude', 'Longitude'])\nAll input csv files are merged to /home/chetana/gridmet_test_run/testing_all_ready.csv\n   Unnamed: 0_x  Latitude  Longitude  ...        date  AMSR_SWE AMSR_Flag\n0             0      49.0   -125.000  ...  2022-01-01         0       241\n1             1      49.0   -124.964  ...  2022-01-01         0       241\n2             2      49.0   -124.928  ...  2022-01-01         0       241\n3             3      49.0   -124.892  ...  2022-01-01         0       241\n4             4      49.0   -124.856  ...  2022-01-01         0       241\n\n[5 rows x 29 columns]\n",
  "history_begin_time" : 1691530721883,
  "history_end_time" : 1691530735952,
  "history_notes" : null,
  "history_process" : "b63prf",
  "host_id" : "tq3z35",
  "indicator" : "Done"
},{
  "history_id" : "kv4j2vobm6i",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717607,
  "history_end_time" : 1691530721102,
  "history_notes" : null,
  "history_process" : "zh38b6",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "1wcta3ni493",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717609,
  "history_end_time" : 1691530721103,
  "history_notes" : null,
  "history_process" : "wdh394",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "c49svedtlrp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717632,
  "history_end_time" : 1691530721103,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "8xm2v2acuqx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717667,
  "history_end_time" : 1691530721103,
  "history_notes" : null,
  "history_process" : "ilbqzg",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "o0cru5ieczh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717681,
  "history_end_time" : 1691530721103,
  "history_notes" : null,
  "history_process" : "do86ae",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "c26xgfqg6vu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717683,
  "history_end_time" : 1691530721103,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "02b17spfycw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717685,
  "history_end_time" : 1691530721103,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "v128rf64zm2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717687,
  "history_end_time" : 1691530721103,
  "history_notes" : null,
  "history_process" : "br9etb",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "zqzpwyvlumu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717698,
  "history_end_time" : 1691530721103,
  "history_notes" : null,
  "history_process" : "c2xkhz",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "ypdy3k72gq9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717700,
  "history_end_time" : 1691530721104,
  "history_notes" : null,
  "history_process" : "doinnd",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "kxtdua0088g",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717702,
  "history_end_time" : 1691530721104,
  "history_notes" : null,
  "history_process" : "b7a4fu",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "y5e19nri51r",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717703,
  "history_end_time" : 1691530721104,
  "history_notes" : null,
  "history_process" : "gnpbdq",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "znorl58miy7",
  "history_input" : "import numpy as np\nimport pandas as pd\nfrom osgeo import gdal\nimport warnings\nimport rasterio\nimport csv\nfrom rasterio.transform import Affine\nfrom scipy.ndimage import sobel, gaussian_filter\n\n# Set the warning filter globally to ignore the FutureWarning\nwarnings.simplefilter(\"ignore\", FutureWarning)\n\ndef lat_lon_to_pixel(lat, lon, geotransform):\n    x = int((lon - geotransform[0]) / geotransform[1])\n    y = int((lat - geotransform[3]) / geotransform[5])\n    return x, y\n\n\ndef calculate_slope_aspect_for_single(elevation_data, pixel_size_x, pixel_size_y):\n    # Calculate slope using the Sobel operator\n    slope_x = np.gradient(elevation_data, pixel_size_x, axis=1)\n    slope_y = np.gradient(elevation_data, pixel_size_y, axis=0)\n    slope_rad = np.arctan(np.sqrt(slope_x ** 2 + slope_y ** 2))\n    slope_deg = np.degrees(slope_rad)\n\n    # Calculate aspect (direction of the steepest descent)\n    aspect_rad = np.arctan2(slope_y, -slope_x)\n    aspect_deg = (np.degrees(aspect_rad) + 360) % 360\n\n    return slope_deg, aspect_deg\n\n\ndef save_as_geotiff(data, output_file, src_file):\n    with rasterio.open(src_file) as src_dataset:\n        profile = src_dataset.profile\n        transform = src_dataset.transform\n\n        # Update the data type, count, and set the transform for the new dataset\n        profile.update(dtype=rasterio.float32, count=1, transform=transform)\n\n        # Create the new GeoTIFF file\n        with rasterio.open(output_file, 'w', **profile) as dst_dataset:\n            # Write the data to the new GeoTIFF\n            dst_dataset.write(data, 1)\n  \n\ndef calculate_slope_aspect(dem_file):\n    with rasterio.open(dem_file) as dataset:\n        # Read the DEM data as a numpy array\n        dem_data = dataset.read(1)\n\n        # Get the geotransform to convert pixel coordinates to geographic coordinates\n        transform = dataset.transform\n\n        # Calculate the slope and aspect using numpy\n        dx, dy = np.gradient(dem_data, transform[0], transform[4])\n        slope = np.arctan(np.sqrt(dx ** 2 + dy ** 2)) * (180.0 / np.pi)\n        aspect = np.arctan2(-dy, dx) * (180.0 / np.pi)\n\n        # Adjust aspect values to range from 0 to 360 degrees\n        aspect[aspect < 0] += 360\n        print(f\"slope shape: {slope.shape}\")\n        print(f\"aspect shape: {aspect.shape}\")\n        \n        \n    return slope, aspect\n  \ndef calculate_curvature(elevation_data, pixel_size_x, pixel_size_y):\n    # Calculate curvature using the Laplacian operator\n    curvature_x = np.gradient(np.gradient(elevation_data, pixel_size_x, axis=1), pixel_size_x, axis=1)\n    curvature_y = np.gradient(np.gradient(elevation_data, pixel_size_y, axis=0), pixel_size_y, axis=0)\n    curvature = curvature_x + curvature_y\n\n    return curvature\n\n  \ndef calculate_curvature(dem_file, sigma=1):\n    with rasterio.open(dem_file) as dataset:\n        # Read the DEM data as a numpy array\n        dem_data = dataset.read(1)\n\n        # Calculate the gradient using the Sobel filter\n        dx = sobel(dem_data, axis=1, mode='constant')\n        dy = sobel(dem_data, axis=0, mode='constant')\n\n        # Calculate the second derivatives using the Sobel filter\n        dxx = sobel(dx, axis=1, mode='constant')\n        dyy = sobel(dy, axis=0, mode='constant')\n\n        # Calculate the curvature using the second derivatives\n        curvature = dxx + dyy\n\n        # Smooth the curvature using Gaussian filtering (optional)\n        curvature = gaussian_filter(curvature, sigma)\n\n    return curvature\n  \ndef calculate_gradients(dem_file):\n    with rasterio.open(dem_file) as dataset:\n        # Read the DEM data as a numpy array\n        dem_data = dataset.read(1)\n\n        # Calculate the gradients along the North and East directions\n        dy, dx = np.gradient(dem_data, dataset.res[0], dataset.res[1])\n\n        # Calculate the Northness and Eastness\n        northness = np.arctan(dy / np.sqrt(dx**2 + dy**2))\n        eastness = np.arctan(dx / np.sqrt(dx**2 + dy**2))\n\n    return northness, eastness\n  \n  \ndef geotiff_to_csv(geotiff_file, csv_file, column_name):\n    # Open the GeoTIFF file\n    with rasterio.open(geotiff_file) as dataset:\n        # Get the pixel values as a 2D array\n        data = dataset.read(1)\n\n        # Get the geotransform to convert pixel coordinates to geographic coordinates\n        transform = dataset.transform\n\n        # Get the width and height of the GeoTIFF\n        height, width = data.shape\n\n        # Open the CSV file for writing\n        with open(csv_file, 'w', newline='') as csvfile:\n            csvwriter = csv.writer(csvfile)\n\n            # Write the CSV header\n            csvwriter.writerow(['Latitude', 'Longitude', 'x', 'y', column_name])\n\n            # Loop through each pixel and extract latitude, longitude, and image value\n            for y in range(height):\n                for x in range(width):\n                    # Get the pixel value\n                    image_value = data[y, x]\n\n                    # Convert pixel coordinates to geographic coordinates\n                    lon, lat = transform * (x, y)\n\n                    # Write the data to the CSV file\n                    csvwriter.writerow([lat, lon, x, y, image_value])\n\n  \ndef read_elevation_data(file_path, result_dem_csv_path, result_dem_feature_csv_path):\n    neighborhood_size=4\n    df = pd.read_csv(file_path)\n    \n    dataset = rasterio.open(geotiff_file)\n    data = dataset.read(1)\n\n    # Get the width and height of the GeoTIFF\n    height, width = data.shape\n    \n    # Create an empty DataFrame with column names\n    columns = ['lat', 'lon', 'elevation', 'slope', 'aspect', 'curvature', 'northness', 'eastness']\n    all_df = pd.DataFrame(columns=columns)\n    \n    all_df.to_csv(result_dem_feature_csv_path)\n    print(f\"DEM and other columns are saved to file {result_dem_feature_csv_path}\")\n    return all_df\n\n\n  \n# Usage example:\nresult_dem_csv_path = \"/home/chetana/gridmet_test_run/dem_template.csv\"\nresult_dem_feature_csv_path = \"/home/chetana/gridmet_test_run/dem_all.csv\"\n\n\ndem_file = \"/home/chetana/gridmet_test_run/dem_file.tif\"\nslope_file = '/home/chetana/gridmet_test_run/slope_file.tif'\naspect_file = '/home/chetana/gridmet_test_run/aspect_file.tif'\ncurvature_file = '/home/chetana/gridmet_test_run/curvature_file.tif'\nnorthness_file = '/home/chetana/gridmet_test_run/northness_file.tif'\neastness_file = '/home/chetana/gridmet_test_run/eastness_file.tif'\n\n\nslope, aspect = calculate_slope_aspect(dem_file)\ncurvature = calculate_curvature(dem_file)\nnorthness, eastness = calculate_gradients(dem_file)\n\n# Save the slope and aspect as new GeoTIFF files\nsave_as_geotiff(slope, slope_file, dem_file)\nsave_as_geotiff(aspect, aspect_file, dem_file)\nsave_as_geotiff(curvature, curvature_file, dem_file)\nsave_as_geotiff(northness, northness_file, dem_file)\nsave_as_geotiff(eastness, eastness_file, dem_file)\n\ngeotiff_to_csv(dem_file, dem_file+\".csv\", \"Elevation\")\ngeotiff_to_csv(slope_file, slope_file+\".csv\", \"Slope\")\ngeotiff_to_csv(aspect_file, aspect_file+\".csv\", \"Aspect\")\ngeotiff_to_csv(curvature_file, curvature_file+\".csv\", \"Curvature\")\ngeotiff_to_csv(northness_file, northness_file+\".csv\", \"Northness\")\ngeotiff_to_csv(eastness_file, eastness_file+\".csv\", \"Eastness\")\n\n# List of file paths for the CSV files\ncsv_files = [dem_file+\".csv\", slope_file+\".csv\", aspect_file+\".csv\", \n             curvature_file+\".csv\", northness_file+\".csv\", eastness_file+\".csv\"]\n\n# Initialize an empty list to store all dataframes\ndfs = []\n\n# Read each CSV file into separate dataframes\nfor file in csv_files:\n    df = pd.read_csv(file, encoding='utf-8')\n    dfs.append(df)\n\n# Merge the dataframes based on the latitude and longitude columns\nmerged_df = dfs[0]  # Start with the first dataframe\nfor i in range(1, len(dfs)):\n    merged_df = pd.merge(merged_df, dfs[i], on=['Latitude', 'Longitude', 'x', 'y'])\n\n# Save the merged dataframe to a new CSV file\nmerged_df.to_csv(result_dem_feature_csv_path, index=False)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/znorl58miy7/western_us_dem.py\", line 3, in <module>\n    from osgeo import gdal\nModuleNotFoundError: No module named 'osgeo'\n",
  "history_begin_time" : 1691530718020,
  "history_end_time" : 1691530721104,
  "history_notes" : null,
  "history_process" : "oon4sb",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "nibbkdskyno",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717708,
  "history_end_time" : 1691530721104,
  "history_notes" : null,
  "history_process" : "fa7e4u",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "du1mkll78qt",
  "history_input" : "#############################################\n# Process Name: gridmet_station_only\n# Person Assigned: Gokul Prathin A\n# Last Changes On: 1st July 2023\n#############################################\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport netCDF4 as nc\nimport urllib.request\nfrom datetime import datetime, timedelta, date\nfrom snowcast_utils import test_start_date\n\n\ndef get_current_year():\n    now = datetime.now()\n    current_year = now.year\n    return current_year\n\n#year_list = [get_current_year()]\nselected_yr = datetime.strptime(test_start_date, \"%y-%m-%d\")\n\n\nyear_list = [selected_yr.year]\n\ndef remove_files_in_folder(folder_path):\n    # Get a list of files in the folder\n    files = os.listdir(folder_path)\n\n    # Loop through the files and remove them\n    for file in files:\n        file_path = os.path.join(folder_path, file)\n        if os.path.isfile(file_path):\n            os.remove(file_path)\n            print(f\"Deleted file: {file_path}\")\n\ndef download_file(url, target_file_path, variable):\n    try:\n        with urllib.request.urlopen(url) as response:\n            print(f\"Downloading {url}\")\n            file_content = response.read()\n        save_path = target_file_path\n        with open(save_path, 'wb') as file:\n            file.write(file_content)\n        print(f\"File downloaded successfully and saved as: {save_path}\")\n    except Exception as e:\n        print(f\"An error occurred while downloading the file: {str(e)}\")\n\n\ndef download_gridmet_of_specific_variables():\n    # make a directory to store the downloaded files\n    \n\n    base_metadata_url = \"http://www.northwestknowledge.net/metdata/data/\"\n    variables_list = ['tmmn', 'tmmx', 'pr', 'vpd', 'etr', 'rmax', 'rmin', 'vs']\n\n    for var in variables_list:\n        for y in year_list:\n            download_link = base_metadata_url + var + '_' + '%s' % y + '.nc'\n            target_file_path = os.path.join(folder_name, var + '_' + '%s' % y + '.nc')\n            if not os.path.exists(target_file_path):\n                download_file(download_link, target_file_path, var)\n            else:\n                print(f\"File {target_file_path} exists\")\n\nfolder_name = '/home/chetana/gridmet_test_run/gridmet_climatology'\nif not os.path.exists(folder_name):\n    os.makedirs(folder_name)\nremove_files_in_folder(folder_name)\ndownload_gridmet_of_specific_variables()\n\n",
  "history_output" : "",
  "history_begin_time" : 1691530718186,
  "history_end_time" : 1691530721105,
  "history_notes" : null,
  "history_process" : "drwmbo",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "o2g0avrkgxl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717713,
  "history_end_time" : 1691530721106,
  "history_notes" : null,
  "history_process" : "2n7b06",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "phfyq2kt45s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717714,
  "history_end_time" : 1691530721106,
  "history_notes" : null,
  "history_process" : "bwdy3s",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "qnshf9ujhyx",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1691530721106,
  "history_notes" : null,
  "history_process" : "2wkl6e",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "c3z0snyagae",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1691530721107,
  "history_notes" : null,
  "history_process" : "i2fynz",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "7iwtno4ulqa",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717719,
  "history_end_time" : 1691530721108,
  "history_notes" : null,
  "history_process" : "2o6cp8",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "v9eucpzsj0r",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717739,
  "history_end_time" : 1691530721108,
  "history_notes" : null,
  "history_process" : "5wzgx5",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "03ocxny05ry",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717762,
  "history_end_time" : 1691530721109,
  "history_notes" : null,
  "history_process" : "76ewp5",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "re8pxz4pkca",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717764,
  "history_end_time" : 1691530721109,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "q087n8t57q4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717766,
  "history_end_time" : 1691530721109,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "q8vc6qw9zz4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717768,
  "history_end_time" : 1691530721109,
  "history_notes" : null,
  "history_process" : "rvqv35",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "9bdr612p2ld",
  "history_input" : "import h5py\nimport subprocess\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom snowcast_utils import test_start_date\n\nwestern_us_coords = '/home/chetana/gridmet_test_run/dem_file.tif.csv'\n\ndef find_closest_index(target_latitude, target_longitude, lat_grid, lon_grid):\n    lat_diff = np.abs(lat_grid - target_latitude)\n    lon_diff = np.abs(lon_grid - target_longitude)\n\n    # Find the indices corresponding to the minimum differences\n    lat_idx, lon_idx = np.unravel_index(np.argmin(lat_diff + lon_diff), lat_grid.shape)\n\n    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]\n\nif __name__ == \"__main__\":\n  df = pd.DataFrame(columns=['date', 'lat', 'lon', 'AMSR_SWE', 'AMSR_Flag'])\n  date = test_start_date\n  he5_date = date.replace(\".\", \"\")\n  cmd = f\"curl --output /home/chetana/gridmet_test_run/amsr_testing/testing_amsr_file.he5 -b ~/.urs_cookies -c ~/.urs_cookies -L -n -O https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/{date}/AMSR_U2_L3_DailySnow_B02_{he5_date}.he5\"\n  print(f'running command {cmd}')\n  subprocess.run(cmd, shell=True)\n  file = h5py.File('/home/chetana/gridmet_test_run/amsr_testing/testing_amsr_file.he5', 'r')\n  hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n  lat = hem_group['lat'][:]\n  lon = hem_group['lon'][:]\n  swe = hem_group['Data Fields/SWE_NorthernDaily'][:]\n  flag = hem_group['Data Fields/Flags_NorthernDaily'][:]\n  date = datetime.strptime(date, '%Y.%m.%d')\n  \n  western_us_df = pd.read_csv(western_us_coords)\n  for idx, row in western_us_df.iterrows():\n    target_lat = row['Latitude']\n    target_lon = row['Longitude']\n    closest_lat_idx, closest_lon_idx, closest_lat, closest_lon = find_closest_index(target_lat, target_lon, lat, lon)\n    closest_swe = swe[closest_lat_idx, closest_lon_idx]\n    closest_flag = flag[closest_lat_idx, closest_lon_idx]\n    df.loc[len(df.index)] = [date,\n                             target_lat, target_lon,\n                            closest_swe, closest_flag]\n  df.to_csv('/home/chetana/gridmet_test_run/testing_ready_amsr.csv', index=False)\n  \n  print('completed amsr testing data collection.')\n\n    ",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/9bdr612p2ld/amsr_testing_realtime.py\", line 6, in <module>\n    from snowcast_utils import test_start_date\n  File \"/home/chetana/gw-workspace/9bdr612p2ld/snowcast_utils.py\", line 5, in <module>\n    import seaborn as sns\nModuleNotFoundError: No module named 'seaborn'\n",
  "history_begin_time" : 1691530718009,
  "history_end_time" : 1691530721109,
  "history_notes" : null,
  "history_process" : "0n26v2",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
}]
